[{"categories":["tekton"],"content":"借助 Trigger 组件实现通过外部事件触发指定流水线","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"本文主要记录了创建 Tekton pipeline 之后，如何解放双手，借助 Trigger 组件实现通过外部事件（比如代码仓库的 commit 事件）触发指定流水线，而不是手动创建 pipelinerun 来触发。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:0:0","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"1. Trigger 是什么？ 根据前面 task 和 pipeline 的教程 Tekton教程(二)—构建流水线：Task \u0026 Pipeline 基本使用 可以知道，想要触发某个 task 或者 pipeline 必须手动创建一个 taskrun 或者 pipelinerun，可以说是自动化了但又没完全自动化。 今天介绍的组件 Trigger 则可以实现通过外部事件来触发对应的任务，比如有代码提交时触发自动构建以及部署任务。 Trigger 组件就是用来解决这个触发问题的，它可以从各种来源的事件中检测并提取需要信息，然后根据这些信息来创建 TaskRun 和 PipelineRun，还可以将提取出来的信息传递给它们以满足不同的运行要求。 gitlab、github 的 webhook 就是一种最常用的外部事件，通过 Trigger 组件就监听这部分事件从而实现在提交代码后自动运行某些任务。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:1:0","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"核心模块 其核心模块如下： EventListener：时间监听器，是外部事件的入口 ，通常需要通过HTTP方式暴露，以便于外部事件推送，比如配置Gitlab的Webhook。 Trigger：指定当 EventListener 检测到事件发生时会发生什么，它会定义 TriggerBinding、TriggerTemplate 以及可选的 Interceptor。 TriggerTemplate：用于模板化资源，根据传入的参数实例化 Tekton 对象资源，比如 TaskRun、PipelineRun等。 TriggerBinding：用于捕获事件中的字段并将其存储为参数，然后会将参数传递给 TriggerTemplate。 ClusterTriggerBinding：和 TriggerBinding 相似，用于提取事件字段，不过它是集群级别的对象。 Interceptors：拦截器，在 TriggerBinding 之前运行，用于负载过滤、验证、转换等处理，只有通过拦截器的数据才会传递给TriggerBinding。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:1:1","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"工作流程 具体工作流程如下： EventListener 用于监听外部事件（具体触发方式为 http），外部事件产生后被 EventListener 捕获，然后进入处理过程。 首先会由 Interceptors 来进行处理（如果有配置 interceptor 的话），对负载过滤、验证、转换等处理，类似与 http 中的 middleware。 Interceptors 处理完成后无效的事件就会被直接丢弃，剩下的有效事件则交给 TriggerBinding 处理， TriggerBinding 实际上就是从事件内容中提取对应参数，然后将参数传递给 TriggerTemplate。 TriggerTemplate 则根据预先定义的模版以及收到的参数创建 TaskRun 或者 PipelineRun 对象。 TaskRun 或者 PipelineRun 对象创建之后就会触发对应 task 或者 pipeline 运行，整个流程就全自动了。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:1:2","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"2. Trigger 组件中的模块 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:2:0","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"EventListener 一个简单的 EventListener 定义如下： apiVersion: triggers.tekton.dev/v1beta1 kind: EventListener metadata: name: trigger-pipeline-eventlistener spec: resources: kubernetesResource: serviceType: NodePort serviceAccountName: tekton-triggers-gitlab-sa triggers: - bindings: - ref: tr-pipeline-binding template: ref: tr-pipeline-template 参数详解： 首先 spec.resources.kubernetesResource.serviceType 定义了这个 EventListener 接收外部事件的 svc 的类型，这里选择 nodePort 编译外部调用。 外部事件的触发都是通过 http 调用实现的，也就是说这个 EventListener 通过 svc 暴露出去一个接口，只要调用该接口就算是触发了一个事件。 然后 spec.serviceAccountName 则是指定了这个 EventListener 使用的 serviceAccount，因为 EventListener 最终会创建 taskrun、pipelinerun 同时会查询一些其他信息，因此需要为其配置一个 serviceAccount，同时还需要为这个 serviceAccount 赋予相应的权限。 最后 spec.triggers.bindings 就是关联到这个 EventListener 的 TriggerBinding，而 spec.triggers.template 则是关联的 TriggerTemplate。 二者都是通过名字进行关联。 参数分析后，这个 EventListener 的含义就比较明显了，任何由该 EventListener 监听的事件都交给 tr-pipeline-binding 这个 TriggerBinding 来解析参数，然后将参数传递给 tr-pipeline-template 这个 TriggerTemplate 创建对应 taskrun 或者 pipelinerun。过程中需要 的权限由 tekton-triggers-gitlab-sa 这个 serviceAccount 提供。同时这个 EventListener 对外暴露的 service 类型为 NodePort。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:2:1","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"TriggerBinding 一个简单的 TriggerBinding 定义如下： apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerBinding metadata: name: pipeline-binding spec: params: - name: gitrevision value: $(body.head_commit.id) - name: gitrepositoryurl value: $(body.repository.url) - name: contenttype value: $(header.Content-Type) 这个 TriggerBinding 的 spec.params 则定义了该 TriggerBinding 会尝试从事件内容中提取的参数。 spec.params.name 定义了参数名，spec.params.value 则定义了参数的取值方式。 比如参数 gitrevision 的 value 为 $(body.head_commit.id) 就是说去 body.head_commit.id 这个字段的内容作为自己的值。 Event body 一般是一个 json 格式的数据 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:2:2","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"TriggerTemplate 一个简单的 TriggerTemplate 定义如下： apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerTemplate metadata: name: pipeline-template spec: params: - name: gitrevision description: The git revision default: main - name: gitrepositoryurl description: The git repository url - name: message description: The message to print default: This is the default message - name: contenttype description: The Content-Type of the event resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: simple-pipeline-run- spec: pipelineRef: name: simple-pipeline params: - name: message value: $(tt.params.message) - name: contenttype value: $(tt.params.contenttype) - name: git-revision value: $(tt.params.gitrevision) - name: git-url value: $(tt.params.gitrepositoryurl) workspaces: - name: git-source emptyDir: {} 相信看到这个 yaml 定义基本能猜到具体是做什么的，spec.params 定义了这个 TriggerTemplate 需要哪些参数，需要注意的是 TriggerTemplate 和 TriggerBinding 中的参数是通过名字关联的，也就是说这里的参数名必须和 TriggerBinding 里定义的参数名一致才能对应上。 spec.resourcetemplates 则定义了这个 TriggerTemplate 最终会创建的资源模版，比如这里就会创建一个 PipelineRun，然后 pipelinerun 可以从 TriggerTemplate 中获取参数，比如 $(tt.params.gitrepositoryurl) 就是获取 gitrepositoryurl 这个参数，这里的 tt 应该就是 TriggerTemplate 的缩写。 到这里整个参数的传递流程就清晰了： PipelineRun 的参数也是从 TriggerTemplate 中取的 TriggerTemplate 中的参数值来源于 TriggerBinding TriggerBinding 中的参数则是从 event body 中提取到的 也就是说最终生成的 pipelinerun 要什么参数在触发对应事件时就要相应的携带过来，然后经过 TriggerBinding、TriggerTemplate 最终进入到 pipelinerun，然后进入到 pipeline，最后进入到对应的 task。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:2:3","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"RBAC 这里为 EventListener 中使用的 serviceAccout 赋予权限。 apiVersion: v1 kind: ServiceAccount metadata: name: tekton-triggers-gitlab-sa --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: tekton-triggers-gitlab rules: # Permissions for every EventListener deployment to function - apiGroups: [\"triggers.tekton.dev\"] resources: [\"eventlisteners\", \"triggerbindings\", \"triggertemplates\",\"clustertriggerbindings\", \"clusterinterceptors\",\"interceptors\",\"triggers\"] verbs: [\"get\",\"list\",\"watch\"] - apiGroups: [\"\"] # secrets are only needed for Github/Gitlab interceptors, serviceaccounts only for per trigger authorization resources: [\"configmaps\", \"secrets\", \"serviceaccounts\"] verbs: [\"get\", \"list\", \"watch\"] # Permissions to create resources in associated TriggerTemplates - apiGroups: [\"tekton.dev\"] resources: [\"pipelineruns\", \"pipelineresources\", \"taskruns\"] verbs: [\"create\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tekton-triggers-gitlab-binding subjects: - kind: ServiceAccount name: tekton-triggers-gitlab-sa namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: tekton-triggers-gitlab ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:2:4","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"3. Demo ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:3:0","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"创建 Trigger 组件 创建一个简单的 Trigger 进行测试，hello-trigger.yaml 完整内容如下： apiVersion: triggers.tekton.dev/v1beta1 kind: EventListener metadata: name: trigger-pipeline-eventlistener spec: resources: kubernetesResource: serviceType: NodePort serviceAccountName: tekton-triggers-gitlab-sa triggers: - bindings: - ref: tr-pipeline-binding template: ref: tr-pipeline-template --- apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerBinding metadata: name: tr-pipeline-binding spec: params: - name: username value: $(body.username) --- apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerTemplate metadata: name: tr-pipeline-template spec: params: - name: username description: user for say hello default: tekton resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: hello-goodbye- spec: params: - name: username value: $(tt.params.username) pipelineRef: name: hello-goodbye 注意：TriggerTemplate 里的 pipeline 也要使用 generateName，否则名字相同、内容也相同的 pipelinerun 就会被忽略掉。 Apply 到 k8s kubectl apply -f hello-trigger.yaml 查看创建好的 eventlistener 及其 svc [root@caas-console ~]# kubectl get eventlistener NAME ADDRESS AVAILABLE REASON READY REASON trigger-pipeline-eventlistener http://el-trigger-pipeline-eventlistener.default.svc.cluster.local:8080 False MinimumReplicasUnavailable False [root@caas-console ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE el-trigger-pipeline-eventlistener NodePort 10.108.222.103 \u003cnone\u003e 8080:30262/TCP,9000:30330/TCP 2m2s 可以看到确实创建了一个 NodePort 类型的 svc，接下来我们只需要调用这个接口即可触发事件。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:3:1","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"调用接口触发事件 正常情况下一般是把这个 endpoint 配置到 gitlab、github 仓库中作为 webhook，不过手动调用也可以触发，这里简单起见就直接用 curl 来调用。 内网测试，直接通过 svc 的 clusterIP 调用即可 curl http://10.108.222.103:8080 -d '{\"username\":\"17x\"}' 结果如下： [root@caas-console ~]# curl http://10.108.222.103:8080 -d '{\"username\":\"17x\"}' {\"eventListener\":\"trigger-pipeline-eventlistener\",\"namespace\":\"default\",\"eventListenerUID\":\"333aff20-2045-43b3-9fe7-78473840aced\",\"eventID\":\"fd21920b-848a-4f9e-a0b1-b14555ce4136\"} 可以看到调用成功了，并且返回了这个事件由哪个 eventListener 监听的，具体咋那个 namespace 下，本次事件的 id 等信息。 查看是否创建出了 pipelinerun [root@caas-console ~]# kubectl get pipelinerun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-goodbye-run-50649b48-ddd3-434f-ba70-aa27c87f9342 Unknown Running 4s Pipelinerun 已经成功创建出来了，说明 Trigger 是正常工作的。 查看任务执行情况 [root@tekton ~]# kubectl get po hello-goodbye-2x2jt-goodbye-pod 0/1 Completed 0 9s hello-goodbye-2x2jt-hello-pod 0/1 Completed 0 18s [root@tekton ~]# kubectl logs hello-goodbye-2x2jt-hello-pod Hello 17x [root@tekton ~]# kubectl logs hello-goodbye-2x2jt-goodbye-pod Goodbye 17x! 可以看到两个 task 都已经完成了，而且根据日志打印的 username 为 17x，和我们用 curl 触发事件时传递的参数是一样的，说明 TriggerBinding 以及 TriggerTemplate 都是正常的。 至此，tekton 中的 trigger 组件教程就结束了。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:3:2","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"4. 小结 本文主要讲了如何配置 Tekton Trigger 组件实现通过外部事件触发流水线。 相关知识点： 1）什么是 Trigger：它可以从各种来源的事件中检测并提取需要信息，然后根据这些信息来创建 TaskRun 和 PipelineRun。 2）Trigger 组件中的模块：包括 EventListener、TriggerBinding、TriggerTemplate、Interceptors 等。 3）Trigger 组件的具体工作流程如下： 1）EventListener 通过 svc 方式对外暴露一个接口， 2）外部系统调用该接口并传递参数以触发一个事件 3）EventListener 收到事件后首先由 Interceptors 进行验证、转换等处理，然后交给 TriggerBinding 4）TriggerBinding 从接收到的 event body 中解析出对应参数并传递给 TriggerTemplate 5）TriggerTemplate 根据接受到的参数以及模版创建出对应的 taskrun 或者 pipelinerun 等资源，用于触发对应任务。 6）Taskrun 或者 pipelinerun 创建后就进入到之前两个模块的范围了，Trigger 的任务就算是完成了。 ","date":"2023-01-15","objectID":"/posts/tekton/03-tekton-trigger/:4:0","tags":["tekton"],"title":"Tekton教程(三)---解放双手：使用 Trigger 自动触发流水线","uri":"/posts/tekton/03-tekton-trigger/"},{"categories":["tekton"],"content":"tekton 中 task 和 pipeline 具体使用说明","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"本文主要记录云原生的 CI/CD 框架 Tekton 中的 task 和 pipeline 资源对象的基本使用，通过 task 构建基本任务单元，然后使用 pipeline 将多个任务组合构建成一个完整的流水线，最后在通过 pipelinerun 触发相应任务。 在前一篇文章：Tekton教程(一)—云原生 CICD: Tekton 初体验 的最后我们运行了一个简单的 demo，创建了一个简单的 task 并通过 taskrun 进行触发，最后查看 od 日志观察运行结果。对于初次接触 Tekton 的用户可能一时不能理解，不过相信看到这篇文章之后就能够对 Tekton 整个流水线的定义与构建有相应的认识了。 tekton 流水线的构建包括以下两个核心对象的使用： 1）task：task 对象是 tekton 中任务的最小单位 taskrun：task 的关联对象，创建一个 task 之后并不会真的运行，需要使用 taskrun 对象来真正执行。 2）pipeline：pipeline 则是多个 task 的组合 pipelinerun：同 task，pipeline 也需要创建 pipelinerun 对象才会执行。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:0:0","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"1. task \u0026 taskrun Task 用于定义具体的任务，task 是 tekton 中描述任务的最小单位。 一个 task 最好只做一件事，这样能更好的复用 task。 一个 task 可以做这些事情： 打印一句话 调用一次 http 请求 克隆一个镜像仓库 编译一个 go 程序 Build 一个镜像 … Taskrun 用于触发具体的任务。 Taskrun 有两个作用： 触发一次任务 为任务提供具体参数 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:1:0","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"简单的 task 定义 一个简单的 task 完整 yaml 如下： apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: echo image: alpine script: | #!/bin/sh echo \"Hello World\" 核心部分为 spec.steps，一个任务可以由多个步骤组成。 上述 task 对应的 taskrun 完整 yaml 如下： apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: hello-task-run- spec: taskRef: name: hello 可以看到 taskrun 中通过 spec.taskRef.name 来关联 task generateName 需要注意的是 taskrun 这里我们没有指定 name，而是用的 generateName，这样在创建该对象时 k8s 会自动以generateName 为前缀生成该对象的完整名字。 同时该对象也必须使用 create 命令来创建，而不是 apply。 为什么需要用 generateName？ 因为一个 taskrun 只能触发一次任务运行，而一个任务我们可能会反复运行，如果我们在 taskrun 中写死一个名字，就会导致该任务只会触发一次，就算 apply 多次都会因为内容没有任何变化而直接被忽略掉。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:1:1","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"带参数的 task 定义 Task 中除了运行固定脚本、命令之外，还可以从外部传递参数，就像这样： 一个带参数的 task 定义如下： apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: params: - name: username type: string steps: - name: echo image: alpine script: | #!/bin/sh echo \"$(params.username)\" 这个 task 定义中增加了 spec.params 字段用于声明该任务需要哪些参数，然后在 steps 中使用 $(params.paramsName) 语法获取对应参数。 该任务对应的 taskrun 定义如下： apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: hello-task-run- spec: taskRef: name: hello params: - name: username value: \"17x\" 可以看到除了最基本的 spec.taskRef 之外新增了一个 spec.params 字段用于定义 task 中用到的参数的具体值。 即：task 中最终这个参数的值则是通过 taskrun 指定。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:1:2","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"Demo 跑一个简单的 task 来体验一下。 Create task 首先创建一个简单的 task,hello-task.yaml 内容如下： apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: params: - name: username type: string steps: - name: echo image: alpine script: | #!/bin/sh echo \"Hello $(params.username)\" apply 到集群里 kubectl apply -f hello-task.yaml 查看创建结果 [root@caas-console ~]# kubectl get task NAME AGE hello 6m15s Create taskRun 然后创建一个 taskrun 来触发该任务，同时提供 task 需要的参数，hello-taskrun.yaml 完整内容如下： apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: hello-task-run- spec: taskRef: name: hello params: - name: username value: \"17x\" kubectl create -f hello-taskrun.yaml 查看创建结果 [root@caas-console ~]# kubectl get taskrun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-task-run-fg6k8 Unknown Running 8s 可以看到 taskrun 创建成功，由于还在运行中，所以 SUCCEEDED 字段为 Unknown，等运行完成后就会根据结果改为 true 或者 false。 查看运行结果 同时查看 pod 可以看到为这个任务启动了一个 pod 来运行，由于任务比较简单所以已经运行完成了。 [root@caas-console ~]# kubectl get po NAME READY STATUS RESTARTS AGE hello-task-run-fg6k8-pod 0/1 Completed 0 79s 我们可以看下这个 pod 的日志，是否打印出了 “Hello 17x” 这句话。 [root@caas-console ~]# kubectl logs hello-task-run-fg6k8-pod Hello 17x 果然打印出了 “Hello 17x”,说明任务运行成功。 至此 task 相关教程就结束了，除了简单的执行一个脚本之外 task 还能做很多事情，就交给大家自行探索了。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:1:3","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"2. pipeline \u0026 pipelinerun Pipeline 翻译为流水线，它可以看做是多个 task 的组合。 一般我们的任务都比较复杂，由多个步骤组成，虽然可以把每个步骤对应为 task 中的一个 step，但是这样不利于 task 复用，所以一般不这样做。 推荐用法：将比较通用的步骤单独定义为 task，然后使用 pipeline 将多个 task 编排为一个流水线 ，这样既能使用功能，也更利于 task 复用。 比如一个构建镜像的 pipeline 就包含两个步骤： Git clone 拉取代码 构建镜像并上传 可以直接在一个 task 里写两个 step，但是这两个步骤都是比较通用的，因此最好是单独定义成 task。 pipelinerun 和 taskrun 基本一致，用于触发流水线以及为 pipeline 提供必要的参数。 pipelinerun 之于 pipelinerun 等于 taskrun 之于 task。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:2:0","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"pipeline 定义 一个简单的 pipeline 如下： apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: hello-goodbye spec: params: - name: username type: string tasks: - name: hello taskRef: name: hello - name: goodbye runAfter: - hello taskRef: name: goodbye params: - name: username value: $(params.username) 可以看到，pipeline 中通过 spec.tasks 指定多个 task，每个 task 里通过 taskRef.name 关联到具体的 task 实例。 由于某个task 是需要参数的，因此需要在 pipeline 的 spec.params 里定义好需要的参数，然后在 spec.tasks 里也需要再次定义 params，不过 task 里直接通过 $(params.username) 获取具体值**。** 注意：需要注意的是，tasks 中的任务不保证先后顺序，因此如果不同任务之间有依赖关系可以使用 runAfter 字段来指定先后关系。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:2:1","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"pipelinerun 定义 之前 pipeline 对应的 pipelinerun 定义如下： apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: hello-goodbye-run spec: pipelineRef: name: hello-goodbye params: - name: username value: \"Tekton\" 可以看到 pipelinerun 中通过 spec.pipelineRef.name 来关联到具体 pipeline，同时通过 spec.params 指定具体参数的值。 和 taskrun 类似 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:2:2","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"Demo Create goodbye task Pipeline 一般需要多个task，因此我们再创建一个 task，goodbye-task.yaml 完成内容如下： apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: goodbye spec: params: - name: username type: string steps: - name: goodbye image: ubuntu script: | #!/bin/bash echo \"Goodbye $(params.username)!\" kubectl apply -f goodbye-task.yaml 查看一下当前定义好的 task： [root@caas-console ~]# kubectl get task NAME AGE goodbye 20s hello 32m 可以看到已经有两个 task 了，可以编排 pipeline 了。 Create pipeline 使用 hello 和 goodbye 两个 task 来构建一个 pipeline，hello-goodbye-pipeline.yaml 完成内容如下： apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: hello-goodbye spec: params: - name: username type: string tasks: - name: hello taskRef: name: hello params: - name: username value: $(params.username) - name: goodbye runAfter: - hello taskRef: name: goodbye params: - name: username value: $(params.username) 由于两个 task 都需要参数，因此两个 task 里都需要增加 params 字段。 kubectl apply -f hello-goodbye-pipeline.yaml 查看创建好的 pipeline [root@caas-console ~]# kubectl get pipeline NAME AGE hello-goodbye 5s Create pipelinerun 然后创建 pipelinerun 来真正执行这个 pipeline，hello-goodbye-run.yaml 完整内容如下： apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: hello-goodbye-run spec: pipelineRef: name: hello-goodbye params: - name: username value: \"Tekton\" 这里我们把 username 参数改成 tekton。 kubectl create -f hello-goodbye-run.yaml 查看状态 [root@caas-console ~]# kubectl get pipelinerun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-goodbye-runqrpch Unknown Running 6s 查看运行结果 查看具体的 pod NAME READY STATUS RESTARTS AGE hello-goodbye-runqrpch-goodbye-pod 0/1 Completed 0 34s hello-goodbye-runqrpch-hello-pod 0/1 Completed 0 43s 可以看到，hello-pod 和 goodbye-pod 都已经是 Completed 状态了，说明 pipeline 已经执行完毕，再次查看 pipelinerun 的状态 [root@caas-console ~]# kubectl get pipelinerun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-goodbye-runqrpch True Succeeded 87s 49s SUCCEEDED 字段已经变成 True 了。 分别查看两个 pod 的日志，确实下是否真的执行成功 [root@caas-console ~]# kubectl logs hello-goodbye-runqrpch-hello-pod Hello Tekton [root@caas-console ~]# kubectl logs hello-goodbye-runqrpch-goodbye-pod Goodbye Tekton! 确实是按照定义输出了对应内容，说明 pipeline 真的执行完成了。 至此，tekton pipeline 相关教程就结束了，后续进阶的话就是定义的 pipeline 更加复杂罢了。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:2:3","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"3. 小结 Tekton 流水线构建主要分为以下两个部分： Task \u0026 taskrun task：定义一个任务 taskrun：触发指定任务并传递参数 Pipeline \u0026 pipelinerun pipeline：编排多个任务，组成一个流水线以实现某个功能 pipelinerun：类似于 taskrun，触发指定 pipeline 并传递参数 一句话描述：通过 task 定义基本任务单元，通过 pipeline 将多个任务进行组合，构建出一个能实现某个完整功能的流水线，最后通过 pipelinerun 触发对应流水线的运行。 ","date":"2023-01-08","objectID":"/posts/tekton/02-tekton-task-pipeline/:3:0","tags":["tekton"],"title":"Tekton教程(二)---构建流水线：Task \u0026 Pipeline 基本使用","uri":"/posts/tekton/02-tekton-task-pipeline/"},{"categories":["tekton"],"content":"在 k8s 集群中快速部署 tekton pipeline、trigger、dashboard 等组件","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"本文主要记录了如何在 k8s 上快速部署云原生的 CI/CD 框架 tekton 的各个组件，包括 pipeline、trigger、dashboard 等，最后运行了一个简单的 demo 以体验 tekton 的功能。 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:0:0","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"1. Tekton 是什么？ Tekton 是一款功能非常强大而灵活的开源的 CI/CD 云原生框架。Tekton 的前身是 Knative 项目的 build-pipeline 项目，这个项目是为了给 build 模块增加 pipeline 的功能，但是随着不同的功能加入到 Knative build 模块中，build 模块越来越变得像一个通用的 CI/CD 系统，于是，索性将 build-pipeline 剥离出 Knative，就变成了现在的 Tekton，而 Tekton 也从此致力于提供全功能、标准化的云原生 CI/CD 解决方案。 即：Tekton 是云原生的 CI/CD 框架，是云原生的的 CI/CD 解决方案。 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:1:0","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"优点 Tekton 为 CI/CD 系统提供了诸多好处： 可定制：Tekton 是完全可定制的，具有高度的灵活性，我们可以定义非常详细的构建块目录，供开发人员在各种场景中使用。 比较基础的功能，应该是所有 CI/CD 系统都有这个功能 可重复使用：Tekton 是完全可移植的，任何人都可以使用给定的流水线并重用其构建块，可以使得开发人员无需\"造轮子\"就可以快速构建复杂的流水线。 所有工作都运行在 pod 中，对外部系统无依赖，无缝迁移 可扩展：Tekton Catalog是社区驱动的 Tekton 构建块存储库，我们可以使用 Tekton Catalog中定义的组件快速创建新的流水线并扩展现有管道。 dockerhub 之于 docker，用于存放 Tekton 自己的模块 标准化：Tekton 在你的 Kubernetes 集群上作为扩展安装和运行，并使用完善的 Kubernetes 资源模型，Tekton 工作负载在 Kubernetes Pod 内执行。 云原生体现之一，完全兼容 k8s 标准，对环境没有要求，只要是标准 k8s 集群就能运行 伸缩性：要增加工作负载容量，只需添加新的节点到 k8s 集群即可，Tekton 可随集群扩展，无需重新定义资源分配或对管道进行任何其他修改。 云原生体现之一，完全利用 k8s 的能力 感觉云原生的 CI/CD 框架指的应该是 tekton 完全兼容 k8s 资源模型，可以在任意标准 k8s 集群中部署，同时能够充分利用 k8s 的各项能力，比如伸缩性，只要 k8s 集群添加节点即可，tekton 不需要做任何工作。 即：Tekton 能够很好的和 k8s 配合。 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:1:1","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"组件 Tekton 由一些列组件组成： Tekton Pipelines：是 Tekton 的基础，它定义了一组 Kubernetes CRD 作为构建块，我们可以使用这些对象来组装 CI/CD 流水线。 Tekton Triggers：允许我们根据事件来实例化流水线，例如，可以我们在每次将 PR 合并到 GitHub 仓库的时候触发流水线实例和构建工作。 Tekton CLI：提供了一个名为 tkn的命令行界面，它构建在 Kubernetes CLI 之上，运行和 Tekton 进行交互。 Tekton Dashboard：是 Tekton Pipelines的基于 Web 的一个图形界面，可以线上有关流水线执行的相关信息。 Tekton Catalog：是一个由社区贡献的高质量 Tekton 构建块（任务、流水线等）存储库，可以直接在我们自己的流水线中使用这些构建块。 Tekton Hub：是一个用于访问 Tekton Catalog的 Web 图形界面工具。 Tekton Operator：是一个 Kubernetes Operator，可以让我们在 Kubernetes 集群上安装、更新、删除 Tekton 项目。 因此我们只要部署 Tekton Pipelines 以及 Tekton Triggers 就能拥有完成的 CI/CD 流程，为了便于查看可以在部署一个 Tekton Dashboard。 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:1:2","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"2. 部署 本文主要记录 tektoncd 的部署过程。整个部署实际很简单，不过 tektoncd 所有镜像默认存放在 gcr.io 的，导致国内使用比较困难，镜像问题算是部署中最大的一个难点。 因此在写这篇文章时将当前用到的所有镜像都推送到了 dockerhub/lixd96 这个用户下，大家可以直接使用，同时文章里提供了脚本用于将官方 yaml 中的镜像替换为 dockerhub/lixd96 下的镜像，从而解决使用官方 yaml 部署导致的无法从 gcr.io 拉取镜像的问题。 根据这篇文章操作，不出意外的话应该几分钟就能完成部署，整个过程非常丝滑~ 本文部署的各个组件版本分别是 Pipeline v0.43.2 Trigger v0.22.1 Dashboard v0.31.0 由于国内的特殊环境，整个部署过程分为以下几部分： 1）下载对应版本的 yaml 文件 2）替换 yaml 文件中的镜像 3）从 dockerhub 拉取镜像 这一步可以忽略，只是习惯在部署前先准备好镜像 4）Apply 到 k8s 集群 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:2:0","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"环境准备 首先我们需要一个 k8s 集群，单节点即可。 如果没有 k8s 环境的话可以参数这篇文章：Kubernetes教程(十一)—使用 KubeClipper 通过一条命令快速创建 k8s 集群，快速搭建一个。 使用 kubeclipper 工具，大概两三分钟就能创建出一个 k8s 集群，本文的演示环境也是通过该工具推出来的，非常好用~ ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:2:1","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"部署 Pipeline 下载 0.43.2 版本的 yaml curl https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.43.2/release.yaml -o pipeline.yaml 使用 sed 命令挨个替换镜像 sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/controller:v0.43.2@sha256:e1a541216f70bfc519739e056111d0f69e7959913e28ccbf98ce9fe2fd0dd406/lixd96\\/tektoncd-pipeline-cmd-controller:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/resolvers:v0.43.2@sha256:5ea2565c256a5085ee422d4778166fd1fe0f985ff6e3816542728379433f30db/lixd96\\/tektoncd-pipeline-cmd-resolvers:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/webhook:v0.43.2@sha256:e2bc5e55370049efa5ed3e16868ecec65fb9cdb6df0fd7e08568a8b6f3193186/lixd96\\/tektoncd-pipeline-cmd-webhook:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/kubeconfigwriter:v0.43.2@sha256:449fae542ca42a94171c7e6fe41af4451c62126743f77b47f09bbcecc932145e/lixd96\\/tektoncd-pipeline-cmd-kubeconfigwriter:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/git-init:v0.43.2@sha256:cd5fb697a91af1883917e5e8ab230566bff60fd1310fb2d0e12badcee7db5db6/lixd96\\/tektoncd-pipeline-cmd-git-init:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/entrypoint:v0.43.2@sha256:50333090b874cdff1706d9f4de9d367270586d91a3204f223ad3c9c8f8b5968b/lixd96\\/tektoncd-pipeline-cmd-entrypoint:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/nop:v0.43.2@sha256:6c99e85668d5c5d383ee341fb22affb71ea2908f5615a3ec0157980ac1891ef4/lixd96\\/tektoncd-pipeline-cmd-nop:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/sidecarlogresults:v0.43.2@sha256:8c7e3dbb3cbfa76e9d291d869d50c93b4b9001dab6e3143d5db7b4e297144814/lixd96\\/tektoncd-pipeline-cmd-sidecarlogresults:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/imagedigestexporter:v0.43.2@sha256:768185690a3c5b5a79c764fe3d66bac8351136a14dd82d9fd7da019789b4ed95/lixd96\\/tektoncd-pipeline-cmd-imagedigestexporter:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/pullrequest-init:v0.43.2@sha256:8f5809192c455ea3a657203337e139482b06ffdef1a32d3ad494d6bcdb7c1465/lixd96\\/tektoncd-pipeline-cmd-pullrequest-init:v0.43.2/' pipeline.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/pipeline\\/cmd\\/workingdirinit:v0.43.2@sha256:707cf41528b19e7b20925fcfe17b1ebf8e61a22fe824df6b79c17b36f81a2d19/lixd96\\/tektoncd-pipeline-cmd-workingdirinit:v0.43.2/' pipeline.yaml 从 dockerhub 拉取镜像 tag=v0.43.2 images=\" controller entrypoint git-init imagedigestexporter kubeconfigwriter nop pullrequest-init resolvers sidecarlogresults webhook workingdirinit \" for image in $images;do ctr --namespace k8s.io image pull docker.io/lixd96/tektoncd-pipeline-cmd-$image:$tag;done apply 到 k8s kubectl apply -f pipeline.yaml 会启动 3 个 pod [root@caas-console ~]# kubectl get po -A|grep tekton tekton-pipelines-resolvers tekton-pipelines-remote-resolvers-774848479b-x6759 1/1 Running 0 44s tekton-pipelines tekton-pipelines-controller-68fb8c9df6-r755w 1/1 Running 0 44s tekton-pipelines tekton-pipelines-webhook-b54b6d464-ppb7f 1/1 Running 0 44s ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:2:2","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"部署 Trigger 先下载 v0.22.1 版本对应的两个 yaml curl https://storage.googleapis.com/tekton-releases/triggers/previous/v0.22.1/release.yaml -o trigger-release.yaml curl https://storage.googleapis.com/tekton-releases/triggers/previous/v0.22.1/interceptors.yaml -o trigger-interceptors.yaml 然后使用 sed 替换镜像 sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/controller:v0.22.1@sha256:47f18d03c08ebc8ef474dd62e7d83ead3c4aa802c72668dafb73fd6afedd305f/lixd96\\/tektoncd-triggers-cmd-controller:v0.22.1/' trigger-release.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/webhook:v0.22.1@sha256:9a124b2ead10a6bc3ae1d32d05b9fe664465cfe6d09830ef89f3987a443a5c86/lixd96\\/tektoncd-triggers-cmd-webhook:v0.22.1/' trigger-release.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/eventlistenersink:v0.22.1@sha256:bd8b2ec63012605739dc74871d1a20634d1055ed3d77864a582a9b5f2d22ab92/lixd96\\/tektoncd-triggers-cmd-eventlistenersink:v0.22.1/' trigger-release.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/controller:v0.22.1@sha256:47f18d03c08ebc8ef474dd62e7d83ead3c4aa802c72668dafb73fd6afedd305f/lixd96\\/tektoncd-triggers-cmd-controller:v0.22.1/' trigger-interceptors.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/eventlistenersink:v0.22.1@sha256:bd8b2ec63012605739dc74871d1a20634d1055ed3d77864a582a9b5f2d22ab92/lixd96\\/tektoncd-triggers-cmd-eventlistenersink:v0.22.1/' trigger-interceptors.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/webhook:v0.22.1@sha256:9a124b2ead10a6bc3ae1d32d05b9fe664465cfe6d09830ef89f3987a443a5c86/lixd96\\/tektoncd-triggers-cmd-webhook:v0.22.1/' trigger-interceptors.yaml sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/triggers\\/cmd\\/interceptors:v0.22.1@sha256:eda7af449fb82b06e952da0f5c0d1c2a3eddbab041e43065d37f67523c60c494/lixd96\\/tektoncd-triggers-cmd-interceptors:v0.22.1/' trigger-interceptors.yaml 接着拉取镜像 tag=v0.22.1 dockerUserName=lixd96 images=\" controller eventlistenersink interceptors webhook \" for image in $images;do ctr --namespace k8s.io image pull docker.io/$dockerUserName/tektoncd-triggers-cmd-$image:$tag;done 最后 apply 到 k8s kubectl apply -f trigger-release.yaml kubectl apply -f trigger-interceptors.yaml Trigger 也会启动 3 个 pod [root@caas-console ~]# kubectl get po -A|grep tekton-triggers tekton-pipelines tekton-triggers-controller-5969f786d6-pbrkl 1/1 Running 0 2m16s tekton-pipelines tekton-triggers-core-interceptors-77d6499b44-svbsb 1/1 Running 0 2m13s tekton-pipelines tekton-triggers-webhook-67559d98cf-2nz98 1/1 Running 0 2m16s ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:2:3","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"部署 Dashboard 为了对用户更友好，Tekton 也有一个Dashboard，可以使用如下命令进行安装： curl https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.31.0/tekton-dashboard-release.yaml -o tekton-dashboard-release.yaml 同样的，替换镜像 sed -i 's/gcr.io\\/tekton-releases\\/github.com\\/tektoncd\\/dashboard\\/cmd\\/dashboard:v0.31.0@sha256:454a405aa4f874a0c22db7ab47ccb225a95addd3de904084e35c5de78e4f2c48/lixd96\\/tektoncd-dashboard-cmd-dashboard:v0.31.0/' tekton-dashboard-release.yaml 然后拉取镜像 tag=v0.31.0 dockerUserName=lixd96 images=\" dashboard \" for image in $images;do ctr --namespace k8s.io image pull docker.io/$dockerUserName/tektoncd-dashboard-cmd-$image:$tag;done 最后 apply 到 k8s kubectl apply -f tekton-dashboard-release.yaml Dashboard 只会启动一个 pod [root@caas-console ~]# kubectl get po -A|grep tekton-dashboard tekton-pipelines tekton-dashboard-6ddf4d8556-k8cfz 1/1 Running 0 23s 外部访问则需要把对应 service 改成 nodePort 类型 kubectl -n tekton-pipelines patch svc tekton-dashboard -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl -n tekton-pipelines get svc tekton-dashboard 打开浏览器，访问对应节点 IP+NodePort 即可进入 tekton 的 dashboard。 打开后界面大概是这样子的： 通过界面可以更加直观的看到 tekton 相关的各个资源对象。 ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:2:4","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["tekton"],"content":"3. Demo 简单启动一个 task 测试一下 tekton 能否正常运行。 执行以下命令创建一个 task 对象： cat \u003c\u003c EOF | kubectl apply -f - apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: demo spec: steps: - name: echo image: alpine script: | #!/bin/sh echo \"Hello Tekton\" EOF 内容也很简单，就是打印出 “Hello Tekton” 这句话。 查看一下 [root@caas-console ~]# kubectl get task NAME AGE demo 4s 执行一下命令创建一个 taskrun，以触发 之前的 task： cat \u003c\u003c EOF | kubectl create -f - apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: demo-task-run- spec: taskRef: name: demo EOF 查看一下 [root@caas-console ~]# kubectl get taskrun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME demo-task-run-82kdw True Succeeded 23s 14s 因为这个 task 很简单，所以都已经跑完了。 最后在查看以下具体的 pod 日志，看看是否真的打印出了这句话 [root@caas-console ~]# kubectl get pod NAME READY STATUS RESTARTS AGE demo-task-run-82kdw-pod 0/1 Completed 0 2m38s [root@caas-console ~]# kubectl logs demo-task-run-82kdw-pod Hello Tekton 可以看到，确实打印出了”Hello Tekton“ 这句话，至此说明我们部署的 tenton 是能够正常运行的。 到这里 tektoncd 的部署就完成了，如果理解不了这个 demo 中做的事情也没关系，后续会有 Tekton 的使用教程~ ","date":"2023-01-01","objectID":"/posts/tekton/01-deploy-tekton/:3:0","tags":["tekton"],"title":"Tekton教程(一)---云原生 CICD: Tekton 初体验","uri":"/posts/tekton/01-deploy-tekton/"},{"categories":["Kubernetes"],"content":"通过Kubeadm部署K8s集群","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"本文主要记录了如何使用 kind 在本地快速部署一个 k8s集群，包括 kind 的基本使用、大致原理以及注事事项等。 repo：https://github.com/kubernetes-sigs/kind website：https://kind.sigs.k8s.io/ 一句话描述，什么是 kind： kind is a tool for running local Kubernetes clusters using Docker container “nodes”. Kind 是一个通过使用 docker 容器模拟节点来创建本地 k8s 集群的工具。 ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"1. QuickStart ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Install docker Kind 使用 docker 来启动容器，因此需要先安装 docker，可自行安装，也可以用下面这个脚本安装。 通过以下命令创建 install-docker.sh 脚本文件： cat \u003c\u003c EOF \u003e install-docker.sh #!/bin/bash set -e # centos 一键安装 docker 脚本. #卸载旧版本 yum remove -y docker docker-common docker-selinux docker-engine #安装需要的软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 #添加yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装最新版docker yum install -y docker-ce #配置镜像加速器 mkdir -p /etc/docker echo '{ \"registry-mirrors\": [ \"https://reg-mirror.qiniu.com\", \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\", \"https://docker.mirrors.ustc.edu.cn\" ] }' \u003e /etc/docker/daemon.json #启动并加入开机启动 systemctl enable docker --now docker version echo \"docker install finish.\" EOF 运行脚本，安装 docker： sh install-docker.sh ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:1","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Install kind from binary kind 只是一个二进制文件，因此下载下来放到 bin 目录即可。 curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:2","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Install kubectl Kind 只负责创建集群（会配置好 kubeconfig），后续操作集群的话需要手动安装 kubectl. curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x kubectl sudo mv kubectl /usr/local/bin ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:3","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Create aio cluster 一切就绪之后，使用 kind 创建集群即可。 kind 0.17.0 默认用的是 v.1.25.3 版本 k8s，可以先手动拉镜像,因为默认的镜像地址不一定能拉下来。 docker pull kindest/node:v1.25.3 然后创建集群时指定刚才拉下来的镜像 kind create cluster --image kindest/node:v1.25.3 --name aio -v 5 不出意外的话，一两分钟就可以创建好。 创建好之后就可以使用 kubectl 进行操作了，kind 会把 kubeconfig 也配置好，使用起来和真正的集群基本一致。 ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:4","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"List cluster Kind 自身也保存了集群数据的，使用以下命令进行查看。 kind get clusters ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:5","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Delete cluster 删除一个集群也很简单，通过集群名即可 # syntax：kind delete cluster --name ${clustername} kind delete cluster --name aio ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:6","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"Create ha cluster 默认创建的集群只有一个 master 节点，不过可以通过配置文件创建 HA 集群。 使用以下命令生成一个 kind-ha.yaml 配置文件： 配置文件内容很简单，nodes 字段里多指定几个节点就行了。 cat \u003c\u003cEOF \u003e kind-ha.yaml # a cluster with 3 control-plane nodes and 3 workers kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: control-plane - role: control-plane - role: worker - role: worker - role: worker EOF 以上配置文件指定了创建一个 3 master 3 worker 的 k8s 集群。同时，在 HA master 下， 它还额外部署了一个 Nginx，用来提供负载均衡 vip。 注意：这里的 HA 并不是真正意义上的 HA，毕竟所有 node 都跑在一个节点上的，如果底层的硬件或者 Docker 出问题那么整个集群都会挂掉。 因此仅供本地测试使用。 创建时通过 --config 指定该配置文件： kind create cluster --image kindest/node:v1.25.3 --name ha --config kind-ha.yaml ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:1:7","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"2. 实现原理 Kind 使用一个docker 容器来模拟一个 node，在 docker 容器里面跑 systemd ，并用 systemd 托管 kubelet 以及 containerd，然后通过容器内部的 kubelet 把其他 K8s 组件，比如 kube-apiserver、etcd 等跑起来，最后在部署上 CNI 整个集群就完成了 Kind 内部也是使用的 kubeadm 进行部署。 我们可以进入到 docker 容器进行查看： [root@kind ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9a55b2ea333 kindest/node:v1.25.3 \"/usr/local/bin/entr…\" About a minute ago Up About a minute 127.0.0.1:33275-\u003e6443/tcp aio-control-plane [root@kind ~]# docker exec -it c9a55b2ea333 /bin/bash 先看下 containerd root@aio-control-plane:/# systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2022-12-10 04:42:01 UTC; 4min 8s ago Docs: https://containerd.io Main PID: 243 (containerd) Tasks: 122 Memory: 799.4M CGroup: /docker/c9a55b2ea333c9fd7ffcce78a6a96c0b1c9d618f4ecdf1bf3568334684b7186c/system.slice/containerd.service ├─ 243 /usr/local/bin/containerd ├─ 371 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 648e35fa086d1e73a07688fa742be18ee3cbd72f12412153329fd0cf43693b7e -address /run/containerd/containerd.sock ├─ 398 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c9b803db61f0194f84e451615dea5cd463b921590b83d044ef7ecdce7c62608e -address /run/containerd/containerd.sock ├─ 401 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 831789d91a8142289e4263eb0dbba4b9ff4b9d36f930c41248c4fc77e3aeeb6d -address /run/containerd/containerd.sock ├─ 429 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 25d98fdf10c4198c8ddcf7065eed292bcc6a0b1dfd069b640a50b5f307a551de -address /run/containerd/containerd.sock ├─ 862 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b880b9a217fb747602beadc45890efbee81fadc8d6d6d5487c0428c3c5e2a1fc -address /run/containerd/containerd.sock ├─ 888 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 79a95b6fa3be26c317fe8e4144420caa0db537ed5fb748be28acf497fdc15a8e -address /run/containerd/containerd.sock ├─1208 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 0eb055283fcef14662c9e4e4497f33ea9b506f2455df2c8a7839a0fce6b83f1d -address /run/containerd/containerd.sock ├─1231 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 0e58541ef79169ce4f17f27493e671e90c940c52e12b786faaf108aaa742c8a9 -address /run/containerd/containerd.sock └─1311 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 6689352dad1bfa8e57522f231deefd8fd2169a765f428b6ad1c1c58b82c8dbf1 -address /run/containerd/containerd.sock 再看下 kubelt root@aio-control-plane:/# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Sat 2022-12-10 04:42:32 UTC; 55s ago Docs: http://kubernetes.io/docs/ Process: 748 ExecStartPre=/bin/sh -euc if [ -f /sys/fs/cgroup/cgroup.controllers ]; then create-kubelet-cgroup-v2; fi (code=exited, status=0/SUCCESS) Process: 749 ExecStartPre=/bin/sh -euc if [ ! -f /sys/fs/cgroup/cgroup.controllers ] \u0026\u0026 [ ! -d /sys/fs/cgroup/systemd/kubelet ]; then mkdir -p /sys/fs/cgroup/systemd/kubelet; fi (code=exited, status=0/SUCCESS) Main PID: 750 (kubelet) Tasks: 14 (limit: 4662) Memory: 40.1M CPU: 2.485s CGroup: /docker/c9a55b2ea333c9fd7ffcce78a6a96c0b1c9d618f4ecdf1bf3568334684b7186c/kubelet.slice/kubelet.service └─750 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock --node-ip=172.18.0.2 --node-labels= --pod-infra-container-image=registry.k8s.io/pause:3.8 --provider-id=kind://docker/aio/aio-control-plane --fail-swap-on=false --cgroup-root=/kubelet 可以看到，在容器里确实以 systemd 方式启动了一个containerd 和 kubelet，然后其他组件以静态 pod 的方式被 kube","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"3. 注意事项 由于 kind 是通过 docker 容器模拟 node 来部署集群的，因此和普通集群有一些差异。主要包括以下几个方面： 文件系统 kind cluster 中无法直接访问宿主机上的文件 kind cluster 中无法直接使用宿主机上的镜像 网络 比如无法在宿主机直接访问 kind cluster 中的服务 以上问题 kind 都提供了相应的解决方案，比如镜像导入、端口映射、目录映射等等，具体见下文。 ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"镜像导入 Q：宿主机上使用 docker images 查看明明有镜像，但是 kubectl apply 却要去拉镜像？ A：这是由于 kind 中的节点实际都是 docker 容器导致的，和宿主机是隔离的。在宿主机拉取镜像后还需要通过 kind load 方式加载到 node 里面（容器里）去,否则部署的应用会再次拉取镜像。 相关命令如下： 以下命令都是在宿主机上执行 # 查看 node 里的镜像列表 docker exec -it dev-control-plane crictl images # 导入镜像到指定集群 kind load docker-image caas4/etcd:3.5.5 --name aio ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"如何访问集群中的服务 由于 kind 是把 node 运行在 docker 里的， 因此即使在 k8s 集群中使用 nodePort 方式将服务暴露出来，在宿主机上依旧无法访问。 因为这里暴露的 nodePort 其实是 docker 容器的端口，如果 docker 容器启动时没有将端口映射出来依旧无法访问 有两种解决方案： 进入到对应 network namespace 端口映射 进入到对应 network namespace 由于kind 集群的 node 是以 docker 容器运行的，那么只需要进入到该容器的 network namespace 就可以访问到 集群中暴露出来的 nodePort。 当然，clusterIP 应该还是无法访问的。 相关命令如下： containerID=xxx pid=$(docker inspect -f {{.State.Pid}} $containerID) echo $pid nsenter -n -t $pid 端口映射 Kind 在启动集群时可以指定将某些端口映射出来，因为是通过 docker 跑的，所以后续应该只是把参数传给 docker 了。 类似于指定了 docker run -p 参数。 注意：由于 docker 容器启动后不能修改端口映射，因此 kind 集群创建后也不能修改端口映射了。 配置文件如下： kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 30000 hostPort: 30000 protocol: TCP 在参数 extraPortMappings 中定义要映射的端口即可，多个节点则需要挨个定义。 定义了端口映射后再启动集群，对应的 docker 容器就会直接把端口暴露出来了 [root@kind ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 54c8bb471113 kindest/node:v1.25.3 \"/usr/local/bin/entr…\" 10 minutes ago Up 10 minutes 0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp, 0.0.0.0:30000-\u003e30000/tcp, 127.0.0.1:42259-\u003e6443/tcp portmapping-control-plane 这样就可以在宿主机上访问到集群中的服务了。 小结 由于端口映射只能在创建集群时指定，创建后无法修改，因此在测试时建议直接使用方式一，进入到对应 network namespace 进行访问。 等后续端口固定好之后再通过端口映射一次性搞定。 ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:3:3","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"目录映射 和端口映射类似，kind cluster 里的 hostpath 指的是 docker 里的文件系统，而想要用宿主机上的文件还需要再映射一次。 最终还是会把参数传递给 docker，类似于 docker run -v 参数。 同样只能在创建集群时指定，配置文件如下： apiVersion: kind.x-k8s.io/v1alpha4 kind: Cluster nodes: - role: control-plane extraMounts: - hostPath: /home/bill/work/foo containerPath: /foo 通过 extraMounts 参数指定需要映射的目录。 通过该配置文件启动的 docker 容器就会挂载相应目录到容器里。 集群启动后进入 docker 容器查看，目录映射已经生效： [root@kind ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5e10f9cbc1bd kindest/node:v1.25.3 \"/usr/local/bin/entr…\" 7 minutes ago Up 7 minutes 127.0.0.1:42521-\u003e6443/tcp volumemapping-control-plane [root@kind ~]# docker exec -it 5e10f9cbc1bd ls /foo # 这个目录是宿主机上的，能在容器里看到，说明成功了 systemd-private-668bfa87d90144a1a43345805f73cd6a-chronyd.service-Tuqj3U ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:3:4","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"4.小结 本文主要介绍了 如果快速使用 kind 在本地创建一个 k8s 集群，及其原理和注意事项等。 如果需要在本地测试的话推荐使用 kind 如果需要真正的创建集群的话现在主流工具则是 kubeadm 如果觉得 kubeadm 也比较麻烦的话可以试一下 kubeclipper，基于 kubeadm 工具进行二次封装, 完全兼容原生 Kubernetes, 以图形化方式提供在企业自有基础设施中快速部署 K8S 集群和持续化全生命周期管理（安装、卸载、升级、扩缩容、远程访问等）能力。 ","date":"2022-12-10","objectID":"/posts/kubernetes/15-kind-kubernetes-in-docker/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(十五)---使用 kind 在本地快速部署一个 k8s集群","uri":"/posts/kubernetes/15-kind-kubernetes-in-docker/"},{"categories":["Kubernetes"],"content":"pv dynamic provision,存储卷全流程详解","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"本文主要对 PV 从创建到挂载全流程进行了简单分析，包括存储卷的动态供应(Dynamically Provision)以及最终被挂载到 Pod 中被我们的app 所使用。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"1. 概述 本文主要分析以下两个步骤： 1）PV 的创建：即在 k8s 中创建一个 pv 对象 2）PV 的挂载：即创建 Pod 指定使用该 PVC，Pod 启动后 PV 被挂载到 Pod 中 其中创建部分又可以分为两种： 静态供应：即管理员手动创建 PV 动态供应：由 k8s 根据 PVC 自动创建对应 PV 主流的方式为 动态供应，毕竟管理员无法预估集群使用者需要什么样的 PV，也就不可能提前创建好一模一样的 PV，开发人员又可能不了解存储，无法自己创建，最终只能等 Pod 启动时在通知管理员创建 PV ，这样就太麻烦了。 环境准备 这里需要一个部署好了存储插件的 k8s 集群。 推荐使用 kubeclipper 安装集群，具体可以参考 Kubernetes教程(十一)—使用 KubeClipper 通过一条命令快速创建 k8s 集群 同时这里有一个 安装 NFS 的脚本，等会可以作为存储后端使用 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"2. PV 的创建 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"1. 创建 PVC 首先创建一个 pvc。 这里需要注意 storageClassName，如果是用 kubeclipper 安装的，那么默认的名字就是 nfs-sc。 cat \u003e pvc.yaml \u003c\u003c EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs-sc EOF kubectl apply -f pvc.yaml ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"2. PVController 绑定 PV \u0026 PVC PVC 创建之后就轮到 PersistentVolumeController 登场了。 PVController 即 PersistentVolumeController，是 kube-controller-manager 的一部分 PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。 相关源码 pkg/controller/volume/persistentvolume/pv_controller_base.go 由于这时候还没有对应的 pv，因此会一直绑定不了，直到有 pv 创建出来。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"3. Dynamically Provision \u0026 external-provisioner 创建 pvc 之后同时也会进入 dynamically provision 流程。 如果有部署 CSI 的话。 部署的 CSI Plugin 里有一个叫做 external-provisioner 的pod，这时候 external-provisioner 就起作用了，他会 watch pvc 对象，因为这里我们创建了 PVC，那么 provisioner 就会收到相应事件，然后根据 PVC 中的 storageClassName 拿到对应 StorageClass，然后根据 StorageClass 中的 provisioner 字段拿到对应 provisioner，如果发现是自己处理的，就调用 CSI Plugin 的 CreateVolume 方法创建出 volume。 CSI Plugin CreateVolume 接口则由具体厂商实现，比如 阿里云实现的 CreateVolume 可能就是在阿里云上创建了一块云盘。 创建成功后就在 k8s 里创建对应的 PV 来指代这个 volume。最后 PV 创建之后，PVController 就 可以将二者进行绑定。 这里的 bind 只是修改了 pvc 和 pv 对象的字段 查看创建的 pvc 和 pv [root@ee ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-test-pvc Bound pvc-047acd58-808e-4f26-83c6-df59e8e00e65 1Gi RWO nfs-sc 3m49s [root@ee ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-047acd58-808e-4f26-83c6-df59e8e00e65 1Gi RWO Delete Bound default/csi-test-pvc nfs-sc 3m59s 可以看到 pvc 中的 VolumeName 就是 pv 的名字，同时已经进入 Bound 状态，说明在 PV 被创建出来之后 PVController 立马就将二者进行绑定了。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:2:3","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"3. 创建 Pod 使用该 PVC 然后创建一个 pod 使用该 pvc cat \u003e pod.yaml \u003c\u003c EOF apiVersion: v1 kind: Pod metadata: name: test-pod1 spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: csi-test-pvc containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: task-pv-storage EOF kubectl apply -f pod.yaml 这里随便创建了一个 Pod，用于测试。 根据 pod.yaml 可知，是通过 claimName 来指定要使用的 PVC 的，然后前面 PVC 和 PV 绑定的时候会把 PV 的名字填到 PVC 的 spec.volumeName 字段上，因此这里又可以找到对应的 PV，然后就进入 Volume 挂载流程了。 Pod 创建后 k8s-scheduler 会把 pod 调度到某个 node 上。 从 etcd 角度看就是更新了 pod 的 spec.nodeName 字段。 Pod 被调度到某节点后，并不会立刻被创建出来，因为对应的 PV 还需要经过 attach、mount 两个阶段才能被 pod 使用，因此在创建 Pod 前还需要对 PV 进行处理。 像 nfs 这种文件系统则不需要 attach 阶段，不过这里为了流程完整性，还是分析一下。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"1. Attach attach 由 ad controller 以及 external-attacher 两个组件共同完成，如下图所示： AD Controller ADController 即 AttachDetach Controller，负责 PV 的 attach/detach 处理。 大致逻辑：AD Controller 会获取当前 Node 上的 Pod，然后 Pod 的 Volume 列表计算出 该 Node 上的 PV 列表，然后和 node.Status.VolumesAttached 值进行对比，没有 attach 的话就执行 attach 操作。 Attach 之前还会做一个 MultiAttach check，如果是 RWO 类型的 Volume，然后还已经在别的节点 attach 了，那么这里直接报错。具体流程见后续的源码分析部分。 为了进行解耦，AD Controller 并不会直接调用 CSI Plugin 去执行 attach，而是创建一个叫做 VolumeAttachment 的对象，大概是长这样的： apiVersion: storage.k8s.io/v1 kind: VolumeAttachment metadata: creationTimestamp: \"2022-11-10T02:55:16Z\" name: csi-ac6258582f9363a9af542dbde1161db00bddf492ea1522e6a92c2f35c3d815c5 resourceVersion: \"142813\" uid: 713ee91b-d256-45c9-a9ba-5d11ad3b7e5c spec: attacher: nfs.csi.k8s.io nodeName: ee source: persistentVolumeName: pvc-047acd58-808e-4f26-83c6-df59e8e00e65 status: attached: false 里面记录了 3 个重要信息，attacher、nodeName 和 persistentVolumeName，后续会用到,然后 status.attached 则是用于记录是否真的 attach 了。 等 external-attacher 处理完成后，ADController 就会更新 node.Status.VolumesAttached，把这个 Volume 记录上去。 更新后的 node 信息如下： 已省略其他信息 [root@ee ~]# kubectl get node ee -oyaml apiVersion: v1 kind: Node metadata: name: ee spec: podCIDR: 10.96.0.0/24 podCIDRs: - 10.96.0.0/24 status: volumesAttached: - devicePath: \"\" name: kubernetes.io/csi/nfs.csi.k8s.io^172.20.151.105#tmp/nfs/data/lxd-host#pvc-047acd58-808e-4f26-83c6-df59e8e00e65# volumesInUse: - kubernetes.io/csi/nfs.csi.k8s.io^172.20.151.105#tmp/nfs/data/lxd-host#pvc-047acd58-808e-4f26-83c6-df59e8e00e65# kubelet 启动参数–enable-controller-attach-detach AD Cotroller 与 kubelet 中的 volume manager 逻辑相似，都可以做 Attach/Detach 操作，但是 kube-controller-manager 与 kubelet 中只会有一个组件做 Attach/Detach 操作，具体则是通过 kubelet 启动参数 –enable-controller-attach-detach 来设置。 设置为 true 表示启用 kube-controller-manager 的 AD controller 来做 Attach/Detach 操作，同时禁用 kubelet 执行 Attach/Detach 操作。 默认值为 true，即让 kube-controller-manager 的 AD controller来做 Attach/Detach 操作。 注意：这个是 kubelet 的配置，而每个节点都会运行一个 kubelet，也就是说不同的节点可以有不同的配置，比如 node1 用AD controller来做 Attach/Detach ，node2 则让 kubelet 来做。 Q：不知道这个配置有什么作用，难道是历史遗留问题？？ A：从这个 Issue #10 来看，好像是和 flexvolume 有关， flexvolume 的 Attach/Detach 只能在对应节点做，因此需要配置该参数为 false ，让 kubelet 来完成 Attach/Detach 操作。不过现在都是 CSI 了应该不用管了。 external-attacher VolumeAttachment 创建后就交给 external-attacher 了，external-attacher 会 watch VolumeAttachment 对象。根据 .spec.attacher 判断是不是需要自己处理，如果是则 调用 CSI Plugin 的 ControllerPublishVolume 方法，将 .spec.persistentVolumeName 这个 Volume attach 到 .spec.nodeName 这个节点上。 具体实现就看 CSI Plugin 了，可能是简单的调用了一个 云厂商的 API。 Attach 成功后(即调用 ControllerPublishVolume 方法没有报错) external-attacher 就会把 .status.attached 改为 true。 至此 attach 阶段就完成了，进入后续的 mount 阶段。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"2. Mount mount 阶段则由 kubelet 中的 volume manager 来完成。 kubelet \u0026 volume manager Mount 阶段则由对应节点的 kubelet 中的 volume manager 处理。 volume manager 获取 node.Status.VolumesAttached 属性值，发现 volume 已被标记为 attached； 就会进行 mount 操作，调用 CSI 的 NodeStageVolume 和 NodePublishVolume 接口。 kubelet 会调用 VolumeManager，为 pods 准备存储设备，存储设备就绪会挂载存储设备到 pod 所在的节点上，并在容器启动的时候挂载在容器指定的目录中；同时，删除卸载不在使用的存储。 相关代码如下： func (rc *reconciler) reconcile() { // Unmounts are triggered before mounts so that a volume that was // referenced by a pod that was deleted and is now referenced by another // pod is unmounted from the first pod before being mounted to the new // pod. rc.unmountVolumes() // Next we mount required volumes. This function could also trigger // attach if kubelet is responsible for attaching volumes. // If underlying PVC was resized while in-use then this function also handles volume // resizing. rc.mountOrAttachVolumes() // Ensure devices that should be detached/unmounted are detached/unmounted. rc.unmountDetachDevices() } 整个 reconcile 包括 3 个方法 unmountVolumes：遍历 node 上的 mountedVolume，检查 pod 还在不在，如果 pod 不在了就把对应 volume unmount 了。 mountOrAttachVolumes：遍历需要 mount 或者 attach 的 volume，看下是不是真的 mount 或者 attach了，没有就执行 mount 或者 attach。 unmountDetachDevices：如果某个设备的多个 volume 都是 unmount 状态那就把该设备 detach 掉。 在 mountOrAttachVolumes 方法中，由于设备需要先 attach，因此 kubelet 这里会一直阻塞，产生一个叫做 VolumeNotAttached 的错误，这个就要靠前面的 ADController 了，ADController 把 Volume Attach 之后，kubelet 这边就开始 mount。 如果这里 CSI Driver 中的 requiredAttach 配置为 false 则这里会默认 Volume 已经 attach 了，然后开始 mount。 具体源码 pkg/volume/util/operationexecutor/operation_generator.go#L1508-L1522 Kubelet 会根据具体的 PV 名字找到对应的 CSI Driver，然后 调用 CSI 的 NodeStageVolume 和 NodePublishVolume 接口完成 mount。 具体源码 pkg/volume/util/operationexecutor/operation_generator.go#L530-L748 NodeStageVolume 被封装到了 MountDevice 方法里，而 NodePublishVolume 则是封装到了 MapPodDevice 里。 至此，存储相关的基本流程就结束了，至于删除则是按照相反的流程执行。 还有下面这两个 Sidecar 没有讲解，不过看名字应该也能知道大概是做什么的： external-snapshotter ：存储卷快照 external-resizer：存储卷扩容 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"4. 小结 k8s中涉及存储的组件主要有：attach/detach controller、pv controller、volume manager、volume plugins、scheduler。每个组件分工明确： attach/detach controller：负责对 volume 进行 attach/detach persistent volume controller：负责处理 pv/pvc 对象，包括 pv 的 provision/delete kubelet volume manager：主要负责对 volume 进行 mount/unmount volume plugins：包含 k8s 原生的和各厂商的的存储插件 具体流程： 一共可以分为 3 个步骤： 1）Create 用户创建 PV 后 external-provisioner pod watch 到 pvc 创建事件 ，并根据具体信息调用 CSI Plugin 的 CreateVolume 方法创建对应 PV，以实现动态供应。 PV 创建后由 PVController 将 PV 和 PVC 进行绑定 2）Attach 创建 Pod，Pod 的 spec.Volume 中指定要使用的 PVC 的名字，pod 创建后由 Scheduler 调度到指定节点。 然后 ADController 则会根据当前 Node 上已经 Attach 的 Volume 以及当前 Node 上的 Pod 列表中提取出的 Volume 列表进行调谐，把Pod 用到的但是当前没有 Attach 的 Volume 进行 Attach。实际为创建一个 VolumeAttachment 对象。 由 external-attacher 来 watch 这个 VolumeAttachment 对象，并根据里面的信息来调用 CSI Plugin 的 ControllerPublishVolume 方法进行 Attch。 3）Mount Volume Attach 完成后则由 Kubelet 中的 Volume Manager 来执行 Mount 操作，最终则是调用 CSI Plugin 的 NodeStageVolume 和 NodePublishVolume 方法 以上就是 Pod 使用 PV 的基本流程。 ","date":"2022-11-11","objectID":"/posts/kubernetes/14-pv-dynamic-provision-process/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(十四)---PV 从创建到挂载全流程详解","uri":"/posts/kubernetes/14-pv-dynamic-provision-process/"},{"categories":["Kubernetes"],"content":"Vertical Pod Autoscaler,垂直 Pod 自动扩缩容","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"本文主要记录 k8s 中与 HPA 相对应的 VPA 功能，包括架构、大致实现以及 demo。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"什么是 VPA？ VPA 全称 Vertical Pod Autoscaler,即垂直 Pod 自动扩缩容,它根据容器资源使用率自动设置 CPU 和内存的requests, 以便为每个 Pod 提供适当的资源。 既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。 使用 VPA 的意义: Pod 资源用其所需，提升集群节点使用效率; 不必运行基准测试任务来确定 CPU 和内存请求的合适值; VPA可以随时调整CPU和内存请求,无需人为操作，因此可以减少维护时间。 注意：VPA目前还没有生产就绪，在使用之前需要了解资源调节对应用的影响。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"VPA架构图 VPA 主要包括两个组件： 1）VPA Controller Recommendr：给出 pod 资源调整建议 Updater：对比建议值和当前值，不一致时驱逐 Pod 2）VPA Admission Controller Pod 重建时将 Pod 的资源请求量修改为推荐值 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"VPA 工作流程 VPA 工作流程如下图所示： 首先 Recommender 会根据应用当前的资源使用情况以及历史的资源使用情况，计算接下来可能的资源使用阈值，如果计算出的值和当前值不一致则会给出一条资源调整建议。 然后 Updater 则根据这些建议进行调整，具体调整方法为： 1）Updater 根据建议发现需要调整，然后调用 api 驱逐 Pod 2）Pod 被驱逐后就会重建，然后再重建过程中VPA Admission Controller 会进行拦截，根据 Recommend 来调整 Pod 的资源请求量 3）最终 Pod 重建出来就是按照推荐资源请求量重建的了。 根据上述流程可知，调整资源请求量需要重建 Pod，这是一个破坏性的操作，所以 VPA 还没有生产就绪。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"Recommenderd 设计理念 推荐模型(MVP) 假设内存和CPU利用率是独立的随机变量，其分布等于过去 N 天观察到的变量(推荐值为 N=8 以捕获每周峰值)。 对于 CPU，目标是将容器使用率超过请求的高百分比(例如95%)时的时间部分保持在某个阈值(例如1%的时间)以下。 在此模型中，CPU 使用率 被定义为在短时间间隔内测量的平均使用率。测量间隔越短，针对尖峰、延迟敏感的工作负载的建议质量就越高。 最小合理分辨率为1/min,推荐为1/sec。 对于内存，目标是将特定时间窗口内容器使用率超过请求的概率保持在某个阈值以下(例如，24小时内低于1%)。 窗口必须很长(≥24小时)以确保由 OOM 引起的驱逐不会明显影响(a)服务应用程序的可用性(b)批处理计算的进度(更高级的模型可以允许用户指定SLO来控制它)。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"VPA的不足 VPA的成熟度还不足 更新正在运行的 Pod 资源配置是 VPA 的一项试验性功能，会导致 Pod 的重建和重启，而且有可能被调度到其他的节点上。 VPA 不会驱逐没有在副本控制器管理下的 Pod。 目前 VPA 不能和监控 CPU 和内存度量的Horizontal Pod Autoscaler (HPA) 同时运行,除非 HPA 只监控其他定制化的或者外部的资源度量。 VPA 使用 admission webhook 作为其准入控制器。如果集群中有其他的 admission webhook,需要确保它们不会与 VPA 发生冲突。准入控制器的执行顺序定义在 APIServer 的配置参数中。 VPA 会处理出现的绝大多数 OOM 的事件，但不保证所有的场景下都有效。 VPA 性能尚未在大型集群中进行测试。 VPA 对 Pod 资源 requests 的修改值可能超过实际的资源上限，例如节点资源上限、空闲资源或资源配额，从而造成 Pod 处于 Pending 状态无法被调度。 同时使用集群自动伸缩(ClusterAutoscaler) 可以一定程度上解决这个问题。 多个 VPA 同时匹配同一个 Pod 会造成未定义的行为。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"In-Place Update of Pod Resources 当前 VPA 需要重建 Pod 才能调整 resource.requst，因此局限性会比较大，毕竟频繁重建 Pod 可能会对业务稳定性有影响。 社区在 2019 年就有人提出In-Place Update of Pod Resources 功能，最新进展见 #1287,根据 issue 中的描述，最快在 k8s v1.26 版本就能完成 Alpha 版本。 该功能实现后对 VPA 来说是一个巨大的优化，毕竟一直破坏性的重建 Pod 风险还是有的。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"VPA Demo 首先肯定是要有一个 k8s 集群，可以参考Kubernetes教程(十一)—使用 KubeClipper 通过一条命令快速创建 k8s 集群 快速创建一个。 为了和本文保持一致，建议使用 centos7.9 + k8s v1.23.6 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"openssl VPA 需要 openssl 1.1.1 及以上版本，安装前先检查一下。 $ openssl version OpenSSL 1.0.2k-fips 26 Jan 2017 openssl 1.1.1 一键安装脚本如下： #!/bin/bash set -e # centos一键安装openssl 1.1.1 版本 # 下载相关依赖 yum update -y yum install -y wget tar make gcc perl pcre-devel zlib-devel # 下载 openssl 1.1.1 版本源码 wget https://www.openssl.org/source/openssl-1.1.1g.tar.gz tar zxvf openssl-1.1.1g.tar.gz cd openssl-1.1.1g # 开始编译 ./config --prefix=/usr --openssldir=/etc/ssl --libdir=lib no-shared zlib-dynamic make \u0026\u0026 make install # 查看是否安装成功 openssl version echo 'openssl install finish.' ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:4:1","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"Metrics Server VPA 依赖于 Metrics Server 提供的指标信息，因此需要使用以下命令在集群中安装 Metrics Server： # 从官方仓库获取yaml文件 curl https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.1/components.yaml -o metrics-server.yaml # 替换镜像 sed -i 's/k8s.gcr.io\\/metrics-server\\/metrics-server:v0.6.1/dyrnq\\/metrics-server:v0.6.1/' metrics-server.yaml 添加--kubelet-insecure-tls flag 以跳过 tls，具体修改如下： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls # 增加该 flag - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s image: dyrnq/metrics-server:v0.6.1 # 替换为国内能访问的镜像 确认当前集群可以使用 kubectl top 功能 kubectl top node ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:4:2","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"启动 vpa VPA 与 k8s 版本兼容性如下： VPA version Kubernetes version 0.12 1.25+ 0.11 1.22 - 1.24 0.10 1.22+ 0.9 1.16+ 当前使用的是 k8s 1.23.6 版本，根据 VPA 兼容性，这个版本的 k8s 需要使用 VPA 0.11 版本。 先拉一下代码 git clone -b vpa-release-0.11 https://github.com/kubernetes/autoscaler.git 然后把里面用到的 gcr 镜像都替换以下，全部换成 dockerhub 的 sed -i 's/k8s.gcr.io\\/autoscaling/k8sgcrioautoscaling/' deploy/admission-controller-deployment.yaml sed -i 's/k8s.gcr.io\\/autoscaling/k8sgcrioautoscaling/' deploy/recommender-deployment.yaml sed -i 's/k8s.gcr.io\\/autoscaling/k8sgcrioautoscaling/' deploy/updater-deployment.yaml 然后使用仓库里的脚本启动 VPA cd vertical-pod-autoscaler ./hack/vpa-up.sh VPA 一共包含 3 个组件： 1）recommender 2）updater 3）admission-controller 检测是否全部启动 $ kubectl -n kube-system get po vpa-admission-controller-778b7f4fff-8f6ql 1/1 Running 0 7m45s vpa-recommender-5479996844-2f6gg 1/1 Running 0 7m46s vpa-updater-745454495c-lv8bg 1/1 Running 0 7m47s ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:4:3","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"测试 首先创建一个 deployment,pod 里跑了一个死循环，主要用于占用 cpu cat \u003e deploy.yaml \u003c\u003c EOF apiVersion: apps/v1 kind: Deployment metadata: name: hamster spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: gcmirrors/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes \u003e/dev/null; sleep 0.5s; done\" EOF kubectl apply -f deploy.yaml 启动后查看 pod 的 cpu 占用情况 [root@vpa vertical-pod-autoscaler]# kubectl top pod NAME CPU(cores) MEMORY(bytes) hamster-6f6567cc69-jwbts 503m 1Mi hamster-6f6567cc69-qnhph 504m 1Mi 可以看到很快就已经超过设置的阈值了。 然后创建一个 vpa对象，测试是否会自动更新资源请求量： cat \u003e vpa.yaml \u003c\u003c EOF apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa spec: # recommenders field can be unset when using the default recommender. # When using an alternative recommender, the alternative recommender's name # can be specified as the following in a list. # recommenders: # - name: 'alternative' targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster resourcePolicy: containerPolicies: - containerName: '*' minAllowed: cpu: 100m memory: 50Mi maxAllowed: cpu: 1 memory: 500Mi controlledResources: [\"cpu\", \"memory\"] EOF kubectl apply -f vap.yaml 创建 vpa 对象后耐心等待一会，vpa 就会给出推荐建议 $ kubectl describe vpa Status: Conditions: Last Transition Time: 2022-10-15T06:30:43Z Status: True Type: RecommendationProvided Recommendation: # 可以看到当前已经给出了调整建议 Container Recommendations: Container Name: hamster Lower Bound: Cpu: 211m Memory: 262144k Target: Cpu: 627m Memory: 262144k Uncapped Target: Cpu: 627m Memory: 262144k Upper Bound: Cpu: 1 Memory: 500Mi Events: \u003cnone\u003e 然后之前创建的 pod 就会使用新的 resource.request 值重建，当前 pod 情况如下： [root@vpa vertical-pod-autoscaler]# kubectl get po NAMESPACE NAME READY STATUS RESTARTS AGE default hamster-6f6567cc69-qnhph 1/1 Running 0 4m38s default hamster-6f6567cc69-t8rc9 1/1 Running 0 47s 根据上面的时间可以看出来第二个 pod 已经被重建过了，查看一下当前的 resource.request kubectl get po hamster-6f6567cc69-t8rc9 -oyaml spec: containers: - args: - -c - while true; do timeout 0.5s yes \u003e/dev/null; sleep 0.5s; done command: - /bin/sh image: gcmirrors/ubuntu-slim:0.1 imagePullPolicy: IfNotPresent name: hamster resources: requests: cpu: 627m memory: 262144k 可以看到当前的 resources.request 就是前面给出的建议值，说明 VPA 功能正常。 至此 VPA demo 结束。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:4:4","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"小结 本文主要分析了 VPA 的架构、工作流程以及 VPA 的不足，最后使用 demo 进行了演示。 当前 VPA 限制还比较多，特别是 In-Place Update of Pod Resources 功能缺失的情况下，需要重建 Pod 才能实现资源请求量的调整，影响比较大。 总之当前版本的 VPA 还不是很推荐使用。 ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"参考 vpa-design-proposal In-Place Update of Pod Resources ","date":"2022-10-14","objectID":"/posts/kubernetes/12-vpa/:6:0","tags":["Kubernetes"],"title":"Kubernetes教程(十二)---VPA:垂直 Pod 自动扩缩容","uri":"/posts/kubernetes/12-vpa/"},{"categories":["Kubernetes"],"content":"Flannel host-gw 模式以及 Calico GBP 模式大致实现","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"本文主要介绍了 Kubernetes 中的 Pure Layer 3 网络方案。其中的典型例子，莫过于 Flannel 的 host-gw 模式和 Calico 项目了，本文将大致分析 Flannel 的 host-gw 模式和 Calico 项目来探索Kubernetes 中的纯三层（Pure Layer 3）网络方案。 本文写于 2021-03-27 第二次更新于 2022-09-04，增加了一些自己的理解,并修复了一些错误 Kubernetes ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"1. Flannel host-gw 模式 数据流向如下图所示： 当你设置 Flannel 使用 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例： $ ip route ... 10.244.1.0/24 via 10.168.0.3 dev eth0 这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。 所谓下一跳地址就是：如果 IP 包从主机 A 发到主机 B，需要经过路由设备 X 的中转。那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址。 而从 host-gw 示意图中我们可以看到，这个下一跳地址对应的，正是我们的目的宿主机 Node 2。 一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。 这样，这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上。 而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.3，即 Infra-container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 Infra-container-2 当中。 可以看到，host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。 也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。 而且 Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。 而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。 host-gw 模式能够正常工作的核心，就在于 IP 包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的 MAC 地址。这样，它就会经过二层网络到达目的宿主机。 所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。 问题来了，为什么需要二层连通？ 如果二层不连通，那么只能通过三层传输(即：IP 包)，到其他设备或者路由器的时候是不认识这个容器 IP 的，因此这个包肯定会丢。 注：Node1 和 Node2 认识是因为 Calico 在上面加了对应的路由规则。 因此必须要二层连通才行。 ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"2. Calico BGP 模式 Calico 当前算是主流的 k8s CNI 实现。 实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。 也就是说，Calico 也会在每台宿主机上，添加一个格式如下所示的路由规则： \u003c目的容器IP地址段\u003e via \u003c网关的IP地址\u003e dev eth0 其中，网关的 IP 地址，正是目的容器所在宿主机的 IP 地址。 而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的 IP 地址，找到它所对应的、“下一跳”的网关。 不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器” BGP 来自动地在整个集群中分发路由信息。 BGP（Border Gateway Protocol）即边界网关协议，是运行于TCP上的自治系(互联网AS)统路由协议。 简单理解：所谓 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。 BGP 在每个边界网关（路由器）上运行，彼此之间通信更新路由表信息，而 BGP 的这个能力，正好可以取代 Flannel 维护主机上路由表的功能。 而且，BGP 这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非 Flannel 自己的方案可比。 Calico 由三个部分组成： 1）Calico 的 CNI 插件。这就是 Calico 与 Kubernetes 对接的部分。 2）Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。 3）BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。 除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。 二者只是不同的实现，功能上并没有差别。Calico 通过设置路由规则，将数据包直接路由到对应 veth 设备，Flannel 则是先路由到网桥，在从网桥转发到对应 veth 设备。 Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 前缀开头）。 此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示： 10.233.2.3 dev cali5863f3 scope link 即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。 基于上述原因，Calico 项目在宿主机上设置的路由规则，肯定要比 Flannel 项目多得多。 不过，Flannel host-gw 模式使用 CNI 网桥的主要原因，其实是为了跟 VXLAN 模式保持一致。否则的话，Flannel 就需要维护两套 CNI 插件了。 有了这样的 Veth Pair 设备之后，容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。接下来的流程就跟 Flannel host-gw 模式完全一致了。 其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。 Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。 需要注意的是，Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N²的规模快速增长，从而给集群本身的网络带来巨大的压力。 所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫作 Route Reflector 的模式。 在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。 对于 Calico BGP 模式来说，同样要求集群宿主机之间是二层连通的。 举个例子，假如我们有两台处于不同子网的宿主机 Node 1 和 Node 2，对应的 IP 地址分别是 192.168.1.2 和 192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个 IP 地址之间是可以相互通信的。 而我们现在的需求，还是 Container 1 要访问 Container 4。 按照我们前面的讲述，Calico 会尝试在 Node 1 上添加如下所示的一条路由规则： 10.233.2.0/16 via 192.168.2.2 eth0 上面这条规则里的下一跳地址是 192.168.2.2，可是它对应的 Node 2 跟 Node 1 却根本不在一个子网里，没办法通过二层网络把 IP 包发送到下一跳地址。 在这种情况下，你就需要为 Calico 打开 IPIP 模式。 ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"3. Calico IPIP 模式 Calico 的 IPIP 模式也是隧道中的一种。 在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示： 10.233.2.0/24 via 192.168.2.2 tunl0 这一次，要负责将 IP 包发出去的设备，变成了 tunl0，Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。 P 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，如下所示： 其中，经过封装后的新的 IP 包的目的地址（图中的 Outer IP Header 部分），正是原 IP 包的下一跳地址，即 Node 2 的 IP 地址：192.168.2.2，而原 IP 包本身，则会被直接封装成新 IP 包的 Payload。 这样，原先从容器到 Node 2 的 IP 包，就被伪装成了一个从 Node 1 到 Node 2 的 IP 包。 由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。 这时，Node 2 的网络内核栈会使用 IPIP 驱动进行解包，从而拿到原始的 IP 包。然后，原始 IP 包就会经过 Node 2 上由 Calico 设置的路由规则和 Veth Pair 设备到达目的容器内部。 以上，就是 Calico 项目主要的工作原理了。 额外的封包和解包工作会导致集群网络性能下降，在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。 ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"4. 小结 三层网络主要通过维护路由规则，将数据包直接转发到对应的宿主机上。 Flannel host-gw 主要通过 etcd 中的子网信息来维护路由规则； Calico BGP 模式则通过 BGP 协议收集路由信息，由 Felix 进程来维护； Calico IPIP 模式也是一直隧道模式，主要通过在 IP 包外再封装一层 IP 包来实现。 使用场景 在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。 1）如果是在公有云上，由于宿主机网络本身比较“直白”，一般推荐更加简单的 Flannel host-gw 模式。 2）但不难看到，在私有部署环境里，Calico 项目才能够覆盖更多的场景，并为你提供更加可靠的组网方案和架构思路。 ​ 三层和隧道的异同： 相同之处是都实现了跨主机容器的三层互通，不同之处是三层通过配置下一条主机的路由规则来实现互通，因此需要二层连通，而隧道则是通过封包和解包来实现，因此不需要二层连通。 三层的优点：少了封包和解包的过程，性能肯定是更高的。 三层的缺点：需要自己想办法维护路由规则。 隧道的优点：简单，原因是大部分工作都是由 Linux 内核的模块实现了，应用层面工作量较少。 隧道的缺点：主要的问题就是性能低。 是否应用隧道的本质其实是在网络传输的中间过程中，只靠原本的数据包的路由信息(mac,ip)等，中间网络设备是否能够顺利识别并转发。 像二层不通的情况下，用 BGP 模式直接在宿主机设置路由是无法到达的，因此就需要该用 IPIP 模式走隧道了。 ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"5. 参考 https://kubernetes.io/docs/concepts/cluster-administration/networking/ https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c https://feisky.gitbooks.io/kubernetes/content/network/flannel/flannel.html 深入剖析Kubernetes 专栏 ","date":"2022-09-04","objectID":"/posts/kubernetes/03-pure-layer-3-network/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(三)---纯三层网络方案","uri":"/posts/kubernetes/03-pure-layer-3-network/"},{"categories":["Kubernetes"],"content":"Kubernetes 集群网络方案 Flannel 核心原理详解","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"本文主要记录了 Kubernetes 集群网络方案之 Flannel 核心原理详解，包括其隧道方案中的两种：UDP 实现和 VXLAN 实现。 本文写于 2021-03-20，(感觉自己会了)，(现在看来之前并不是真的会) 第二次更新于 2022-09-03，增加了一些自己的理解，(感觉自己又行了) ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:0:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"1. 概述 Docker 的默认配置下，不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的。 Docker 单机容器网络具体实现可以参考： Docker教程(四)—容器网络实现分析 以及 Docker教程(十)—Docker 单机(桥接)网络实现 这两篇文章。 为了解决这个容器跨主通信的问题，k8s 制定了 CNI 规范，然后社区里依据该规范出现了各种各样的容器网络方案。其中 Flannel 是最早实现的，也是最简单的一个，因此我们使用 Flannel 来分析。 Flannel 项目是 CoreOS 公司主推的容器网络方案。目前，Flannel 支持三种后端实现，分别是： 1）VXLAN； 2）host-gw； 3）UDP。 其中 UDP 和 VXLAN 都是隧道模式，host-gw 则是纯三层网络方案。 UDP 模式，是 Flannel 项目最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。不过，Flannel 之所以最先选择 UDP 模式，就是因为这种模式是最直接、也是最容易理解的容器跨主网络实现。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:1:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"2. Flannel UDP 模式 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"0. 例子 在这个例子中，我有两台宿主机。 宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。 宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。 我们现在的任务，就是让 container-1 访问 container-2。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:1","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"1. 大致流程 UDP 方案具体流程如下图所示： ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:2","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"2. flannel0 该方案使用时会在各个 Work 节点上运行一个Flannel 进程，同时创建一个 flannel0 设备 ，而这个 flannel0 它是一个 TUN 设备（Tunnel 设备）。 在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。 当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。 这是一个从内核态向用户态的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。 这是一个从用户态向内核态的流动方向。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:3","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"3. Subnet 子网（Subnet) 是 Flannel 项目里一个非常重要的概念。 事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。 在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。 而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： $ etcdctl ls /coreos.com/network/subnets /coreos.com/network/subnets/100.96.1.0-24 /coreos.com/network/subnets/100.96.2.0-24 /coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），然后从 Etcd 中找到这个子网对应的宿主机的 IP 地址，如下所示： $ etcdctl get /coreos.com/network/subnets/100.96.2.0-24 {\"PublicIP\":\"10.168.0.3\"} 即根据容器 IP 确定子网，根据子网确定目标宿主机 IP。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:4","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"4. 具体步骤 step 1：容器到宿主机 container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 step 2：宿主机路由到 flannel0 设备 这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。 Flannel 已经在宿主机上创建出了一系列的路由规则。 以 Node 1 为例，如下所示： # 在Node 1上 $ ip route default via 10.168.0.1 dev eth0 100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0 100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.1 10.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 由于我们的 IP 包的目的地址是 100.96.2.3，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。 step 3：flanneld 进程转发给 Node2 flannel0 设备收到 IP 包后转给 flanned 进程。然后，flanneld 根据这个 IP 包的目的地址，是 100.96.2.3，去 etcd 中查询到对应的宿主机IP，就是 Node2，因此会把它发送给了 Node 2 宿主机，不过发送之前会对该 IP 包进行封装。 因为当前这个包源地址是 container-1 的 IP 100.96.1.2，目的地址是 container-2 的 IP 100.96.2.3，这样直接发送出去肯定是到不了的。 step 4：封装UDP包 flanneld 进程会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。 由于 flanneld 进程监听的是 8285 端口，所以会发送给 Node2 的 8285 端口。 step 5：Node2 解析并处理UDP包 Node2 上的 flanneld 进程收到这个 UDP 包之后就可以从里面解析出container-1 发来的原 IP 包。 解析后将其发送给 flannel0 设备，flannel0 则会将其转发给操作系统内核。 step 6：内核处理IP包 内核收到这个 IP 包之后，内核网络栈就会负责处理这个 IP 包。具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。 该路由规则同样由 Flannel 维护。 而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： # 在Node 2上 $ ip route default via 10.168.0.1 dev eth0 100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0 100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.1 10.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 step 7：容器网络 IP 包到 docker0 网桥后的流程就属于容器网络了，这里不在过多讲解，具体参考开篇提到的两篇文章。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:5","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"5. 分析 实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。 而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： 1）第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态； 2）第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程； 3）第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。 此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因。 所以说，我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行。这也是为什么，Flannel 后来支持的VXLAN 模式，逐渐成为了主流的容器网络方案的原因。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:2:6","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"3. Flannel VXLAN VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。 所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。 VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。 当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。 简单理解就是：把 二层数据包 封装之后通过 三层网络发送，然后对应设备收到后进行解包拿到里面的 二层数据包，这样只要 三层网络连通就可以实现了，相当于把 二层网络的范围扩大到了 三层网络。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"1. VTEP 而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。 而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。 因此 VXLAN 模式的效率会比 UDP 模式高不少 具体流程如下： 可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:1","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"2. 路由规则 每台宿主机上的 flanneld 进程会负责维护相关的路由规则。比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... 10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。 10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。 即：Flannel1 设备会在当前宿主机增加指向 flannel 网络中其他节点的路由规则。 类似于 UDP 模式中的 Subnet，前者是把对应节点 IP 存储在 etcd中，后者则是直接通过路由规则指定。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:2","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"3. ARP 记录 flanneld 进程启动时，会自动把当前节点上的 ARP 记录发送给当前 flannel 网络中的其他节点。 后续将 IP 包封装成 二层数据帧的时候，用到的目的 MAC 地址就是从这里查询的。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:3","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"4. 网桥设备 flannel.1 设备实际还要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。 flannel.1 设备的 FDB 则由 flanneld 进程维护。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:4","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"5. 具体步骤 step 1：容器到宿主机 和 UDP 模式一样，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，根据容器内的路由规则，会先出现在 docker0 网桥。 step 2：宿主机路由到 flannel1 设备 然后被路由到本机 flannel.1 设备进行处理。也就是说，来到了“隧道”的入口。 为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。 根据前面提到的 路由规则，知道这个 IP 包要发给 10.1.16.0 ，即 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。 step 3：封装为 2 层数据帧 为了方便叙述，后续把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设备”和“目的 VTEP 设备”。 而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。 所以在我们的例子中，“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”。 在正常网络里是由 内核网络栈进行封装的，比如某机器收到一个 三层数据包,对比目的 IP 地址，发现就是内部某局域网的，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址，将这个 三层数据包 封装成 二层数据帧。 但是这里的 二层网络 使我们虚拟出来的，因此为了让 VTEP 设备收到的是二层数据帧，我们需要自己来处理封包的逻辑。 前面路由记录中我们知道了“目的 VTEP 设备”的 IP 地址，这里就可以使用 IP 地址查询对应的 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。 这也就是为什么 flanneld 进程启动后要把本地 ARP 记录发送给其他节点。 # 在Node 1上 $ ip neigh show dev flannel.1 10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT 可以看到：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。 有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。 step 4：将二层数据帧封装为外部数据帧，通过 UDP 发送出去 上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为**“内部数据帧”(Inner Ethernet Frame)**。 所以接下来，Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。 我们把这次要封装出来的、宿主机对应的数据帧称为**“外部数据帧”(Outer Ethernet Frame)**。 为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。 而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1。 这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。 step 5：flannel1 设备转发 UDP 包 flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。 通过 bridge fdb 命令查看 flannel.1 设备的FDB # 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询 $ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37 5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent 可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即： 往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。 所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。 step 6: 宿主机封包并发送 宿主机把我们准备好的 UDP 包，增加 IP 头组成一个IP包，IP 头中的IP则是前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。 然后增加二层数据帧头，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。 接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。 step 7：Node 2 解包 Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。 而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:5","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"6. 分析 与 UDP 实现相比，VXLAN 方式所有封包工作都在内核态完成，省去了 内核态用户态切换的消耗，拥有较高的效率。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:3:6","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"4. 小结 本章主要分析了 Flannel 网络中的 UDP 和 VXLAN 实现。 具体实现 通过在现有的 三层网络 上构建虚拟的 二层网络，将容器中发出的三层数据包封装为而层数据帧，然后通过宿主机网卡发送出去。接收方接收到之后再由对应的程序进行解包处理，得到原始数据包。 两种实现方式差异 相同点：都是在各个节点上运行 flanneld 进程和 flannel 设备来处理相关网络包。 差异点： UDP 方式实现有 3 次内核态用户态切换导致效率低，而 VXLAN 方式则全在内核态处理，效率较高。 VXLAN 模式组建的覆盖网络，其实就是一个由不同宿主机上的 VTEP 设备，也就是 flannel.1 设备组成的虚拟二层网络。对于 VTEP 设备来说，它发出的“内部数据帧”就仿佛是一直在这个虚拟的二层网络上流动。这，也正是覆盖网络的含义。 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:4:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"5. 参考 flannel-docs-backends.md https://kubernetes.io/docs/concepts/cluster-administration/networking/ https://github.com/flannel-io/flannel https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c https://feisky.gitbooks.io/kubernetes/content/network/flannel/flannel.html 深入剖析 Kubernetes 专栏 ","date":"2022-09-03","objectID":"/posts/kubernetes/02-cluster-network/:5:0","tags":["Kubernetes","Network"],"title":"Kubernetes教程(二)---集群网络之 Flannel 核心原理","uri":"/posts/kubernetes/02-cluster-network/"},{"categories":["Kubernetes"],"content":"使用 KubeClipper 通过一条命令快速创建 k8s 集群","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"本文主要记录了如何使用开源项目 KubeClipper 通过一条命令快速创建 k8s 集群。 2022-12-12，KubeClipper 发布 release-1.3.1 版本，本文也同步进行更新 1.3.1 版本最大提升在于部署体验以及安装速度上的优化，同时一些错误提示也更加人性化，使用体验方便有较大提升。 部署参数大幅减少：1.3.1 版本 aio 部署甚至不需要任何参数，kcctl deploy 即可完成部署，多节点部署也只需要提供两三个参数即可，较之前体验上有较大提升。 集群安装速度大幅提升：之前单节点集群需要 3 分钟，5节点高可用集群需要 9分钟，1.3.1 版本单节点只需要两分钟，5 节点也只需要4分钟。整个部署时间并不会随着节点增加而线性增加。 之前写了一篇使用 kubeadm 创建集群的文章： Kubernetes教程(一)—使用 kubeadm 创建 k8s 集群(containerd)，整个流程下来其实还是比较麻烦的，体验并不是太好，今天给大家推荐一个优秀的开源项目：KubeClipper 。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"1. 什么是 KubeClipper KubeClipper 旨在提供易使用、易运维、极轻量、生产级的 Kubernetes 多集群全生命周期管理服务，让运维工程师从繁复的配置和晦涩的命令行中解放出来，实现一站式管理跨区域、跨基础设施的多 K8S 集群。 KubeClipper 的 3个优势： 1）易使用：图形化页面 提供 Web 控制台，通过点点鼠标即可创建一个 k8s 集群 2）极轻量：架构简单，少依赖 不依赖 ansible，使用方便 3）生产级：易用性和专业性兼顾 提供在线、离线、代理方式部署以及多版本 K8S、CRI、CNI 选择 提供了离线部署方式对国内用户极为友好，毕竟网络才是国内用户装 k8s 的一大难题。 关于该项目的更多信息见：Github Repo 以及官方的这篇介绍文章：KubeClipper——轻量便捷的 Kubernetes 多集群全生命周期管理工具 一句话概括：KubeClipper 是一个轻量便捷的 Kubernetes 多集群全生命周期管理工具。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"2. 快速开始 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"准备工作 KubeClipper 本身并不会占用太多资源，但是为了后续更好的运行 Kubernetes 建议硬件配置不低于最低要求。 您仅需参考以下对机器硬件和操作系统的要求准备一台主机。 硬件推荐配置 确保您的机器满足最低硬件要求：CPU \u003e= 2 核，内存 \u003e= 2GB。 操作系统：CentOS 7.x / Ubuntu 18.04 / Ubuntu 20.04。 节点要求 节点必须能够通过 SSH 连接。 节点上可以使用 sudo / curl / wget / tar / gzip命令。 建议您的操作系统处于干净状态（不安装任何其他软件），否则可能会发生冲突。 实际上这些要求基本上不用特别处理，随便找一台机器都满足条件，本文就使用一台 2C4G 的 CentOS 7.9 机器来部署。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"安装 KubeClipper 下载 kcctl KubeClipper 提供了命令行工具🔧 kcctl，我们先安装 kcctl，安装命令如下： curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn VERSION=v1.3.1 bash - 通过以下命令检测是否安装成功: kcctl version 开始安装 然后使用 kcctl 安装 kubeclipper，一条命令即可,模板如下： kcctl deploy kcctl 将检查安装环境，若满足条件则会在当前节点安装 kubeclipper，在打印出如下的 KubeClipper banner 后即表示安装完成。 _ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 安装过程中需要去阿里云下载离线安装包，大概 1 分钟即可下载完成。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"登录控制台 安装完成后，打开浏览器，访问 http://$IP 即可进入 KubeClipper 控制台。 您可以使用默认帐号密码 admin / Thinkbig1 进行登录。 您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。 文中所有操作均通过命令行工具完成，至于控制台相关功能则由大家自行探索了~ ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:2:3","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"安装 K8S 集群 部署成功后可以使用 kcctl 工具或者通过控制台创建 k8s 集群， 这里咱们使用 kcctl 工具进行创建。 命令行，程序猿最后的倔强。 首先使用默认账号密码进行登录获取 token，便于后续 kcctl 和 kc-server 进行交互。 kcctl login -H http://localhost -u admin -p Thinkbig1 查看当前 agent 节点 [root@iZbp1cz9txcv4n6uluew1jZ ~]# kcctl get node +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ | ID | HOSTNAME | REGION | IP | OS/ARCH | CPU | MEM | +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ | f22d488f-af17-47cb-ab55-07f8c4cce5f0 | server1 | default | 172.20.175.140 | linux/amd64 | 2 | 3645Mi | +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ 然后使用以下命令创建 k8s 集群: # 其中 IP 为上一步中的 server1 节点 IP kcctl create cluster --name demo --master 172.20.175.140 --untaint-master 大概两分钟左右即可完成集群创建,可以使用以下命令查看实时日志： while true; do logDir=\"/var/log/kc-agent\"; op=$(ll $logDir -t|grep -v total|head -n 1|awk '{print $9}'); echo -e \"\\n当前操作 ID：\"$op; latestSteps=$(ll \"$logDir/$op\" -t|grep -v total|head -n 1|awk '{print $9}'); echo -e \"当前最新步骤：\"$latestSteps\"\\n\"; tail -n 20 \"$logDir/$op/$latestSteps\"; sleep 3; done 或者使用以下命令查看集群状态 kcctl get cluster -o yaml|grep status -A5 也可以进入控制台查看实时日志。 进入 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。 $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} 至此，我们的单节点版本就算是结束了，相比之下确实比使用原生 kubeadm 体验好多了。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:2:4","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"3. 多节点版本 为了保证高可用，k8s 集群至少需要 3个 master 节点，同时 kubeclipper 使用 etcd 作为后端存储，为了保证高可用，也建议至少使用 3 节点及以上来部署。 同时生产环境一般建议 kubeclipper server 节点单独存在，不建议将 server 节点同时作为 agent 节点使用。 节点规划如下： 3 台 作为 kubeclipper server 部署使用 5 台 作为 kubeclipper agent 节点，部署 k8s 使用。 资源有限，依旧是 2C4G 的 CentOS 7.9 机器 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"安装 KubeClipper 同样的，还是需要先安装 kcctl 工具，安装方式和之前是一样的，一条命令搞定： kcctl 工具只需要在任意一台节点上安装即可，只要能使用 ssh 访问其他节点即可。 为了和本教程保持一致，建议在其中的一台 server 节点上安装。 curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn VERSION=v1.3.1 bash - 接着使用 kcctl 部署 KubeClipper，其模板如下所示： kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) (--server SERVER_NODES) (--agent AGENT_NODES) 需要手动指定 server 节点以及 agent 节点。 若使用 密码 方式则命令如下所示: kcctl deploy --passwd $SSH_PASSWD --server SERVER_NODES --agent AGENT_NODES 私钥 方式如下： kcctl deploy --pk-file $SSH_PRIVATE_KEY --server SERVER_NODES --agent AGENT_NODES 本教程使用 密码 方式进行部署，具体命令如下： # 3 个 server 节点，5 个 agent 节点，多个 IP 之间使用逗号分隔 servers=172.20.175.140,172.20.175.142,172.20.175.146 agents=172.20.175.145,172.20.175.143,172.20.175.144,172.20.175.139,172.20.175.141 kcctl deploy --server $servers --agent $agents --passwd Thinkbig1 同样的，在打印出如下的 KubeClipper banner 后即表示安装完成。 _ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"创建高可用 K8S 集群 这里我们同样使用 kcctl 工具，通过命令行方式进行创建。老规矩，使用默认账号密码进行登录： kcctl login -H http://localhost -u admin -p Thinkbig1 然后查看一下当前的 agent 节点信息： [root@server1 ~]# kcctl get node +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ | ID | HOSTNAME | REGION | IP | OS/ARCH | CPU | MEM | +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ | a0e33d98-fd43-4e69-9700-3bc201b16a3b | agent1 | default | 172.20.175.145 | linux/amd64 | 2 | 3645Mi | | 4a62dc30-b1a5-42b3-87e2-373733e6ae71 | agent2 | default | 172.20.175.143 | linux/amd64 | 2 | 3645Mi | | b5a45667-38fd-4448-ad59-2c93bc049f37 | agent5 | default | 172.20.175.141 | linux/amd64 | 2 | 3645Mi | | 2f5a889a-8ee3-4f7d-9a41-ba55761c89bc | agent4 | default | 172.20.175.139 | linux/amd64 | 2 | 3645Mi | | ea9790f7-add6-4f96-997a-6ef1061b6907 | agent3 | default | 172.20.175.144 | linux/amd64 | 2 | 3645Mi | +--------------------------------------+----------+---------+----------------+-------------+-----+--------+ 接下来就可以使用 kcctl create cluster 命令创建集群了,命令模版如下： kcctl create cluster (--name CLUSTER_NAME) (--master NODES) [--worker NODES][ --untaint-master] 更多信息见 kcctl create cluster -h 在本教程中，我们将 agent1、2、3 作为 master 节点，agent4、5作为 worker 节点，完整命令如下： masters=172.20.175.145,172.20.175.143,172.20.175.144 workers=172.20.175.139,172.20.175.141 kcctl create cluster --name demo --master $masters --worker $workers 对于这样一个 5 节点的集群大概 3 到 4 分钟即可创建完成。 可以 ssh 到第一个 master 节点对应的 agent 节点（即 agent1）使用以下命令查看实时日志： while true; do logDir=\"/var/log/kc-agent\"; op=$(ll $logDir -t|grep -v total|head -n 1|awk '{print $9}'); echo -e \"\\n当前操作 ID：\"$op; latestSteps=$(ll \"$logDir/$op\" -t|grep -v total|head -n 1|awk '{print $9}'); echo -e \"当前最新步骤：\"$latestSteps\"\\n\"; tail -n 20 \"$logDir/$op/$latestSteps\"; sleep 3; done 或者使用以下命令查看集群状态 kcctl get cluster -o yaml|grep status -A5 也可以进入控制台查看实时日志。 进入 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。 ssh 到任意 master 节点上查看集群状态： [root@agent1 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} [root@agent1 ~]# kubectl get node NAME STATUS ROLES AGE VERSION agent1 Ready control-plane,master 3m38s v1.23.6 agent2 Ready control-plane,master 3m13s v1.23.6 agent3 Ready control-plane,master 3m12s v1.23.6 agent4 Ready \u003cnone\u003e 2m18s v1.23.6 agent5 Ready \u003cnone\u003e 2m17s v1.23.6 可以看到，5 节点都处于 Ready 状态，说明集群安装成功。 ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Kubernetes"],"content":"4. 小结 本文演示了如何通过 KubeClipper 使用简单的命令创建 K8S 集群。 整个流程可以简单的分为 3 个步骤： 1）安装 kcctl #不指定VERSION则默认安装master分支构建版本 curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn VERSION=v1.3.1 bash - 2）使用 kcctl 部署 kubeclipper kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) (--server SERVER_NODES) (--agent AGENT_NODES) 3）使用 kcctl 或控制台创建 K8S 集群 kcctl create cluster (--name CLUSTER_NAME) (--master NODES) [--worker NODES][ --untaint-master] 文中所有操作均通过命令行工具完成，至于控制台相关功能则由大家自行探索了~ 如果觉得体验还不错的话，请不要吝啬你的 star ，KubeClipper 还处于快速成长阶段，欢迎大家积极参与~ ","date":"2022-08-20","objectID":"/posts/kubernetes/11-install-by-kubeclipper/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群","uri":"/posts/kubernetes/11-install-by-kubeclipper/"},{"categories":["Linux"],"content":"如何使用 SSH 隧道进行端口转发","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"本章主要介绍了什么是 SSH 隧道以及如何使用 SSH 隧道，包括 SSH 隧道加密数据传输以及绕过防火墙。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:0:0","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"1. 什么是 SSH 隧道 SSH 隧道是 SSH 中的一种机制，它能够将其他 TCP 端口的网络数据通过 SSH 连接来转发，并且自动提供了相应的加密及解密服务。因为 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输，因此这一过程也被叫做“隧道”（tunneling）。 SSH 隧道也可以叫做端口转发 SSH 隧道能够提供两大功能： 1）加密 SSH Client 端至 SSH Server 端之间的通讯数据。 2）突破防火墙的限制完成一些之前无法建立的 TCP 连接。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:1:0","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"本地转发和远程转发 SSH 端口转发自然需要 SSH 连接，而 SSH 连接是有方向的，从 SSH Client 到 SSH Server 。而我们的应用也是有方向的，比如需要连接 MySQL Server 时，MySQL Server 自然就是 Server 端，我们应用连接的方向也是从应用的 Client 端连接到应用的 Server 端。如果这两个连接的方向一致，那我们就说它是本地转发。而如果两个方向不一致，我们就说它是远程转发。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:1:1","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"相关参数 “-L选项”：local，表示使用本地端口转发创建 ssh 隧道 “-R选项”：remote，表示使用远程端口转发创建 ssh 隧道 “-D选项”：dynamic，表示使用动态端口转发创建 ssh 隧道 “-N选项”： 表示创建隧道以后不连接到 sshServer端，通常与”-f”选项连用 “-f选项”：表示在后台运行ssh隧道，通常与”-N”选项连用 “-g选项”：表示 ssh 隧道对应的转发端口将监听在主机的所有IP中，不使用”-g选项”时，转发端口默认只监听在主机的本地回环地址中，”-g” 表示开启网关模式，远程端口转发中，无法开启网关功能 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:1:2","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"2. 演示 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:2:0","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"本地转发 一般是 server 端有 IP 能访问时用本地转发。 将服务器 172.20.150.199 上的 8888 端口映射到本地的 8888 端口，即将本地 8888 端口流量转发到服务器 172.20.150.199 上的 8888 端口。 比如不能直接访问 server 的 8888 端口，此时就可以使用 ssh 隧道来突破该限制。 在服务器 172.20.148.199 上启动一个 http server： python -m SimpleHTTPServer 8888 然后在本地执行以下命令，开启 ssh 隧道。 # 以下两条命令皆可 ssh -N -L 8888:localhost:8888 root@172.20.148.199 该命令表示在本地和 172.20.148.199 之间建立 ssh 隧道，由 ssh 客户端监听本地的 8888 端口并将流量转发到 172.20.148.199 的 sshServer，最终由 sshServer 在转发到 8888 端口 每部分参数具体含义如下： “-N选项”： 表示创建隧道以后不连接到 sshServer 端 “-L选项”：local，表示使用本地端口转发创建 ssh 隧道 8888:localhost:8888：本地的 8888 端口会被转发到 localhost:8888 端口上去。 这里的 localhost 实际上指监听本地回环地址，如果要监听其他 IP 也可以手动指定 root@172.20.148.199：我们创建的 ssh 隧道是连接到 172.20.148.199 上的 root 用户的 测试一下，请求本地 8888 端口能否访问到 远程服务器上的 http server。 curl http://localhost:8888 这时转发数据流是： client 上的应用客户端将数据发送到本地的 8888 端口上， client 上的 ssh 客户端将本地 8888 端口收到的数据加密后发送到 server 端的 ssh server上，ssh server 会解密收到的数据并将之转发到监听的 8888 端口 最后再将从 http server 返回的数据原路返回以完成整个流程。 可以看到，本地已经开启的 8888 端口的监听。 [root@kc-1 ~]# netstat -tunlp|grep 8888 tcp 0 0 127.0.0.1:8888 0.0.0.0:* LISTEN 10933/ssh tcp6 0 0 ::1:8888 :::* LISTEN 10933/ssh 现在默认是在监听本地回环地址，可以设置监听非本地回环地址,比如执行下面这个命令就可以在 ip 192.168.10.205 上监听： ssh -N -L 192.168.10.205:8888:192.168.10.20:8888 root@192.168.10.20 具体如下： [root@kc-1 ~]# netstat -tunlp|grep 8888 tcp 0 0 192.168.10.205:8888 0.0.0.0:* LISTEN 11576/ssh 如果你觉得这还不够，希望所有 IP 地址的 8888 端口都被监听，那么可以在建立隧道时开启”网关功能”，使用”-g”选项可以开启”网关功能”： ssh -g -N -L 8888:192.168.10.20:8888 root@192.168.10.20 监听情况如下： [root@kc-1 ~]# netstat -tunlp|grep 8888 tcp 0 0 0.0.0.0:8888 0.0.0.0:* LISTEN 13287/ssh tcp6 0 0 :::8888 :::* LISTEN 13287/ssh 至此，我们已经了解 ssh 本地转发功能了，接下来看一下远程转发。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:2:1","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"远程转发 当 server 端没有可以直接访问的 IP 时就无法使用本地转发了，如果本地有 IP 可以直接访问那么还可以使用远程转发。 在服务器 172.20.148.199 上启动一个 http server： python -m SimpleHTTPServer 8888 然后假设本地有一个 server 端能访问的 IP 172.20.150.199，然后在 server 上执行以下命令将端口转发到本地端口 ssh -N -R 8888:localhost:8888 root@172.20.150.199 可以看做是在 server 端执行的一个本地转发🙃，因为 server 端没有 ip 所以需要反过来操作。 端口监听都是在本地发生，但是执行命令的机器却变成了 server 端。 命令执行后会在本地(172.20.150.199)监听端口，并将流量转发到我们的 server 端。之前本地转发时也是在本地监听端口 将本地服务器(172.20.150.199 ) 的 8888 端口转发到 server 端的 8888 端口。 唯一区别是远程转发模式下 只能监听本地回环地址，不能监听其他IP，也不能开启网关模式，即无法让其他机器通过 本地服务器(172.20.150.199 ) 来访问 server。 由于是在 server 端执行的命令，因此该场景中 server 端扮演的是 ssh client 的角色，而 client 端则是 ssh server。 此时的数据流向为：client 端数据发送到 8888 端口，client 服务器上的 ssh server 监听该端口并对数据进行加密后转发到 server 端的 ssh client，ssh client 解密数据后转发到 server 节点的 8888 端口。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:2:2","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"扩展-跨机器转发 具体场景如下图所示： 如上图所示，我们想要在A与B之间创建隧道，最终通过隧道访问到ServerC中的 http 服务。 ServerAIP：192.168.10.205 ServerBIP：192.168.10.85 ServerCIP：192.168.10.134 ServerA与ServerB上没有开启任何 http 服务，ServerC中开启了 http 服务，监听了 8888 端口。 我们需要在 ServerA 上执行以下命令开启 ssh 隧道： ssh -N -L 8888:192.168.10.134:8888 root@192.168.10.85 执行后 serverA 上已经开始监听 8888 端口了，默认是在本地回环地址上，需要其他机器访问的话可以指定 ip 或者增加 -g 参数开启网关模式。 [root@kc-1 ~]# netstat -tunlp|grep 8888 tcp 0 0 127.0.0.1:8888 0.0.0.0:* LISTEN 32464/ssh tcp6 0 0 ::1:8888 :::* LISTEN 32464/ssh 然后测试一下 $ curl http://localhost:8888 Directory listing for / .bash_history .bash_logout .bash_profile .bashrc .cshrc .pki/ .ssh/ .tcshrc anaconda-ks.cfg original-ks.cfg tmp/ 注意 上述场景中存在一个问题，就是数据安全性的问题，我们之所以使用ssh隧道，就是为了用它来保护明文传输的数据，从而提升安全性，不过，在上例的场景中，只有ServerA与ServerB之间的传输是受ssh隧道保护的，ServerB与ServerC之间的传输，仍然是明文的，所以，如果想要在上述场景中使用ssh隧道进行数据转发，首先要考虑ServerB与ServerC之间的网络是否可靠。 其实，当我们在创建隧道时如果开启了网关功能，那么应用客户端与ServerA之间的通讯也会面临同样的问题。 其实一般这种场景主要是为了绕过防火墙。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:2:3","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"动态转发 对于本地端口转发和远程端口转发，都存在两个一一对应的端口，分别位于 SSH 的客户端和服务端，而动态端口转发则只是绑定了一个本地端口，而目标地址:目标端口则是不固定的。目标地址:目标端口是由发起的请求决定的，比如，请求地址为192.168.1.100:3000，则通过 SSH 转发的请求地址也是192.168.1.100:3000。 在本地执行以下命令， ssh -N -D localhost:2000 root@192.168.10.85 会在本地开启一个 socks 代理，监听 2000 端口 [root@kc-1 tmp]# netstat -tunlp|grep 2000 tcp 0 0 127.0.0.1:2000 0.0.0.0:* LISTEN 25496/ssh tcp6 0 0 ::1:2000 :::* LISTEN 25496/ssh 我们只需要在本地配置上 socks 代理，localhost:2000 即可把所有请求通过 ssh 2000 端口转发到 192.168.10.85 这台机器上去了。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:2:4","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"3. 小结 SSH 有以下功能： 1）保护 tcp 会话，保护会话中明文传输的内容。 2）绕过防火墙或者穿透到内网，访问对应的服务。 常用命令： 创建本地转发模式的ssh隧道，命令如下 ssh -g -N -L forwardingPort:targetIP:targetPort user@sshServerIP 本机上的 forwardingPort 将会被监听，访问本机的 forwardingPort，就相当于访问 targetIP 的 targetPort，ssh隧道建立在本机与 sshServer 之间。 创建远程转发模式的ssh隧道，命令如下 ssh -N -R forwardingPort:targetIP:targetPort user@sshServerIP sshServer 上的 forwardingPort 将会被监听，访问 sshServer 上的 forwardingPort，就相当于访问 targetIP 的 targetPort，ssh 隧道建立在本机与 sshServer 之间。 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:3:0","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Linux"],"content":"4. 参考 SSH Tunneling: Examples, Command, Server Config ssh端口转发：ssh隧道 ","date":"2022-08-13","objectID":"/posts/linux/07-ssh-tunnel/:4:0","tags":["Linux"],"title":"SSH 隧道简明教程","uri":"/posts/linux/07-ssh-tunnel/"},{"categories":["Golang"],"content":"Go exec 包执行命令超时失效问题分析及解决方案","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"本文主要从源码层面分析了 Go exec 包执行命令超时失效问题，找出具体原因并给出相关解决方案。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:0:0","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"现象 使用 os/exec 执行 shell 脚本并设置超时时间，然后到超时时间之后程序并未超时退出，反而一直阻塞。 具体代码如下： func main() { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() // 二者都可以触发 cmd := exec.CommandContext(ctx, \"bash\",\"/root/sleep.sh\") // cmd := exec.CommandContext(ctx, \"bash\",\"-c\",\"echo hello \u0026\u0026 sleep 1200\") out, err := cmd.CombinedOutput() fmt.Printf(\"ctx.Err : [%v]\\n\", ctx.Err()) fmt.Printf(\"error : [%v]\\n\", err) fmt.Printf(\"out : [%s]\\n\", string(out)) } /root/sleep.sh： #!/bin/bash sleep 1200 运行上述代码 [root@kc ~]# go run main.go 会创建一个 bash 进程，bash 进程又会创建一个 sleep 子进程： [root@kc ~]# ps -ef|grep sleep root 15485 15479 0 11:38 pts/1 00:00:00 bash /root/sleep.sh root 15486 15485 0 11:38 pts/1 00:00:00 sleep 1200 root 15491 15239 0 11:38 pts/2 00:00:00 grep --color=auto sleep 等 context 超时之后，bash 进程被 kill 掉，进而 sleep 进程被 1 号进程托管，并且此时程序并未退出。 [root@kc ~]# ps -ef|grep sleep root 15486 1 0 11:38 pts/1 00:00:00 sleep 1200 root 15499 15239 0 11:38 pts/2 00:00:00 grep --color=auto sleep 手动 kill 掉 sleep 进程 kill 15486 此时程序退出 [root@kc ~]# go run main.go ctx.Err : [context deadline exceeded] error : [signal: killed] out : [] ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:1:0","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"原因分析 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:2:0","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"执行流程 exec.cmd 执行流程如下： 图源： PureLife 首先 go 中调用 fork 创建子进程，在子进程中执行具体命令，并通过管道和子进程进行连接，子进程将结果输出到管道，go 从管道中读取。 go 与 /bin/bash 之间通过两个管道进行连接，分别用于捕获 stderr 和 stdout 输出，/bin/bash 程序退出后，管道写入端被关闭，从而 go 可以感知到子进程退出，从而立刻返回。 猜想：根据现象可知，创建了两个进程，超时后 bash 进程退出，但是 sleep 进程还在，如果 sleep 进程继续占有管道，那么就可能导致阻塞。后续手动 kill 掉 sleep 进程后程序退出也能印证这一点。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:2:1","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"相关源码 带着这个猜想去查看一下源码，相关源码均在 os/exec/exec.go 中。 CombinedOutput func (c *Cmd) CombinedOutput() ([]byte, error) { if c.Stdout != nil { return nil, errors.New(\"exec: Stdout already set\") } if c.Stderr != nil { return nil, errors.New(\"exec: Stderr already set\") } var b bytes.Buffer c.Stdout = \u0026b c.Stderr = \u0026b err := c.Run() return b.Bytes(), err } func (c *Cmd) Run() error { if err := c.Start(); err != nil { return err } return c.Wait() } CombinedOutput 逻辑很简单，和方法名一样，将 Stdout 和 Stderr 设置为同一个 writer。 Run 方法中则调用了 Start 和 Wait 方法： Start 方法用于启动子进程，启动后立即返回 Wait 方法则阻塞，等待子进程结束并回收资源。 阻塞大概率出现在 Wait 方法中，因此先看 Wait 方法。 Wait Wait 方法具体如下 func (c *Cmd) Wait() error { if c.Process == nil { return errors.New(\"exec: not started\") } if c.finished { return errors.New(\"exec: Wait was already called\") } c.finished = true state, err := c.Process.Wait() if c.waitDone != nil { close(c.waitDone) } c.ProcessState = state var copyError error for range c.goroutine { if err := \u003c-c.errch; err != nil \u0026\u0026 copyError == nil { copyError = err } } c.closeDescriptors(c.closeAfterWait) if err != nil { return err } else if !state.Success() { return \u0026ExitError{ProcessState: state} } return copyError } 根据 debug 得知阻塞点就是 err := \u003c-c.errch 这句。从 errch 中读取错误信息并最终返回给调用者。而 \u003c-ch 命令阻塞的原因只有发送方未准备好，那么 errch 对应的发送方是谁呢，就在 Start 方法中： Start func (c *Cmd) Start() error { // ... if len(c.goroutine) \u003e 0 { c.errch = make(chan error, len(c.goroutine)) for _, fn := range c.goroutine { go func(fn func() error) { c.errch \u003c- fn() }(fn) } } if c.ctx != nil { c.waitDone = make(chan struct{}) go func() { select { case \u003c-c.ctx.Done(): c.Process.Kill() case \u003c-c.waitDone: } }() } //... 第一部分，通过启动后台 goroutine 执行 c.goroutine 中的方法并将错误写入 c.errch，可以猜测一下应该是这里的产生了阻塞，需要继续追踪 c.goroutine 是哪儿来的。 第二部分则是开启了另一个 goroutine，用来监听 context，在超时之后会 kill 掉子进程。 这也符合现象中看到的，超时后 bash 进程被 kill 掉了。 接下来继续追踪 c.goroutine 是哪儿赋值的,同样是在 Start 方法中，前面提到了 go 通过管道来连接子进程以收集结果，具体逻辑就在这里： func (c *Cmd) Start() error { // ... type F func(*Cmd) (*os.File, error) for _, setupFd := range []F{(*Cmd).stdin, (*Cmd).stdout, (*Cmd).stderr} { fd, err := setupFd(c) if err != nil { c.closeDescriptors(c.closeAfterStart) c.closeDescriptors(c.closeAfterWait) return err } c.childFiles = append(c.childFiles, fd) } } 通过 (*Cmd).stdin, (*Cmd).stdout, (*Cmd).stderr 三个方法来分别处理 stdin、stdout、stderr。 这里先忽略掉 stdin，只看 stdout、stderr 具体 stdout、stderr 方法如下： func (c *Cmd) stdout() (f *os.File, err error) { return c.writerDescriptor(c.Stdout) } func (c *Cmd) stderr() (f *os.File, err error) { // 如果 stderr 和 stdout 一样的就不重复处理了 if c.Stderr != nil \u0026\u0026 interfaceEqual(c.Stderr, c.Stdout) { return c.childFiles[1], nil } return c.writerDescriptor(c.Stderr) } 二者都是调用的 writerDescriptor，不过 stderr 中简单判断了一下避免重复处理。 writerDescriptor 方法如下： func (c *Cmd) writerDescriptor(w io.Writer) (f *os.File, err error) { // case1 if w == nil { f, err = os.OpenFile(os.DevNull, os.O_WRONLY, 0) if err != nil { return } c.closeAfterStart = append(c.closeAfterStart, f) return } // case2 if f, ok := w.(*os.File); ok { return f, nil } // case3 pr, pw, err := os.Pipe() if err != nil { return } c.closeAfterStart = append(c.closeAfterStart, pw) c.closeAfterWait = append(c.closeAfterWait, pr) c.goroutine = append(c.goroutine, func() error { _, err := io.Copy(w, pr) pr.Close() // in case io.Copy stopped due to write error return err }) return pw, nil } 有三个分支逻辑： case1：如果没有指定 stderr 或者 stdout 就直接写入 os.DevNull case2：如果指定的 stderr 或者 stdout 是 *os.File 类型也直接返回，后续直接写入该文件 case3：如果前两种情况都不是就进行最后一种情况，也即是最终的阻塞点。创建管道，子进程写入管道写端点，go 中启动一个 goroutine 从管道读端点读取并写入到指定的 stderr 或者 stdout 中。 这里只分析 case3，首先 io.Copy 方法会一直阻塞到 reader 被关闭才会返回，这也就是为什么这里会产生阻塞。 正常情况下 context 超时后，子进程会被 kill 掉，那么管道的写端点自然会被关闭， io.Copy 则在 copy 完成后正常返回，给 c.errch 中发送一个 nil，Wait 方法则从 c.errch 中读取到 error 就返回了，一切正常😄。 但是在之前的 demo 中除了 bash 这个子进程之外还启动了一个 sleep 子子进程，context 超时后，sleep 进程依旧在运行，并且持有管道的写端点，导致 io.Copy 一直等待，最终产生阻塞。 手动 kill 掉 sleep 进程后，管道的写端点被释放，读端点也被关闭，io.Copy 方法返回，Wait 方法才正常退出。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:2:2","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"解决方案 根据上述分析可知，进入 case3 且产生子子进程就会导致阻塞，那么避免进入第三分支或者不产生子子进程即可。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:3:0","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"使用 *os.File 类型接收输出 指定将 stdout、stderr 输出到文件，使用 *os.File 类型即可进入 case2，从而避免阻塞。 该方式存在两个问题： 需要额外处理输出，比如从文件读取并写入到需要的地方 程序退出后 子子进程被 1 号进程托管会继续运行 demo 如下： func main() { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() cmd := exec.CommandContext(ctx, \"bash\", \"/root/sleep.sh\") combinedOutput, err := ioutil.TempFile(\"\", \"stdouterr\") if err != nil { fmt.Println(err) return } defer func() { _ = os.Remove(combinedOutput.Name()) }() cmd.Stdout = combinedOutput cmd.Stderr = combinedOutput err = cmd.Run() if err != nil { fmt.Println(err) } _, err = combinedOutput.Seek(0, 0) var b bytes.Buffer _, err = io.Copy(\u0026b, combinedOutput) if err != nil { fmt.Println(err) return } err = combinedOutput.Close() if err != nil { fmt.Println(err) return } fmt.Println(\"output:\", b.String()) fmt.Printf(\"ctx.Err : [%v]\\n\", ctx.Err()) fmt.Printf(\"error : [%v]\\n\", err) } ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:3:1","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"避免产生子进程 脚本方式 Shell 脚本的 5 种执行方式： 使用绝对路径执行：/root/sleep.sh 使用相对路径执行：./sleep.sh （需要 x 权限） 使用 sh 或 bash 命令来执行：bash /root/sleep.sh 使用 . (空格)脚本名称来执行：. /root/sleep.sh 使用 source 来执行(一般用于生效配置文件)：source /root/sleep.sh 前三种方式都会在新的 bash 进程中执行，后续两种则会在当前 bash 进程中执行。 感兴趣的可以在终端执行上面 5 条命令试一下，前 3 种都会出现 bash 进程和 sleep 进程，后两种则只会产生 sleep 进程。使用 echo $$ 打印当前 bash 进程 ID 和 sleep 进程的父进程对比即刻发现二者一致。 因为 Go 中没有 shell 环境因此只能用 bash /root/sleep.sh 方式执行，肯定会产生一个新的 bash 进程，该方法无效。 bash -c 方式 bash -c command 方式执行单条命令的时候有相关的优化，是不会产生多个进程的，因此如果将 demo 中的复杂命令或者脚本拆分成多个命令执行也可以实现。 单条命令和多条命令对比具体如下： [root@kc ~]# bash -c \"sleep 1200\" [root@kc ~]# ps -ef|grep sleep root 16449 15583 0 17:24 pts/1 00:00:00 sleep 1200 root 16451 15239 0 17:24 pts/2 00:00:00 grep --color=auto sleep 单条命令只会启动一个 sleep 进程 root@kc ~]# bash -c \"echo hello \u0026\u0026 sleep 1200\" [root@kc ~]# ps -ef|grep sleep root 16452 15583 0 17:24 pts/1 00:00:00 bash -c echo hello \u0026\u0026 sleep 1200 root 16453 16452 0 17:24 pts/1 00:00:00 sleep 1200 root 16455 15239 0 17:24 pts/2 00:00:00 grep --color=auto sleep 多条命令会启动一个 bash 进程和一个 sleep 进程。 原因 单条命令时：首先启动一个 bash 进程 然后发现是一个简单的命令，作为一种优化，它会调用exec然后在不 fork 的情况下执行该命令，然后将子 shell 替换为 sleep 命令。 多条命令时：需要使用子 shell 来处理\u0026\u0026操作符，它需要等待第一个命令终止的 SIGCHLD，然后决定是否需要运行第二个命令，因此不能将子 shell 替换为 sleep 命令，所以会有两个进程。 \u0026\u0026 表示前一条命令执行成功后才执行后续命令。 具体见 shell.c 第 1370 行 因此我们只需要将 demo 中的命令拆分为以下两条命令分两次执行即可避免产生子进程 bash -c 'echo hello' bash -c 'sleep 1200' 不过该方法改动比较大，如果脚本比较复杂基本没法用。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:3:2","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"手动 kill 所有子进程 除此之外还可以手动 kill 掉相关的子子进程，这样程序也可以正常返回。 通过将 cmd 的 Setpgid 设置为 true，从而创建新的进程组 根据 linux kill(2) 定义，指定 pid 为负数时会给这个进程组中的所有进程发送信号 根据以上两个定义我们就可以手动 kill 掉所有的子进程了。 func main() { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() cmd := exec.CommandContext(ctx, \"bash\", sh) cmd.SysProcAttr = \u0026syscall.SysProcAttr{Setpgid: true} go func() { select { case \u003c-ctx.Done(): // cmd.Process.Kill() err := syscall.Kill(-cmd.Process.Pid, syscall.SIGKILL) if err != nil { fmt.Printf(\"kill error : [%v]\\n\", err) } } }() output, err := cmd.CombinedOutput() if err != nil { fmt.Println(err) return } fmt.Println(\"output:\", string(output)) fmt.Printf(\"ctx.Err : [%v]\\n\", ctx.Err()) fmt.Printf(\"error : [%v]\\n\", err) } 该方法相比之下影响比较小，也没有子子进程遗留，比较完美，推荐使用。 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:3:3","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"社区提案 该问题其实很早就存在了，最早可以追溯到这个 2017 年的 Issue #23019，不过为了保持向后兼容，在方案上一直没有达成共识，最新提案见这个 Issue #50436，根据 #53400 中的最新消息，该提案可能会在 Go 1.20 中实现。 大致方案为在 exec.Cmd 中添加一个 Interrupt(os.Signal) 字段，在 context 超时后将这个信号发送给子进程以关闭所有子进程。 // Context is the context that controls the lifetime of the command // (typically the one passed to CommandContext). Context context.Context // If Interrupt is non-nil, Context must also be non-nil and Interrupt will be // sent to the child process when Context is done. // // If the command exits with a success code after the Interrupt signal has // been sent, Wait and similar methods will return Context.Err() // instead of nil. // // If the Interrupt signal is not supported on the current platform // (for example, if it is os.Interrupt on Windows), Start may fail // (and return a non-nil error). Interrupt os.Signal // If WaitDelay is non-zero, the command's I/O pipes will be closed after // WaitDelay has elapsed after either the command's process has exited or // (if Context is non-nil) Context is done, whichever occurs first. // If the command's process is still running after WaitDelay has elapsed, // it will be terminated with os.Kill before the pipes are closed. // // If the command exits with a success code after pipes are closed due to // WaitDelay and no Interrupt signal has been sent, Wait and similar methods // will return ErrWaitDelay instead of nil. // // If WaitDelay is zero (the default), I/O pipes will be read until EOF, // which might not occur until orphaned subprocesses of the command have // also closed their descriptors for the pipes. WaitDelay time.Duration ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:3:4","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Golang"],"content":"小结 现象 使用 os/exec 执行 shell 脚本并设置超时时间，然后到超时时间之后程序并未超时退出，反而一直阻塞。 原因 os/exec 包执行命令时会创建子进程，通过管道连接子进程以收集命令执行结果，goroutine 从管道中读取命令输出，超时后会 kill 掉子进程，从而关闭管道，管道被关闭后 goroutine 则自动退出。 如果存在子子进程，占有管道则会导致 kill 掉子进程后管道依旧未能释放，读取输出的 goroutine 被阻塞，最终导致程序超时后也无法返回。 触发机制 需要满足以下两个条件： 1）cmd.stdout、cmd.stderr 非 nil 且不是 *os.File 类型 不满足该条件则不会进入阻塞路径 2）命令会产生子进程 没有子进程则不会继续占用管道 解决方案 1.使用临时文件接收结果，破坏条件1 只是解决阻塞问题，但是残留后台进程会继续运行 2.拆分复杂命令分别执行，破坏条件2 3.手动监听超时后 kill 掉整个进程组，手动补救 ","date":"2022-07-08","objectID":"/posts/go/exex-cmd-timeout/:4:0","tags":["Golang"],"title":"Go exec 包执行命令超时失效问题分析及解决方案","uri":"/posts/go/exex-cmd-timeout/"},{"categories":["Blog"],"content":"基于 Github Action 自动构建 Hugo 博客","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"本文主要记录了如何配置 Github Action 实现 Hugo 博客自动部署。 GitHub Actions 快速入门 hugo quick-start ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:0:0","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"1. 概述 Hugo 都是静态博客，即最终生成的是静态页面，而所谓部署就是把这些静态文件放到 web 服务器(比如 Nginx、Caddy) 的对应目录就行了。 因此整个 Github Action 只需要做两件事： 1）编译，生成静态文件 2）部署，把静态文件移动到合适的位置 比如放到某个云服务器上 或者放到 Github Pages 然后我们再通过 git push 来触发 Github Action 就可以了。 ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:1:0","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"2. 具体实现 ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:2:0","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"添加 Github Action 需要在仓库根目录下创建 .github/workflow 这个二级目录，然后在 workflow 下以 yml 形式配置 Github Action。 具体可以参考 这个仓库 需要指定 action 触发条件，这里就设置为 push 触发，具体如下： on: push: branches: - main # Set a branch to deploy pull_request: 以上表示在 main分支收到 push 事件时执行该 action。 如果是之前创建的仓库，可能需要改成 master 分支。 另外我们可以直接在 marketplace 找别人配置好的 action 来使用，就更加方便了，以下是本教程用到的 action actions/checkout hugo-setup github-pages-action rsync 我们要做的就是把这些单独的 action 进行组合，以实现自动部署。 ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:2:1","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"发布到 Github Pages 静态博客可以直接用 Github Pages，比较简单，缺点就是国内访问会比较慢，甚至于直接打不开。 action 文件如下 name: GitHub Pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.100.2' # 是否启用 hugo extend extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 整个 Action 一个包含 4 个步骤： 1）拉取代码 2）准备 hugo 环境 3）使用 hugo 编译生成静态文件 4）把生成的静态文件发布到 Github Pages 其他都不需要改，唯一需要注意的是 Hugo 的版本以及是否启用 hugo 扩展。 建议改成和自己当前使用的版本，否则可能会出现兼容性问题。 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.100.2' extended: true ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:2:2","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"发布到云服务器 发布到云服务器和发布到 Github Pages 差不多，只有最后 deploy 这个步骤不一样。 发布到云服务器有很多种实现方式： 1）直接 scp 拷贝到对应目录 2）rsync 同步 这里用的是 rsync 方式，需要在服务器上安装 rsync。这里用的是 centos7，自带了 rsync, 只是没有启动，只需要启动就好，其他系统应该也默认安装了。 systemctl enable rsyncd --now action 文件如下 name: Custom Server on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.100.2' extended: true - name: Build run: hugo --minify - name: Deploy uses: burnett01/rsync-deployments@5.2 with: switches: -avzr --delete path: ./public remote_path: /var/www/html/ # 需要先手动在远程主机创建该目录，否则会执行失败 remote_host: ${{ secrets.DEPLOY_HOST }} # 远程主机 IP remote_port: ${{ secrets.DEPLOY_PORT }} # ssh 端口，默认为 22 remote_user: ${{ secrets.DEPLOY_USER }} # ssh user remote_key: ${{ secrets.DEPLOY_KEY }} # ssh 私钥 为了安全起见，敏感数据都通过 secrets 方式引用，需要在对应仓库中创建这些 secret。 ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:2:3","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Blog"],"content":"3. 测试 随便修改点内容，执行提交 echo hello \u003e tmp.txt git add . git commit -m \"test action\" 然后打开 github action 页面查看，可以看到已经在执行了 点开可以查看执行日志 到此，整个配置就完成了，具体细节可以参考 这个仓库 ","date":"2022-06-11","objectID":"/posts/blog/01-github-action-deploy-hugo/:3:0","tags":["Hugo"],"title":"基于 Github Action 自动构建 Hugo 博客","uri":"/posts/blog/01-github-action-deploy-hugo/"},{"categories":["Kubernetes"],"content":"k8s volume 的实现原理，挂载流程等","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"本文主要记录了 k8s volume 的实现原理,包括远程卷的 Attach、Mount 过程，CRI Mount Volume 实现以及 PV、PVC、StorageClass 持久化存储体系运作流程。 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"1. 背景 容器中文件是在磁盘上临时存放的，容器重启后就会丢失，为了解决这个问题引入了 Volume 的概念。 k8s 中的 volume 和 docker 中的 volume 类似，只不过管理上更加系统化。 k8s 中的 Volume 属于 Pod 内部共享资源存储，生命周期和 Pod 相同，与 Container 无关，即使 Pod 上的容器停止或者重启，Volume 不会受到影响，但是如果 Pod 终止，那么这个 Volume 的生命周期也将结束。 这样的存储无法满足有状态服务的需求，于是推出了 Persistent Volume，故名思义，持久卷是能将数据进行持久化存储的一种资源对象。它是独立于Pod的一种资源，是一种网络存储，它的生命周期和 Pod 无关。云原生的时代，持久卷的种类也包括很多，iSCSI，RBD，NFS，以及CSI, CephFS, OpenSDS, Glusterfs, Cinder 等网络存储。 可以看出，在kubernetes 中支持的持久卷基本上都是网络存储，只是支持的类型不同。 k8s 中支持多种类型的卷(持久卷)，比如： 本地的普通卷 emptyDir hostpath 本地持久化卷 local 远程文件系统或者块存储 nfs rbd 一些特殊卷 configMap secret ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"2. 挂载流程 整个挂载流程需要 kube-controller-manager、 kubelet 以及 cri 共同完成，一个完整的流程大致可分为以下 两个步骤： 1）远程存储挂载至宿主机 1）Attach 由 controller 执行，将远程块存储作为一个远程磁盘挂载到宿主机 2）Mount 由 kubelet 执行，将远程磁盘格式化并挂载到 Volume 对应的宿主机目录 一般是在 /var/lib/kubelet 目录下 2）将宿主机目录挂载至 Pod 中 由 CRI 执行，将宿主机上的目录挂载到 Pod 中 第一步是在准备 volume（宿主机目录），第二步才是真正的挂载操作。 根据 volume 类型的不同，准备工作也不完全一样，比如 NFS 本身就是文件系统就可以省略 Attach，而 hostpath 本身就是挂载到宿主机目录，甚至连 Mount 这一步都不需要，完全不需要准备工作，CRI 直接挂载即可。 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"1. 远程存储挂载至宿主机 只有远程存储才会需要执行该步骤，对应到 k8s 的 api 就是只有使用 PV 的时候才会有这个流程。 该步骤分为两个阶段： 阶段一：Attach，挂载磁盘 阶段二：Mount，格式化磁盘并 mount 至宿主机目录 最终准备好的 volume 都会放在这个目录下： /var/lib/kubelet/pods/\u003cPod的ID\u003e/volumes/kubernetes.io~\u003cVolume类型\u003e/\u003cVolume名字\u003e Attach Attach 阶段由 AttachDetachController + external-attacher 完成。 一般缩写为 ADController，是 VolumeController 的一部分，所以也是 kube-controller-manager 的一部分。 AttachDetachController 不断地检查每一个 Pod 对应的 PV 和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作，为虚拟机挂载远程磁盘。Kubernetes 提供的可用参数是 nodeName，即宿主机的名字。 实际上只是创建了一个 VolumeAttachment 对象。 external-attacher 组件观察到由上一步 AttachDetachController 创建的 VolumeAttachment 对象，如果其 .spec.Attacher 中的 Driver name 指定的是自己同一 Pod 内的 CSI Plugin，则调用 CSI Plugin 的ControllerPublish 接口进行 Volume Attach。 对于 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么该阶段就是调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。 这相当于执行： $ gcloud compute instances attach-disk \u003c虚拟机名字\u003e --disk \u003c远程磁盘名字\u003e 其实就是调用云厂商提供的 API 会虚拟机加一个磁盘。 Mount Mount 阶段由 VolumeManagerReconciler 完成。 由于需要在具体的 node 上执行，因此这是 kubelet 组件的一部分。 kubelet 通过 VolumeManager 启动 reconcile loop，当观察到有新的使用 PersistentVolumeSource 为 CSI 的 PV 的 Pod 调度到本节点上，于是调用 reconcile 函数进行 Attach/Detach/Mount/Unmount 相关逻辑处理。 将磁盘设备格式化并挂载到 Volume 宿主机目录。Kubernetes 提供的可用参数是 dir，即 Volume 的宿主机目录。 对于块存储来说相当于执行以下命令： # 通过 lsblk 命令获取磁盘设备 ID $ sudo lsblk # 格式化成 ext4 格式 $ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/\u003c磁盘设备ID\u003e # 挂载到挂载点 $ sudo mkdir -p /var/lib/kubelet/pods/\u003cPod的ID\u003e/volumes/kubernetes.io~\u003cVolume类型\u003e/\u003cVolume名字\u003e mount /dev/\u003c磁盘设备ID\u003e /var/lib/kubelet/pods/\u003cPod的ID\u003e/volumes/kubernetes.io~\u003cVolume类型\u003e/\u003cVolume名字\u003e 对于 NFS 来说，相当于执行以下命令： 因为已经是文件系统，所以可以省去格式化那一步 $ mount -t nfs \u003cNFS服务器地址\u003e:/ /var/lib/kubelet/pods/\u003cPod的ID\u003e/volumes/kubernetes.io~\u003cVolume类型\u003e/\u003cVolume名字\u003e ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"2. CRI 将 Volume 挂载到 Pod 中 挑选 CRI 中比较熟悉的 docker 来分析，从 docker 挂载卷出发研究 k8s 中的卷挂载流程。 如果想要在容器中挂载宿主机目录的话，就要带上-v参数，以下面这条命令为例： docker run -v /home:/test ... 它的具体的实现过程如下： 创建容器进程并开启 Mount namespace int pid = clone(main_function, stack_size, CLONE_NEWNS | SIGCHLD, NULL); 将宿主机目录挂载到容器进程的目录中来 mount(\"/home\", \"/test\", \"\", MS_BIND, NULL) 此时虽然开启了 mount namespace，只代表主机和容器之间 mount 点隔离开了，容器仍然可以看到主机的文件系统目录。 通过 bind mount 将宿主机目录挂载到容器目录。 调用 pivot_root 或 chroot，改变容器进程的根目录。至此，容器再也看不到宿主机的文件系统目录了。 出去 namespace 和 rootfs 相关的逻辑，挂载卷的核心逻辑其实就是调用 linux 的 bind mount api，将宿主机的目录挂载到容器中的目录上。 具体实现见之前写的一个 简易的 docker ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"3. 小结 volume 挂载流程如下图所示： 1）用户创建 Pod，指定使用某个 PVC 2）AttachDetachController watch 到有 Pod 创建，且 Pod 指定的 PVC 对应的 PV 还没有执行 Attach，那么就执行 Attach 操作，实际为创建一个 VolumeAttachment 对象 3）external-attacher sidecar watch 到有 VolumeAttachment 创建，就调用 CSI Plugin 的ControllerPublish 接口完成 Attach 步骤 4）kubelet 通过 VolumeManager 启动 reconcile loop，当观察到有新的使用 PersistentVolumeSource 为CSI 的 PV 的 Pod 调度到本节点上，于是调用 reconcile 函数进行 Mount/Unmount 相关逻辑处理。 5）volume 准备完成后，调用 CRI 接口创建 Pod，将 volume 通过 Mounts 参数传递过去，最终由 CRI 将 volume 对应的宿主机目录挂载到 Pod 中 先挖个坑， CSI Plugin 相关的后续在分析,先简单贴个图 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:2:3","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"3. PV \u0026 PVC \u0026 StorageClass k8s 中的 PV \u0026 PVC \u0026 StorageClass 等概念就是用于描述这套持久化存储体系的。 PV ：持久化存储数据卷 PVC：PV 使用请求 StorageClass：PV 的创建模板 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"1. 背景 在 PV \u0026 PVC 出现之前，k8s 其实也是支持持久化存储的。 比如要使用一个 hostpath 类型的 volume，pod yaml 只需要这么写： apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name : busybox image: registry.fjhb.cn/busybox imagePullPolicy: IfNotPresent command: - sleep - \"3600\" volumeMounts: - mountPath: /busybox-data name: data volumes: - hostPath: path: /tmp name: data 而如果我们要用 Ceph RDB 类型的 Volume，那么 Pod yaml 文件就可以这样写： apiVersion: v1 kind: Pod metadata: name: rbd spec: containers: - image: kubernetes/pause name: rbd-rw volumeMounts: - name: rbdpd mountPath: /mnt/rbd volumes: - name: rbdpd rbd: monitors: - '10.16.154.78:6789' - '10.16.154.82:6789' - '10.16.154.83:6789' pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring imageformat: \"2\" imagefeatures: \"layering\" 这样越算是用上持久化存储了，但是这样有很大的弊端： 其一，如果不懂得 Ceph RBD 的使用方法，那么这个 Pod 里 Volumes 字段，你十有八九也完全看不懂。 其二，这个 Ceph RBD 对应的存储服务器的地址、用户名、授权文件的位置，也都被轻易地暴露给了全公司的所有开发人员，这是一个典型的信息被“过度暴露”的例子。 k8s 为了降低了用户声明和使用持久化 Volume 的门槛，引入了 PV 和 PVC API 对象。 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"2. 相关概念 PV PV 描述的是持久化存储数据卷。 假设我们在远程存储服务那边创建一块空间，用于作为某个 Pod 的 Volume，比如：NFS 下的某个目录或者某个 Ceph RDB 服务。 PV 对象就是用于代表持久化存储数据卷的。 假设现在我准备了一个 NFS 用来做持久化存储，为了让 k8s 知道这个东西，我需要创建一个 PV 对象: apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" 其中指明了 nfs 的 server 和 path 等信息，有了这部分信息就可以使用这个 NFS 服务了。 由于需要关联到专业的存储知识，一般由运维人员创建。 PVC PVC 描述的是对持久化存储数据卷的需求，作为一个开发人员，可能不懂存储，因此创建 PV 需要的这些字段不知道怎么填。 为了减轻使用负担，k8s 推出 PV 的同时还推出了 PVC，一个 pvc demo 如下： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi 内容非常简单，只需要指定需要的访问模式以及需要的存储空间大小即可，完全不用管专业的存储知识。 使用简单，不需要存储知识，一般由开发人员自己创建。 StorageClass StorageClass 其中的一个作用是关联 PV 和 PVC，只有 StorageClass 相同的 PV 和 PVC 才会被绑定在一起。 另一个作用则是充当 PV 的模板，用于实现动态创建 PV，也就是 k8s 中的 Dynamic Provisioning 机制。 将 PV 和 PVC 分开后使用上确实简单了，但是由于二者分别由不同用户创建，很可能出现创建了 PVC 找不到相匹配的 PV 的情况，毕竟运维人员也不知道需要那些类型的 PV。 一般是运维人员发现问题后再去收到创建出对应的 PV。 为了解决这个问题， k8s 又推出了 StorageClass 以及 Dynamic Provisioning 机制。 根据 PVC 以及 StorageClass 自动创建 PV，极大降低了运维人员的工作量。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-sc annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: archiveOnDelete: \"false\" reclaimPolicy: \"Delete\" provisioner: k8s-sigs.io/nfs-subdir-external-provisioner-nfs-sc 注意：光有 StorageClass 是没法自动创建出 PV 的，还需要一个配套的 provisioner组件才行。 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"3. 运作流程 PV \u0026 PVC \u0026 StorageClass 下的存储体系具体运作流程如下： 1. 创建 StorageClass \u0026 provisioner 首先由运维人员创建对应 StorageClass 并在 K8S 集群中运行配套的 provisioner 组件。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-sc annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: archiveOnDelete: \"false\" reclaimPolicy: \"Delete\" provisioner: k8s-sigs.io/nfs-subdir-external-provisioner-nfs-sc StorageClass 中的 provisioner 指明了要使用的 provisioner，然后以 deployment 方式部署对应的 provisioner： 当然这里还需要 RBAC 权限等配置，比较多先省略了,完整配置见 nfs-subdir-external-provisioner apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner-nfs-sc labels: app: nfs-client-provisioner-nfs-sc namespace: kube-system spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner-nfs-sc template: metadata: labels: app: nfs-client-provisioner-nfs-sc spec: serviceAccountName: nfs-client-provisioner tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - \"\" containers: - name: nfs-client-provisioner image: caas4/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner-nfs-sc - name: NFS_SERVER value: 172.20.151.105 - name: NFS_PATH value: /tmp/nfs/data volumes: - name: nfs-client-root nfs: server: 172.20.151.105 path: /tmp/nfs/data 其中以环境变量的方式指明了 PROVISIONER_NAME 为 k8s-sigs.io/nfs-subdir-external-provisioner-nfs-sc，以及 NFS 的相关参数。 StorageClass 中的 provisioner: k8s-sigs.io/nfs-subdir-external-provisioner-nfs-sc 和这里的 PROVISIONER_NAME 是对应的，因此可以找到对应的 provisioner。 2. 创建 PVC 然后开发人员创建需要的 PVC，比如 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs-sc accessModes: - ReadWriteMany resources: requests: storage: 1Mi 需要注意的一点是 storageClassName 必须和前面创建的 StorageClass 对应。 PVC 创建之后就轮到 PersistentVolumeController 登场了。 PersistentVolumeController 会根据 PVC 寻找对应的 PV 来进行绑定，没有的话自然无法绑定。 这时候我们前面创建的 provisioner 就起作用了，他会 watch pvc 对象，比如这里我们创建了 PVC，provisioner 就会收到相应事件，然后根据 PVC 中的 storageClassName 找到对应 StorageClass，然后根据 StorageClass中的 provisioner 字段找到对应 provisioner，发现是自己处理的，就 调用 CSI Plugin 的接口 CreateVolume 创建出 volume，然后在 k8s 里创建对应的 PV 来指代这个 volume。 CSI Plugin CreateVolume 接口则由具体厂商实现，比如 阿里云实现的 CreateVolume 可能就是在阿里云上创建了一块云盘。 最后 PV 创建之后，PersistentVolumeController 就将二者进行绑定。 3. 创建 Pod PVC 和 PV 绑定之后就可以使用了，创建一个 Pod 来使用这个 PVC： kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: busybox:stable command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim 这里就是通过 claimName 来指定要使用的 PVC。 Pod 创建之后 k8s 就可以根据 claimName 找到对应 PVC，然后 PVC 绑定的时候会把 PV 的名字填到 spec.volumeName 字段上，因此这里又可以找到对应的 PV，然后就进入到第二节中的挂载流程了。 4. 小结 至此，k8s 的这套持久化存储体系运作流程就算是完成了。流程如下图所示： 1）管理员预先创建好 StorageClass 和 Provisioner 2）用户创建 PVC 3）根据 PVC 中的 StorageClass 最终找到 Provisioner 4）Provisioner 创建 PV 5）PersistentVolumeController 绑定 PVC 和 PV 6）创建 Pod 时指定 PVC，随后进入第二节的持久卷挂载流程 ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:3:3","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"4. 参考 persistent-volumes storage-classes CSI Spec ","date":"2022-06-02","objectID":"/posts/kubernetes/09-volume/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(九)---Volume 实现原理","uri":"/posts/kubernetes/09-volume/"},{"categories":["Kubernetes"],"content":"使用 kubeadm 从头搭建一个使用 containerd 作为容器运行时的 Kubernetes 集群","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"本文记录了使用 kubeadm 从头搭建一个使用 containerd 作为容器运行时的 Kubernetes 集群的过程。 本文创建于 2021.3.12，更新于 2022.5.28 推荐使用 KubeClipper 来创建 k8s 集群，一条命令搞定：Kubernetes教程(十一)—使用 KubeClipper 通过一条命令快速创建 k8s 集群 本次实验用到的机器如下： 主机名 系统版本 内核版本 配置 IP 角色 k8s-1 CentOS 7.9 5.18 2C4G 192.168.2.131 master k8s-2 CentOS 7.9 5.18 2C4G 192.168.2.132 worker k8s-3 CentOS 7.9 5.18 2C4G 192.168.2.133 worker Linux Kernel 版本需要 4.x 以上,否则 calico 可能无法正常启动。 软件版本 containerd：1.5.11 libseccomp 2.5.1 k8s：1.23.5 kubeadm kubelet kubectl calico：3.22 各个组件不同版本可能存在不兼容情况，调整组件版本时请注意。 本文主要分为以下几个部分： 1）安装 containerd(所有节点) 2）安装 kubeadm、kubelet、kubectl(所有节点) 3）初始化 master 节点(k8s-1) 4）加入到集群(k8s-2,k8s-3) 5）部署 calico(k8s-1) 6）k8s 简单体验(k8s-1) ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"0. 安装 containerd 在所有节点上执行该步骤 containerd 官方文档 getting-started k8s 官方文档 container-runtimes#containerd ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"1. 安装 安装和配置的先决条件： cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 应用 sysctl 参数而无需重新启动 sudo sysctl --system yum 方式 安装yum-utils包（提供yum-config-manager 实用程序）并设置稳定的存储库。 这部分参考如何安装 Docker：在 CentOS 上安装 Docker 引擎，安装的时候只安装 containerd 即可。 sudo yum install -y yum-utils sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install containerd -y 二进制方式 这里我使用的系统是 CentOS 7.9，首先需要安装 libseccomp 依赖。 先查看系统有没有 libseccomp 软件包 [root@k8s-1 ~]# rpm -qa | grep libseccomp libseccomp-2.3.1-4.el7.x86_64 centos 7.9 默认安装了 2.3 版本，不过太旧了需要升级 # 卸载原来的 [root@k8s-1 ~]# rpm -e libseccomp-2.3.1-4.el7.x86_64 --nodeps #下载高于2.4以上的包 [root@k8s-1 ~]# wget http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm #安装 [root@k8s-1 ~]# rpm -ivh libseccomp-2.5.1-1.el8.x86_64.rpm #查看当前版本 [root@k8s-1 ~]# rpm -qa | grep libseccomp libseccomp-2.5.1-1.el8.x86_64 然后去 containerd 的 release 页面下载对应的压缩包。 有两种压缩包 containerd-1.5.11-linux-amd64.tar.gz 单独的 containerd cri-containerd-cni-1.5.11-linux-amd64.tar.gz containerd + runc 安装了 containerd 也要安装 runc，所以这里直接下载第二个打包好的就行。 wget https://github.com/containerd/containerd/releases/download/v1.5.11/cri-containerd-cni-1.5.11-linux-amd64.tar.gz 可以通过 tar 的 -t 选项直接看到压缩包中包含哪些文件： [root@localhost ~]# tar -tf cri-containerd-cni-1.5.11-linux-amd64.tar.gz etc/ etc/cni/ etc/cni/net.d/ etc/cni/net.d/10-containerd-net.conflist etc/systemd/ etc/systemd/system/ etc/systemd/system/containerd.service etc/crictl.yaml usr/ usr/local/ usr/local/bin/ usr/local/bin/containerd-shim-runc-v2 usr/local/bin/containerd-shim usr/local/bin/crictl usr/local/bin/ctr usr/local/bin/containerd-shim-runc-v1 usr/local/bin/containerd usr/local/bin/ctd-decoder usr/local/bin/critest usr/local/bin/containerd-stress usr/local/sbin/ usr/local/sbin/runc ... 可以看到里面有 containerd 和 runc ，而且目录也是设置好了的，直接解压到各个目录中去，甚至不用手动配置环境变量。 tar -C / -zxvf cri-containerd-cni-1.5.11-linux-amd64.tar.gz ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:1:1","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2. 修改配置 生成默认 containerd 配置文件 sudo mkdir -p /etc/containerd # 生成默认配置文件并写入到 config.toml 中 containerd config default | sudo tee /etc/containerd/config.toml 使用 systemd cgroup 驱动程序 注意：cri 使用的 cgroup 和 kubelet 使用的 cgroup 最好是一致的，如果使用 kubeadm 安装的那么 kubelet 也默认使用 systemd cgroup。 结合 runc 使用 systemd cgroup 驱动，在 /etc/containerd/config.toml 中设置 vim /etc/containerd/config.toml # 把配置文件中的 SystemdCgroup 修改为 true [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true #一键替换 sed 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 用国内源替换 containerd 默认的 sand_box 镜像，编辑 /etc/containerd/config.toml [plugins] ..... [plugins.\"io.containerd.grpc.v1.cri\"] ... sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.5\" #一键替换 # 需要对路径中的/ 进行转移，替换成\\/ sed 's/k8s.gcr.io\\/pause/registry.aliyuncs.com\\/google_containers\\/pause/g' /etc/containerd/config.toml 配置镜像加速器地址 然后再为镜像仓库配置一个加速器，需要在 cri 配置块下面的 registry 配置块下面进行配置 registry.mirrors：（注意缩进） 比较麻烦，只能手动替换了 镜像来源： registry-mirrors [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] # 添加下面两个配置 [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://ekxinbbh.mirror.aliyuncs.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://gcr.k8s.li\"] ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:1:2","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3. 测试 启动 containerd systemctl daemon-reload systemctl enable containerd --now 启动完成后就可以使用 containerd 的本地 CLI 工具 ctr 和了，比如查看版本： ctr version ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:1:3","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"1. k8s 环境准备 在所有节点上执行该步骤 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"关闭交换空间 不关则会出现以下错误：failed to run Kubelet: running with swap on is not supported, please disable swap! sudo swapoff -a sed -ri 's/.*swap.*/#\u0026/' /etc/fstab ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"关闭防火墙 systemctl stop firewalld \u0026\u0026 systemctl disable firewalld systemctl stop NetworkManager \u0026\u0026 systemctl disable NetworkManager ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"禁用 SELinux 将 SELinux 设置为 permissive 模式（相当于将其禁用）， 这是允许容器访问主机文件系统所必需的 sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:3","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"允许 iptables 检查桥接流量 确保 br_netfilter 模块被加载。 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:4","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"配置 hosts 这里需要根据自己环境的节点 hostname 和 ip 来调整 # hostnamectl set-hostname xxx 修改 hostname cat \u003e\u003e /etc/hosts \u003c\u003c EOF 192.168.2.131 k8s-1 192.168.2.132 k8s-2 192.168.2.133 k8s-3 EOF ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:2:5","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2. 安装 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2.1 安装 kubeadm、kubelet 和 kubectl 在所有节点上执行该步骤 配置 yum 源 官网提供的 google 源一般用不了，这里直接换成阿里的源： cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF 然后执行安装 # --disableexcludes 禁掉除了kubernetes之外的别的仓库 # 由于官网未开放同步方式, 替换成阿里源后可能会有索引 gpg 检查失败的情况, 这时请带上`--nogpgcheck`选项安装 # 指定安装 1.23.5 版本 sudo yum install -y kubelet-1.23.5 kubeadm-1.23.5 kubectl-1.23.5 --disableexcludes=kubernetes --nogpgcheck kubelet 设置开机启动 sudo systemctl enable kubelet --now ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2.2 初始化主节点 在 k8s-1 上执行该步骤 生成 kubeadm.yaml 文件 首先导出 kubeadm 配置文件并修改 kubeadm config print init-defaults --kubeconfig ClusterConfiguration \u003e kubeadm.yml 然后对配置文件做以下修改： nodeRegistration.criSocket：1.23.5 版本默认还是用的 docker，由于我们用的是 containerd，所以需要改一下 nodeRegistration.name：节点名，改成主节点的主机名（即 k8s-1） localAPIEndpoint.advertiseAddress：这个就是 apiserver 的地址，需要修改为主节点的 IP imageRepository：镜像仓库，默认是国外的地址，需要替换成国内源 kubernetesVersion：调整版本号和之前安装的 kubeadm 一致 networking.podSubnet：新增子网信息，固定为 192.168.0.0/16，主要方便后续安装 calico 具体如下： vim kubeadm.yml 修改后 yaml 如下： apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: # 修改为主节点IP地址 advertiseAddress: 192.168.2.131 bindPort: 6443 nodeRegistration: # 修改为 containerd criSocket: /run/containerd/containerd.sock imagePullPolicy: IfNotPresent # 节点名改成主节点的主机名 name: k8s-1 taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd # 换成国内的源 imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration # 修改版本号 必须对应 kubernetesVersion: 1.24.1 networking: # 新增该配置 固定为 192.168.0.0/16，用于后续 Calico网络插件 podSubnet: 192.168.0.0/16 dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 拉取镜像 查看所需镜像列表 [root@k8s-1 ~]# kubeadm config images list --config kubeadm.yml registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.5 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.5 registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.5 registry.aliyuncs.com/google_containers/kube-proxy:v1.23.5 registry.aliyuncs.com/google_containers/pause:3.6 registry.aliyuncs.com/google_containers/etcd:3.5.1-0 registry.aliyuncs.com/google_containers/coredns:v1.8.6 先手动拉取镜像 kubeadm config images pull --config kubeadm.yml 有时候发现一直拉不下来，也没有报错，就一直搁这阻塞着，可以通过以下命令测试 # ctr --debug images pull {image} ctr --debug images pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.5 最后发现直接用 ctr images pull 可以拉下来，那就手动拉吧，脚本如下： for i in `kubeadm config images list --config kubeadm.yml`;do ctr images pull $i;done 执行初始化 镜像拉取下来后就可以开始安装了 执行以下命令初始化主节点 # --config=kubeadm.yml 指定配置文件 # --upload-certs 上传证书，可以在后续执行加入节点时自动分发证书文件 # tee kubeadm-init.log 将日志保存到文件 kubeadm init --config=kubeadm.yml --upload-certs | tee kubeadm-init.log 输出如下： # 出现这个就说明安装成功了 Your Kubernetes control-plane has initialized successfully! # 执行下面的命令配置 kubeconfig To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf # 配置 pod 网络的命令 You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ # node 节点加入集群需要执行如下指令 Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.2.131:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:d53020265c2bae4f691258966b3d35f99a9cc2dc530514888d85e916b2844525 按照提示配置 kubeconfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 配置好后查看一下 node 状态 [root@k8s-1 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-1 NotReady control-plane 109s v1.23.5 状态为 NotReady，因为此时还没有安装网络插件。 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2.3 Node节点加入集群 在 k8s-2 和 k8s-3 上执行该步骤，将节点加入到集群中。 将 worker 加入到集群中很简单，只需要在对应节点上安装 kubeadm，kubectl，kubelet 三个工具，然后使用 kubeadm join 命令加入即可。 先在 k8s-2 节点执行 # 这就是前面安装Master节点时日志中的提示 kubeadm join 192.168.2.131:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:d53020265c2bae4f691258966b3d35f99a9cc2dc530514888d85e916b2844525 输出如下： This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 在 master 节点查询 [root@k8s-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-1 NotReady control-plane 3m46s v1.23.5 k8s-2 NotReady \u003cnone\u003e 20s v1.23.5 然后把 k8s-3 也加进来 [root@k8s-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-1 NotReady control-plane 3m46s v1.23.5 k8s-2 NotReady \u003cnone\u003e 20s v1.23.5 k8s-3 NotReady \u003cnone\u003e 17s v1.23.5 到此基本安装完成，后续就是部署 calico 了。 此时由于没有安装网络插件，所有节点都还处于 NotReady 状态。 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:3:3","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"2.4 FAQ 1）kubelet 报错找不到节点 Failed while requesting a signed certificate from the master: cannot create certificate signing request: Unauthorized \"Error getting node\" err=\"node \\\"k8s-master\\\" not found\" 因为第一次安装的时候生成了证书，但是第一次安装失败了，第二次安装 kubeadm 又生成了新证书，导致二者证书对不上，于是出现了 Unauthorized 的错误，然后没有把 node 信息注册到 api server，然后就出现了第二个错误，node not found。 每次安装失败后，都需要执行 kubeadm reset 重置环境。 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:3:4","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3. 安装 Calico 具体参考：install calico ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3.1 下载配置文件并拉取镜像 所有节点都需要执行该步骤 第一步获取官方给的 yaml 文件 curl https://projectcalico.docs.tigera.io/archive/v3.22/manifests/calico.yaml -O 可能是网络问题，导致 calico 相关镜像一直拉取超时，最终 pod 无法启动，所以建议提前手动拉取镜像。 手动拉取也很慢，不过最终还是会成功，不过直接超时报错。 查看一共需要哪些镜像 [root@k8s-1 ~]# cat calico.yaml |grep docker.io|awk {'print $2'} docker.io/calico/cni:v3.23.1 docker.io/calico/cni:v3.23.1 docker.io/calico/node:v3.23.1 docker.io/calico/kube-controllers:v3.23.1 手动拉取 for i in `cat calico.yaml |grep docker.io|awk {'print $2'}`;do ctr images pull $i;done 最后查看一下，确定是否拉取下来了 [root@k8s-2 ~]# ctr images ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/calico/cni:v3.23.1 application/vnd.docker.distribution.manifest.list.v2+json sha256:26802bb7714fda18b93765e908f2d48b0230fd1c620789ba2502549afcde4338 105.4 MiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le - docker.io/calico/kube-controllers:v3.23.1 application/vnd.docker.distribution.manifest.list.v2+json sha256:e8b2af28f2c283a38b4d80436e2d2a25e70f2820d97d1a8684609d42c3973afb 53.8 MiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le - docker.io/calico/node:v3.23.1 application/vnd.docker.distribution.manifest.list.v2+json sha256:d2c1613ef26c9ad43af40527691db1f3ad640291d5e4655ae27f1dd9222cc380 73.0 MiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le - ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:4:1","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3.2 配置网卡名 在 k8s-1 上执行该步骤 calico 默认会找 eth0网卡，如果当前机器网卡不是这个名字，可能会无法启动，需要手动配置以下。 [root@k8s-1 ~]# ip a 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 我这里网卡名是 ens33，不符合默认条件，需要修改 calico.yaml 手动指定一下。 vi calico.yaml 然后直接搜索 CLUSTER_TYPE，找到下面这段 - name: CLUSTER_TYPE value: \"k8s,bgp\" 然后添加一个和 CLUSTER_TYPE 同级的 **IP_AUTODETECTION_METHOD **字段，具体如下： # value 就是指定你的网卡名字，我这里网卡是 ens33，然后直接配置的通配符 ens.* - name: IP_AUTODETECTION_METHOD value: \"interface=ens.*\" ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:4:2","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3.3 部署 在 k8s-1上执行该步骤 kubectl apply -f calico.yaml 如果不错意外的话等一会 calico 就安装好了，可以通过以下命令查看： [root@k8s-1 ~]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6c75955484-hhvh6 1/1 Running 0 7m37s kube-system calico-node-5xjqd 1/1 Running 0 7m37s kube-system calico-node-6lnd6 1/1 Running 0 7m37s kube-system calico-node-vkgfr 1/1 Running 0 7m37s kube-system coredns-6d8c4cb4d-8gxsf 1/1 Running 0 20m kube-system coredns-6d8c4cb4d-m596j 1/1 Running 0 20m kube-system etcd-k8s-1 1/1 Running 0 20m kube-system kube-apiserver-k8s-1 1/1 Running 0 20m kube-system kube-controller-manager-k8s-1 1/1 Running 1 (6m16s ago) 20m kube-system kube-proxy-5qj6j 1/1 Running 0 20m kube-system kube-proxy-rhwb7 1/1 Running 0 20m kube-system kube-proxy-xzswm 1/1 Running 0 20m kube-system kube-scheduler-k8s-1 1/1 Running 1 (5m56s ago) 20m calico 开头的以及 coredns 都跑起来就算完成。 kubectl get pod ${POD_NAME} -n ${NAMESPACE} -o yaml | kubectl replace --force -f - kubectl get pod calico-node-68fnx -n kube-system -o yaml | kubectl replace --force -f - kubectl get pod calico-node-5d6zb -n kube-system -o yaml | kubectl replace --force -f - kubectl get pod calico-kube-controllers-56cdb7c587-blc9l -n kube-system -o yaml | kubectl replace --force -f - ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:4:3","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"3.4 FAQ calico controller 无法启动，报错信息如下： client.go 272: Error getting cluster information config ClusterInformation=\"default\" error=Get \"https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\": context deadline exceeded 查看对应 pod 日志发现有一个错误，提示内核版本过低，需要 4.x 版本才行。于是更新内核版本只会就可以了 写本文时安装的是 5.18 版本内核，所有应该不会出现这个问题。 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:4:4","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"4. 检查集群状态 在 k8s-1 上执行该步骤 检查各组件运行状态 [root@k8s-1 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} 查看集群信息 [root@k8s-1 ~]# kubectl cluster-info Kubernetes control plane is running at https://192.168.2.131:6443 CoreDNS is running at https://192.168.2.131:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 查看节点状态 [root@k8s-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-1 Ready control-plane,master 22m v1.23.5 k8s-2 Ready \u003cnone\u003e 21m v1.23.5 k8s-3 Ready \u003cnone\u003e 21m v1.23.5 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"5. 运行第一个容器实例 在 k8s-1 上执行该步骤 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:6:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"5.1 创建Deployment nginx-deployment.yaml 文件内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: # 创建2个nginx容器 replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 创建实例 [root@k8s-1 ~]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created 查看 pod $ kubectl get pods # 输出如下，需要等待一小段时间，STATUS 为 Running 即为运行成功 [root@k8s-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-79fccc485-7czxp 0/1 ContainerCreating 0 37s nginx-deployment-79fccc485-hp565 0/1 ContainerCreating 0 37s 查看 deployment [root@k8s-1 ~]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2/2 2 2 50s ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:6:1","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"5.2 创建 Service 创建一个 service [root@k8s-1 ~]# kubectl expose deployment nginx-deployment --port=80 --type=LoadBalancer --name=nginx-svc service/nginx-deployment exposed 也可以通过 配置文件方式创建，nginx-vc.yaml 文件内容如下： --- apiVersion: v1 kind: Service metadata: name: nginx-svc labels: name: nginx-svc spec: type: LoadBalancer ports: - port: 80 targetPort: 80 protocol: TCP selector: app: nginx kubectl apply -f nginx-svc.yaml 查看 service [root@k8s-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 25m # 由此可见，Nginx 服务已成功发布并将 80 端口映射为 30842 nginx-deployment LoadBalancer 10.102.137.171 \u003cpending\u003e 80:30842/TCP 40s 查看 service 详情 [root@k8s-1 ~]# kubectl describe service nginx-deployment Name: nginx-deployment Namespace: default Labels: app=nginx Annotations: \u003cnone\u003e Selector: app=nginx Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.102.137.171 IPs: 10.102.137.171 Port: \u003cunset\u003e 80/TCP TargetPort: 80/TCP NodePort: \u003cunset\u003e 30842/TCP Endpoints: 192.168.13.65:80,192.168.200.193:80 Session Affinity: None External Traffic Policy: Cluster Events: \u003cnone\u003e ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:6:2","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"5.3 验证 通过浏览器访问任意服务器 # 端口号为第五步中的端口号 http://192.168.2.131:30842/ 此时 Kubernetes 会以负载均衡的方式访问部署的 Nginx 服务，能够正常看到 Nginx 的欢迎页即表示成功。容器实际部署在其它 Node 节点上，通过访问 Node 节点的 IP:Port 也是可以的。 ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:6:3","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"5.4 重置环境 删除 deployment [root@k8s-1 ~]# kubectl delete deployment nginx-deployment deployment.apps \"nginx-deployment\" deleted 删除 services deployment 移除了 但是 services 中还存在，所以也需要一并删除。 [root@k8s-1 ~]# kubectl delete service nginx-svc service \"nginx-deployment\" deleted 至此，实验完成，感谢阅读~ ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:6:4","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Kubernetes"],"content":"6. 相关文档 install-kubeadm create-cluster-kubeadm Install Calico containerd#getting-started ","date":"2022-05-28","objectID":"/posts/kubernetes/01-install/:7:0","tags":["Kubernetes"],"title":"Kubernetes教程(一)---使用 kubeadm 创建 k8s 集群(containerd)","uri":"/posts/kubernetes/01-install/"},{"categories":["Docker"],"content":"通过 veth pair、bridge、iptables 等技术手动实现docker 桥接网络模型","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"本文主要通过 veth pair、bridge、iptables 等技术手动实现 docker 桥接网络模型，以加深对 docker 网络的理解。 跟着《自己动手写 docker》从零开始实现一个简易版的 docker，主要用于加深对 docker 的理解。 源码及相关教程见 Github。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:0:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"1. 概述 Docker有多种网络模型。对于单机上运行的多个容器，可以使用缺省的 bridge 网络驱动。 我们按照下图创建网络拓扑，让容器之间网络互通，从容器内部可以访问外部资源，同时，容器内可以暴露服务让外部访问。 桥接网络的一个拓扑结构如下： 上述网络拓扑实现了：让容器之间网络互通，从容器内部可以访问外部资源，同时，容器内可以暴露服务让外部访问。 根据网络拓扑图可以看到，容器内的数据通过 veth pair 设备传递到宿主机上的网桥上，最终通过宿主机的 eth0 网卡发送出去（或者再通过 veth pair 进入到另外的容器），而接收数据的流程则恰好相反。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:1:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"2. 预备知识 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:2:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"veth pair 相关笔记： veth-pair Veth是成对出现的两张虚拟网卡，从一端发送的数据包，总会在另一端接收到。利用Veth的特性，我们可以将一端的虚拟网卡\"放入\"容器内，另一端接入虚拟交换机。这样，接入同一个虚拟交换机的容器之间就实现了网络互通。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:2:1","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"bridge 相关笔记：Linux bridge 我们可以认为Linux bridge就是虚拟交换机，连接在同一个bridge上的容器组成局域网，不同的bridge之间网络是隔离的。 docker network create [NETWORK NAME]实际上就是创建出虚拟交换机。 交换机是工作在数据链路层的网络设备，它转发的是二层网络包。最简单的转发策略是将到达交换机输入端口的报文，广播到所有的输出端口。当然更好的策略是在转发过程中进行学习，记录交换机端口和MAC地址的映射关系，这样在下次转发时就能够根据报文中的MAC地址，发送到对应的输出端口。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:2:2","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"NAT 相关笔记：iptables NAT（Network Address Translation），是指网络地址转换。 因为容器中的IP和宿主机的IP是不一样的，为了保证发出去的数据包能正常回来，需要对IP层的源IP/目的IP进行转换。 SNAT：源地址转换 DNAT：目的地址转换 SNAT 比如上图中的 eth0 ip 是 183.69.215.18，而容器 dockerA 的 IP 却是 172.187.0.2。 因此容器中发出来的数据包，源IP肯定是 172.187.0.2，如果就这样不处理直接发出去，那么接收方处理后发回来的响应数据包的 目的IP 自然就会填成 172.187.0.2，那么我们肯定接收不到这个响应了。 DNAT 如果发出去做了 SNAT，源IP改成了宿主机的 183.69.215.18，那么回来的响应数据包目的IP自然就是183.69.215.18，我们可以成功收到这个响应，但是如果直接把源IP是183.69.215.18的数据包发到容器里面去，由于容器IP是172.187.0.2,那肯定不会处理这个包，所以宿主机收到响应包只会需要进行DNAT，将目的IP地址改成容器中的172.187.0.2。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:2:3","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"3. 演示 实验环境 Ubuntu 20.04 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:3:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"环境准备 首先需要创建对应的容器，veth pair 设备以及 bridge 设备 并分配对应 IP。 创建“容器” 从前面的背景知识了解到，容器的本质是 Namespace + Cgroups + rootfs。因此本实验我们可以仅仅创建出Namespace网络隔离环境来模拟容器行为： $ sudo ip netns add ns1 $ sudo ip netns add ns2 $ sudo ip netns show ns2 ns1 创建Veth pairs $ sudo ip link add veth0 type veth peer name veth1 $ sudo ip link add veth2 type veth peer name veth3 查看以下 $ ip addr .... 7: veth1@veth0: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 96:c5:68:6d:fd:42 brd ff:ff:ff:ff:ff:ff 8: veth0@veth1: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2e:93:7e:33:b0:ed brd ff:ff:ff:ff:ff:ff 9: veth3@veth2: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 36:15:80:fa:3c:8b brd ff:ff:ff:ff:ff:ff 10: veth2@veth3: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether e2:31:15:64:bd:39 brd ff:ff:ff:ff:ff:ff 将Veth的一端放入“容器” 将 veth 的一端移动到对应的 Namespace 就相当于把这张网卡加入到’容器‘里了。 $ sudo ip link set veth0 netns ns1 $ sudo ip link set veth2 netns ns2 查看宿主机上的网卡 $ ip addr 7: veth1@if8: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 96:c5:68:6d:fd:42 brd ff:ff:ff:ff:ff:ff link-netns ns1 9: veth3@if10: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 36:15:80:fa:3c:8b brd ff:ff:ff:ff:ff:ff link-netns ns2 发现少了两个，然后查看以下对应Namespace里的网卡 $ sudo ip netns exec ns1 ip addr 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 3: sit0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 8: veth0@if7: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2e:93:7e:33:b0:ed brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ sudo ip netns exec ns2 ip addr 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 3: sit0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 10: veth2@if9: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether e2:31:15:64:bd:39 brd ff:ff:ff:ff:ff:ff link-netnsid 0 可以看到，veth0和veth2确实已经放到“容器”里去了。 创建bridge 一般使用brctl进行管理，不是自带的工具，需要先安装一下： $ sudo apt-get install bridge-utils 创建bridge br0： $ sudo brctl addbr br0 将Veth的另一端接入bridge $ sudo brctl addif br0 veth1 $ sudo brctl addif br0 veth3 查看接入效果： $ sudo brctl show bridge name bridge id STP enabled interfaces br0 8000.361580fa3c8b no veth1 veth3 可以看到，两个网卡veth1和veth3已经“插”在bridge上。 分配IP并启动 为bridge分配IP地址，激活上线 $ sudo ip addr add 172.18.0.1/24 dev br0 $ sudo ip link set br0 up 为\"容器“内的网卡分配IP地址，并激活上线 docker0容器： $ sudo ip netns exec ns1 ip addr add 172.18.0.2/24 dev veth0 $ sudo ip netns exec ns1 ip link set veth0 up docker1容器： $ sudo ip netns exec ns2 ip addr add 172.18.0.3/24 dev veth2 $ sudo ip netns exec ns2 ip link set veth2 up Veth另一端的网卡激活上线 $ sudo ip link set veth1 up $ sudo ip link set veth3 up ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:3:1","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"测试 容器互通 测试从容器docker0 ping 容器docker1，测试之前先用 tcpdump 抓包，等会好分析： $ sudo tcpdump -i br0 -n 在新窗口执行 ping 命令： $ sudo ip netns exec ns1 ping -c 3 172.18.0.3 br0上的抓包数据如下： tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes 12:35:18.285705 ARP, Request who-has 172.18.0.3 tell 172.18.0.2, length 28 12:35:18.285903 ARP, Reply 172.18.0.3 is-at e2:31:15:64:bd:39, length 28 12:35:18.285908 IP 172.18.0.2 \u003e 172.18.0.3: ICMP echo request, id 13829, seq 1, length 64 12:35:18.286034 IP 172.18.0.3 \u003e 172.18.0.2: ICMP echo reply, id 13829, seq 1, length 64 12:35:19.309392 IP 172.18.0.2 \u003e 172.18.0.3: ICMP echo request, id 13829, seq 2, length 64 12:35:19.309589 IP 172.18.0.3 \u003e 172.18.0.2: ICMP echo reply, id 13829, seq 2, length 64 12:35:20.349350 IP 172.18.0.2 \u003e 172.18.0.3: ICMP echo request, id 13829, seq 3, length 64 12:35:20.349393 IP 172.18.0.3 \u003e 172.18.0.2: ICMP echo reply, id 13829, seq 3, length 64 12:35:23.309404 ARP, Request who-has 172.18.0.2 tell 172.18.0.3, length 28 12:35:23.309517 ARP, Reply 172.18.0.2 is-at 2e:93:7e:33:b0:ed, length 28 可以看到，先是172.18.0.2发起的ARP请求，询问172.18.0.3的MAC地址，然后是ICMP的请求和响应，最后是172.18.0.3的ARP请求。因为接在同一个bridge br0上，所以是二层互通的局域网。 同样，从容器docker1 ping 容器docker0也是通的： sudo ip netns exec docker1 ping -c 3 172.18.0.2 宿主机访问容器 在“容器”docker0内启动服务，监听80端口： $ sudo ip netns exec ns1 nc -lp 80 在宿主机上执行telnet，可以连接到docker0的80端口： $ telnet 172.18.0.2 80 Trying 172.18.0.2... Connected to 172.18.0.2. Escape character is '^]'. 可以联通。 容器访问外网 这部分稍微复杂一些，需要配置 NAT 规则。 1）配置容器内路由 需要配置容器内的路由，这样才能把网络包从容器内转发出来。具体就是：将bridge设置为“容器”的缺省网关。让非172.18.0.0/24网段的数据包都路由给bridge，这样数据就从“容器”跑到宿主机上来了。 $ sudo ip netns exec ns1 route add default gw 172.18.0.1 veth0 $ sudo ip netns exec ns2 route add default gw 172.18.0.1 veth2 查看“容器”中的路由规则 $ sudo ip netns exec ns1 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.18.0.1 0.0.0.0 UG 0 0 0 veth0 172.18.0.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 可以看到，非 172.18.0.0 网段的数据都会走默认规则，也就是发送给网关 172.18.0.1。 2）宿主机开启转发功能并配置转发规则 在宿主机上配置内核参数，允许IP forwarding，这样才能把网络包转发出去。 $ sudo sysctl net.ipv4.conf.all.forwarding=1 还有就是要配置 iptables FORWARD规则 首先确认iptables FORWARD的缺省策略： $ sudo iptables -t filter -L FORWARD Chain FORWARD (policy ACCEPT) target prot opt source destination 一般都是 ACCEPT，如果如果缺省策略是DROP，需要设置为ACCEPT： sudo iptables -t filter -P FORWARD ACCEPT 3）宿主机配置 SNAT 规则 $ sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/24 ! -o br0 -j MASQUERADE 上面的命令的含义是：在nat表的POSTROUTING链增加规则，当数据包的源地址为172.18.0.0/24网段，出口设备不是br0时，就执行MASQUERADE动作。 MASQUERADE也是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换，而SNAT动作必须在命令中指定固定的IP地址。 测试能否访问： $ sudo ip netns exec ns1 ping -c 3 www.baidu.com PING www.a.shifen.com (14.215.177.38) 56(84) bytes of data. 64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=53 time=35.9 ms 64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=2 ttl=53 time=35.6 ms 64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=3 ttl=53 time=36.2 ms 外部访问容器 外部访问容器需要进行 DNAT，把目的IP地址从宿主机地址转换成容器地址。 $ sudo iptables -t nat -A PREROUTING ! -i br0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.18.0.2:80 上面命令的含义是：在nat表的PREROUTING链增加规则，当输入设备不是br0，目的端口为80时，做目的地址转换，将宿主机IP替换为容器IP。 测试一下 在“容器”docker0内启动服务： $ sudo ip netns exec ns1 nc -lp 80 在和宿主机同一个局域网的远程主机访问宿主机IP:80 $ telnet 192.168.2.110 80 确认可以访问到容器内启动的服务。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:3:2","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"环境恢复 删除虚拟网络设备 $ sudo ip link set br0 down $ sudo brctl delbr br0 $ sudo ip link del veth1 $ sudo ip link del veth3 iptablers和Namesapce的配置在机器重启后被清除。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:3:3","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"4. 小结 本文主要通过 Linux 提供的各种虚拟设备以及 iptables 模拟出了 docker bridge网络模型，并测试了几种场景的网络互通。实际上docker network 就是使用了veth、Linux bridge、iptables等技术，帮我们创建和维护网络。 具体分析一下： 首先 docker 就是一个进程，主要利用 Linux Namespace 进行隔离。 为了跨 Namespace 通信，就用到了 Veth pair。 然后过个容器都使用 Veth pair 联通的话，不好管理，所以加入了 Linux Bridge，所有 veth 都直接和 bridge 连接，这样就好管理多了。 然后容器和外部网络要进行通信，于是又要用到 iptables 的 NAT 规则进行地址转换。 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:4:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"5. 参考 iptables 笔记 veth-pair 笔记 Docker bridge networks Docker单机网络模型动手实验 ","date":"2022-03-26","objectID":"/posts/docker/10-bridge-network/:5:0","tags":["Docker"],"title":"Docker教程(十)---Docker 单机(桥接)网络实现","uri":"/posts/docker/10-bridge-network/"},{"categories":["Docker"],"content":"Union File System中的 overlayfs 演示，以及docker 通过 overlayfs构建 rootfs流程分析","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"本文主要介绍了 Docker 的另一个核心技术：Union File System。主要包括对 overlayfs 的演示，以及分析 docker 是如何借助 ufs 实现容器 rootfs 的。 跟着《自己动手写 docker》从零开始实现了一个简易版的 docker，主要用于加深对 docker 的理解。 源码及相关教程见 Github。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:0:0","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"1. 概述 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:1:0","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"Union File System Union File System ，简称 UnionFS 是一种为 Linux FreeBSD NetBSD 操作系统设计的，把其他文件系统联合到一个联合挂载点的文件系统服务。 它使用 branch 不同文件系统的文件和目录“透明地”覆盖，形成 个单一一致的文件系统。这些branches或者是read-only或者是read-write的，所以当对这个虚拟后的联合文件系统进行写操作的时候，系统是真正写到了一个新的文件中。看起来这个虚拟后的联合文件系统是可以对任何文件进行操作的，但是其实它并没有改变原来的文件，这是因为unionfs用到了一个重要的资管管理技术叫写时复制。 写时复制（copy-on-write，下文简称CoW），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。它的思想是，如果一个资源是重复的，但没有任何修改，这时候并不需要立即创建一个新的资源；这个资源可以被新旧实例共享。创建新资源发生在第一次写操作，也就是对资源进行修改的时候。通过这种资源共享的方式，可以显著地减少未修改资源复制带来的消耗，但是也会在进行资源修改的时候增减小部分的开销。 举个例子 假设我们存在2个目录X,Y，里面分别有A，B文件，那么UFS的作用就是将这两个目录合并，并且重新挂载的Z上,这样在Z目录上就可以同时看到A和B文件。这就是联合文件系统，目的就是将多个文件联合在一起成为一个统一的视图。 然后我们在Z目录中删除B文件，同时，在A文件中增加一些内容，如添加Hello字符串。此时可以发现，X内的A文件新增了Hello,并且新增了一条B被删除的记录，但是Y中的B并没有任何变化。这是UFS的一个重要特性。在所有的联合起来的目录中，只有第一个目录是有写的权限，即我们不管如何的去对Z进行修改操作，都只能对第一个联合进来的X修改，对Y是没有权限修改的。 但是如果我们在Z中对Y中的文件进行了修改，它虽然没有权限去修改Y目录中的文件，但是它会在第一层目录添加一个记录来记录更改内容。 Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件： $ tree . ├── A │ ├── a │ └── x └── B ├── b └── x 然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上： $ mkdir C $ mount -t aufs -o dirs=./A:./B none ./C 这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起： $ tree ./C ./C ├── a ├── b └── x 可以看到，在这个合并后的目录 C 里，有 a、b、x 三个文件，并且 x 文件只有一份。这，就是“合并”的含义。此外，如果你在目录 C 里对 a、b、x 文件做修改，这些修改也会在对应的目录 A、B 中生效。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:1:1","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"常见实现 AUFS AuFS 的全称是 Another UnionFS，后改名为 Alternative UnionFS，再后来干脆改名叫作 Advance UnionFS。AUFS完全重写了早期的UnionFS 1.x，其主要目的是为了可靠性和性能，并且引入了一些新的功能，比如可写分支的负载均衡。AUFS的一些实现已经被纳入UnionFS 2.x版本。 AUFS 只是 Docker 使用的存储驱动的一种，除了 AUFS 之外，Docker 还支持了不同的存储驱动，包括 aufs、devicemapper、overlay2、zfs 和 vfs 等等，在最新的 Docker 中，overlay2 取代了 aufs 成为了推荐的存储驱动，但是在没有 overlay2 驱动的机器上仍然会使用 aufs 作为 Docker 的默认驱动。 overlayfs Overlayfs 是一种类似 aufs 的一种堆叠文件系统，于 2014 年正式合入 Linux-3.18 主线内核，目前其功能已经基本稳定（虽然还存在一些特性尚未实现）且被逐渐推广，特别在容器技术中更是势头难挡。 Overlayfs 是一种堆叠文件系统，它依赖并建立在其它的文件系统之上（例如 ext4fs 和 xfs 等等），并不直接参与磁盘空间结构的划分，仅仅将原来底层文件系统中不同的目录进行“合并”，然后向用户呈现。 简单的总结为以下3点： （1）上下层同名目录合并； （2）上下层同名文件覆盖； （3）lower dir文件写时拷贝。 这三点对用户都是不感知的。 假设我们有 dir1 和 dir2 两个目录： dir1 dir2 / / a a b c 然后我们可以把 dir1 和 dir2 挂载到 dir3上，就像这样： dir3 / a b c 需要注意的是：在 overlay 中 dir1 和 dir2 是有上下关系的。lower 和 upper 目录不是完全一致，有一些区别，具体见下一节。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:1:2","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"2. overlayfs 演示 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:2:0","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"环境准备 具体演示如下： 创建一个如下结构的目录： . ├── lower │ ├── a │ └── c ├── merged ├── upper │ ├── a │ └── b └── work 具体命令如下： $ mkdir ./{merged,work,upper,lower} $ touch ./upper/{a,b} $ touch ./lower/{a,c} 然后进行 mount 操作： # -t overlay 表示文件系统为 overlay # -o lowerdir=./lower,upperdir=./upper,workdir=./work 指定 lowerdir、upperdir以及 workdir这3个目录。 # 其中 lowerdir 是自读的，upperdir是可读写的， $ sudo mount \\ -t overlay \\ overlay \\ -o lowerdir=./lower,upperdir=./upper,workdir=./work \\ ./merged 此时目录结构如下： . ├── lower │ ├── a │ └── c ├── merged │ ├── a │ ├── b │ └── c ├── upper │ ├── a │ └── b └── work └── work 可以看到，merged 目录已经可以同时看到 lower和upper中的文件了，而由于文件a同时存在于lower和upper中，因此被覆盖了，只显示了一个a。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:2:1","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"修改文件 虽然 lower 和 upper 中的文件都出现在了 merged 目录，但是二者还是有区别的。 lower 为底层目录，只提供数据，不能写。 upper 为上层目录，是可读写的。 测试： # 分别对 merged 中的文件b和c写入数据 # 其中文件 c 来自 lower，b来自 upper $ echo \"will-persist\" \u003e ./merged/b $ echo \"wont-persist\" \u003e ./merged/c 修改后从 merged 这个视图进行查看： $ cat ./merged/b will-persist $ cat ./merged/c wont-persist 可以发现，好像两个文件都被更新了，难道上面的结论是错的？ 再从 upper 和 lower 视角进行查看： $ cat ./upper/b will-persist $ cat ./lower/c (empty) 可以发现 lower 中的文件 c 确实没有被改变。 那么 merged 中查看的时候，文件 c 为什么有数据呢？ 由于 lower 是不可写的，因此采用了 CoW 技术，在对 c 进行修改时，复制了一份数据到 overlay 的 upperdir，即这里的 upper 目录，进入 upper 目录查看是否存在 c 文件： [root@iZ2zefmrr626i66omb40ryZ upper]$ ll total 8 -rw-r--r-- 1 root root 0 Jan 18 18:50 a -rw-r--r-- 1 root root 13 Jan 18 19:10 b -rw-r--r-- 1 root root 13 Jan 18 19:10 c [root@iZ2zefmrr626i66omb40ryZ upper]$ cat c wont-persist 从 lower copy 到 upper，也叫做 copy_up ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:2:2","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"删除文件 首先往 lower 中增加文件 f [root@iZ2zefmrr626i66omb40ryZ lower]$ echo fff \u003e\u003e f [root@iZ2zefmrr626i66omb40ryZ lower]$ ls ../merged/ f 果然 lower 中添加后，merged 中也能直接看到了，然后再 merged 中去删除文件 f： [root@iZ2zefmrr626i66omb40ryZ lower]$ cd ../merged/ [root@iZ2zefmrr626i66omb40ryZ merged]$ rm -rf f # merged 中删除后 lower 中文件还在 [root@iZ2zefmrr626i66omb40ryZ merged]$ ls ../lower/ a c e f # 而 upper 中出现了一个大小为0的c类型文件f [root@iZ2zefmrr626i66omb40ryZ merged]# ls -l ../upper/ total 0 c--------- 1 root root 0, 0 Jan 18 19:28 f 可以发现，overlay 中删除 lower 中的文件，其实也是在 upper 中创建一个标记，表示这个文件已经被删除了，测试一下： [root@iZ2zefmrr626i66omb40ryZ merged]$ rm -rf ../upper/f [root@iZ2zefmrr626i66omb40ryZ merged]$ ls f [root@iZ2zefmrr626i66omb40ryZ merged]$ cat f fff 把 upper 中的大小为0的f文件给删掉后，merged中又可以看到lower中 f 了，而且内容也是一样的。 说明 overlay 中的删除其实是标记删除。再 upper 中添加一个删除标记，这样该文件就被隐藏了，从merged中看到的效果就是文件被删除了。 删除文件或文件夹时，会在 upper 中添加一个同名的 c 标识的文件，这个文件叫 whiteout 文件。当扫描到此文件时，会忽略此文件名。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:2:3","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"添加文件 最后再试一下添加文件 # 首先在 merged 中创建文件 g [root@iZ2zefmrr626i66omb40ryZ merged]$ echo ggg \u003e\u003e g [root@iZ2zefmrr626i66omb40ryZ merged]$ ls g # 然后查看 upper，发现也存在文件 g [root@iZ2zefmrr626i66omb40ryZ merged]$ ls ../upper/ g # 在查看内容，发送是一样的 [root@iZ2zefmrr626i66omb40ryZ merged]$ cat ../upper/g ggg 说明 overlay 中添加文件其实就是在 upper 中添加文件。 测试一下删除会怎么样呢： [root@iZ2zefmrr626i66omb40ryZ merged]$ rm -rf ../upper/g [root@iZ2zefmrr626i66omb40ryZ merged]$ ls f 把 upper 中的文件 g 删除了，果然 merged 中的文件 g 也消失了。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:2:4","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"3. docker 是如何使用 overlay 的？ ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:3:0","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"大致流程 每一个Docker image都是由一系列的read-only layers组成： image layers的内容都存储在Docker hosts filesystem的/var/lib/docker/aufs/diff目录下 而/var/lib/docker/aufs/layers目录则存储着image layer如何堆栈这些layer的metadata。 docker支持多种graphDriver，包括vfs、devicemapper、overlay、overlay2、aufs等等，其中最常用的就是aufs了，但随着linux内核3.18把overlay纳入其中，overlay的地位变得更重。 docker info命令可以查看docker的文件系统。 $ docker info # ... Storage Driver: overlay2 #... 比如这里用的就是 overlay2. 例如，假设我们有一个由两层组成的容器镜像： layer1: layer2: /etc /bin myconf.ini my-binary 然后，在容器运行时将把这两层作为 lower 目录，创建一个空upper目录，并将其挂载到某个地方： sudo mount \\ -t overlay \\ overlay \\ -o lowerdir=/layer1:/layer2,upperdir=/upper,workdir=/work \\ /merged 最后将/merged用作容器的 rootfs。 这样，容器中的文件系统就完成了。 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:3:1","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"具体分析 以构建镜像方式演示以下 docker 是如何使用 overlayfs 的。 先拉一下 Ubuntu:20.04 的镜像： $ docker pull ubuntu:20.04 20.04: Pulling from library/ubuntu Digest: sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322 Status: Downloaded newer image for ubuntu:20.04 docker.io/library/ubuntu:20.04 然后写个简单的 Dockerfile ： FROM ubuntu:20.04 RUN echo \"Hello world\" \u003e /tmp/newfile 开始构建： $ docker build -t hello-ubuntu . Sending build context to Docker daemon 2.048kB Step 1/2 : FROM ubuntu:20.04 ---\u003e ba6acccedd29 Step 2/2 : RUN echo \"Hello world\" \u003e /tmp/newfile ---\u003e Running in ee79bb9802d0 Removing intermediate container ee79bb9802d0 ---\u003e 290d8cc1f75a Successfully built 290d8cc1f75a Successfully tagged hello-ubuntu:latest 查看构建好的镜像： $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-ubuntu latest 290d8cc1f75a 13 minutes ago 72.8MB ubuntu 20.04 ba6acccedd29 3 months ago 72.8MB 使用docker history命令，查看镜像使用的 image layer 情况： $ docker history hello-ubuntu IMAGE CREATED CREATED BY SIZE COMMENT 290d8cc1f75a 22 seconds ago /bin/sh -c echo \"Hello world\" \u003e /tmp/newfile 12B ba6acccedd29 3 months ago /bin/sh -c #(nop) CMD [\"bash\"] 0B \u003cmissing\u003e 3 months ago /bin/sh -c #(nop) ADD file:5d68d27cc15a80653… 72.8MB missing ”标记的 layer ，是自 Docker 1.10 之后，一个镜像的 image layer image history 数据都存储在 个文件中导致的，这是 Docker 官方认为 正常行为。 可以看到，290d8cc1f75a 这一层在最上面，只用了 12Bytes，而下面的两层都是共享的，这也证明了AUFS是如何高效使用磁盘空间的。 然后去找一下具体的文件： docker默认的存储目录是/var/lib/docker,具体如下： [root@iZ2zefmrr626i66omb40ryZ docker]$ ls -al total 24 drwx--x--x 13 root root 167 Jul 16 2021 . drwxr-xr-x. 42 root root 4096 Oct 13 15:07 .. drwx--x--x 4 root root 120 May 24 2021 buildkit drwx-----x 7 root root 4096 Jan 17 20:25 containers drwx------ 3 root root 22 May 24 2021 image drwxr-x--- 3 root root 19 May 24 2021 network drwx-----x 53 root root 12288 Jan 17 20:25 overlay2 drwx------ 4 root root 32 May 24 2021 plugins drwx------ 2 root root 6 Jul 16 2021 runtimes drwx------ 2 root root 6 May 24 2021 swarm drwx------ 2 root root 6 Jan 17 20:25 tmp drwx------ 2 root root 6 May 24 2021 trust drwx-----x 5 root root 266 Dec 29 14:31 volumes 在这里，我们只关心image和overlay2就足够了。 image：镜像相关 overlay2：docker 文件所在目录，也可能不叫这个名字，具体和文件系统有关，比如可能是 aufs 等。 先看 image目录： docker会在/var/lib/docker/image目录下按每个存储驱动的名字创建一个目录，如这里的overlay2。 [root@iZ2zefmrr626i66omb40ryZ docker]$ cd image/ [root@iZ2zefmrr626i66omb40ryZ image]$ ls overlay2 # 看下里面有哪些文件 [root@iZ2zefmrr626i66omb40ryZ image]$ tree -L 2 overlay2/ overlay2/ ├── distribution │ ├── diffid-by-digest │ └── v2metadata-by-diffid ├── imagedb │ ├── content │ └── metadata ├── layerdb │ ├── mounts │ ├── sha256 │ └── tmp └── repositories.json 这里的关键地方是imagedb和layerdb目录，看这个目录名字，很明显就是专门用来存储元数据的地方。 layerdb：docker image layer 信息 imagedb：docker image 信息 因为 docker image 是由 layer 组成的，而 layer 也已复用，所以分成了 layerdb 和 imagedb。 先去 imagedb 看下刚才构建的镜像： $ cd overlay2/imagedb/content/sha256 $ ls [root@iZ2zefmrr626i66omb40ryZ sha256]# ls 0c7ea9afc0b18a08b8d6a660e089da618541f9aa81ac760bd905bb802b05d8d5 61ad638751093d94c7878b17eee862348aa9fc5b705419b805f506d51b9882e7 // .... 省略 b20b605ed599feb3c4757d716a27b6d3c689637430e18d823391e56aa61ecf01 60d84e80b842651a56cd4187669dc1efb5b1fe86b90f69ed24b52c37ba110aba ba6acccedd2923aee4c2acc6a23780b14ed4b8a5fa4e14e252a23b846df9b6c1 可以看到，都是 64 位的ID，这些就是具体镜像信息，刚才构建的镜像ID为290d8cc1f75a,所以就找290d8cc1f75a开头的文件： [root@iZ2zefmrr626i66omb40ryZ sha256]$ cat 290d8cc1f75a4e230d645bf03c49bbb826f17d1025ec91a1eb115012b32d1ff8 {\"architecture\":\"amd64\",\"config\":{\"Hostname\":\"\",\"Domainname\":\"\",\"User\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"bash\"],\"Image\":\"sha256:ba6acccedd2923aee4c2acc6a23780b14ed4b8a5fa4e14e252a23b846df9b6c1\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"OnBuild\":null,\"Labels\":null},\"container\":\"ee79bb9802d0ff311de6d606fad35fa7e9ab0c1cb4113837a50571e79c9454df\",\"container_config\":{\"Hostname\":\"\",\"Domainname\":\"\",\"User\":\"\",\"AttachStdin\":false,","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:3:2","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Docker"],"content":"4. 参考 a practical look into overlayfs overlayfs.txt docker-overlay2文件系统 ","date":"2022-03-19","objectID":"/posts/docker/09-ufs-overlayfs/:4:0","tags":["Docker"],"title":"Docker教程(九)---如何使用UFS(overlayfs)实现rootfs","uri":"/posts/docker/09-ufs-overlayfs/"},{"categories":["Golang"],"content":"golang chan 特性以及源码分析","date":"2022-03-12","objectID":"/posts/go/chan/","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"本文主要介绍了 Go 语言(golang)中的 chan，并从源码层面分析其具体实现，包括创建 channel，发送数据，接收数据以及相关调度等。 以下分析基于 Go 1.17.5 ","date":"2022-03-12","objectID":"/posts/go/chan/:0:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"1. 概述 官方对 chan 的描述如下： A channel provides a mechanism for concurrently executing functions to communicate by sending and receivingvalues of a specified element type. The value of an uninitialized channel is nil. chan 提供了一种并发通信机制，用于生产和消费某一指定类型数据，未初始化的 chan 的值是nil。 ","date":"2022-03-12","objectID":"/posts/go/chan/:1:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"2. 特性与实现 Chan 是 Go 里面的一种数据结构，具有以下特性： goroutine-safe，多个 goroutine 可以同时访问一个 channel 而不会出现并发问题 可以用于在 goroutine 之间存储和传递值 其语义是先入先出（FIFO） 可以导致 goroutine 的 block 和 unblock ","date":"2022-03-12","objectID":"/posts/go/chan/:2:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"内部结构 chan 内部结构如下图： 主要包含以下几个部分： 1）circular queue：循环队列，用于存储数据 2）send index 记录发送的位置 3）receive index 记录接收的位置 4）mutex 锁，用于实现 goroutine safe。 buf 的具体实现很简单，就是一个环形队列，使用 sendx 和 recvx 分别用来记录发送、接收的 offset，然后通过 mutex 互斥锁来保证并发安全。 ","date":"2022-03-12","objectID":"/posts/go/chan/:2:1","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"创建 chan chan 使用 make 进行初始化，第一个参数指定 chan 中的元素类型，第二个参数用于指定 chan 的缓冲区大小。 ch := make(chan string, 3) 上述代码中 make 返回的 ch 实际上是一个指向 heap 中真正的 chan 对象的指针。 chan（即 hchan 结构体） 默认会被分配在堆上，make 返回的只是一个指向该对象的指针。 这也是为什么我们可以在函数之间传递 chan，而不是 chan 的指针。 ","date":"2022-03-12","objectID":"/posts/go/chan/:2:2","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"发送、接收与关闭 func main() { ch := make(chan Task, 3) for _,task := range hellaTasks { taskCh \u003c- task // 发送 } close(tashCh) // 关闭 } func worker(ch) { for { task:= \u003c-taskCh // 接收 process(task) } } main goroutine 发送 task 到 chan，然后 worker goroutine 从 chan 中接收 task 并处理，最后 main goroutine 发送完成后关闭 chan。 具体发送过程如下： 1）acquire 加锁 2）enqueue，将 task 对象拷贝到数组里 3）release 释放锁 对于 chan 的关闭，最佳实践是由发送方进行关闭。 接收过程： 1）acquire 加锁 2）dequeue 将 task 对象从 数组 中 拷贝出来赋值给用户用于接收的对象 task:= \u003c-taskCh,比如这里就是拷贝出来赋值给 task 3）release 释放锁 整个过程中没有任何共享内存，数据都是通过 copy 进行传递,这遵循了 Go 并发设计中很核心的一个理念： Do not communicate by sharing memory; instead, share memory by communicating. ","date":"2022-03-12","objectID":"/posts/go/chan/:2:3","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"阻塞与唤醒 hchan 中的 buf 数组大小就是 make chan 时指定的大小。 当 buf 满之后再往 chan 中发送值就会阻塞。 复习一下 goroutine 调度：G 阻塞之后并不会阻塞 M。M 会先把这个 G 暂停(gopark)，然后把执行栈切换到 g0，g0 会执行 schedule() 函数，从当前 M 绑定的 P 中查找有没有可以执行的G，有就捞出来继续执行。 先发后收 假设 chan 中已经有 3 个 task 了,然后我们再试着往里面发送一个 taskCh \u003c- task runtime 会调用 gopark 将这个 goroutine(姑且称作G1) 切换到 wait 状态。 什么时候会被唤醒呢？ hchan 结构体中还有 sendq、recvq 两个列表，分别记录了等待发送或者接收的 goroutine，如下图所示： type hchan struct { recvq waitq // list of recv waiters sendq waitq // list of send waiters } type waitq struct { first *sudog last *sudog } 比如前面被阻塞的 G1 就会存入 sendq 假设此时 G2 从 chan 中取走一个消息 task:= \u003c-taskCh G2 取走一个消息后就会找到 sendq 中的第一个对象，把待发送的 elem 直接写入 buf 数组。然后 调用 goready 把对应的 goroutine G1 设置为 runnable 状态。 先收后发 之前是先发送，后接收。现在看一下先接收后发送的情况。 task:= \u003c-taskCh G2 直接从空的 chan 中取消息，同样会被阻塞,然后被写入到 hchan 的 recqv 中。 注意：elem 这里的 t 存的是 G2 栈里的地址。 然后 G1 往 chan 中发送一条消息。 taskCh \u003c- task 按照上面的逻辑应该是，将 task 写入 buf 数组后，，再把 recvq 中的第一个 goroutine G2 唤醒。 但是 Go 官方这里进行了优化，可以说是一个骚操作。因为 recvq 里的 elem 对象 t 存的就是接收者的内存地址。 所以我们可以直接把 G1 发送来的 task 写入 elem 对应的 t 里，即在 G1 里修改 G2 的栈对象。 因为这个时候 G2 还是 gopark，处于 waiting 状态，所以不会出问题。 正常情况下因为不知道两个线程谁先谁后，这样改肯定会出问题。但是在 go runtime 这里，肯定是 G2 先执行，满足 happen-before 所以不存在问题。 通过这样一个骚操作省去了发送和接收时的两次加解锁和内存拷贝。 ","date":"2022-03-12","objectID":"/posts/go/chan/:2:4","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"特性实现原理 到此我们应该明白了 chan 的这些特性的实现原理 goroutine-safe. hchan mutex，通过加锁来避免数据竞争。 可以用于在 goroutine 之间存储和传递值，以及先入先出（FIFO）语义。 copying into and out of hchan buffer 可以导致 goroutine 的 block 和 unblock 通过 sudog queues 来记录阻塞的 goroutine。 通过 runtime scheduler(gopark, goready)来实现阻塞与唤醒。 ","date":"2022-03-12","objectID":"/posts/go/chan/:2:5","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"3. 源码分析 chan 的所有相关代码都在runtime/chan.go中，还是比较好找的。 ","date":"2022-03-12","objectID":"/posts/go/chan/:3:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"内部结构 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } type waitq struct { first *sudog last *sudog } 其中的 sendx/recvx 、sendq/recvq、buf以及 lock是核心字段，前面都有介绍过，应该比较熟悉了。 ","date":"2022-03-12","objectID":"/posts/go/chan/:3:1","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"创建 在源码中通道的创建由 makechan 方法实现： func makechan(t *chantype, size int) *hchan {} 然后还有两个包装方法： //go:linkname reflect_makechan reflect.makechan func reflect_makechan(t *chantype, size int) *hchan { return makechan(t, size) } func makechan64(t *chantype, size int64) *hchan { if int64(int(size)) != size { panic(plainError(\"makechan: size out of range\")) } return makechan(t, int(size)) } 内部都是调用的 makechan 方法。 func makechan(t *chantype, size int) *hchan { elem := t.elem // 编译器检查 typesize 和 align if elem.size \u003e= 1\u003c\u003c16 { throw(\"makechan: invalid channel element type\") } if hchanSize%maxAlign != 0 || elem.align \u003e maxAlign { throw(\"makechan: bad alignment\") } // 计算存放数据元素的内存大小以及是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u003e maxAlloc-hchanSize || size \u003c 0 { panic(plainError(\"makechan: size out of range\")) } var c *hchan switch { case mem == 0: // chan的size为0，或者每个元素占用的大小为0（比如struct{}大小就是0，不占空间） // 这种情况就不需要单独为buf分配空间 c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 如果队列中不存在指针，那么每个元素都需要被存储并占用空间，占用大小为前面乘法算出来的mem // 同时还要加上hchan本身占用的空间大小，加起来就是整个hchan占用的空间大小 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) // 把buf指针指向空的hchan占用空间大小的末尾 c.buf = add(unsafe.Pointer(c), hchanSize) default: // 如果chan中的元素是指针类型的数据，为buf单独开辟mem大小的空间，用来保存所有的数据 c = new(hchan) c.buf = mallocgc(mem, elem, true) } // 元素大小、类型以及缓冲区大小赋值 c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) // 初始化锁 lockInit(\u0026c.lock, lockRankHchan) return c } 具体流程如下： 1）首先是编译器检查，包括通道元素类型的size以及通道和元素的对齐，然后计算存放数据元素的内存大小以及是否溢出 2）然后根据不同条件进行内存分配 总体的原则是：总内存大小 = hchan需要的内存大小 + 元素需要的内存大小 队列为空或元素大小为0：只需要开辟的内存空间为hchan本身的大小 元素不是指针类型：需要开辟的内存空间=hchan本身大小+每个元素的大小*申请的队列长度 元素是指针类型：这种情况下buf需要单独开辟空间，buf占用内存大小为每个元素的大小*申请的队列长度 3）最后则对chan的其他字段赋值 ","date":"2022-03-12","objectID":"/posts/go/chan/:3:2","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"发送 发送数据到channel时，直观的理解是将数据放到chan的环形队列中，不过go做了一些优化： 先判断是否有等待接收数据的groutine，如果有，直接将数据发给Groutine，唤醒groutine，就不放入队列中了。 这样省去了两次内存拷贝和加锁的开销 当然还有另外一种情况就是：队列如果满了，那就只能放到队列中等待，直到有数据被取走才能发送。 调用链 chan 的发送逻辑涉及到5个方法： func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) {} func chansend1(c *hchan, elem unsafe.Pointer) {…} func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {…} func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) {…} func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) {…} chansend1 方法是 go编译代码中c \u003c- x这种写法的入口点，即当我们编写代码 c \u003c- x其实就是调用此方法。 这四个方法的调用关系：chansend1 -\u003e chansend -\u003e send -\u003e sendDirect 具体发送逻辑在chansend这个方法里，然后真正使用的方法其实是对该方法的一层包装。 func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) { return chansend(c, elem, false, getcallerpc()) } chansend func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // 判断 channel 是否为 nil if c == nil { if !block {// 如果非阻塞，直接返回 false return false } // 当向 nil channel 发送数据时，会调用 gopark // 而 gopark 会将当前的 goroutine 休眠，并用过第一个参数的 unlockf 来回调唤醒 // 但此处传递的参数为 nil，因此向 channel 发送数据的 goroutine 和接收数据的 goroutine 都会阻塞，进而死锁 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\"unreachable\") } if raceenabled { racereadpc(c.raceaddr(), callerpc, funcPC(chansend)) } // 对于不阻塞的 send，快速检测失败场景 // 如果 channel 未关闭且 channel 没有多余的缓冲空间。这可能是： // 1. channel 是非缓冲型的，且等待接收队列里没有 goroutine // 2. channel 是缓冲型的，但循环数组已经装满了元素 // 主要用于 select 语句中，涉及到指令重排队+可观测性 if !block \u0026\u0026 c.closed == 0 \u0026\u0026 full(c) { return false } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } // 加锁,避免竞争 lock(\u0026c.lock) // 检查 channel 是否已关闭，不允许向关闭的 channel 发送数据 if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"send on closed channel\")) // 直接panic } // 从 recvq 队首取出一个接收者，如果存在接收者，就绕过环形队列（buf）直接把 ep 拷贝给 sg，并释放锁 // 这就是前面提到的，官方做的一个优化，如果有goroutine在等待就直接把数据给该goroutine，没必要在写到buf，然后接收者又从buf中拷贝出来 if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true } // 到这里说明当前没有等待状态的接收者 // 如果环形队列还未满 if c.qcount \u003c c.dataqsiz { // 拿到 sendx 索引的位置 qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } // 直接把数据从 qp 拷贝到 qp，就是把数据拷贝到环形队列中 typedmemmove(c.elemtype, qp, ep) // 维护 snedx 的值，因为是环形队列，所以到最大值时就重置为0 c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } //qcount即当前chan中的元素个数 c.qcount++ unlock(\u0026c.lock) return true } // 到这里说明环形队列已经满了 // 如果还是要非阻塞的方式发送，就只能返回错误了 if !block { unlock(\u0026c.lock) return false } // 到这里说明缓存队列满了，然后调用法指定是阻塞方式进行发送 // channel 满了，发送方会被阻塞。接下来会构造一个 sudog gp := getg() // 获取当前 goroutine mysg := acquireSudog()// 从对象池获取 sudog mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // 把发送的数据(ep)、当前g(gp)、已经当前这个chan(c)都存到sudog中 mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c // 保存当前 sudog，下面要用到做校验 gp.waiting = mysg gp.param = nil // 把这个sudog存入sendq队列 c.sendq.enqueue(mysg) atomic.Store8(\u0026gp.parkingOnChan, 1) // 调用gopark，挂起当前的 g，将当前的 g 移出调度器的队列 gopark(chanparkcommit, unsafe.Pointer(\u0026c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // 等到有接收者从chan中取值的时候，这个发送的g又会被重新调度，然后从这里开始继续执行 KeepAlive(ep) // 检验是否为当前的 sudog if mysg != gp.waiting { throw(\"G waiting list is corrupted\") } gp.waiting = nil gp.activeStackChans = false // 这里sudog中的success表示的是当前这个通道上是否进行过通信 // 为 true 则说明是真正的唤醒，chan上有活动（有数据写进来，或者有数据被读取出去） // 为 false 则说明是假的唤醒，即当前唤醒是否关闭chan导致的 // 这里主要根据这个值判断chan是否被关闭了 closed := !mysg.success gp.param = nil if mysg.releasetime \u003e 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil // 将 sudog 放回对象池 releaseSudog(mysg) if closed { // 如果chan被关闭了也是直接panic if c.closed == 0 { throw(\"chansend: spurious wakeup\") } panic(plainError(\"send on closed channel\")) } return true } 核心逻辑 如果recvq不为空，从recvq中取出一个等待接收数据的Groutine，直接将数据发送给该Groutine 如果recvq为空，才将数据放入buf中 如果buf已满，则将要发送的数据和当前的Groutine打包成Sudog对象放入sendq，并将groutine置为等待状态 等goroutine再次被调度时程序继续执行 send 然后追踪一下 send 方法： func send(c *hchan, sg *sudog, ep unsafe.Pointer, un","date":"2022-03-12","objectID":"/posts/go/chan/:3:3","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"接收 从channel读取数据的流程和发送的类似，基本是发送操作的逆操作。 这里同样存在和send一样的优化：从channel读取数据时，不是直接去环形队列中去数据，而是先判断是否有等待发送数据的groutine。如果有，直接将groutine出队列，取出数据返回，并唤醒groutine。如果没有等待发送数据的groutine，再从环形队列中取数据。 调用链 chan的接收涉及到7个方法： func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool) {} func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool) {} func chanrecv1(c *hchan, elem unsafe.Pointer) {…}， func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) {…} func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) {…} func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) {…} func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) {…} 按照发送时的套路可知，只有 chanrecv 是具体逻辑，上面几个都是包装方法： //go:linkname reflect_chanrecv reflect.chanrecv func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool) { return chanrecv(c, elem, !nb) } func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool) { return chanrecv(c, elem, false) } //go:nosplit func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } //go:nosplit func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } 接收操作有两种写法，一种带 “ok”，反应 channel 是否关闭； 一种不带 “ok”，这种写法，当接收到相应类型的零值时无法知道是真实的发送者发送过来的值，还是 channel 被关闭后，返回给接收者的默认类型的零值。 两种写法，都有各自的应用场景。 经过编译器的处理后，这两种写法最后对应源码里的就是不带ok的chanrecv1和带ok的chanrecv2这两个函数。 chanrecv // chanrecv 函数接收 channel c 的元素并将其写入 ep 所指向的内存地址。 // 如果 ep 是 nil，说明忽略了接收值。比如 \u003c-ch 这样，没有接收取到的值 // 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false) // 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false) // 否则，用返回值填充 ep 指向的内存地址。返回 (true, true) // 如果 ep 非空，则应该指向堆或者函数调用者的栈 func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { // 如果是一个 nil 的 channel if c == nil { // 如果不阻塞，直接返回 (false, false) if !block { return } // 否则，接收一个 nil 的 channel，调用gopark将goroutine 挂起 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\"unreachable\") // 被挂起之后不会执行到这一句 } // 这块主要用在 select 语句中，先大概了解下，比较难懂。。。 // 快速路径: 在不需要锁的情况下检查失败的非阻塞操作 // 注意到 channel 不能由已关闭转换为未关闭，则失败的条件是： // 1. channel 是非缓冲型的，recvq 队列为空 // 2. channel 是缓冲型的，buf 为空 if !block \u0026\u0026 empty(c) { // 此处的 c.closed 必须在条件判断之后进行验证， // 因为指令重排后，如果先判断 c.closed，得出 channel 未关闭，无法判断失败条件中channel 是已关闭还是未关闭（从而需要 atomic 操作） if atomic.Load(\u0026c.closed) == 0 { return } // 再次检查 channel 是否为空 if empty(c) { // 接收者不为 nil 时返回该类型的零值 if ep != nil { // typedmemclr 逻辑是根据类型清理相应地址的内存 typedmemclr(c.elemtype, ep) } // 返回（true,fasle） // 返回值1--true：表示被 select case 选中， // 返回值2--fasle 表示是否正常收到数据 return true, false } } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } // 加锁，保证并发安全 lock(\u0026c.lock) // channel 已关闭，并且循环数组 buf 里没有元素 // 这里可以处理非缓冲型关闭 和 缓冲型关闭但 buf 无元素的情况 // 也就是说即使是关闭状态，但在缓冲型的 channel， // buf 里有元素的情况下还能接收到元素 if c.closed != 0 \u0026\u0026 c.qcount == 0 { unlock(\u0026c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } // 等待发送队列里有 goroutine 存在，说明 buf 是满的 // 这有可能是： // 1. 非缓冲型的 channel // 2. 缓冲型的 channel，但 buf 满了 // 针对 1，直接进行内存拷贝（从 sender goroutine -\u003e receiver goroutine） // 针对 2，接收到循环数组头部的元素，并将发送者的元素放到循环数组尾部 if sg := c.sendq.dequeue(); sg != nil { recv(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true, true } // chan的buf 里有元素，可以正常接收 if c.qcount \u003e 0 { // 直接从循环数组里找到要接收的元素 qp := chanbuf(c, c.recvx) // ep != nil表示代码里，没有忽略要接收的值 // 即接收的代码不是 \"\u003c- ch\"，而是 \"val \u003c- ch\"这种，ep 指向 val if ep != nil { typedmemmove(c.elemtype, ep, qp) } // 清理掉循环数组里相应位置的值 typedmemclr(c.elemtype, qp) // 维护接收游标 c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } // buf 数组里的元素个数减 1 c.qcount-- // 处理完成，解锁返回 unlock(\u0026c.lock) return true, true } // 到这里说明chan的buf里没有数据了，如果是非阻塞接收就直接返回了 if !block { unlock(\u0026c.lock) return false, false } // 接下来就是要被阻塞的情况了 // 和发送类似的，构造一个 sudog gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // 这里需要注意一下，ep就是我们用来接收值得对象 // 这里把ep直接存到sudog.elem字段上 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg // 这个waiting同样是用来唤醒后做校验的 mysg.g = ","date":"2022-03-12","objectID":"/posts/go/chan/:3:4","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"3. 关闭 调用链 close 就比较简单了，相关方法就两个： //go:linkname reflect_chanclose reflect.chanclose func reflect_chanclose(c *hchan) { closechan(c) } func closechan(c *hchan){} 其中一个还是包装方法，真正逻辑就在 clsoechan 里。 每个逻辑都有一个 reflect_xxx 的方法，根据名字猜测是反射的时候用的。 closechan func closechan(c *hchan) { // 关闭一个nil的chan直接panic if c == nil { panic(plainError(\"close of nil channel\")) } // 同样是先加锁 lock(\u0026c.lock) // 判断一下是否被关闭过了，关闭一个已经关闭的chan也是直接panic if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"close of closed channel\")) } // 修改closed标记为，表示chan已经被关闭了 c.closed = 1 // gList 是通过 g.schedlink 链接 G 的列表，一个 G 只能是一次在一个 gQueue 或 gList 上 // gList 模拟的是栈操作（FILO） // gQueue 模拟的是队列操作（FIFO） var glist gList // 释放所有的接收者 for { sg := c.recvq.dequeue() // sg == nil，表示接收队列已为空，跳出循环 if sg == nil { break } // 如果 elem 不为空说明未忽略接收值，赋值为该类型的零值 if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // 释放所有的发送者 for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026c.lock) // 循环读取 glist 里面的数据，挨个唤醒 for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } 核心流程： 设置关闭状态 唤醒所有等待读取chanel的协程 所有等待写入channel的协程，抛出异常 ","date":"2022-03-12","objectID":"/posts/go/chan/:3:5","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"4. 小结 ","date":"2022-03-12","objectID":"/posts/go/chan/:4:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"存储实现 chan 内部使用一个环形队列实现存储，使用 sendx或recvx进行发送或读取。 ","date":"2022-03-12","objectID":"/posts/go/chan/:4:1","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"并发安全 使用 mutex 保证并发安全。 ","date":"2022-03-12","objectID":"/posts/go/chan/:4:2","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"调度 使用 sendq 和 recvq来暂存由于发送或接收而被阻塞的goroutine。 send/recv的时候都会判断recvq/sendq是否有goroutine正在等待，有则优先处理。 发送 发送的时候发现recvq有goroutine正在等待，说明此时chan的buf是空的，或者chan是个非缓存chan，根本没有buf。 对于发送来说，不管是buf为空还是chan没有buf都是一样的处理逻辑。 此时会直接从recvq中取出第一个g，然后把本次要发送的数据直接写给这个接收者g，并调用goready把这个g唤醒。 接收 如果接收的时候发现sendq有goroutine正在等待，说明buf满了，或者chan是个非缓存chan，根本没有buf。 对于接收来说buf满了或者chan没有buf二者的处理逻辑就不太一样了。 因为需要保证顺序,buf满了就不能直接去读sender的数据了，只能从buf中去。 如果是buf满了：那么会先从buf中读一个值出来(腾一个位置出来)，然后把sender发送的值写入buf，并唤醒这个sender g。 如果是没有buf的无缓存chan：那就直接把sender要发送的数据取出来，作为本次取到的数据，然后唤醒sender g。 通过研究底层的源码实现才发现，chan 其实没有那么复杂，底层实现逻辑很清晰。 如果有调度基础的话，看起来就比较简单。 ","date":"2022-03-12","objectID":"/posts/go/chan/:4:3","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["Golang"],"content":"5. 参考 understanding-channels-kavya-joshi 图解Golang channel源码 Go夜读-第 56 期 channel \u0026 select 源码分析 Go源码阅读 | channel 设计与实现 ","date":"2022-03-12","objectID":"/posts/go/chan/:5:0","tags":["Golang"],"title":"Go语言之 chan 源码分析","uri":"/posts/go/chan/"},{"categories":["etcd"],"content":"etcd raft 算法实现之leader选取源码分析","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"本文主要通过源码分析了 etcd 中 leader选举的具体实现。 raft 是针对 paxos 的简化版本，拆解为三个核心问题： 1）Leader 选举； 2）日志复制； 3）正确性保证； 这里简单回顾以下 Leader 选举的逻辑。 更多 raft 信息可以参考这两篇文章 Raft 算法概述 etcd教程(九)—Raft 算法具体实现 在 raft 中，一个节点任一时刻都会处于以下三个状态之一： Leader leader 处理所有来自客户端的请求(如果客户端访问 follower，会把请求重定向到 leader) Follower follower 是消极的，他们不会主动发出请求而仅仅对来自 leader 和 candidate 的请求作出回应。 Candidate Candidate 状态用来选举出一个 leader。 在正常情况下会只有一个 leader，其他节点都是 follower。 Raft 使用心跳机制来触发 leader 选举，具体状态转换流程如图： 可以看到： 所有节点启动时都是follower状态； 在一段时间内如果没有收到来自 leader 的心跳，从 follower 切换到 candidate，且 term+1并发起选举； 如果收到 majority 的投票（含自己的一票）则切换到 leader 状态； 如果发现其他节点 term 比自己更新，则主动切换到 follower。 以下分析记录 etcd v3.5.1 为了便于阅读，省略了无关代码，只保留主干部分。 接下来就看下 etcd 中是怎么实现的吧。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:0:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"1. 节点初始化 // raft/raft.go 318行 func newRaft(c *Config) *raft { r := \u0026raft{ id: c.ID, lead: None, isLearner: false, raftLog: raftlog, maxMsgSize: c.MaxSizePerMsg, maxUncommittedSize: c.MaxUncommittedEntriesSize, prs: tracker.MakeProgressTracker(c.MaxInflightMsgs), electionTimeout: c.ElectionTick, heartbeatTimeout: c.HeartbeatTick, logger: c.Logger, checkQuorum: c.CheckQuorum, preVote: c.PreVote, readOnly: newReadOnly(c.ReadOnlyOption), disableProposalForwarding: c.DisableProposalForwarding, } if !IsEmptyHardState(hs) { r.loadState(hs) } r.becomeFollower(r.Term, None) return r } 首先根据配置文件，构造一个 raft 对象，然后如果有持久化数据的话就同步一下。 重点是r.becomeFollower(r.Term, None),说明节点启动的时候默认都是 follower 。 追踪下去，看下做了些什么： // raft/raft.go 686行 func (r *raft) becomeFollower(term uint64, lead uint64) { // 这是一个func，主要是状态机的处理逻辑 r.step = stepFollower r.reset(term) // 这就是 选举相关的逻辑 r.tick = r.tickElection r.lead = lead r.state = StateFollower } ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:1:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"2. 超时后开启选举 tickElection主要是判断是否能够开始选举Leader,实际是由外部驱动的： // raft/raft.go 645行 func (r *raft) tickElection() { // 每次计数加一 r.electionElapsed++ // 如果条件允许(并不是所有节点都可以参与Leader选举的)，并且已经超时，那么就开始选举 if r.promotable() \u0026\u0026 r.pastElectionTimeout() { r.Step(pb.Message{From: r.id, Type: pb.MsgHup}) } } // raft/raft.go 1714行 func (r *raft) pastElectionTimeout() bool { // 超时时间到了就可以开始去选举Leader了 return r.electionElapsed \u003e= r.randomizedElectionTimeout } // raft/raft.go 1718行 func (r *raft) resetRandomizedElectionTimeout() { // 再固定的超时时间上增加一个随机值，避免出现所有节点同时超时的情况 r.randomizedElectionTimeout = r.electionTimeout + globalRand.Intn(r.electionTimeout) } 超时时间居然并不是用时间来处理，而是每次+1。 超时时间用计数来实现这个有点巧妙，可以避免当前机器突然卡了以下然后导致超时的问题。 用计时的话：机器卡了一下，然后卡回来可能就超时了 用计数的话：机器卡的时候计数就不会加了，卡回来也不会超时。 实际上由于该方法是外部调用的，所以外部比如每毫秒调用一次，那么实际上超时时间单位就是 ms 了。 同时还通过一个简单的随机算法，使得每个节点的超时时间不一致，巧妙地避免出现所有节点同时超时，然后又同时发起选举，最后都把票投给自己，导致一直选不出 Leader 的情况。 时间错开后，第一个节点超时发起选举后，其他节点还没超时，此时有很大概率就直接选出 Leader（如果发起选取的节点满足其他条件的话）。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:2:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"3. 选举处理逻辑 // raft/raft.go 847行 func (r *raft) Step(m pb.Message) error { switch m.Type { case pb.MsgHup: if r.preVote { // 如果开启了预选机制，则进入预选流程 r.hup(campaignPreElection) } else { // 否则直接进入选举流程 r.hup(campaignElection) } } } preVote也是 etcd 中新增的功能，主要用于避免无效选举，以提升集群的稳定性。 因为有的节点注定不会成为 Leader，真的进行选举也是白给，所以先预选一下，在预选阶段就拦截掉部分无效选举。 // raft/raft.go 760行 func (r *raft) hup(t CampaignType) { if r.state == StateLeader { // 已经是 Leader 就不能再发起选举了 return } if !r.promotable() { // 不满足参与 Leader 选举的条件也是不让选 return } // 如果还有配置修改没有应用，也不能选 ents, err := r.raftLog.slice(r.raftLog.applied+1, r.raftLog.committed+1, noLimit) if n := numOfPendingConf(ents); n != 0 \u0026\u0026 r.raftLog.committed \u003e r.raftLog.applied { return } // 到此，正式开始选举 r.campaign(t) } // raft/raft.go 785行 func (r *raft) campaign(t CampaignType) { var term uint64 var voteMsg pb.MessageType // 根据始预选还是分别处理 if t == campaignPreElection { r.becomePreCandidate() // 切换到 PreCandidate 状态 voteMsg = pb.MsgPreVote term = r.Term + 1 } else { r.becomeCandidate() // 切换到 Candidate 状态 voteMsg = pb.MsgVote term = r.Term } if _, _, res := r.poll(r.id, voteRespMsgType(voteMsg), true); res == quorum.VoteWon { // We won the election after voting for ourselves (which must mean that // this is a single-node cluster). Advance to the next state. if t == campaignPreElection { r.campaign(campaignElection) } else { r.becomeLeader() } return } // 向所有节点发送消息 var ids []uint64 { idMap := r.prs.Voters.IDs() ids = make([]uint64, 0, len(idMap)) for id := range idMap { ids = append(ids, id) } sort.Slice(ids, func(i, j int) bool { return ids[i] \u003c ids[j] }) } for _, id := range ids { if id == r.id { // 本节点除外 continue } r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), voteMsg, id, r.Term) var ctx []byte if t == campaignTransfer { ctx = []byte(t) } r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx}) } } 主要做了两件事： 切换到 Candidate 状态 发送投票消息给其他节点 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:3:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"4. 预选逻辑 这里的预选逻辑额外拿出来分析以下，raft paper 中没有，是 etcd 中的一个优化。 if t == campaignPreElection { r.becomePreCandidate() voteMsg = pb.MsgPreVote term = r.Term + 1 } else { r.becomeCandidate() voteMsg = pb.MsgVote term = r.Term } 首先看一下 r.becomePreCandidate()和r.becomeCandidate()有什么区别： // raft/raft.go 695行 func (r *raft) becomeCandidate() { r.step = stepCandidate r.reset(r.Term + 1) r.tick = r.tickElection r.Vote = r.id r.state = StateCandidate } // raft/raft.go 708行 func (r *raft) becomePreCandidate() { r.step = stepCandidate r.prs.ResetVotes() r.tick = r.tickElection r.lead = None r.state = StatePreCandidate } 重点是，follower 切换成为 candidate 调用了 r.reset(r.Term + 1)，把 term +1 了。 而在预选中切换成 PreCandidate 则没有，只是在发送消息的时候，PreCandidate 把消息中的 term +1 而已。 因为预选可能不会通过，如果该节点无法联系上系统中的大多数节点那么这次状态切换会导致 term 毫无意义的增大，所以没有增加。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:4:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"5. 投票结果处理 // raft/raft.go 1376行 func stepCandidate(r *raft, m pb.Message) error { // 同样是预选和正选状态判定 var myVoteRespType pb.MessageType if r.state == StatePreCandidate { myVoteRespType = pb.MsgPreVoteResp } else { myVoteRespType = pb.MsgVoteResp } // 处理投票结构 switch m.Type { case myVoteRespType: gr, rj, res := r.poll(m.From, m.Type, !m.Reject) switch res { // 获取到了过半的选票 case quorum.VoteWon: // 如果是预选那么此时就可以开始正式选举 if r.state == StatePreCandidate { r.campaign(campaignElection) } else { // 如果是正选那么选举成功，切换成 leader r.becomeLeader() r.bcastAppend() } case quorum.VoteLost: // 如果失败了，不管是预选还是正选都切换成 follower r.becomeFollower(r.Term, None) } } return nil } 投票结果有三种： VoteWon：成功，如果是预选那么此时就可以开始正式选举，如果是正选就成为 leader 了 VoteLost：失败，不管是预选还是正选都切换成 follower VotePending：其实还有一种情况，即同意或者拒绝的票数都没到阈值，还需要进一步等待后续投票。不过Switch中没有写出来，即匹配不到上面的任何一种情况时就啥也不做。 成为 Leader 后还调用了r.bcastAppend()方法，发送了一条广播日志。 如果没有有效日志，那么也会广播一条空的 Message，具体原因在后面。 这个在 Raft Paper 中是有提到的，主要在 State Machine Safety这一条中。 如果一条日志成功复制到大多数节点上，leader 就知道可以 commit 了。如果leader 在 commit 之前崩溃了，新的 leader 将会尝试完成复制这条日志。然而一个 leader 不可能立刻推导出之前 term 的 entry 已经 commit 了。所以 Raft 算法做了以下限制： 某个 leader 选举成功之后，不会直接提交前任 leader 时期的日志，而是通过提交当前任期的日志的时候“顺手”把之前的日志也提交了，具体怎么实现了，在 log matching 部分有详细介绍。 为了避免 leader 在整个任期中都没有收到客户端请求，导致日志一直没有被提交的情况，leader 会在在任期开始的时候发立即尝试复制、提交一条空的 log。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:5:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"6. Leader 心跳 Follower 检测到心跳超时后就会开始选举 Leader，Leader 自然需要不断的给 Follower 发送心跳以保证自己的 Leader 地位。 // raft/raft.go 724行 func (r *raft) becomeLeader() { // 成为 Leader 后设置了另一个 tick r.tick = r.tickHeartbeat // ... } // raft/raft.go 657行 func (r *raft) tickHeartbeat() { r.heartbeatElapsed++ // 可以看到同样是用的计数来代表时间 r.electionElapsed++ if r.state != StateLeader { return } // 如果超了就给 Follower 发一个心跳消息过去 if r.heartbeatElapsed \u003e= r.heartbeatTimeout { r.heartbeatElapsed = 0 if err := r.Step(pb.Message{From: r.id, Type: pb.MsgBeat}); err != nil { } } } // raft/raft.go 847行 func (r *raft) Step(m pb.Message) error { switch m.Type { default: err := r.step(r, m) if err != nil { return err } } return nil } 实际上这个类型，啥也匹配不到，最终会进入 Default 逻辑，调用 step 方法。 Leader 的 step 方法如下： // raft/raft.go 991行 func stepLeader(r *raft, m pb.Message) error { switch m.Type { case pb.MsgBeat: r.bcastHeartbeat() return nil } 广播发送心跳： // raft/raft.go 525行 func (r *raft) bcastHeartbeat() { lastCtx := r.readOnly.lastPendingRequestCtx() if len(lastCtx) == 0 { r.bcastHeartbeatWithCtx(nil) } else { r.bcastHeartbeatWithCtx([]byte(lastCtx)) } } 此时去看下 Follower 收到心跳后会做什么呢： // raft/raft.go 1421行 func stepFollower(r *raft, m pb.Message) error { switch m.Type { case pb.MsgHeartbeat: r.electionElapsed = 0 // 直接把计数清0 r.lead = m.From r.handleHeartbeat(m) // 然后回复心跳消息 } // raft/raft.go 1513行 func (r *raft) handleHeartbeat(m pb.Message) { r.raftLog.commitTo(m.Commit) r.send(pb.Message{To: m.From, Type: pb.MsgHeartbeatResp, Context: m.Context}) } 处理很简单，就是重置 electionElapsed 然后再回复一个心跳响应消息。 根据前面 Follower 的逻辑中，每次调用 tick 时，electionElapsed 会+1，如果超过阈值就会发起选举。 然后 Leader 心跳消息时会直接将 electionElapsed 重置，所以如果 Leader 正常运行，Follower 永远不会触发选举。 根据上述逻辑可以知道：Leader 的 tick 发送间隔要小于 Follower 的 tick 间隔，不然 Follower 都检测到超时了，Leader 还不发送心跳消息过去，整个集群就无法正常运行了。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:6:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["etcd"],"content":"7. 小结 1）选举流程 Follower tick 超时 产生 MsgHup 广播 MsgVote 消息 接收 MsgVoteResp VoteWin：切换到 Leader VoteLost：切换会 Follower 2）心跳流程 Leader tick 超时 产生 MsgBeat 广播 MsgHeartbeat 消息 Follower 清零 tick 计数 etcd 中新增了预选功能，主要用于避免无效的选举对集群稳定性产生影响。 通过将各个节点超时时间随机化，来避免同时开启选举，然后瓜分选票，最终一直无法选出 Leader 的情况。 通过计数方式实现超时，也比较巧妙。 ","date":"2022-03-05","objectID":"/posts/etcd/15-raft-leader-election/:7:0","tags":["etcd"],"title":"etcd教程(十五)---leader选取源码分析","uri":"/posts/etcd/15-raft-leader-election/"},{"categories":["Docker"],"content":"Cgroups 概念讲解及简单演示","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"本章主要记录了 Cgroups 相关的命令和操作，最后用一个 Demo 演示了如何用 Go 语言操作 Cgroups。 主要是之前学习 Cgroups 的时候没有找到相关命令，这里简单记录一下 Cgroup 具体是如何创建删除及管理的。 准备跟着《自己动手写 docker》这本书从零开始实现一个简易版的 docker，加深对 docker 的理解。 源码及相关教程见 Github，欢迎 star。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:0:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"1. hierarchy ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:1:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"创建 由于 Linux Cgroups 是基于内核中的 cgroup virtual filesystem 的，所以创建 hierarchy 其实就是将其挂载到指定目录。 语法为: mount -t cgroup -o subsystems name /cgroup/name 其中 subsystems 表示需要挂载的 cgroups 子系统 /cgroup/name 表示挂载点（一般为具体目录） 这条命令同在内核中创建了一个 hierarchy 以及一个默认的 root cgroup。 例如： $ mkdir cg1 $ mount -t cgroup -o cpuset cg1 ./cg1 比如以上命令就是挂载一个 cg1 的 hierarchy 到 ./cg1 目录，如果指定的 hierarchy 不存在则会新建。 hierarchy 创建的时候就会就会自动创建一个 cgroup 以作为 cgroup树中的 root 节点。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:1:1","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"删除 删除 hierarchy 则是卸载。 语法为：umount /cgroup/name /cgroup/name 表示挂载点（一般为具体目录） 例如： $ umount ./cg1 以上命令就是卸载 ./cg1 这个目录上挂载的 hierarchy，也就是前面挂载的 cg。 hierarchy 卸载后，相关的 cgroup 都会被删除。 不过 cg1 目录需要手动删除。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:1:2","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"文件含义 hierarchy 挂载后会生成一些文件，具体如下： 为了避免干扰，未关联任何 subsystem $ mkdir cg1 $ mount -t cgroup -o none,name=cg1 cg1 ./cg1 $ tree cg1 cg1 ├── cgroup.clone_children ├── cgroup.procs ├── cgroup.sane_behavior ├── notify_on_release ├── release_agent └── tasks 具体含义如下： cgroup.clone_children：这个文件只对cpuset subsystem有影响，当该文件的内容为1时，新创建的cgroup将会继承父cgroup的配置，即从父cgroup里面拷贝配置文件来初始化新cgroup，可以参考这里 cgroup.procs：当前cgroup中的所有进程ID，系统不保证ID是顺序排列的，且ID有可能重复 cgroup.sane_behavior：具体功能不详，可以参考这里和这里 notify_on_release：该文件的内容为1时，当cgroup退出时（不再包含任何进程和子cgroup），将调用release_agent里面配置的命令。 新cgroup被创建时将默认继承父cgroup的这项配置。 release_agent：里面包含了cgroup退出时将会执行的命令，系统调用该命令时会将相应cgroup的相对路径当作参数传进去。 注意：这个文件只会存在于root cgroup下面，其他cgroup里面不会有这个文件。 相当于配置一个回调用于清理资源。 tasks：当前cgroup中的所有线程ID，系统不保证ID是顺序排列的 cgroup.procs 和 tasks 的区别见 cgroup 操作章节。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:1:3","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"release_agent 当一个cgroup里没有进程也没有子cgroup时，release_agent将被调用来执行cgroup的清理工作。 具体操作流程： 首先需要配置 notify_on_release 以开启该功能。 然后将脚本内容写入到 release_agent 中去。 最后cgroup退出时（不再包含任何进程和子cgroup）就会执行 release_agent 中的命令。 #创建新的cgroup用于演示 dev@ubuntu:~/cgroup/demo$ sudo mkdir test #先enable release_agent dev@ubuntu:~/cgroup/demo$ sudo sh -c 'echo 1 \u003e ./test/notify_on_release' #然后创建一个脚本/home/dev/cgroup/release_demo.sh， #一般情况下都会利用这个脚本执行一些cgroup的清理工作，但我们这里为了演示简单，仅仅只写了一条日志到指定文件 dev@ubuntu:~/cgroup/demo$ cat \u003e /home/dev/cgroup/release_demo.sh \u003c\u003c EOF #!/bin/bash echo \\$0:\\$1 \u003e\u003e /home/dev/release_demo.log EOF #添加可执行权限 dev@ubuntu:~/cgroup/demo$ chmod +x ../release_demo.sh #将该脚本设置进文件release_agent dev@ubuntu:~/cgroup/demo$ sudo sh -c 'echo /home/dev/cgroup/release_demo.sh \u003e ./release_agent' dev@ubuntu:~/cgroup/demo$ cat release_agent /home/dev/cgroup/release_demo.sh #往test里面添加一个进程，然后再移除，这样就会触发release_demo.sh dev@ubuntu:~/cgroup/demo$ echo $$ 27597 dev@ubuntu:~/cgroup/demo$ sudo sh -c 'echo 27597 \u003e ./test/cgroup.procs' dev@ubuntu:~/cgroup/demo$ sudo sh -c 'echo 27597 \u003e ./cgroup.procs' #从日志可以看出，release_agent被触发了，/test是cgroup的相对路径 dev@ubuntu:~/cgroup/demo$ cat /home/dev/release_demo.log /home/dev/cgroup/release_demo.sh:/test ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:1:4","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"2. cgroup ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:2:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"创建 创建 cgroup 很简单，在父 cgroup 或者 hierarchy 目录下新建一个目录就可以了。 具体层级关系就和目录层级关系一样。 # 创建子cgroup cgroup-cpu $ mkdir cgroup-cpu $ cd cgroup-cpu # 创建cgroup-cpu的子cgroup $ mkdir cgroup-cpu-1 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:2:1","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"删除 删除也很简单，删除对应目录即可。 注意：是删除目录 rmdir，而不是递归删除目录下的所有文件。 如果有多层 cgroup 则需要先删除子 cgroup，否则会报错： $ rmdir cgroup-cpu # 如果cgroup中有进程正在本限制，也会出现这个错误，需要先停掉对应进程，或者把进程移动到另外的 cgroup 中(比如父cgroup) rmdir: failed to remove 'cgroup-cpu': Device or resource busy 先删除子 cgroup 就可以了： $ rmdir cg1 $ cd ../ $ rmdir cgroup-cpu 也可以借助 libcgroup 工具来创建或删除。 使用 libcgroup 工具前，请先安装 libcgroup 和 libcgroup-tools 数据包 redhat系统安装： $ yum install libcgroup $ yum install libcgroup-tools ubuntu系统安装: $ apt-get install cgroup-bin # 如果提示cgroup-bin找不到，可以用 cgroup-tools 替换 $ apt-get install cgroup-tools 具体语法： # controllers就是subsystem # path可以用相对路径或者绝对路径 $ cgdelete controllers:path 例如： $ cgcreate cpu:./mycgroup $ cgdelete cpu:./mycgroup ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:2:2","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"添加进程 创建新的 cgroup 后，就可以往里面添加进程了。注意下面几点： 在一颗 cgroup 树里面，一个进程必须要属于一个 cgroup。 所以不能凭空从一个 cgroup 里面删除一个进程，只能将一个进程从一个 cgroup 移到另一个 cgroup 新创建的子进程将会自动加入父进程所在的 cgroup。 这也就是 tasks 和 cgroup.proc 的区别。 从一个 cgroup 移动一个进程到另一个 cgroup 时，只要有目的 cgroup 的写入权限就可以了，系统不会检查源 cgroup 里的权限。 用户只能操作属于自己的进程，不能操作其他用户的进程，root 账号除外。 #--------------------------第一个shell窗口---------------------- #创建一个新的cgroup dev@ubuntu:~/cgroup/demo$ sudo mkdir test dev@ubuntu:~/cgroup/demo$ cd test #将当前bash加入到上面新创建的cgroup中 dev@ubuntu:~/cgroup/demo/test$ echo $$ 1421 dev@ubuntu:~/cgroup/demo/test$ sudo sh -c 'echo 1421 \u003e cgroup.procs' #注意：一次只能往这个文件中写一个进程ID，如果需要写多个的话，需要多次调用这个命令 #--------------------------第二个shell窗口---------------------- #重新打开一个shell窗口，避免第一个shell里面运行的命令影响输出结果 #这时可以看到cgroup.procs里面包含了上面的第一个shell进程 dev@ubuntu:~/cgroup/demo/test$ cat cgroup.procs 1421 #--------------------------第一个shell窗口---------------------- #回到第一个窗口，随便运行一个命令，比如 top dev@ubuntu:~/cgroup/demo/test$ top #这里省略输出内容 #--------------------------第二个shell窗口---------------------- #这时再在第二个窗口查看，发现top进程自动加入了它的父进程（1421）所在的cgroup dev@ubuntu:~/cgroup/demo/test$ cat cgroup.procs 1421 16515 dev@ubuntu:~/cgroup/demo/test$ ps -ef|grep top dev 16515 1421 0 04:02 pts/0 00:00:00 top dev@ubuntu:~/cgroup/demo/test$ #在一颗cgroup树里面，一个进程必须要属于一个cgroup， #所以我们不能凭空从一个cgroup里面删除一个进程，只能将一个进程从一个cgroup移到另一个cgroup， #这里我们将1421移动到root cgroup dev@ubuntu:~/cgroup/demo/test$ sudo sh -c 'echo 1421 \u003e ../cgroup.procs' dev@ubuntu:~/cgroup/demo/test$ cat cgroup.procs 16515 #移动1421到另一个cgroup之后，它的子进程不会随着移动 #--------------------------第一个shell窗口---------------------- ##回到第一个shell窗口，进行清理工作 #先用ctrl+c退出top命令 dev@ubuntu:~/cgroup/demo/test$ cd .. #然后删除创建的cgroup dev@ubuntu:~/cgroup/demo$ sudo rmdir test ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:2:3","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"cgroup.procs vs tasks #创建两个新的cgroup用于演示 dev@ubuntu:~/cgroup/demo$ sudo mkdir c1 c2 #为了便于操作，先给root账号设置一个密码，然后切换到root账号 dev@ubuntu:~/cgroup/demo$ sudo passwd root dev@ubuntu:~/cgroup/demo$ su root root@ubuntu:/home/dev/cgroup/demo# #系统中找一个有多个线程的进程 root@ubuntu:/home/dev/cgroup/demo# ps -efL|grep /lib/systemd/systemd-timesyncd systemd+ 610 1 610 0 2 01:52 ? 00:00:00 /lib/systemd/systemd-timesyncd systemd+ 610 1 616 0 2 01:52 ? 00:00:00 /lib/systemd/systemd-timesyncd #进程610有两个线程，分别是610和616 #将616加入c1/cgroup.procs root@ubuntu:/home/dev/cgroup/demo# echo 616 \u003e c1/cgroup.procs #由于cgroup.procs存放的是进程ID，所以这里看到的是616所属的进程ID（610） root@ubuntu:/home/dev/cgroup/demo# cat c1/cgroup.procs 610 #从tasks中的内容可以看出，虽然只往cgroup.procs中加了线程616， #但系统已经将这个线程所属的进程的所有线程都加入到了tasks中， #说明现在整个进程的所有线程已经处于c1中了 root@ubuntu:/home/dev/cgroup/demo# cat c1/tasks 610 616 #将616加入c2/tasks中 root@ubuntu:/home/dev/cgroup/demo# echo 616 \u003e c2/tasks #这时我们看到虽然在c1/cgroup.procs和c2/cgroup.procs里面都有610， #但c1/tasks和c2/tasks中包含了不同的线程，说明这个进程的两个线程分别属于不同的cgroup root@ubuntu:/home/dev/cgroup/demo# cat c1/cgroup.procs 610 root@ubuntu:/home/dev/cgroup/demo# cat c1/tasks 610 root@ubuntu:/home/dev/cgroup/demo# cat c2/cgroup.procs 610 root@ubuntu:/home/dev/cgroup/demo# cat c2/tasks 616 #通过tasks，我们可以实现线程级别的管理，但通常情况下不会这么用， #并且在cgroup V2以后，将不再支持该功能，只能以进程为单位来配置cgroup #清理 root@ubuntu:/home/dev/cgroup/demo# echo 610 \u003e ./cgroup.procs root@ubuntu:/home/dev/cgroup/demo# rmdir c1 root@ubuntu:/home/dev/cgroup/demo# rmdir c2 root@ubuntu:/home/dev/cgroup/demo# exit exit 结论：将线程ID加到 cgroup1的 cgroup.procs 时，会把线程对应进程ID加入 cgroup.procs 且还会把当前进程下的全部线程ID加入到 tasks 中。 这里看起来，进程和线程好像效果是一样的。 区别来了，如果此时把某个线程ID移动到另外的 cgroup2 的 tasks 中，会自动把 线程ID对应的进程ID加入到 cgroup2 的 cgroup.procs 中，且只把对应线程加入 tasks 中。 此时 cgroup1和cgroup2 的 cgroup.procs 都包含了同一个进程ID，但是二者的 tasks 中却包含了不同的线程ID。 这样就实现了线程粒度的控制。但通常情况下不会这么用，并且在cgroup V2以后，将不再支持该功能，只能以进程为单位来配置cgroup。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:2:4","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"3. 演示 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:3:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"Docker 是如何使用 cgroup 的 我们知道 Docker 是通过 Cgroups 实现容器资源限制和监控的，那么具体是怎么用的呢？ 先启动一个容器： [root@iZ2zefmrr626i66omb40ryZ memory]# docker run -itd -m 128m nginx da82f9ebd384730dda7f831b4331c9e55893c100c83c0c9b0ce112436aa93416 这里通过docker run -m参数设置了内存限制。 实际上 docker 会在 memory cgroup 上创建一个叫 docker 的子 cgroup $ ls -l /sys/fs/cgroup/memory/docker/ -rw-r--r-- 1 root root 0 Jan 6 19:53 cgroup.clone_children --w--w--w- 1 root root 0 Jan 6 19:53 cgroup.event_control -rw-r--r-- 1 root root 0 Jan 6 19:53 cgroup.procs # 可以发现这一长串ID和创建容器时打印的是一致的 drwxr-xr-x 2 root root 0 Jan 6 19:56 da82f9ebd384730dda7f831b4331c9e55893c100c83c0c9b0ce112436aa93416 # 省略其他文件 说明 docker 是为每个容器创建了一个子 cgroup 来单独限制。 [root@iZ2zefmrr626i66omb40ryZ docker]# cd da82f9ebd384730dda7f831b4331c9e55893c100c83c0c9b0ce112436aa93416/ [root@iZ2zefmrr626i66omb40ryZ da82f9ebd384730dda7f831b4331c9e55893c100c83c0c9b0ce112436aa93416]# cat memory.limit_in_bytes 134217728 [root@iZ2zefmrr626i66omb40ryZ da82f9ebd384730dda7f831b4331c9e55893c100c83c0c9b0ce112436aa93416]# 可以发现，这里面限制的内存 134217728/1024/1024 刚好就是我们指定的 128M。 所以 docker 使用 cgroup 其实很简单，就是根据用户指定的参数创建对应的 cgroup 限制。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:3:1","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"Go Demo 其实挺简单的，就是用 Go 翻译了一遍上面的命令。 具体代码如下： // cGroups cGroups初体验 func cGroups() { // /proc/self/exe是一个符号链接，代表当前程序的绝对路径 if os.Args[0] == \"/proc/self/exe\" { // 第一个参数就是当前执行的文件名，所以只有fork出的容器进程才会进入该分支 fmt.Printf(\"容器进程内部 PID %d\\n\", syscall.Getpid()) // 需要先在宿主机上安装 stress 比如 apt-get install stress cmd := exec.Command(\"sh\", \"-c\", `stress --vm-bytes 200m --vm-keep -m 1`) cmd.SysProcAttr = \u0026syscall.SysProcAttr{} cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil { fmt.Println(err) os.Exit(1) } } else { // 主进程会走这个分支 cmd := exec.Command(\"/proc/self/exe\") cmd.SysProcAttr = \u0026syscall.SysProcAttr{Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWNS | syscall.CLONE_NEWPID} cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Start(); err != nil { fmt.Println(err) os.Exit(1) } // 得到 fork 出来的进程在外部namespace 的 pid fmt.Println(\"fork 进程 PID：\", cmd.Process.Pid) // 在默认的 memory cgroup 下创建子目录，即创建一个子 cgroup err := os.Mkdir(filepath.Join(cgroupMemoryHierarchyMount, \"testmemorylimit\"), 0755) if err != nil { fmt.Println(err) } // 将容器加入到这个 cgroup 中，即将进程PID加入到cgroup下的 cgroup.procs 文件中 err = ioutil.WriteFile(filepath.Join(cgroupMemoryHierarchyMount, \"testmemorylimit\", \"cgroup.procs\"), []byte(strconv.Itoa(cmd.Process.Pid)), 0644) if err != nil { fmt.Println(err) os.Exit(1) } // 限制进程的内存使用，往 memory.limit_in_bytes 文件中写入数据 err = ioutil.WriteFile(filepath.Join(cgroupMemoryHierarchyMount, \"testmemorylimit\", \"memory.limit_in_bytes\"), []byte(\"100m\"), 0644) if err != nil { fmt.Println(err) os.Exit(1) } cmd.Process.Wait() } } 首先是一个 if 判断，区分主进程和子进程，分别执行不同逻辑。 主进程：fork 出子进程，并创建 cgroup，然后将子进程加入该 cgrouop 子进程：执行 stress 命令，以消耗内存，便于查看 memory cgroup 的效果 运行并测试： lixd  ~/projects/docker/mydocker main $ go build main.go lixd  ~/projects/docker/mydocker main $ sudo ./main fork 进程 PID： 21827 当前进程 pid 1 stress: info: [7] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd 根据输出可以知道，我们 fork 出的进程，pid 为 21827。 通过pstree -pl查看进程关系： $pstree -pl init(1)─┬─init(8)───init(9)───fsnotifier-wsl(10) ├─init(12)───init(13)─┬─exe(20618)─┬─sh(20623)───stress(20624)───stress(20625) │ │ ├─{exe}(20619) │ │ ├─{exe}(20620) │ │ ├─{exe}(20621) │ │ └─{exe}(20622) └─zsh(14)───sudo(21821)───main(21822)─┬─exe(21827)─┬─sh(21832)───stress(21833)───stress(21834) 可以看到 21827 进程 最终启动了一个 21834 的 stress 进程。 top查看以下内存占用： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 21834 root 20 0 208664 101564 272 D 35.2 1.3 0:14.38 stress 可以看到 RES 101564，也就是刚好100M，说明我们的 cgroup 是有效果的。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:3:2","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"4. 小结 本文主要介绍了 hierarchy 和 cgroup 相关的操作，如创建删除。 接着介绍了 hierarchy 中各个文件含义，重点包括 release_agent 的作用以及 cgroup.procs 和 tasks 的区别。 最后简答介绍了 Docker 是如何使用 cgroup 的，并提供了一个简单的 Go 语言操作 cgroup demo。 ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:4:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"5. 参考 cgroups(7) — Linux manual page Linux Cgroup系列（02）：创建并管理cgroup ","date":"2022-02-26","objectID":"/posts/docker/08-cgroups-3/:5:0","tags":["Docker"],"title":"Docker教程(八)---Cgroups-3-相关命令汇总及Go Demo","uri":"/posts/docker/08-cgroups-3/"},{"categories":["Docker"],"content":"Cgroups 中的pids、cpu、memory 3个 subsystem 演示","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"本章主要演示以下 cgroups 下各个 subsystem 的作用。 根据难易程度，依次演示了 pids 、cpu 和 memory 3 个 subsystem 的使用。 注：本文所有操作在 Ubuntu20.04 下进行。 准备跟着《自己动手写 docker》这本书从零开始实现一个简易版的 docker，加深对 docker 的理解。 源码及相关教程见 Github，欢迎 star。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:0:0","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"1. pids pids subsystem 功能是限制cgroup及其所有子孙cgroup里面能创建的总的task数量。 注意：这里的task指通过fork和clone函数创建的进程，由于clone函数也能创建线程（在Linux里面，线程是一种特殊的进程），所以这里的task也包含线程，本文统一以进程来代表task，即本文中的进程代表了进程和线程 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:0","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"创建子cgroup 创建子cgroup，取名为test #进入目录/sys/fs/cgroup/pids/并新建一个目录，即创建了一个子cgroup lixd  /home/lixd $ cd /sys/fs/cgroup/pids lixd  /sys/fs/cgroup/pids $ sudo mkdir test 再来看看test目录下的文件 lixd  /sys/fs/cgroup/pids $ cd test #除了上一篇中介绍的那些文件外，多了两个文件 lixd  /sys/fs/cgroup/pids/test $ ls cgroup.clone_children cgroup.procs notify_on_release pids.current pids.events pids.max tasks 下面是这两个文件的含义： pids.current: 表示当前cgroup及其所有子孙cgroup中现有的总的进程数量 pids.max: 当前cgroup及其所有子孙cgroup中所允许创建的总的最大进程数量 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:1","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"限制进程数 首先是将当前bash加入到cgroup中，并修改pids.max的值，为了便于测试，这里就限制为1： #--------------------------第一个shell窗口---------------------- # 将当前bash进程加入到该cgroup root@DESKTOP-9K4GB6E:/sys/fs/cgroup/pids/test# echo $$ \u003e cgroup.procs #将pids.max设置为1，即当前cgroup只允许有一个进程 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/pids/test# echo 1 \u003e pids.max 由于 bash 已经占用了一个进程，所以此时 bash 中已经无法创建新的进程了： root@DESKTOP-9K4GB6E:/sys/fs/cgroup/pids/test# ls bash: fork: retry: Resource temporarily unavailable bash: fork: retry: Resource temporarily unavailable bash: fork: retry: Resource temporarily unavailable 创建新进程失败，于是命令运行失败，说明限制生效。 打开另一个 shell 查看 lixd  /mnt/c/Users/意琦行 $ cd /sys/fs/cgroup/pids/test lixd  /sys/fs/cgroup/pids/test $ ls cgroup.clone_children cgroup.procs notify_on_release pids.current pids.events pids.max tasks lixd  /sys/fs/cgroup/pids/test $ cat pids.current 1 果然，pids.current 为 1，已经到 pids.max 的限制了。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:2","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"当前cgroup和子cgroup之间的关系 当前cgroup中的pids.current和pids.max代表了当前cgroup及所有子孙cgroup的所有进程，所以子孙cgroup中的pids.max大小不能超过父cgroup中的大小。 如果子cgroup中的pids.max设置的大于父cgroup里的大小，会怎么样？ 答案是子cgroup中的进程不光受子cgroup限制，还要受其父cgroup的限制。 #继续使用上面的两个窗口 #--------------------------第二个shell窗口---------------------- #将pids.max设置成2 dev@dev:/sys/fs/cgroup/pids/test$ echo 2 \u003e pids.max #在test下面创建一个子cgroup dev@dev:/sys/fs/cgroup/pids/test$ mkdir subtest dev@dev:/sys/fs/cgroup/pids/test$ cd subtest/ #将subtest的pids.max设置为5 dev@dev:/sys/fs/cgroup/pids/test/subtest$ echo 5 \u003e pids.max #将当前bash进程加入到subtest中 dev@dev:/sys/fs/cgroup/pids/test/subtest$ echo $$ \u003e cgroup.procs #--------------------------第三个shell窗口---------------------- #重新打开一个bash窗口，看一下test和subtest里面的数据 #test里面的数据如下： dev@dev:~$ cd /sys/fs/cgroup/pids/test dev@dev:/sys/fs/cgroup/pids/test$ cat pids.max 2 #这里为2表示目前test和subtest里面总的进程数为2 dev@dev:/sys/fs/cgroup/pids/test$ cat pids.current 2 dev@dev:/sys/fs/cgroup/pids/test$ cat cgroup.procs 3083 #subtest里面的数据如下： dev@dev:/sys/fs/cgroup/pids/test$ cat subtest/pids.max 5 dev@dev:/sys/fs/cgroup/pids/test$ cat subtest/pids.current 1 dev@dev:/sys/fs/cgroup/pids/test$ cat subtest/cgroup.procs 3185 #--------------------------第一个shell窗口---------------------- #回到第一个窗口，随便运行一个命令，由于test里面的pids.current已经等于pids.max了， #所以创建新进程失败，于是命令运行失败，说明限制生效 dev@dev:/sys/fs/cgroup/pids/test$ ls -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: Resource temporarily unavailable #--------------------------第二个shell窗口---------------------- #回到第二个窗口，随便运行一个命令，虽然subtest里面的pids.max还大于pids.current， #但由于其父cgroup “test”里面的pids.current已经等于pids.max了， #所以创建新进程失败，于是命令运行失败，说明子cgroup中的进程数不仅受自己的pids.max的限制，还受祖先cgroup的限制 dev@dev:/sys/fs/cgroup/pids/test/subtest$ ls -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: retry: No child processes -bash: fork: Resource temporarily unavailable ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:3","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"pids.current \u003e pids.max的情况 并不是所有情况下都是pids.max \u003e= pids.current，在下面两种情况下，会出现pids.max \u003c pids.current 的情况： 设置pids.max时，将其值设置的比pids.current小 将其他进程加入到当前cgroup有可能会导致pids.current \u003e pids.max 因为 pids.max 只会在当前cgroup中的进程fork、clone的时候生效，将其他进程加入到当前cgroup时，不会检测pids.max，所以可能触发这种情况 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:4","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"小结 总的来说，pids subsystem 是比较简单的。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:1:5","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"2. cpu 在cgroup里面，跟CPU相关的子系统有 cpusets、cpuacct 和 cpu。 其中 cpuset 主要用于设置 CPU 的亲和性，可以限制 cgroup 中的进程只能在指定的 CPU 上运行，或者不能在指定的 CPU上运行，同时 cpuset 还能设置内存的亲和性。设置亲和性一般只在比较特殊的情况才用得着，所以这里不做介绍。 cpuacct 包含当前 cgroup 所使用的 CPU 的统计信息，信息量较少，有兴趣可以去看看它的文档，这里不做介绍。 本节只介绍 cpu 子系统，包括怎么限制 cgroup 的 CPU 使用上限及相对于其它 cgroup 的相对值。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:2:0","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"创建子 cgroup 通用是创建子目录即可。 #进入/sys/fs/cgroup/cpu并创建子cgroup root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu# cd /sys/fs/cgroup/cpu root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu# mkdir test root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu# cd test/ root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu/test# ls cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 看起来文件比 memory subsystem 还是少一些。 cpu.cfs_period_us \u0026 cpu.cfs_quota_us：两个文件配合起来设置CPU的使用上限，两个文件的单位都是微秒（us）。 cfs_period_us：用来配置时间周期长度 取值范围为1毫秒（ms）到1秒（s） cfs_quota_us：用来配置当前cgroup在设置的周期长度内所能使用的CPU时间数 取值大于1ms即可 默认值为 -1，表示不受cpu时间的限制。 cpu.shares用来设置CPU的相对值（比例），并且是针对所有的CPU（内核），默认值是1024。 假如系统中有两个cgroup，分别是A和B，A的shares值是1024，B的shares值是512，那么A将获得1024/(1204+512)=66%的CPU资源，而B将获得33%的CPU资源。 shares有两个特点： 如果A不忙，没有使用到66%的CPU时间，那么剩余的CPU时间将会被系统分配给B，即B的CPU使用率可以超过33% 如果添加了一个新的cgroup C，且它的shares值是1024，那么A的限额变成了1024/(1204+512+1024)=40%，B的变成了20% 从上面两个特点可以看出： 在闲的时候，shares基本上不起作用，只有在CPU忙的时候起作用，这是一个优点。 由于shares是一个绝对值，需要和其它cgroup的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制CPU使用率。 cpu.stat包含了下面三项统计结果： nr_periods： 表示过去了多少个cpu.cfs_period_us里面配置的时间周期 nr_throttled： 在上面的这些周期中，有多少次是受到了限制（即cgroup中的进程在指定的时间周期中用光了它的配额） throttled_time: cgroup中的进程被限制使用CPU持续了多长时间(纳秒) ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:2:1","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"演示 #继续使用上面创建的子cgroup： test #设置只能使用1个cpu的20%的时间 dev@ubuntu:/sys/fs/cgroup/cpu,cpuacct/test$ sudo sh -c \"echo 50000 \u003e cpu.cfs_period_us\" dev@ubuntu:/sys/fs/cgroup/cpu,cpuacct/test$ sudo sh -c \"echo 10000 \u003e cpu.cfs_quota_us\" #将当前bash加入到该cgroup dev@ubuntu:/sys/fs/cgroup/cpu,cpuacct/test$ echo $$ 5456 dev@ubuntu:/sys/fs/cgroup/cpu,cpuacct/test$ sudo sh -c \"echo 5456 \u003e cgroup.procs\" #在bash中启动一个死循环来消耗cpu，正常情况下应该使用100%的cpu（即消耗一个内核） dev@ubuntu:/sys/fs/cgroup/cpu,cpuacct/test$ while :; do echo test \u003e /dev/null; done #--------------------------重新打开一个shell窗口---------------------- #通过top命令可以看到5456的CPU使用率为20%左右，说明被限制住了 #不过这时系统的%us+%sy在10%左右，那是因为我测试的机器上cpu是双核的， #所以系统整体的cpu使用率为10%左右 dev@ubuntu:~$ top Tasks: 139 total, 2 running, 137 sleeping, 0 stopped, 0 zombie %Cpu(s): 5.6 us, 6.2 sy, 0.0 ni, 88.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 499984 total, 15472 free, 81488 used, 403024 buff/cache KiB Swap: 0 total, 0 free, 0 used. 383332 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 5456 dev 20 0 22640 5472 3524 R 20.3 1.1 0:04.62 bash #这时可以看到被限制的统计结果 dev@ubuntu:~$ cat /sys/fs/cgroup/cpu,cpuacct/test/cpu.stat nr_periods 1436 nr_throttled 1304 throttled_time 51542291833 # cfs_period_us 值为 10W root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu/test# cat cpu.cfs_period_us 100000 # 往 cfs_quota_us 写入 20000，即限制只能使用20%cpu root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu/test# echo 20000 \u003e cpu.cfs_quota_us # 新开一个窗口，运行一个死循环 $ while : ; do : ; done \u0026 [1] 519 # top 看一下 cpu 占用率，果然是100%了 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 519 lixd 25 5 13444 2912 0 R 100.0 0.0 0:05.66 zsh # 回到第一个shell窗口，限制当前进程的cpu使用率 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu/test# echo 519 \u003e\u003e cgroup.procs # 再切回第二个窗口，发现519进程的cpu已经降到20%了，说明限制生效了 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 519 lixd 25 5 13444 2912 0 R 20.0 0.0 0:31.86 zsh # 查看被限制的统计结果 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/cpu/test# cat cpu.stat nr_periods 2090 nr_throttled 2088 throttled_time 166752684900 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:2:2","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"小结 使用 cgroup 限制 CPU 的使用率比较纠结，用 cfs_period_us \u0026 cfs_quota_us 吧，限制死了，没法充分利用空闲的 CPU，用 shares 吧，又没法配置百分比，极其难控制。总之，使用 cgroup 的 cpu 子系统需谨慎。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:2:3","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"3. memory 相比之下 memory subsystem 就要复杂许多。 为什么需要内存控制 站在一个普通Linux开发者的角度，如果能控制一个或者一组进程所能使用的内存数，那么就算代码有bug，内存泄漏也不会对系统造成影响，因为可以设置内存使用量的上限，当到达这个值之后可以将进程重启。 站在一个系统管理者的角度，如果能限制每组进程所能使用的内存量，那么不管程序的质量如何，都能将它们对系统的影响降到最低，从而保证整个系统的稳定性。 内存控制能控制些什么？ 限制cgroup中所有进程所能使用的物理内存总量 限制cgroup中所有进程所能使用的物理内存+交换空间总量(CONFIG_MEMCG_SWAP)： 一般在server上，不太会用到swap空间，所以不在这里介绍这部分内容。 限制cgroup中所有进程所能使用的内核内存总量及其它一些内核资源(CONFIG_MEMCG_KMEM)： 限制内核内存有什么用呢？其实限制内核内存就是限制当前cgroup所能使用的内核资源，比如进程的内核栈空间，socket所占用的内存空间等，通过限制内核内存，当内存吃紧时，可以阻止当前cgroup继续创建进程以及向内核申请分配更多的内核资源。由于这块功能被使用的较少，本篇中也不对它做介绍。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:0","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"创建子cgroup 在/sys/fs/cgroup/memory下创建一个子目录即创建了一个子cgroup root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory# cd /sys/fs/cgroup/memory root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory# mkdir test root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory# ls test/ cgroup.clone_children memory.kmem.tcp.max_usage_in_bytes memory.oom_control cgroup.event_control memory.kmem.tcp.usage_in_bytes memory.pressure_level cgroup.procs memory.kmem.usage_in_bytes memory.soft_limit_in_bytes memory.failcnt memory.limit_in_bytes memory.stat memory.force_empty memory.max_usage_in_bytes memory.swappiness memory.kmem.failcnt memory.memsw.failcnt memory.usage_in_bytes memory.kmem.limit_in_bytes memory.memsw.limit_in_bytes memory.use_hierarchy memory.kmem.max_usage_in_bytes memory.memsw.max_usage_in_bytes notify_on_release memory.kmem.tcp.failcnt memory.memsw.usage_in_bytes tasks memory.kmem.tcp.limit_in_bytes memory.move_charge_at_immigrate 从上面ls的输出可以看出，除了每个cgroup都有的那几个文件外，和memory相关的文件还不少,这里先做个大概介绍(kernel相关的文件除外)，后面会详细介绍每个文件的作用： cgroup.event_control #用于eventfd的接口 memory.usage_in_bytes #显示当前已用的内存 memory.limit_in_bytes #设置/显示当前限制的内存额度 memory.failcnt #显示内存使用量达到限制值的次数 memory.max_usage_in_bytes #历史内存最大使用量 memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度 memory.stat #显示当前cgroup的内存使用情况 memory.use_hierarchy #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面 memory.force_empty #触发系统立即尽可能的回收当前cgroup中可以回收的内存 memory.pressure_level #设置内存压力的通知事件，配合cgroup.event_control一起使用 memory.swappiness #设置和显示当前的swappiness memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去 memory.oom_control #设置/显示oom controls相关的配置 memory.numa_stat #显示numa相关的内存 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:1","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"添加进程 也是往cgroup中添加进程只要将进程号写入cgroup.procs就可以了。 #重新打开一个shell窗口，避免相互影响 root@DESKTOP-9K4GB6E:~# cd /sys/fs/cgroup/memory/test/ root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo $$ \u003e\u003e cgroup.procs #运行top命令，这样这个cgroup消耗的内存会多点，便于观察 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# top # 后续操作不再在这个窗口进行，避免在这个bash中运行进程影响cgropu里面的进程数及相关统计 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:2","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"设置限额 设置限额很简单，写文件memory.limit_in_bytes就可以了。 echo 1M \u003e memory.limit_in_bytes：限制只能用1M内存 echo -1 \u003e memory.limit_in_bytes：-1则是不限制 #回到第一个shell窗口 #开始设置之前，看看当前使用的内存数量，这里的单位是字节 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.usage_in_bytes 2379776 #设置1M的限额 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 1M \u003e memory.limit_in_bytes #设置完之后记得要查看一下这个文件，因为内核要考虑页对齐, 所以生效的数量不一定完全等于设置的数量 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.usage_in_bytes 950272 #如果不再需要限制这个cgroup，写-1到文件memory.limit_in_bytes即可 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo -1 \u003e memory.limit_in_bytes #这时可以看到limit被设置成了一个很大的数字 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.limit_in_bytes 9223372036854771712 如果设置的限额比当前已经使用的内存少呢？ root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# free -h total used free shared buff/cache available Mem: 7.7Gi 253Mi 7.4Gi 0.0Ki 95Mi 7.3Gi Swap: 2.0Gi 0.0Ki 2.0Gi # 此时用了 1232K root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.usage_in_bytes 1232896 # 限制成500K root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 500k \u003e memory.limit_in_bytes # 再次查看发现现在只用了401K root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.usage_in_bytes 401408 # 发现swap多了1M，说明另外的数据被转移到swap上了 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# free -h total used free shared buff/cache available Mem: 7.7Gi 254Mi 7.4Gi 0.0Ki 94Mi 7.3Gi Swap: 2.0Gi 1.0Mi 2.0Gi #这个时候再来看failcnt，发现有381次之多(隔几秒再看这个文件，发现次数在增长) root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.failcnt 381 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.failcnt 385 #再看看memory.stat（这里只显示部分内容），发现物理内存用了400K， #但有很多pgmajfault以及pgpgin和pgpgout，说明发生了很多的swap in和swap out root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.stat swap 946176 # 946K 差不多刚好是内存中少的量 pgpgin 30492 pgpgout 30443 pgfault 23859 pgmajfault 12507 从上面的结果可以看出，当物理内存不够时，就会触发memory.failcnt里面的数量加1，但进程不会被kill掉，那是因为内核会尝试将物理内存中的数据移动到swap空间中，从而让内存分配成功。 如果设置的限额过小，就算swap out部分内存后还是不够会怎么样？ #--------------------------第一个shell窗口---------------------- # 限制到100k root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 100K \u003e memory.limit_in_bytes #--------------------------第二个shell窗口---------------------- # 尝试执行 top 发现刚运行就被Kill了 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# top Killed 从上面的这些测试可以看出，一旦设置了内存限制，将立即生效，并且当物理内存使用量达到limit的时候，memory.failcnt的内容会加1，但这时进程不一定就会 被kill掉，内核会尽量将物理内存中的数据移到swap空间上去，如果实在是没办法移动了（设置的limit过小，或者swap空间不足），默认情况下，就会kill掉 cgroup里面继续申请内存的进程。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:3","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"触发控制 通过修改memory.oom_control文件，可以控制 subsystem 在物理内存达到上限时的行为。文件中包含以下3个参数： oom_kill_disable：是否启用 oom kill 0：关闭 1：开启 under_oom：表示当前是否已经进入oom状态，也即是否有进程被暂停了。 oom_kill：oom 后是否执行 kill 1：启动，oom 后直接 kill 掉对应进程 2：关闭：当内核无法给进程分配足够的内存时，将会暂停该进程直到有空余的内存之后再继续运行。同时会更新 under_oom 状态 注意：root cgroup的oom killer是不能被禁用的 为了演示OOM-killer的功能，创建了下面这样一个程序，用来向系统申请内存，它会每秒消耗1M的内存。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e #include \u003cunistd.h\u003e #define MB (1024 * 1024) int main(int argc, char *argv[]) { char *p; int i = 0; while(1) { p = (char *)malloc(MB); memset(p, 0, MB); printf(\"%dM memory allocated\\n\", ++i); sleep(1); } return 0; } 保存上面的程序到文件~/mem-allocate.c，然后编译并测试 #--------------------------第一个shell窗口---------------------- #编译上面的文件 dev@dev:/sys/fs/cgroup/memory/test$ gcc ~/mem-allocate.c -o ~/mem-allocate #设置内存限额为5M dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 5M \u003e memory.limit_in_bytes\" #将当前bash加入到test中，这样这个bash创建的所有进程都会自动加入到test中 dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo $$ \u003e\u003e cgroup.procs\" #默认情况下，memory.oom_control的值为0，即默认启用oom killer dev@dev:/sys/fs/cgroup/memory/test$ cat memory.oom_control oom_kill_disable 0 under_oom 0 #为了避免受swap空间的影响，设置swappiness为0来禁止当前cgroup使用swap dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 0 \u003e memory.swappiness\" #当分配第5M内存时，由于总内存量超过了5M，所以进程被kill了 dev@dev:/sys/fs/cgroup/memory/test$ ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated Killed #设置oom_control为1，这样内存达到限额的时候会暂停 dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 1 \u003e\u003e memory.oom_control\" #跟预期的一样，程序被暂停了 dev@dev:/sys/fs/cgroup/memory/test$ ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated #--------------------------第二个shell窗口---------------------- #再打开一个窗口 dev@dev:~$ cd /sys/fs/cgroup/memory/test/ #这时候可以看到memory.oom_control里面under_oom的值为1，表示当前已经oom了 dev@dev:/sys/fs/cgroup/memory/test$ cat memory.oom_control oom_kill_disable 1 under_oom 1 #修改test的额度为7M dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 7M \u003e memory.limit_in_bytes\" #--------------------------第一个shell窗口---------------------- #再回到第一个窗口，会发现进程mem-allocate继续执行了两步，然后暂停在6M那里了 dev@dev:/sys/fs/cgroup/memory/test$ ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated 5M memory allocated 6M memory allocated # 创建上面的文件并编译 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# vim ~/mem-allocate.c root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# gcc ~/mem-allocate.c -o ~/mem-allocate # 限制5M的上限 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 5M \u003e memory.limit_in_bytes #将当前bash加入到test中，这样这个bash创建的所有进程都会自动加入到test中 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo $$ \u003e\u003e cgroup.procs #默认情况下，会启用oom killer root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.oom_control oom_kill_disable 0 under_oom 0 oom_kill 1 #为了避免受swap空间的影响，设置swappiness为0来禁止当前cgroup使用swap root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 0 \u003e memory.swappiness #当分配第5M内存时，由于总内存量超过了5M，所以进程被kill了 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated Killed #设置oom_control为1，这样内存达到限额的时候会暂停 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 1 \u003e\u003e memory.oom_control #跟预期的一样，程序被暂停了 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated #--------------------------第二个shell窗口---------------------- #再打开一个窗口 dev@dev:~$ cd /sys/fs/cgroup/memory/test/ #这时候可以看到memory.oom_control里面under_oom的值为1，表示当前已经oom了 root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# cat memory.oom_control oom_kill_disable 1 under_oom 1 oom_kill 2 #修改test的额度为7M root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# echo 7M \u003e memory.limit_in_bytes # 切换会第一个窗口，发送程序又跑了两步，停在了6M root@DESKTOP-9K4GB6E:/sys/fs/cgroup/memory/test# ~/mem-allocate 1M memory allocated 2M memory allocated 3M memory allocated 4M memory allocated 5M memory allocated 6M memory alloc","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:4","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"其他 进程迁移（migration） 当一个进程从一个cgroup移动到另一个cgroup时，默认情况下，该进程已经占用的内存还是统计在原来的cgroup里面，不会占用新cgroup的配额，但新分配的内存会统计到新的cgroup中（包括swap out到交换空间后再swap in到物理内存中的部分）。 我们可以通过设置memory.move_charge_at_immigrate让进程所占用的内存随着进程的迁移一起迁移到新的cgroup中。 enable： echo 1 \u003e memory.move_charge_at_immigrate disable：echo 0 \u003e memory.move_charge_at_immigrate 注意: 就算设置为1，但如果不是thread group的leader，这个task占用的内存也不能被迁移过去。换句话说，如果以线程为单位进行迁移，必须是进程的第一个线程，如果以进程为单位进行迁移，就没有这个问题。 当memory.move_charge_at_immigrate被设置成1之后，进程占用的内存将会被统计到目的cgroup中，如果目的cgroup没有足够的内存，系统将尝试回收目的cgroup的部分内存（和系统内存紧张时的机制一样，删除不常用的file backed的内存或者swap out到交换空间上，请参考Linux内存管理），如果回收不成功，那么进程迁移将失败。 注意：迁移内存占用数据是比较耗时的操作。 移除cgroup 当memory.move_charge_at_immigrate为0时，就算当前cgroup中里面的进程都已经移动到其它cgropu中去了，由于进程已经占用的内存没有被统计过去，当前cgroup有可能还占用很多内存，当移除该cgroup时，占用的内存需要统计到谁头上呢？答案是依赖memory.use_hierarchy的值，如果该值为0，将会统计到root cgroup里；如果值为1，将统计到它的父cgroup里面。 force_empty 当向memory.force_empty文件写入0时（echo 0 \u003e memory.force_empty），将会立即触发系统尽可能的回收该cgroup占用的内存。该功能主要使用场景是移除cgroup前（cgroup中没有进程），先执行该命令，可以尽可能的回收该cgropu占用的内存，这样迁移内存的占用数据到父cgroup或者root cgroup时会快些。 memory.swappiness 该文件的值默认和全局的swappiness（/proc/sys/vm/swappiness）一样，修改该文件只对当前cgroup生效，其功能和全局的swappiness一样，请参考Linux交换空间中关于swappiness的介绍。 注意：有一点和全局的swappiness不同，那就是如果这个文件被设置成0，就算系统配置的有交换空间，当前cgroup也不会使用交换空间。 memory.use_hierarchy 该文件内容为0时，表示不使用继承，即父子cgroup之间没有关系；当该文件内容为1时，子cgroup所占用的内存会统计到所有祖先cgroup中。 如果该文件内容为1，当一个cgroup内存吃紧时，会触发系统回收它以及它所有子孙cgroup的内存。 注意: 当该cgroup下面有子cgroup或者父cgroup已经将该文件设置成了1，那么当前cgroup中的该文件就不能被修改。 #当前cgroup和父cgroup里都是1 dev@dev:/sys/fs/cgroup/memory/test$ cat memory.use_hierarchy 1 dev@dev:/sys/fs/cgroup/memory/test$ cat ../memory.use_hierarchy 1 #由于父cgroup里面的值为1，所以修改当前cgroup的值失败 dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 0 \u003e ./memory.use_hierarchy\" sh: echo: I/O error #由于父cgroup里面有子cgroup（至少有当前cgroup这么一个子cgroup）， #修改父cgroup里面的值也失败 dev@dev:/sys/fs/cgroup/memory/test$ sudo sh -c \"echo 0 \u003e ../memory.use_hierarchy\" sh: echo: I/O error memory.soft_limit_in_bytes 有了hard limit（memory.limit_in_bytes），为什么还要soft limit呢？hard limit是一个硬性标准，绝对不能超过这个值，而soft limit可以被超越，既然能被超越，要这个配置还有啥用？先看看它的特点 当系统内存充裕时，soft limit不起任何作用 当系统内存吃紧时，系统会尽量的将cgroup的内存限制在soft limit值之下（内核会尽量，但不100%保证） 从它的特点可以看出，它的作用主要发生在系统内存吃紧时，如果没有soft limit，那么所有的cgroup一起竞争内存资源，占用内存多的cgroup不会让着内存占用少的cgroup，这样就会出现某些cgroup内存饥饿的情况。如果配置了soft limit，那么当系统内存吃紧时，系统会让超过soft limit的cgroup释放出超过soft limit的那部分内存（有可能更多），这样其它cgroup就有了更多的机会分配到内存。 从上面的分析看出，这其实是系统内存不足时的一种妥协机制，给次等重要的进程设置soft limit，当系统内存吃紧时，把机会让给其它重要的进程。 注意： 当系统内存吃紧且cgroup达到soft limit时，系统为了把当前cgroup的内存使用量控制在soft limit下，在收到当前cgroup新的内存分配请求时，就会触发回收内存操作，所以一旦到达这个状态，就会频繁的触发对当前cgroup的内存回收操作，会严重影响当前cgroup的性能。 memory.pressure_level 这个文件主要用来监控当前cgroup的内存压力，当内存压力大时（即已使用内存快达到设置的限额），在分配内存之前需要先回收部分内存，从而影响内存分配速度，影响性能，而通过监控当前cgroup的内存压力，可以在有压力的时候采取一定的行动来改善当前cgroup的性能，比如关闭当前cgroup中不重要的服务等。目前有三种压力水平： low 意味着系统在开始为当前cgroup分配内存之前，需要先回收内存中的数据了，这时候回收的是在磁盘上有对应文件的内存数据。 medium 意味着系统已经开始频繁为当前cgroup使用交换空间了。 critical 快撑不住了，系统随时有可能kill掉cgroup中的进程。 如何配置相关的监听事件呢？和memory.oom_control类似，大概步骤如下： 利用函数eventfd(2)创建一个event_fd 打开文件memory.pressure_level，得到pressure_level_fd 往cgroup.event_control中写入这么一串：\u003cevent_fd\u003e \u003cpressure_level_fd\u003e \u003clevel\u003e 然后通过读event_fd得到通知 注意： 多个level可能要创建多个event_fd，好像没有办法共用一个（本人没有测试过） Memory thresholds 我们可以通过cgroup的事件通知机制来实现对内存的监控，当内存使用量穿过（变得高于或者低于）我们设置的值时，就会收到通知。使用方法和memory.oom_control类似，大概步骤如下： 利用函数eventfd(2)创建一个event_fd 打开文件memory.usage_in_bytes，得到usage_in_bytes_fd 往cgroup.event_control中写入这么一串：\u003cevent_fd\u003e \u003cusage_in_bytes_fd\u003e \u003cthreshold\u003e 然后通过读event_fd得到通知 stat file 这个文件包含的统计项比较细，需要一些内核的内存管理知识才能看懂，这里就不介绍了（怕说错）。详细信息可以参考Memory Resource Controller中的“5.2 stat file”。这里有几个需要注意的地方： 里面total开头的统计项包含了子cgroup的数据（前提条件是memory.use_hierarchy等于1）。 里面的’rss + file_mapped\"才约等于是我们常说的RSS（ps aux命令看到的RSS） 文件（动态库和可执行文件）及共享内存可以在多个进程之间共享，不过它们只会统计到他们的owner cgroup中的file_mapped去。（不确定是怎么定义owner的，但如果看到当前cgroup的file_mapped值很小，说明共享的数据没有算到它头上，而是其它的cgroup） ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:5","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"小结 本节没有介绍 swap 和 kernel 相关的内容，不过在实际使用过程中一定要留意 swap 空间，如果系统使用了交换空间，那么设置限额时一定要注意一点，那就是当 cgroup 的物理空间不够时，内核会将不常用的内存 swap out 到交换空间上，从而导致一直不触发 oom killer，而是不停的 swap out／in，导致 cgroup 中的进程运行速度很慢。 如果一定要用交换空间，最好的办法是限制 swap+物理内存 的额度，虽然我们在这篇中没有介绍这部分内容，但其使用方法和限制物理内存是一样的，只是换做写文件 memory.memsw.limit_in_bytes 罢了。 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:3:6","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"4. 参考 cgroups(7) — Linux manual page 美团技术团队—Linux资源管理之cgroups简介 Red Hat—资源管理指南 ","date":"2022-02-25","objectID":"/posts/docker/07-cgroups-2/:4:0","tags":["Docker"],"title":"Docker教程(七)---Cgroups-2-subsystem演示","uri":"/posts/docker/07-cgroups-2/"},{"categories":["Docker"],"content":"Cgroups 概念讲解及简单演示","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"Cgroups 是 linux 内核提供的功能，由于牵涉的概念比较多，所以不太容易理解。本文试图通过简单的描述和 Demo 帮助大家理解 Cgroups 。 注：本文所有操作在 Ubuntu20.04 下进行。 准备跟着《自己动手写 docker》这本书从零开始实现一个简易版的 docker，加深对 docker 的理解。 源码及相关教程见 Github，欢迎 star。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:0:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"1. 什么是 Cgroups Cgroups 是 Linux 下的一种将进程按组进行管理的机制，它提供了对一组进程及将来子进程的资源限制控制和统计的能力。 这些资源包括 CPU、内存、存储、网络等。通过 Cgroups 可以方便地限制某个进程的资源占用，并且可以实时地监控进程的监控与统计信息 Cgroups 分 v1 和 v2 两个版本： v1 实现较早，功能比较多，但是由于它里面的功能都是零零散散的实现的，所以规划的不是很好，导致了一些使用和维护上的不便。 v2 的出现就是为了解决 v1 的问题，在最新的4.5内核中，Cgroups v2 声称已经可以用于生产环境了，但它所支持的功能还很有限。 v1 和 v2 可以混合使用，但是这样会更复杂，所以一般没人会这样用。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:1:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"1. 三部分组件 Cgroups 主要包括下面几部分： cgroups本身：cgroup 是对进程分组管理的一种机制，一个 cgroup 包含一组进程，并可以在这个 cgroup上增加 Linux subsystem 的各种参数配置，将一组进程和一组 subsystem 的系统参数关联起来。 subsystem： 一个 subsystem 就是一个内核模块，他被关联到一颗cgroup 树之后，就会在树的每个节点（进程组）上做具体的操作。subsystem 经常被称作\"resource controller\"，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如 perf_event subsystem。到目前为止，Linux 支持 12种 subsystem，比如限制 CPU 的使用时间，限制使用的内存，统计 CPU 的使用情况，冻结和恢复一组进程等，后续会对它们一一进行介绍。 hierarchy：一个 hierarchy 可以理解为一棵 cgroup 树，树的每个节点就是一个进程组，每棵树都会与零到多个 subsystem 关联。在一颗树里面，会包含 Linux 系统中的所有进程，但每个进程只能属于一个节点（进程组）。系统中可以有很多颗 cgroup 树，每棵树都和不同的 subsystem 关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，只是这些进程组和不同的 subsystem 关联。目前 Linux 支持 12种 subsystem，如果不考虑不与任何 subsystem关联的情况（systemd 就属于这种情况），Linux 里面最多可以建12颗cgroup树，每棵树关联一个 subsystem，当然也可以只建一棵树，然后让这棵树关联所有的 subsystem。当一颗 cgroup树不和任何 subsystem 关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd 就是一个这样的例子。 3个部分间的关系 系统在创建了新的 hierarchy 之后,系统中所有的进程都会加入这个 hierarchy 的cgroup根节点,这个 cgroup 根节点是 hierarchy 默认创建的。 一个 subsystem 只能附加到 一 个 hierarchy 上面。 一个 hierarchy 可以附加多个 subsystem 。 一个进程可以作为多个 cgroup 的成员,但是这些 cgroup 必须在不同的 hierarchy 中。 一个进程fork出子进程时,子进程是和父进程在同一个 cgroup 中的,也可以根据需要将其移动到其他 cgroup 中。 个人理解： cgroup 用于对进程进行分组。 hierarchy 则根据继承关系，将多个 cgroup 组成一棵树。 subsystem 则负责资源限制的工作，将 subsystem 和 hierarchy 绑定后，该 hierarchy 上的所有 cgroup 下的进程都会被 subsystem 给限制。 子 cgroup 会继承父 cgroup 的 subsystem，但是子 cgroup 却可以自定义自己的配置 因此：使用时可以直接在某个已存在的 hierarchy 下创建子 cgroup 或者直接创建一个新的 hierarchy 。 注：后续的 cgroup树就指的是 hierarchy，cgroup 则指 hierarchy 上的节点。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:1:1","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"2. 具体架构 看完上面的描述，可能还是搞不清具体的关系，下面几幅图比较清晰的展示了 cgroup 中几部分组件的关系。 这部分内容来源于：美团技术团队—Linux资源管理之cgroups简介 hierarchy、cgroup、subsystem 3者的关系： 比如上图表示两个 hierarchiy，每一个 hierarchiy 中是一颗树形结构，树的每一个节点是一个 cgroup （比如cpu_cgrp, memory_cgrp)。 第一个 hierarchiy attach 了 cpu 子系统和 cpuacct 子系统， 因此当前 hierarchiy 中的 cgroup 就可以对 cpu 的资源进行限制，并且对进程的 cpu 使用情况进行统计。 第二个 hierarchiy attach 了 memory 子系统，因此当前 hierarchiy 中的 cgroup 就可以对 memory 的资源进行限制。 在每一个 hierarchiy 中，每一个节点（cgroup）可以设置对资源不同的限制权重（即自定义配置）。比如上图中 cgrp1 组中的进程可以使用60%的 cpu 时间片，而 cgrp2 组中的进程可以使用20%的 cpu 时间片。 cgroups 和 进程间的关系： 上面这个图从整体结构上描述了进程与 cgroups 之间的关系。最下面的P代表一个进程。 每一个进程的描述符中有一个指针指向了一个辅助数据结构css_set（cgroups subsystem set）。 指向某一个css_set的进程会被加入到当前css_set的进程链表中。一个进程只能隶属于一个css_set，一个css_set可以包含多个进程，隶属于同一css_set的进程受到同一个css_set所关联的资源限制。 上图中的”M×N Linkage”说明的是css_set通过辅助数据结构可以与 cgroups 节点进行多对多的关联。但是 cgroups 的实现不允许css_set同时关联同一个cgroups层级结构下多个节点。 这是因为 cgroups 对同一种资源不允许有多个限制配置。 一个css_set关联多个 cgroups 层级结构的节点时，表明需要对当前css_set下的进程进行多种资源的控制。而一个 cgroups 节点关联多个css_set时，表明多个css_set下的进程列表受到同一份资源的相同限制。 一个节点的控制列表中的所有进程都会受到当前节点的资源限制。同时某一个进程也可以被加入到不同的 cgroups 层级结构的节点中，因为不同的 cgroups 层级结构可以负责不同的系统资源。所以说进程和 cgroup 结构体是一个多对多的关系。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:1:2","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"2. 如何使用 Cgroups cgroup 相关的所有操作都是基于内核中的 cgroup virtual filesystem，使用cgroup 很简单，挂载这个文件系统就可以了。 一般情况下都是挂载到/sys/fs/cgroup目录下，当然挂载到其它任何目录都没关系。 cgroups 以文件的方式提供应用接口，我们可以通过 mount 命令来查看 cgroups 默认的挂载点： [root@iZ2zefmrr626i66omb40ryZ ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) 第一行的 tmpfs 说明 /sys/fs/cgroup 目录下的文件都是存在于内存中的临时文件。 第二行的挂载点 /sys/fs/cgroup/systemd 用于 systemd 系统对 cgroups 的支持。 其余的挂载点则是内核支持的各个子系统的根级层级结构。 需要注意的是，在使用 systemd 系统的操作系统中，/sys/fs/cgroup 目录都是由 systemd 在系统启动的过程中挂载的，并且挂载为只读的类型。换句话说，系统是不建议我们在 /sys/fs/cgroup 目录下创建新的目录并挂载其它子系统的。这一点与之前的操作系统不太一样。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:2:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"1. 相关语法 查看subsystem列表 可以通过查看/proc/cgroups(since Linux 2.6.24)知道当前系统支持哪些subsystem，下面是一个例子： DESKTOP-9K4GB6E# cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 11 1 1 cpu 3 64 1 cpuacct 3 64 1 blkio 8 64 1 memory 9 104 1 devices 5 64 1 freezer 10 4 1 net_cls 6 1 1 perf_event 7 1 1 net_prio 6 1 1 hugetlb 4 1 1 pids 2 68 1 从左到右，字段的含义分别是： subsys_name：subsystem的名字 hierarchy：subsystem所关联到的cgroup树的ID，如果多个subsystem关联到同一颗cgroup树，那么他们的这个字段将一样，比如这里的cpu和cpuacct就一样，表示他们绑定到了同一颗树。如果出现下面的情况，这个字段将为0： 当前subsystem没有和任何cgroup树绑定 当前subsystem已经和cgroup v2的树绑定 当前subsystem没有被内核开启 num_cgroups：subsystem所关联的cgroup树中进程组的个数，也即树上节点的个数 enabled：1表示开启，0表示没有被开启(可以通过设置内核的启动参数“cgroup_disable”来控制subsystem的开启). hierarchy 相关操作 挂载 Linux中，用户可以使用mount命令挂载 cgroups 文件系统： 语法为: mount -t cgroup -o subsystems name /cgroup/name 其中 subsystems 表示需要挂载的 cgroups 子系统 /cgroup/name 表示挂载点 这条命令同在内核中创建了一个 hierarchy 以及一个默认的 root cgroup。 示例： 挂载一个和cpuset subsystem关联的 hierarchy 到 ./cg1 目录 # 首先肯定是创建对应目录 mkdir cg1 # 具体挂载操作--参数含义如下 # -t cgroup 表示操作的是 cgroup 类型， # -o cpuset 表示要关联 cpuset subsystem，可以写0个或多个，0个则是关联全部subsystem， # cg1 为 cgroup 的名字， # ./cg1 为挂载目标目录。 mount -t cgroup -o cpuset cg1 ./cg1 # 挂载一颗和所有subsystem关联的cgroup树到cg1目录 mkdir cg1 mount -t cgroup cg1 ./cg1 #挂载一颗与cpu和cpuacct subsystem关联的cgroup树到 cg1 目录 mkdir cg1 mount -t cgroup -o cpu,cpuacct cg1 ./cg1 # 挂载一棵cgroup树，但不关联任何subsystem，这systemd所用到的方式 mkdir cg1 mount -t cgroup -o none,name=cg1 cg1 ./cg1 卸载 作为文件系统，同样是使用umount 命令卸载。 # 指定路径来卸载，而不是名字。 $ umount /path/to/your/hierarchy 例如 umount /sys/fs/cgroup/hierarchy cgroup 相关操作 创建 cgroup 比较简单，直接在 hierarchy或 cgroup 目录下创建子目录（mkdir）即可。 删除则是删除对应目录（rmdir）。 注：不能直接递归删除对应目录，因为目录中的文件是虚拟的，递归删除时会报错。 也可以借助 libcgroup 工具来创建或删除。 使用 libcgroup 工具前，请先安装 libcgroup 和 libcgroup-tools 数据包 redhat系统安装： $ yum install libcgroup $ yum install libcgroup-tools ubuntu系统安装: $ apt-get install cgroup-bin # 如果提示cgroup-bin找不到，可以用 cgroup-tools 替换 $ apt-get install cgroup-tools 具体语法： # controllers就是subsystem # path可以用相对路径或者绝对路径 $ cgdelete controllers:path 例如： cgdelete cpu:./mycgroup ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:2:1","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"3. 演示 分别演示以下直接在某个已存在的 hierarchy 下创建子 cgroup 或者直接创建一个新的 hierarchy 两种方式。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:3:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"1. 新 hierarchy 方式 创建 hierarchy 首先，要创建并挂载一个 hierarchy。 # 创建一个目录作为挂载点 lixd  ~ $ mkdir cgroup-test # 创建一个不挂载任何subsystem的hierarchy，由于 name=cgroup-test 的 cgroup 不存在，所以这里会由hierarchy默认创建出来 ✘ lixd  ~ $ sudo mount -t cgroup -o none,name=cgroup-test cgroup-test ./cgroup-test lixd  ~ $ cd cgroup-test lixd  ~/cgroup-test $ ls # 可以发现多了几个文件 cgroup.clone_children cgroup.procs cgroup.sane_behavior notify_on_release release_agent tasks 这些文件就是 hierarchy 中 cgroup 根节点的配置项。具体含义如下： cgroup.clone_ children, cpuset 的subsystem会读取这个配置文件，如果这个值是1 (默认是0)，子cgroup才会继承父cgroup的cpuset的配置。 cgroup.procs是树中当前节点cgroup中的进程组ID，现在的位置是在根节点，这个文件中会有现在系统中所有进程组的ID。 notify_ on_ release 和release agent 会一起使用。 notify_on_release 标识当这个cgroup 最后一个进程退出的时候是否执行了release_agent; release_agent 则是一个路径，通常用作进程退出之后自动清理掉不再使用的cgroup。 tasks 标识该cgroup下面的进程ID，如果把一个进程ID写到tasks文件中，便会将相应的进程加入到这个cgroup中。 创建子 cgroup 然后，从刚创建好的 hierarchy 上 cgroup 根节点中扩展出两个子 cgroup： # 创建子cgroup cgroup-1 lixd  ~/cgroup-test $ sudo mkdir cgroup-1 # 创建子cgroup cgroup-1 lixd  ~/cgroup-test $ sudo mkdir cgroup-2 lixd  ~/cgroup-test $ tree . ├── cgroup-1 │ ├── cgroup.clone_children │ ├── cgroup.procs │ ├── notify_on_release │ └── tasks ├── cgroup-2 │ ├── cgroup.clone_children │ ├── cgroup.procs │ ├── notify_on_release │ └── tasks ├── cgroup.clone_children ├── cgroup.procs ├── cgroup.sane_behavior ├── notify_on_release ├── release_agent └── tasks 可以看到，在一个 cgroup 的目录下创建文件夹时，Kernel 会把文件夹标记为这个 cgroup 的子 cgroup，它们会继承父 cgroup 的属性。 在 cgroup 中添加和移动进程 一个进程在一个Cgroups的hierarchy中，只能在一个cgroup节点上存在，系统的所有进程都会默认在根节点上存在。 想要将进程移动到其他cgroup节点，只需要将进程ID写到目标cgroup节点的tasks文件中即可。 将当前shell所在进程添加到 tasks： cgroup-test#cd cgroup-1 # 需要 root 权限 cgroup-1# echo $$ \u003e\u003e tasks cgroup-1# cat tasks 7575 cgroup-1# cat /proc/7575/cgroup 14:name=cgroup-test:/cgroup-1 # 可以看到该进程已经被加入到cgroup中了 13:rdma:/ 12:pids:/ 11:hugetlb:/ 10:net_prio:/ 9:perf_event:/ 8:net_cls:/ 7:freezer:/ 6:devices:/ 5:blkio:/a 4:cpuacct:/ 3:cpu:/ 2:cpuset:/ 1:memory:/ 0::/ 通过subsystem限制 cgroup中的进程 在上面创建 hierarchy 的时候，这个 hierarchy 并没有关联到任何的subsystem ,所以没办法通过那个 hierarchy 中的 cgroup 节点限制进程的资源占用。 即 只能在创建 hierarchy 时指定要关联哪些 subsystem，创建后就无法修改。 其实系统默认已经为每个 subsystem 创建了一个默认的 hierarchy,比如memory 的 hierarchy。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:3:1","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"2. 子 cgroup 方式 在很多使用 systemd 的系统中，systemd 已经帮我们将各个 subsystem 和cgroup 树关联并挂载好了： DESKTOP-9K4GB6E# mount |grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu) cgroup on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma) cgroup-test on /home/lixd/cgroup-test type cgroup (rw,relatime,name=cgroup-test) 因此我们可以直接在对应 cgroup 树下创建子 cgroup 即可。 直接进到 /sys/fs/cgroup/cpu 目录创建 cgroup-cpu 子目录即可： DESKTOP-9K4GB6E# cd /sys/fs/cgroup/cpu DESKTOP-9K4GB6E# mkdir cgroup-cpu DESKTOP-9K4GB6E# cd cgroup-cpu DESKTOP-9K4GB6E# ls cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 简单跑个程序测试一下,执行下面这条命令 DESKTOP-9K4GB6E# while : ; do : ; done \u0026 [1] 12887 显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 12887。 查看一下CPU占用： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 12887 root 25 5 14912 1912 0 R 100.0 0.0 0:33.31 zsh 果然这个 PID=12887 的进程占用了差不多 100% 的 CPU。 结下来我们就通过 Cgroups 对其进行限制，这里就用前面创建的 cgroup-cpu 控制组。 我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： DESKTOP-9K4GB6E# cat /sys/fs/cgroup/cpu/cgroup-cpu/cpu.cfs_quota_us -1 DESKTOP-9K4GB6E# cat /sys/fs/cgroup/cpu/cgroup-cpu/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： $ echo 20000 \u003e /sys/fs/cgroup/cpu/cgroup-cpu/cpu.cfs_quota_us 这样意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。 接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了： $ echo 12887 \u003e /sys/fs/cgroup/cpu/cgroup-cpu/tasks 使用 top 指令查看一下 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 12887 root 25 5 14912 1912 0 R 20.3 0.0 2:51.05 zsh 果然CPU被限制到了20%。 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:3:2","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"4. 小结 Cgroups 是 Linux 下的一种将进程按组进行管理的机制，它提供了对一组进程及将来子进程的资源限制控制和统计的能力。 cgroups 分为以下三个部分： cgroup 本身：对进程进行分组 hierarchy：将 cgroup 形成树形结构 subsystem：真正起到限制作用的部组件 使用步骤： 1）创建 cgroup 2）配置 subsystem 参数 3）将进程加入到该 cgroup ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:4:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"5. 参考 cgroups(7) — Linux manual page Control groups series by Neil Brown 美团技术团队—Linux资源管理之cgroups简介 Red Hat—资源管理指南 Linux Cgroup系列（01）：Cgroup概述 ","date":"2022-02-18","objectID":"/posts/docker/06-cgroups-1/:5:0","tags":["Docker"],"title":"Docker教程(六)---Cgroups-1-初体验","uri":"/posts/docker/06-cgroups-1/"},{"categories":["Docker"],"content":"Linux namespace 概述","date":"2022-02-11","objectID":"/posts/docker/05-namespace/","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"本文主要介绍了 Linux namespace 以及如何使用 Go 语言操作 namespace。 Docker 基础知识之 Namespace 篇。 主要内容为： 1）Namespace 介绍 2）Go 语言操作 Namespace 演示 准备跟着《自己动手写 docker》这本书从零开始实现一个简易版的 docker，加深对 docker 的理解。 源码及相关教程见 Github，欢迎 star。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:0:0","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"1. Namespace 概述 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:0","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"简介 Linux Namespace 是 Linux 提供的一种内核级别环境隔离的方法，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个 namespace 中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。 Namespace 就是对资源的隔离 目前，Linux 内核里面实现了 8 种不同类型的 namespace。 分类 系统调用参数 隔离内容 相关内核版本 Mount namespaces CLONE_NEWNS Mount points Linux 2.4.19 UTS namespaces CLONE_NEWUTS Hostname and NIS domain name Linux 2.6.19 IPC namespaces CLONE_NEWIPC System V IPC, POSIX message queues Linux 2.6.19 PID namespaces CLONE_NEWPID Process IDs Linux 2.6.24 Network namespaces CLONE_NEWNET Network devices, stacks, ports, etc. 始于Linux 2.6.24 完成于 Linux 2.6.29 User namespaces CLONE_NEWUSER User and group IDs 始于 Linux 2.6.23 完成于 Linux 3.8) Cgroup namespace CLONE_NEWCGROUP Cgroup root directory Linux 4.6 Time namespace CLONE_NEWTIME Boot and monotonic Linux 5.6 注意： 由于 Cgroup namespace 在 4.6 的内核中才实现，并且和 cgroup v2 关系密切，现在普及程度还不高，比如 docker 现在就还没有用它。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:1","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"相关API 和 namespace 相关的函数只有四个，这里简单的看一下： clone setns unshare ioctl_ns clone： 创建一个新的进程并把他放到新的namespace中： int clone(int (*fn)(void *), void *stack, int flags, void *arg, ... /* pid_t *parent_tid, void *tls, pid_t *child_tid */ ); /* flags： 指定一个或者多个上面的CLONE_NEW*（当然也可以包含跟namespace无关的flags）， 这样就会创建一个或多个新的不同类型的namespace， 并把新创建的子进程加入新创建的这些namespace中。 */ setns： 将当前进程加入到已有的namespace中 int setns(int fd, int nstype); /* fd： 指向/proc/[pid]/ns/目录里相应namespace对应的文件， 表示要加入哪个namespace nstype： 指定namespace的类型（上面的任意一个CLONE_NEW*）： 1. 如果当前进程不能根据fd得到它的类型，如fd由其他进程创建， 并通过UNIX domain socket传给当前进程， 那么就需要通过nstype来指定fd指向的namespace的类型 2. 如果进程能根据fd得到namespace类型，比如这个fd是由当前进程打开的， 那么nstype设置为0即可 */ unshare: 使当前进程退出指定类型的 namespace，并加入到新创建的 namespace（相当于创建并加入新的namespace） int unshare(int flags); /* flags： 指定一个或者多个上面的CLONE_NEW*， 这样当前进程就退出了当前指定类型的namespace并加入到新创建的namespace */ ioctl_ns：查询 namespace 信息。 new_fd = ioctl(fd, request); /* fd: 指向/proc/[pid]/ns/目录里相应namespace对应的文件 request: NS_GET_USERNS: 返回指向拥有用户的文件描述符namespace fd引用的命名空间 NS_GET_PARENT: 返回引用父级的文件描述符由fd引用的命名空间的命名空间。 */ clone 和 unshare 的区别 clone 和 unshare 的功能都是创建并加入新的 namespace， 他们的区别是： unshare 是使当前进程加入新的 namespace clone 是创建一个新的子进程，然后让子进程加入新的 namespace，而当前进程保持不变 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:2","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"查看进程所属的 namespaces 系统中的每个进程都有 /proc/[pid]/ns/ 这样一个目录，里面包含了这个进程所属 namespace 的信息，里面每个文件的描述符都可以用来作为 setns 函数(后面会介绍)的参数。 #查看当前bash进程所属的namespace lixd  ~  ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 ipc -\u003e 'ipc:[4026532227]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 mnt -\u003e 'mnt:[4026532241]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 net -\u003e 'net:[4026531992]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 pid -\u003e 'pid:[4026532243]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 pid_for_children -\u003e 'pid:[4026532243]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 lixd lixd 0 Jan 6 19:00 uts -\u003e 'uts:[4026532242]' 以ipc:[4026532227]为例，ipc 是 namespace 的类型，4026532227 是 inode number。 如果两个进程的 ipc namespace 的 inode number一样，说明他们属于同一个 namespace，这条规则对其他类型的 namespace 也同样适用。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:3","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"namespace limit /proc/sys/user 目录中公开了对各种命名空间数量的限制，具体如下： $ tree /proc/sys/user/ /proc/sys/user/ ├── max_cgroup_namespaces ├── max_inotify_instances ├── max_inotify_watches ├── max_ipc_namespaces ├── max_mnt_namespaces ├── max_net_namespaces ├── max_pid_namespaces ├── max_user_namespaces └── max_uts_namespaces $ cat /proc/sys/user/max_pid_namespaces 6784 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:4","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"namespace lifetime 当一个 namespace 中的所有进程都结束或者移出该 namespace 时，该 namespace 将会被销毁。 不过也有一些特殊情况，可以再没有进程的时候保留 namespace： 存在打开的 FD，或者对 /proc/[pid]/ns/* 执行了 bind mount 存在子 namespace 它是一个拥有一个或多个非用户 namespace 的 namespace。 它是一个 PID namespace，并且有一个进程通过 /proc/[pid]/ns/pid_for_children 符号链接引用了这个 namespace。 它是一个 Time namespace，并且有一个进程通过 /proc/[pid]/ns/time_for_children 符号链接引用了这个 namespace。 它是一个 IPC namespace，并且有一个 mqueue 文件系统的 mount 引用了该 namespace 它是一个 PIDnamespace，并且有一个 proc 文件系统的 mount 引用了该 namespace ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:1:5","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"2. Go 演示 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:0","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"UTS UTS Namespace主要用来隔离nodename和domainname两个系统标识。在UTS Namespace里面，每个Namespace允许有自己的hostname. 以下程序展示了如何在 Go 中切换 UTS Namespace。 // 注: 运行时需要 root 权限。 func main() { cmd := exec.Command(\"bash\") cmd.SysProcAttr = \u0026syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS, } cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil { log.Fatalln(err) } } 运行并测试 DESKTOP-9K4GB6E# go run main.go root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# 运行后会进入了一个新的 shell 环境。 查看以下是否真的进入了新的 UTS Namespace。 首先使用 pstree查看进程关系： root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# pstree -pl init(1)─┬─init(1272)───init(1273)───server(1274)─┬─{server}(1282) │ ├─{server}(1283) │ ├─{server}(1284) │ ├─{server}(1285) │ ├─{server}(1286) │ ├─{server}(1287) │ ├─{server}(1288) │ └─{server}(1289) ├─init(3701)───init(3702)───zsh(3703)───su(7520)───bash(7521)───zsh(7575)───go(8104)─┬─main(8182)─┬─bash(8187)───pstree(8194) │ │ ├─{main}(8183) │ │ ├─{main}(8184) │ │ ├─{main}(8185) │ │ └─{main}(8186) │ ├─{go}(8105) │ ├─{go}(8106) │ ├─{go}(8107) │ ├─{go}(8108) │ ├─{go}(8109) │ ├─{go}(8110) │ ├─{go}(8111) │ ├─{go}(8112) │ ├─{go}(8117) │ └─{go}(8143) ├─init(3763)───init(3764)───zsh(3765) ├─init(5171)───init(5172)───fsnotifier-wsl(5173) ├─init(7459)───init(7460)───bash(7461)───su(7476)───bash(7477) ├─{init}(5) └─{init}(6) 主要关注这条： ├─init(3701)───init(3702)───zsh(3703)───su(7520)───bash(7521)───zsh(7575)───go(8104)─┬─main(8182)─┬─bash(8187) main 程序 pid 为 8182,后续新创建的 bash pid 为 8187，现在查看二者 uts 是否相同即可： root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# readlink /proc/8182/ns/uts uts:[4026532242] root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# readlink /proc/8187/ns/uts uts:[4026532386] 可以发现二者确实不在一个 UTS Namespace 中。由于 UTS Namespace hostname 做了隔离 所以在这个环境内修改 hostname 应该不影响外部主机， 下面来做 下实验。 在这个新的 bash 环境中修改 hostname root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# hostname bash root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# hostname bash 新开一个在宿主机上查看 hostname： lixd ~ $ hostname DESKTOP-9K4GB6E 可以看到外部的 hostname 并没有被修改影响，由此可了解 UTS Namespace 的作用。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:1","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"IPC IPC Namespace 用来隔离 sys V IPC和 POSIX message queues。每个 IPC Namespace 都有自己的 Sys V IPC 和 POSIX message queues。 微调一下程序，只是修改了 Cloneflags，新增了 CLONE_NEWIPC，表示同时创建 IPC Namespace。 // 注: 运行时需要 root 权限。 func main() { cmd := exec.Command(\"bash\") cmd.SysProcAttr = \u0026syscall.SysProcAttr{ // Cloneflags: syscall.CLONE_NEWUTS, Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, } cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil { log.Fatalln(err) } } 运行并测试： # 先查看宿主机上的 ipc message queue DESKTOP-9K4GB6E# ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages # 然后创建一个 DESKTOP-9K4GB6E# ipcmk -Q Message queue id: 0 # 再次查看，发现有了 DESKTOP-9K4GB6E# ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages 0x70ffd07c 0 root 644 0 0 运行程序进入新的 shell DESKTOP-9K4GB6E# go run main.go root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages 可以发现，在新的 Namespace 中已经看不到宿主机上的 message queue 了。说明 IPC Namespace 创建成功，IPC 已经被隔离。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:2","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"PID PID Namespace是用来隔离进程ID的。同样一个进程在不同的PID Namespace里可以拥有不同的PID。这样就可以理解，在docker container 里面，使用ps -ef经常会发现，在容器内，前台运行的那个进程PID是1，但是在容器外，使用ps -ef会发现同样的进程却有不同的PID，这就是PID Namespace做的事情。 再次调整程序，增加 PID flags： // 注: 运行时需要 root 权限。 func main() { cmd := exec.Command(\"bash\") cmd.SysProcAttr = \u0026syscall.SysProcAttr{ // Cloneflags: syscall.CLONE_NEWUTS, // Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID, } cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil { log.Fatalln(err) } } 运行并测试： DESKTOP-9K4GB6E# go run main.go root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# pstree -pl init(1)─┬─init(1272)───init(1273)───server(1274)─┬─{server}(1282) │ ├─{server}(1283) │ ├─{server}(1284) │ ├─{server}(1285) │ ├─{server}(1286) │ ├─{server}(1287) │ ├─{server}(1288) │ └─{server}(1289) ├─init(3701)───init(3702)───zsh(3703)───su(7520)───bash(7521)───zsh(7575)───go(9103)─┬─main(9184)─┬─bash(9189)───pstree(9196) │ │ ├─{main}(9185) │ │ ├─{main}(9186) │ │ ├─{main}(9187) │ │ └─{main}(9188) │ ├─{go}(9104) │ ├─{go}(9105) │ ├─{go}(9106) │ ├─{go}(9107) │ ├─{go}(9108) │ ├─{go}(9109) │ ├─{go}(9110) │ ├─{go}(9111) │ ├─{go}(9112) │ └─{go}(9120) ├─init(3763)───init(3764)───zsh(3765) ├─init(5171)───init(5172)───fsnotifier-wsl(5173) ├─init(7459)───init(7460)───bash(7461)───su(7476)───bash(7477) ├─init(8201)───init(8202)───zsh(8203) ├─{init}(5) └─{init}(6) 可以看到 main 函数 pid 为 9184，而新开的 bash pid 为 9189。 然后再新开的 bash 中查看自己的 pid： 这里只能使用 echo $$ 命令查看，ps、top 等命令会查看到其他 Namespace 中的信息。 root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# echo $$ 1 发现 pid 是1，说明再新开的 PID Namespace 中只有一个 bash 这个进程，而且被伪装成了 1 号进程。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:3","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"Mount Mount Namespace用来隔离各个进程看到的挂载点视图。在不同Namespace的进程中，看到的文件系统层次是不一样的。 在Mount Namespace中调用mount()和umount()仅仅只会影响当前Namespace内的文件系统，而对全局的文件系统是没有影响的。 看到这里，也许就会想到chroot()，它也是将某一个子目录变成根节点。但是，Mount Namespace不仅能实现这个功能，而且能以更加灵活和安全的方式实现。 需要注意的是，Mount Namespace 的 flag 是CLONE_NEWNS,直接是 NEWNS 而不是 NEWMOUNT,因为 Mount Namespace 是 Linux 中实现的第一个 Namespace，当时也没想到后续会有很多类型的 Namespace 加入。 再次修改代码，增加 Mount Namespace 的 flag cmd.SysProcAttr = \u0026syscall.SysProcAttr{ // Cloneflags: syscall.CLONE_NEWUTS, // Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, // Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID, Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS, } 运行并测试： 首先运行程序并在新的 bash 环境中查看 /proc DESKTOP-9K4GB6E# go run main.go root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# ls /proc 1 3764 7476 9476 cmdline driver kallsyms loadavg net swaps vmallocinfo 1272 3765 7477 9557 config.gz execdomains kcore locks pagetypeinfo sys vmstat 1273 5171 7520 9562 consoles filesystems key-users mdstat partitions sysvipc zoneinfo 1274 5172 7521 9569 cpuinfo fs keys meminfo sched_debug thread-self 3701 5173 7575 acpi crypto interrupts kmsg misc schedstat timer_list 3702 7459 8201 buddyinfo devices iomem kpagecgroup modules self tty 3703 7460 8202 bus diskstats ioports kpagecount mounts softirqs uptime 3763 7461 8203 cgroups dma irq kpageflags mtrr stat version 可以看到，有一大堆文件，这是因为现在查看到的其实是宿主机上的 /proc 目录。 现在把 proc 目录挂载到当前 Namespace 中来： root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# mount -t proc proc /proc root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# ls /proc 1 cmdline diskstats interrupts key-users loadavg mounts schedstat sysvipc vmallocinfo 10 config.gz dma iomem keys locks mtrr self thread-self vmstat acpi consoles driver ioports kmsg mdstat net softirqs timer_list zoneinfo buddyinfo cpuinfo execdomains irq kpagecgroup meminfo pagetypeinfo stat tty bus crypto filesystems kallsyms kpagecount misc partitions swaps uptime cgroups devices fs kcore kpageflags modules sched_debug sys version 可以看到，少了一些文件，少的主要是数字命名的目录，因为当前 Namespace 下没有这些进程，自然就看不到对应的信息了。 此时就可以通过 ps 命令来查看了： root@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 13:13 pts/2 00:00:00 bash root 11 1 0 13:13 pts/2 00:00:00 ps -ef 可以看到，在当前 Namespace 中 bash 为 1 号进程。 这就说明，当前 Mount Namespace 中的 mount 和外部是隔离的，mount 操作并没有影响到外部，Docker volume 也是利用了这个特性。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:4","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"User User Narespace 主要是隔离用户的用户组ID。也就是说，一个进程的 UserID 和GroupID 在不同的 User Namespace 中可以是不同的。比较常用的是，在宿主机上以一个非root用户运行创建一个User Namespace,然后在User Namespace里面却映射成root用户。这意味着，这个进程在User Namespace里面有root 权限，但是在User Namespace外面却没有root 的权限。 从Linux Kernel 3.8开始，非root进程也可以创建UserNamespace,并且此用户在Namespace里面可以被映射成root，且在Namespace内有root权限。 再次修改代码，增加 User Namespace 的 flag： cmd.SysProcAttr = \u0026syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER, } 运行并测试： 首先在宿主机上查看一个 user 和 group： DESKTOP-9K4GB6E# id uid=0(root) gid=0(root) groups=0(root) 可以看到，此时是 root 用户。 运行程序，进入新的 bash 环境： DESKTOP-9K4GB6E# go run main.go nobody@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) 可以看到，UID 是不同的，说明 User Namespace 生效了。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:5","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"Network Network Namespace 是用来隔离网络设备、IP 地址端口等网络栈的 Namespace。Network Namespace 可以让每个容器拥有自己独立的(虛拟的)网络设备，而且容器内的应用可以绑定到自己的端口，每个 Namespace 内的端口都不会互相冲突。在宿主机上搭建网桥后，就能很方便地实现容器之间的通信，而且不同容器上的应用可以使用相同的端口。 再次修改代码，增加 Network Namespace 的 flag： cmd.SysProcAttr = \u0026syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNET, } 运行并测试： 首先看一下宿主机上的网络设备： DESKTOP-9K4GB6E# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: bond0: \u003cBROADCAST,MULTICAST,MASTER\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 22:2f:1f:8e:f7:72 brd ff:ff:ff:ff:ff:ff 3: dummy0: \u003cBROADCAST,NOARP\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 7e:46:8b:04:23:81 brd ff:ff:ff:ff:ff:ff 4: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 5: sit0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 6: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:15:5d:6e:f2:65 brd ff:ff:ff:ff:ff:ff inet 172.18.167.21/20 brd 172.18.175.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::215:5dff:fe6e:f265/64 scope link valid_lft forever preferred_lft forever 有 lo、eth0 等6 个设备。 然后运行程序： DESKTOP-9K4GB6E# go run main.go nobody@DESKTOP-9K4GB6E:/home/lixd/projects/docker/mydocker$ ip addr 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 3: sit0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 可以发现，新的 Namespace 中只有3个设备了，说明 Network Namespace 生效了。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:2:6","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"3. 小结 1）本质：Linux Namespace 是 Linux 提供的一种内核级别环境隔离的方法，本质就是对全局系统资源的一种封装隔离。 2）使用：Namespace API 一共 4个，最常用的就是 clone，而 Go 已经把 clone 调用给封装好了，使用时只需要传入不同参数即可控制创建不同 Namespace。 ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:3:0","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["Docker"],"content":"4. 参考 overview of Linux namespaces Namespaces in operation, part 1: namespaces overview Linux namespaces Linux_namespaces DOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） ","date":"2022-02-11","objectID":"/posts/docker/05-namespace/:4:0","tags":["Docker"],"title":"Docker教程(五)---namespace 初体验","uri":"/posts/docker/05-namespace/"},{"categories":["etcd"],"content":"etcd watch 机制具体实现及源码分析","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"本文主要通过源码分析了 etcd v3 版本 watch 机制的具体实现（下篇）。 在上一篇 etcd教程(十三)—watch 机制源码分析（上） 中分析了 watch 方法的实现，这篇主要分析 MVCC 模块中的事件产生与推送逻辑的具体实现。 还是开篇贴一下整个的流程图： ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:0:0","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"1. MVCC 在 etcd 启动的时候，WatchableKV 模块会运行 syncWatchersLoop 和 syncVictimsLoop 这两个 goroutine，分别负责不同场景下的事件推送，它们也是 Watch 特性可靠性的核心之一。 // server/etcdserver/server.go 299行 func NewServer(cfg config.ServerConfig) (srv *EtcdServer, err error) { // 省略部分无关代码 srv = \u0026EtcdServer{ readych: make(chan struct{}), Cfg: cfg, // ... consistIndex: b.storage.backend.ci, firstCommitInTerm: notify.NewNotifier(), clusterVersionChanged: notify.NewNotifier(), } // 将 mvcc 中的 KV 对象赋值给了 srv srv.kv = mvcc.New(srv.Logger(), srv.be, srv.lessor, mvccStoreConfig) } 其中的 srv.kv 字段具体类型如下： type EtcdServer struct { // 省略其他字段 kv mvcc.WatchableKV } 根据名字，应该就能猜到这个是用来做什么的了。 继续追踪下去，看 mvcc.New 具体实现： // server/storage/mvcc/watchable_store.go 73 行 func New(lg *zap.Logger, b backend.Backend, le lease.Lessor, cfg StoreConfig) WatchableKV { return newWatchableStore(lg, b, le, cfg) } func newWatchableStore(lg *zap.Logger, b backend.Backend, le lease.Lessor, cfg StoreConfig) *watchableStore { if lg == nil { lg = zap.NewNop() } s := \u0026watchableStore{ store: NewStore(lg, b, le, cfg), victimc: make(chan struct{}, 1), unsynced: newWatcherGroup(), synced: newWatcherGroup(), stopc: make(chan struct{}), } s.store.ReadView = \u0026readView{s} s.store.WriteView = \u0026writeView{s} if s.le != nil { // use this store as the deleter so revokes trigger watch events s.le.SetRangeDeleter(func() lease.TxnDelete { return s.Write(traceutil.TODO()) }) } s.wg.Add(2) // 这里启动了两个 goroutine，分别是 syncWatchersLoop 和 syncVictimsLoop go s.syncWatchersLoop() go s.syncVictimsLoop() return s } 这里启动的两个 goroutine 分别负责了历史事件推送和异常场景重试。 而最新消息推送并不是由后台任务推送，而是在执行 PUT 操作时，由 notify 方法同步推送。 syncWatchersLoop：历史事件推送 syncVictimsLoop：异常场景重试 notify：最新事件推送 这也和上篇中提到的 watcher 分类对应。 后续主要对着两个 goroutine 和 notify 方法进行分析。 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:1:0","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"2. 事件推送实现 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:0","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"watcher 分类 上一篇中说过，etcd 中根据不同场景，对问题进行了分解，将 watcher 按场景分类，实现了轻重分离、低耦合。 具体如下： synced watcher，顾名思义，表示此类 watcher 监听的数据都已经同步完毕，在等待新的变更。 如果你创建的 watcher 未指定版本号 (默认 0)、或指定的版本号大于 etcd sever 当前最新的版本号 (currentRev)，那么它就会保存到 synced watcherGroup 中。 unsynced watcher，表示此类 watcher 监听的数据还未同步完成，落后于当前最新数据变更，正在努力追赶。 如果你创建的 watcher 指定版本号小于 etcd server 当前最新版本号，那么它就会保存到 unsynced watcherGroup 中。 victim watcher：表示 watcher 存在事件堆积导致推送异常 ，需要由异步任务执行重试操作。 从以上介绍中，我们可以将可靠的事件推送机制拆分成最新事件推送、异常场景重试、历史事件推送三个子问题来进行分析。 watcher 状态转换关系如下图所示： ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:1","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"notify 方法 notify 主要实现 最新事件推送。 当你创建完成 watcher 后，执行 put hello 修改操作时，如流程图所示，请求会经过 KVServer、Raft 模块后最终Apply 到状态机，在 MVCC 的 put 事务中，它会将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 在 put 事务结束时，如下面的代码所示，它会将 KeyValue 转换成 Event 事件，然后回调 watchableStore.notify 函数（流程 5）。 notify 会匹配出监听过此 key 并处于 synced watcherGroup 中的 watcher，同时事件中的版本号要大于等于 watcher 监听的最小版本号，才能将事件发送到此 watcher 的事件 channel 中。 注意：为了保证事件顺序，这里只有在 synced watcherGroup 中的 watcher 才会执行立即推送，否则就只能等异常推送历史消息了。 如果在 notify 中直接从 blotdb 中把对应 watcher 的所有 event 都查询出来进行推送，也可以保证先后顺序，但是这样太慢了，会影响到接口的响应时间。因此还是丢到了异步逻辑中。 serverWatchStream 的 sendLoop goroutine 监听到 channel 消息后，读出消息立即推送给 client（流程 6 和 7），至此，完成一个最新修改事件推送。 tx.Put 方法实现如下： func (tw *storeTxnWrite) put(key, value []byte, leaseID lease.LeaseID) { // 省略其他逻辑 tw.s.kvindex.Put(key, idxRev) tw.changes = append(tw.changes, kv) // 将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 } 可以看到，每次修改后，都会使用将修改后的 mvccpb.KeyValue 追加到 changes 数组中。 而 put 事务结束时会调用 End 方法（类似于 commit），具体实现如下： // server/storage/mvcc/watchable_store_txn.go 22 行 func (tw *watchableStoreTxnWrite) End() { changes := tw.Changes() if len(changes) == 0 { tw.TxnWrite.End() return } rev := tw.Rev() + 1 evs := make([]mvccpb.Event, len(changes)) // 将本次事务中的 cahnges 转换成 event for i, change := range changes { evs[i].Kv = \u0026changes[i] if change.CreateRevision == 0 { evs[i].Type = mvccpb.DELETE evs[i].Kv.ModRevision = rev } else { evs[i].Type = mvccpb.PUT } } tw.s.mu.Lock() tw.s.notify(rev, evs) // 调用 notify 方法，通知 watchStream tw.TxnWrite.End() tw.s.mu.Unlock() } 在 end 方法中，将本次事务中的 changes 转换成 event，然后调用 notify 方法通知 watchStream。 具体 notify 方法实现如下： // server/storage/mvcc/watchable_store_txn.go 434 行 func (s *watchableStore) notify(rev int64, evs []mvccpb.Event) { victim := make(watcherBatch) // event 事件进行批量化处理，根据watcher进行分类 // 然后遍历，一个一个推送 for w, eb := range newWatcherBatch(\u0026s.synced, evs) { if eb.revs != 1 { s.store.lg.Panic( \"unexpected multiple revisions in watch notification\", zap.Int(\"number-of-revisions\", eb.revs), ) } if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: rev}) { pendingEventsGauge.Add(float64(len(eb.evs))) } else { // 如果推送失败了就加入到 victim 列表中。 // move slow watcher to victims w.minRev = rev + 1 w.victim = true victim[w] = eb s.synced.delete(w) slowWatcherGauge.Inc() } } s.addVictim(victim) } 先进行批量化处理，然后遍历调用 send 方法，将 event 发送出去。具体 send 方法实现如下： func (w *watcher) send(wr WatchResponse) bool { progressEvent := len(wr.Events) == 0 // 首先是根据 filter 方法，过滤掉不关心的 event if len(w.fcs) != 0 { ne := make([]mvccpb.Event, 0, len(wr.Events)) for i := range wr.Events { filtered := false for _, filter := range w.fcs { if filter(wr.Events[i]) { filtered = true break } } if !filtered { ne = append(ne, wr.Events[i]) } } wr.Events = ne } if !progressEvent \u0026\u0026 len(wr.Events) == 0 { return true } // 然后发送出去 select { case w.ch \u003c- wr: return true default: return false } } 实际上这里的 watcher 就是前面 recvLoop 中收到 create 请求时创建的 watcher，具体如下： func (ws *watchStream) Watch(id WatchID, key, end []byte, startRev int64, fcs ...FilterFunc) (WatchID, error) { // 上文中的 watcher 就是这里创建的，而 watcher.ch 实际就是 watchStream.ch w, c := ws.watchable.watch(key, end, startRev, id, ws.ch, fcs...) } 因此实际上最终 event 被发送到了 watchStream.ch。 然后 sendLoop 不断从 watchStream.ch 中取出 event 并发送给 client。 到此，整个流程就算是串起来了。 notify 中如果发送失败后会将 watcher 添加到 victimGroup 中。这里的发送失败主要是由于 ch 阻塞，导致没发出去，也就是图中的事件堆积。 也就是下面的 select 中走了 default path。 select { case w.ch \u003c- wr: return true default: return false } ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:2","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"syncWatchersLoop syncWatchersLoop 主要负责历史事件推送。 notify 只会处理在 synced watcherGroup 中的 watcher ，如果不在则无法立即接收到 event。 notify 只用于推送最新事件，如果 watcher 还有旧 event 没有推送，而直接推送最新 event 势必无法保证 event 的先后顺序，因此 notify 中只处理了 synced watcherGroup 中的 watcher 。 为了保证效率，notify 也没有从 blotdb 中把对应 watcher 的所有 event 都查询出来再进行推送。 而 syncWatchersLoop 就是负责处理 unsynced 中的 watcher，将这些 watcher 的历史 event 全部推送给 sendLoop，然后将其移动到 synced watcherGroup，以便下次 notify 时就能直接处理。 具体实现如下： // server/storage/mvcc/watchable_store.go 211行 func (s *watchableStore) syncWatchersLoop() { defer s.wg.Done() for { s.mu.RLock() st := time.Now() lastUnsyncedWatchers := s.unsynced.size() s.mu.RUnlock() unsyncedWatchers := 0 if lastUnsyncedWatchers \u003e 0 { unsyncedWatchers = s.syncWatchers() } syncDuration := time.Since(st) waitDuration := 100 * time.Millisecond // more work pending? if unsyncedWatchers != 0 \u0026\u0026 lastUnsyncedWatchers \u003e unsyncedWatchers { // be fair to other store operations by yielding time taken waitDuration = syncDuration } select { case \u003c-time.After(waitDuration): case \u003c-s.stopc: return } } } 每过 100ms 就会对 unsynced watcher 进行一次同步。 具体同步逻辑在s.syncWatchers()方法中： // server/storage/mvcc/watchable_store.go 326行 func (s *watchableStore) syncWatchers() int { s.mu.Lock() defer s.mu.Unlock() if s.unsynced.size() == 0 { return 0 } s.store.revMu.RLock() defer s.store.revMu.RUnlock() curRev := s.store.currentRev compactionRev := s.store.compactMainRev wg, minRev := s.unsynced.choose(maxWatchersPerSync, curRev, compactionRev) minBytes, maxBytes := newRevBytes(), newRevBytes() revToBytes(revision{main: minRev}, minBytes) revToBytes(revision{main: curRev + 1}, maxBytes) tx := s.store.b.ReadTx() tx.RLock() revs, vs := tx.UnsafeRange(schema.Key, minBytes, maxBytes, 0) evs := kvsToEvents(s.store.lg, wg, revs, vs) tx.RUnlock() victims := make(watcherBatch) wb := newWatcherBatch(wg, evs) for w := range wg.watchers { w.minRev = curRev + 1 eb, ok := wb[w] if !ok { s.synced.add(w) s.unsynced.delete(w) continue } if eb.moreRev != 0 { w.minRev = eb.moreRev } if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: curRev}) { pendingEventsGauge.Add(float64(len(eb.evs))) } else { w.victim = true } if w.victim { victims[w] = eb } else { if eb.moreRev != 0 { // stay unsynced; more to read continue } s.synced.add(w) } s.unsynced.delete(w) } s.addVictim(victims) vsz := 0 for _, v := range s.victims { vsz += len(v) } slowWatcherGauge.Set(float64(s.unsynced.size() + vsz)) return s.unsynced.size() } 大致逻辑： 1）从 unsynced watcher group 中选取一组 watcher 2）遍历这组 watcher 得到 minimum revision，并移除掉已经被压缩的 watcher 3）根据第二步中查询到的 minimum revision 查询 键值对并发送这些事件给 watchers 4）最后对这组 watcher 进行判断，若同步完成了就将其从 unsynced watcher group 中移动到 synced watcher group 中 每次都会从所有的 unsynced watcher group 中选出一批 Watcher 进行批处理(组成watchGroup)，在这批 Watcher 中将观察到的最小的 revision.mainID 作为 bbolt 的遍历起始位置。 如果为每个Watcher 单独遍历 bbolt 并从中挑选出属于自己关注的 key ，那么性能就太差了。通过一次性遍历，处理多个 Watcher ，显然可以有效减少遍历的次数。 遍历 bblot 时会反序列化每个 mvccpb.KeyValue 结构， 判断其中的 key 是否属于 watch Group 关注的 key ，而这是由 kvsToEvents 函数完成的： // server/storage/mvcc/watchable_store.go 410行 func kvsToEvents(lg *zap.Logger, wg *watcherGroup, revs, vals [][]byte) (evs []mvccpb.Event) { for i, v := range vals { var kv mvccpb.KeyValue if err := kv.Unmarshal(v); err != nil { lg.Panic(\"failed to unmarshal mvccpb.KeyValue\", zap.Error(err)) } // 如果没有watcher关心这个key就直接返回 if !wg.contains(string(kv.Key)) { continue } // 判断本次事件类型，默认初始化为 PUT ty := mvccpb.PUT // 如果有 Tombstone 标记（即key被标记删除了）就改为 DELETE 事件 if isTombstone(revs[i]) { ty = mvccpb.DELETE // patch in mod revision so watchers won't skip kv.ModRevision = bytesToRev(revs[i]).main } evs = append(evs, mvccpb.Event{Kv: \u0026kv, Type: ty}) } return evs } unsynced watcher group 若同步完成后追上了最新的 revision 就将其移动到 synced watcher group 。 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:3","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"syncVictimsLoop syncVictimsLoop 主要负责异常场景重试。 具体如下： // server/storage/mvcc/watchable_store.go 243行 func (s *watchableStore) syncVictimsLoop() { defer s.wg.Done() for { for s.moveVictims() != 0 { // try to update all victim watchers } s.mu.RLock() isEmpty := len(s.victims) == 0 s.mu.RUnlock() var tickc \u003c-chan time.Time if !isEmpty { tickc = time.After(10 * time.Millisecond) } select { case \u003c-tickc: case \u003c-s.victimc: case \u003c-s.stopc: return } } } 每过 10ms 或者收到通知信息就进行一次循环，尝试处理因消息堆积发送异常而加入到 victimc 列表中的 watcher。 尝试再次把这些 watcher 相关 event 推送出去以清空 victimc 列表。 具体逻辑在s.moveVictims()方法中： // server/storage/mvcc/watchable_store.go 269行 func (s *watchableStore) moveVictims() (moved int) { s.mu.Lock() // 这里先把原victims用临时变量存一下 victims := s.victims // 然后清空原victims s.victims = nil s.mu.Unlock() var newVictim watcherBatch // 直接就是一个遍历发送 for _, wb := range victims { // try to send responses again for w, eb := range wb { // watcher has observed the store up to, but not including, w.minRev rev := w.minRev - 1 if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: rev}) { pendingEventsGauge.Add(float64(len(eb.evs))) } else { // 还是发送失败就临时加入到 newVictim 列表 if newVictim == nil { newVictim = make(watcherBatch) } newVictim[w] = eb continue } moved++ } // assign completed victim watchers to unsync/sync s.mu.Lock() s.store.revMu.RLock() curRev := s.store.currentRev for w, eb := range wb { if newVictim != nil \u0026\u0026 newVictim[w] != nil { // couldn't send watch response; stays victim continue } w.victim = false if eb.moreRev != 0 { w.minRev = eb.moreRev } // 根据 reversion 进行判断，如果追上了就添加到 syncedGroup // 没追上就添加到 unsyncedGroup if w.minRev \u003c= curRev { s.unsynced.add(w) } else { slowWatcherGauge.Dec() s.synced.add(w) } } s.store.revMu.RUnlock() s.mu.Unlock() } // 最后再把本次发送失败的追加到 s.victims 列表中 if len(newVictim) \u003e 0 { s.mu.Lock() s.victims = append(s.victims, newVictim) s.mu.Unlock() } return moved } 逻辑比较简单，就是一个遍历尝试发送，然后把发送失败的再添加 victims。 推送成功后根据 revision 进行判断，该将 watcher 添加到哪个 group： 如果追上了最新 revision 就添加到 syncedGroup 没追上就添加到 unsyncedGroup ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:4","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"小结 执行 put 事务操作时，利用 changes 数组记录下本次事务中修改的所有 KeyValue，最终提交事务时根据 changes 数组，生成对应的 event，然后通过 chan 发送到 sendLoop 中，最终 sendLoop 取出消息并转发给 client。 1）若 watcher 有事件堆积，则加入到 victimGroup，由 syncVictimsLoop 负责进行异步推送。 2）而 usyncedGroup 则由 syncWatchersLoop 负责进行异步推送。 3）推送失败则加入 victimGroup，推送成功后若追上最新 revision 则加入 syncedGroup，否则加入 unsyncedGroup。 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:2:5","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"3. Interval Tree etcd 为了实现高效的事件匹配，使用了 map 来存储单个 key 和 watcher 的对应关系。 而对于监听 key 范围、key 前缀的 watcher 则使用了 区间树来存储。 当收到创建 watcher 请求的时候，它会把 watcher 监听的 key 范围插入到上面的区间树中，区间的值保存了监听同样 key 范围的 watcher 集合 /watcherSet。 当产生一个事件时，etcd 首先需要从 map 查找是否有 watcher 监听了单 key，其次它还需要从区间树找出与此 key 相交的所有区间，然后从区间的值获取监听的 watcher 集合。 区间树支持快速查找一个 key 是否在某个区间内，时间复杂度 O(LogN)，因此 etcd 基于 map 和区间树实现了 watcher 与事件快速匹配，具备良好的扩展性。 etcd 中的相关实现如下： // pkg/adt/interval_tree.go 185行 type IntervalTree interface { // Insert adds a node with the given interval into the tree. Insert(ivl Interval, val interface{}) // Delete removes the node with the given interval from the tree, returning // true if a node is in fact removed. Delete(ivl Interval) bool // Len gives the number of elements in the tree. Len() int // Height is the number of levels in the tree; one node has height 1. Height() int // MaxHeight is the expected maximum tree height given the number of nodes. MaxHeight() int // Visit calls a visitor function on every tree node intersecting the given interval. // It will visit each interval [x, y) in ascending order sorted on x. Visit(ivl Interval, ivv IntervalVisitor) // Find gets the IntervalValue for the node matching the given interval Find(ivl Interval) *IntervalValue // Intersects returns true if there is some tree node intersecting the given interval. Intersects(iv Interval) bool // Contains returns true if the interval tree's keys cover the entire given interval. Contains(ivl Interval) bool // Stab returns a slice with all elements in the tree intersecting the interval. Stab(iv Interval) []*IntervalValue // Union merges a given interval tree into the receiver. Union(inIvt IntervalTree, ivl Interval) } 具体实现这里就不分析了，是基于红黑树实现了。 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:3:0","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"4. 总结 Watch 特性主要分为 serverWatchStream 和 MVCC 两部分。 serverWatchStream serverWatchStream 中包括 sendLoop 和 recvLoop 两个处理循环。 sendLoop 主要接收两种消息。 1） watchStream 推送的 event 消息，sendLoop 收到后并转发给 client 2）recvLoop 发来的 control 消息，包括watcher的create或者cancel，用以维护活跃 watcher 列表 recvLoop 则负责接收 client 的请求： 1）接收 client 的create/cancel watcher 消息，并调用 watchStream 的对应方法 create/cancel watcher 2）最后通过 ctlStream 和 sendLoop 进行通信，以维护 sendLoop 中的活跃 watchID 列表。 MVCC模块 MVCC中的核心模块则是 watchableStore，它通过将 watcher 划分为 synced/unsynced/victim 三类，将问题进行了分解，并通过多个后台异步循环 goroutine 负责不同场景下的事件推送，提供了各类异常等场景下的 Watch 事件重试机制，尽力确保变更事件不丢失、按逻辑时钟版本号顺序推送给 client。 具体流程 1）在 MVCC 的 put 事务中，它会将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 2）在 put 事务结束时调用的 End 方法中，它会将 KeyValue 转换成 Event 事件，然后调用 watchableStore.notify 函数。 3）notify 会匹配出监听过此 key 并处于 synced watcherGroup 中的 watcher，同时事件中的版本号要大于等于 watcher 监听的最小版本号，才能将事件发送到此 watcher 的事件 channel 中。 4）serverWatchStream 的 sendLoop goroutine 监听到 channel 消息后，读出消息立即推送给 client，至此，完成一个最新修改事件推送。 watcher 状态转换 拆分了 3 种不同的 watcherGroup 以执行不同场景下的事件推送。 为了保证效率和事件顺序，notify 中只处理 syncedGroup。 当出现事件堆积，导致 notify 中推送失败时，会从 syncedGroup 中移动到 victimGroup。 syncVictimLoop 则重试 victimGroup 中的 watcher，发送成功后根据 revision 判断添加到哪个 group： 已经追上最新的 revision 则添加到 syncedGroup 否则添加到 usyncedGroup syncWatchersLoop 则定时推送 usyncedGroup 中的 watcher，推送完成后若追上最新 revision 则将其移动到 syncedGroup 。 高效匹配 etcd 基于 map 和区间树数实现了 watcher 与事件快速匹配，保障了大规模场景下的 Watch 机制性能和读写稳定性。 最后再贴一下流程图，相信到这里的话，应该能够看懂了。 ","date":"2022-01-28","objectID":"/posts/etcd/14-watch-analyze-2/:4:0","tags":["etcd"],"title":"etcd教程(十四)---watch 机制源码分析（下）","uri":"/posts/etcd/14-watch-analyze-2/"},{"categories":["etcd"],"content":"etcd watch 机制具体实现及源码分析","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"本文主要通过源码分析了 etcd v3 版本 watch 机制的具体实现（上篇）。 ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:0:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"1. 概述 由于本期内容比较多，分成了上下两篇。 etcd教程(十三)—watch 机制源码分析（上） etcd教程(十四)—watch 机制源码分析（下） 为了避免客户端的反复轮询， etcd 提供了 watch 机制。客户端 watch 一系列 key，当这些被 watch 的 key 更新时， etcd 就会通知客户端。 本文主要为源码分析，，若对etcd watch 机制不熟悉的朋友可以先看下 etcd教程(五)—watch机制原理分析。 以下分析基本 etcd v3.5.1 版本。 ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:1:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"大致流程 整体流程如下图所示，可以分为两个部分: 1）创建 watcher 2）watch 事件推送 1）创建 watcher 发起一个 watch key 请求的时候，etcd 的 gRPCWatchServer 收到 watch 请求后，会创建一个 serverWatchStream, 它负责接收 client 的 gRPC Stream 的 create/cancel watcher 请求 (recvLoop goroutine)，并将从 MVCC 模块接收的 Watch 事件转发给 client(sendLoop goroutine)。 当 serverWatchStream 收到 create watcher 请求后，serverWatchStream 会调用 MVCC 模块的 WatchStream 子模块分配一个 watcher id，并将 watcher 注册到 MVCC 的 WatchableKV 模块。 在 etcd 启动的时候，WatchableKV 模块会运行 syncWatchersLoop 和 syncVictimsLoop goroutine，分别负责不同场景下的事件推送，它们也是 Watch 特性可靠性的核心之一。 2）watch 事件推送 当你创建完成 watcher 后，此时你执行 put hello 修改操作时，请求经过 KVServer、Raft 模块后 Apply 到状态机时，在 MVCC 的 put 事务中，它会将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 在 put 事务结束时，它会将 KeyValue 转换成 Event 事件，然后回调 watchableStore.notify 函数（如下精简代码所示）。notify 会匹配出监听过此 key 并处于 synced watcherGroup 中的 watcher，同时事件中的版本号要大于等于 watcher 监听的最小版本号，才能将事件发送到此 watcher 的事件 channel 中。 serverWatchStream 的 sendLoop goroutine 监听到 channel 消息后，读出消息立即推送给 client，至此，完成一个最新修改事件推送。 根据流程图可知，watch 机制分为图中的蓝色和橙色两部分： 处理 WatchRequest 和发送 WatchResponse 的姑且叫做 serverWatchStream 模块。 处理具体 PUT 操作，并推送事件的 MVCC 模块。 注：etcd 中并没有serverWatchStream这么一个模块，这部分逻辑是在 server 中的，本文暂且称这部分逻辑为 serverWatchStream 模块。 下面就从这两部分进行分析 ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:1:1","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"watcher 状态划分 etcd 中根据不同场景，对问题进行了分解，将 watcher 按场景分类，实现了轻重分离、低耦合。 具体如下： synced watcher，顾名思义，表示此类 watcher 监听的数据都已经同步完毕，在等待新的变更。 如果你创建的 watcher 未指定版本号 (默认 0)、或指定的版本号大于 etcd sever 当前最新的版本号 (currentRev)，那么它就会保存到 synced watcherGroup 中。 unsynced watcher，表示此类 watcher 监听的数据还未同步完成，落后于当前最新数据变更，正在努力追赶。 如果你创建的 watcher 指定版本号小于 etcd server 当前最新版本号，那么它就会保存到 unsynced watcherGroup 中。 victim watcher：表示 watcher 存在事件堆积导致推送异常 ，需要由异步任务执行重试操作。 从以上介绍中，我们可以将可靠的事件推送机制拆分成最新事件推送、异常场景重试、历史事件推送三个子问题来进行分析。 watcher 状态转换关系如下图所示： ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:1:2","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"2. Server ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:2:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"概述 etcd server 启动时会注册多个 gRPC server，watch server 也是其中一个： // server/etcdserver/api/v3rpc/grpc.go 39 行 func Server(s *etcdserver.EtcdServer, tls *tls.Config, interceptor grpc.UnaryServerInterceptor, gopts ...grpc.ServerOption) *grpc.Server { grpcServer := grpc.NewServer(append(opts, gopts...)...) // 省略无关逻辑 // 注册 WatchServer pb.RegisterWatchServer(grpcServer, NewWatchServer(s)) return grpcServer } WatchServer 只用于处理 Watch 方法： type WatchServer interface { Watch(Watch_WatchServer) error } 然后是 etcd 对外提供的 Watch 接口的定义： // api/etcdserverpb/rpc.proto 66 行 service Watch { // Watch watches for events happening or that have happened. Both input and output // are streams; the input stream is for creating and canceling watchers and the output // stream sends events. One watch RPC can watch on multiple key ranges, streaming events // for several watches at once. The entire event history can be watched starting from the // last compaction revision. rpc Watch(stream WatchRequest) returns (stream WatchResponse) { option (google.api.http) = { post: \"/v3/watch\" body: \"*\" }; } } etcd v3 使用的是 gRPC-Gateway 以同时提供 RPC 和 HTTP 服务，不了解的朋友可以看一下 gRPC(Go)教程(七)—利用Gateway同时提供HTTP和RPC服务。 实际上 gRPC-Gateway 这个库就是因为 etcd 更新到 v3 时需要同时提供 RPC 和 HTTP 服务而诞生的。 Watch 接口的大致逻辑为： 当你通过 etcdctl 或 API 发起一个 watch key 请求的时候，etcd 的 gRPCWatchServer 收到 watch 请求后，会创建一个 serverWatchStream, 它负责接收 client 的 gRPC Stream 的 create/cancel watcher 请求 (recvLoop goroutine)，并将从 MVCC 模块接收的 Watch 事件转发给 client(sendLoop goroutine)。 注：并不是每次调用 watch 就会创建一个 serverWatchStream，而是每个 gRPC Stream 只会创建一个，这个创建出来的 serverWatchStream 会接管这个 gRPC Stream 上的所有 watcher。 而一个连接上又可以创建多个 gRPC Stream，这也就是为什么 etcd v3的 watch 在资源占用上比 v2 有大幅降低。 具体如下图所示： ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:2:1","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"Watch 具体实现如下： // server/etcdserver/api/v3rpc/watch.go 152行 func (ws *watchServer) Watch(stream pb.Watch_WatchServer) (err error) { sws := serverWatchStream{ lg: ws.lg, clusterID: ws.clusterID, memberID: ws.memberID, maxRequestBytes: ws.maxRequestBytes, sg: ws.sg, watchable: ws.watchable, ag: ws.ag, gRPCStream: stream, watchStream: ws.watchable.NewWatchStream(), // chan for sending control response like watcher created and canceled. ctrlStream: make(chan *pb.WatchResponse, ctrlStreamBufLen), progress: make(map[mvcc.WatchID]bool), prevKV: make(map[mvcc.WatchID]bool), fragment: make(map[mvcc.WatchID]bool), closec: make(chan struct{}), } sws.wg.Add(1) // 第一个 goroutine sendLoop go func() { sws.sendLoop() sws.wg.Done() }() errc := make(chan error, 1) // 第二个 goroutine recvLoop go func() { if rerr := sws.recvLoop(); rerr != nil { if isClientCtxErr(stream.Context().Err(), rerr) { sws.lg.Debug(\"failed to receive watch request from gRPC stream\", zap.Error(rerr)) } else { sws.lg.Warn(\"failed to receive watch request from gRPC stream\", zap.Error(rerr)) streamFailures.WithLabelValues(\"receive\", \"watch\").Inc() } errc \u003c- rerr } }() select { case err = \u003c-errc: if err == context.Canceled { err = rpctypes.ErrGRPCWatchCanceled } close(sws.ctrlStream) case \u003c-stream.Context().Done(): err = stream.Context().Err() if err == context.Canceled { err = rpctypes.ErrGRPCWatchCanceled } } sws.close() return err } 和流程图一致，创建 serverWatchStream 并分别启动了sendLoop 和 recvLoop。 接下来就分析这两个 Loop。 ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:2:2","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"3. sendLoop sendLoop 主要负责把从 MVCC 模块接收的 Watch 事件转发给 client。 具体如下： // server/etcdserver/api/v3rpc/watch.go 355行 func (sws *serverWatchStream) sendLoop() { // watch ids that are currently active ids := make(map[mvcc.WatchID]struct{}) // watch responses pending on a watch id creation message pending := make(map[mvcc.WatchID][]*pb.WatchResponse) interval := GetProgressReportInterval() progressTicker := time.NewTicker(interval) // 省略部分逻辑 for { select { case wresp, ok := \u003c-sws.watchStream.Chan(): if !ok { return } evs := wresp.Events events := make([]*mvccpb.Event, len(evs)) sws.mu.RLock() needPrevKV := sws.prevKV[wresp.WatchID] sws.mu.RUnlock() for i := range evs { events[i] = \u0026evs[i] if needPrevKV \u0026\u0026 !IsCreateEvent(evs[i]) { opt := mvcc.RangeOptions{Rev: evs[i].Kv.ModRevision - 1} r, err := sws.watchable.Range(context.TODO(), evs[i].Kv.Key, nil, opt) if err == nil \u0026\u0026 len(r.KVs) != 0 { events[i].PrevKv = \u0026(r.KVs[0]) } } } canceled := wresp.CompactRevision != 0 wr := \u0026pb.WatchResponse{ Header: sws.newResponseHeader(wresp.Revision), WatchId: int64(wresp.WatchID), Events: events, CompactRevision: wresp.CompactRevision, Canceled: canceled, } if _, okID := ids[wresp.WatchID]; !okID { // buffer if id not yet announced wrs := append(pending[wresp.WatchID], wr) pending[wresp.WatchID] = wrs continue } mvcc.ReportEventReceived(len(evs)) sws.mu.RLock() fragmented, ok := sws.fragment[wresp.WatchID] sws.mu.RUnlock() var serr error if !fragmented \u0026\u0026 !ok { serr = sws.gRPCStream.Send(wr) } else { serr = sendFragments(wr, sws.maxRequestBytes, sws.gRPCStream.Send) } if serr != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), serr) { sws.lg.Debug(\"failed to send watch response to gRPC stream\", zap.Error(serr)) } else { sws.lg.Warn(\"failed to send watch response to gRPC stream\", zap.Error(serr)) streamFailures.WithLabelValues(\"send\", \"watch\").Inc() } return } sws.mu.Lock() if len(evs) \u003e 0 \u0026\u0026 sws.progress[wresp.WatchID] { // elide next progress update if sent a key update sws.progress[wresp.WatchID] = false } sws.mu.Unlock() case c, ok := \u003c-sws.ctrlStream: if !ok { return } if err := sws.gRPCStream.Send(c); err != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), err) { sws.lg.Debug(\"failed to send watch control response to gRPC stream\", zap.Error(err)) } else { sws.lg.Warn(\"failed to send watch control response to gRPC stream\", zap.Error(err)) streamFailures.WithLabelValues(\"send\", \"watch\").Inc() } return } // track id creation wid := mvcc.WatchID(c.WatchId) if c.Canceled { delete(ids, wid) continue } if c.Created { // flush buffered events ids[wid] = struct{}{} for _, v := range pending[wid] { mvcc.ReportEventReceived(len(v.Events)) if err := sws.gRPCStream.Send(v); err != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), err) { sws.lg.Debug(\"failed to send pending watch response to gRPC stream\", zap.Error(err)) } else { sws.lg.Warn(\"failed to send pending watch response to gRPC stream\", zap.Error(err)) streamFailures.WithLabelValues(\"send\", \"watch\").Inc() } return } } delete(pending, wid) } case \u003c-progressTicker.C: sws.mu.Lock() for id, ok := range sws.progress { if ok { sws.watchStream.RequestProgress(id) } sws.progress[id] = true } sws.mu.Unlock() case \u003c-sws.closec: return } } } 代码比较多，先看个大概流程： for { select { case wresp, ok := \u003c-sws.watchStream.Chan(): case c, ok := \u003c-sws.ctrlStream: case \u003c-progressTicker.C: case \u003c-sws.closec: } } 这样就比较清晰了，for + select 的常规套路。 4个 case 中，最后一个 case 为 channel 关闭时的逻辑： case \u003c-sws.closec: return channel 关闭后就直接退出循环，比较好理解。 倒数第二个 case 为一个定时器，主要用于维护 watcher 的进度（可以看做是类似心跳的东西）。 case \u003c-progressTicker.C: sws.mu.Lock() for id, ok := range sws.progress { if ok { sws.watchStream.RequestProgress(id) } sws.progress[id] = true } sws.mu.Unlock() 因为如果 watch 的 key没有任何变化，那么 watcher 就不会收到任何消息，因为没有新的事件产生。因此 etcd 中 添加了 progress request，定时发送一个进度信息给 watcher，让 watcher 在没有事件产生的时候也能收到一些消息。 // 如果有新的 event 产生，就把 progress 改成 fasle 以忽略掉下次进度更新消息的发送 if len(evs) \u003e 0 \u0026\u0026 sws.progress[wresp.WatchID] { sws.progress[wresp.WatchID] = false } ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:3:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"4. recvLoop recvLoop 主要负责接收 client 的 create/cancel watcher 请求。 具体如下： // server/etcdserver/api/v3rpc/watch.go 238行 func (sws *serverWatchStream) recvLoop() error { for { req, err := sws.gRPCStream.Recv() if err == io.EOF { return nil } if err != nil { return err } switch uv := req.RequestUnion.(type) { case *pb.WatchRequest_CreateRequest: if uv.CreateRequest == nil { break } creq := uv.CreateRequest if len(creq.Key) == 0 { creq.Key = []byte{0} } if len(creq.RangeEnd) == 0 { creq.RangeEnd = nil } if len(creq.RangeEnd) == 1 \u0026\u0026 creq.RangeEnd[0] == 0 { creq.RangeEnd = []byte{} } if !sws.isWatchPermitted(creq) { wr := \u0026pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: creq.WatchId, Canceled: true, Created: true, CancelReason: rpctypes.ErrGRPCPermissionDenied.Error(), } select { case sws.ctrlStream \u003c- wr: continue case \u003c-sws.closec: return nil } } filters := FiltersFromRequest(creq) wsrev := sws.watchStream.Rev() rev := creq.StartRevision if rev == 0 { rev = wsrev + 1 } id, err := sws.watchStream.Watch(mvcc.WatchID(creq.WatchId), creq.Key, creq.RangeEnd, rev, filters...) if err == nil { sws.mu.Lock() if creq.ProgressNotify { sws.progress[id] = true } if creq.PrevKv { sws.prevKV[id] = true } if creq.Fragment { sws.fragment[id] = true } sws.mu.Unlock() } wr := \u0026pb.WatchResponse{ Header: sws.newResponseHeader(wsrev), WatchId: int64(id), Created: true, Canceled: err != nil, } if err != nil { wr.CancelReason = err.Error() } select { case sws.ctrlStream \u003c- wr: case \u003c-sws.closec: return nil } case *pb.WatchRequest_CancelRequest: if uv.CancelRequest != nil { id := uv.CancelRequest.WatchId err := sws.watchStream.Cancel(mvcc.WatchID(id)) if err == nil { sws.ctrlStream \u003c- \u0026pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: id, Canceled: true, } sws.mu.Lock() delete(sws.progress, mvcc.WatchID(id)) delete(sws.prevKV, mvcc.WatchID(id)) delete(sws.fragment, mvcc.WatchID(id)) sws.mu.Unlock() } } case *pb.WatchRequest_ProgressRequest: if uv.ProgressRequest != nil { sws.ctrlStream \u003c- \u0026pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: -1, // response is not associated with any WatchId and will be broadcast to all watch channels } } default: continue } } } 同样也是先看下大致逻辑： for { req, err := sws.gRPCStream.Recv() if err == io.EOF { return nil } if err != nil { return err } switch uv := req.RequestUnion.(type) { case *pb.WatchRequest_CreateRequest: case *pb.WatchRequest_CancelRequest: case *pb.WatchRequest_ProgressRequest: default: continue } 可以看出主要是接收 client 的请求，然后根据请求类型做不同处理。 create 请求： case *pb.WatchRequest_CreateRequest: if uv.CreateRequest == nil { break } // 组装参数 creq := uv.CreateRequest if len(creq.Key) == 0 { // \\x00 is the smallest key creq.Key = []byte{0} } if len(creq.RangeEnd) == 0 { // force nil since watchstream.Watch distinguishes // between nil and []byte{} for single key / \u003e= creq.RangeEnd = nil } if len(creq.RangeEnd) == 1 \u0026\u0026 creq.RangeEnd[0] == 0 { // support \u003e= key queries creq.RangeEnd = []byte{} } // 校验是否能够执行 watch 请求，主要是判断用户有没有操作这个key的权限 if !sws.isWatchPermitted(creq) { // 如果没有权限就发送一个同时携带 Created 和 Canceled 标记的消息给前面的 sendLoop wr := \u0026pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: creq.WatchId, Canceled: true, Created: true, CancelReason: rpctypes.ErrGRPCPermissionDenied.Error(), } select { case sws.ctrlStream \u003c- wr: continue case \u003c-sws.closec: return nil } } // 若有权限则发送一个带 Created 标记的消息给前面的 sendLoop，以创建 watcher filters := FiltersFromRequest(creq) wsrev := sws.watchStream.Rev() rev := creq.StartRevision if rev == 0 { rev = wsrev + 1 } // 创建一个 watcher，并返回其 id id, err := sws.watchStream.Watch(mvcc.WatchID(creq.WatchId), creq.Key, creq.RangeEnd, rev, filters...) if err == nil { sws.mu.Lock() if creq.ProgressNotify { sws.progress[id] = true } if creq.PrevKv { sws.prevKV[id] = true } if creq.Fragment { sws.fragment[id] = true } sws.mu.Unlock() } wr := \u0026pb.WatchResponse{ Header: sws.newResponseHeader(wsrev), WatchId: int6","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:4:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"5. 小结 sendLoop 主要接收两种消息： 1） watchStream 推送的 event 消息，sendLoop 收到后并转发给 client 2）recvLoop 发来的 control 消息，包括watcher的create或者cancel，用以维护活跃 watcher 列表 recvLoop 则负责接收 client 的请求： 1）接收 client 的create/cancel watcher 消息，并调用 watchStream 的对应方法 create/cancel watcher 2）最后通过 ctlStream 和 sendLoop 进行通信，以维护 sendLoop 中的活跃 watchID 列表。 至此上篇结束，剩下的部分在下篇 etcd教程(十四)—watch 机制源码分析（下）。 ","date":"2022-01-21","objectID":"/posts/etcd/13-watch-analyze-1/:5:0","tags":["etcd"],"title":"etcd教程(十三)---watch 机制源码分析（上）","uri":"/posts/etcd/13-watch-analyze-1/"},{"categories":["etcd"],"content":"etcd v3 mvcc 具体实现与源码分析","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"本文主要通过源码分析了 etcd v3 版本 MVCC 的具体实现。 对 etcd mvcc 不太了解的朋友，可以先阅读这篇文章：etcd教程(六)—etcd多版本并发控制 以下分析基于 etcd v3.5.1版本。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:0:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"1. 概述 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:1:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"为什么选择MVCC etcd v3 版本为了解决 v2 版本的 并发性能问题和 watch 机制可靠性问题，因此选择了 MVCC 机制。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:1:1","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"大致实现 etcd 借助 blotdb，以 revision 为 key，在 blotdb 中存储了 key 的多版本数据。 借助 treeIndex 模块，在内存中以 BTree 构建了 keyIndex 结构来关联 key 及其对应的 revisions。 用户操作时，先根据 key 查询 keyIndex 找到对应的 revisions，然后再操作 blotdb。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:1:2","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"整体架构 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:1:3","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"2. treeIndex 模块 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:2:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"相关结构 B-tree 结构如下： type treeIndex struct { sync.RWMutex tree *btree.BTree lg *zap.Logger } 可以看到是基于 Google 开源的 BTree 实现的。 在 treeIndex 中，每个节点的 key 是一个 keyIndex 结构，etcd 就是通过它保存了用户的 key 与版本号的映射关系。 每个 B-tree 节点保存的具体内容如下： type keyIndex struct { key []byte // 用户的key名称 modified revision // 最后一次修改key时的etcd版本号 generations []generation // generation保存了一个key若干代版本号信息，每代中包含对key的多次修改的版本号列表 } keyIndex 中包含用户的 key、最后一次修改 key 时的 etcd 版本号、key 的若干代（generation）版本号信息，每代中包含对 key 的多次修改的版本号列表。 generations 表示一个 key 从创建到删除的过程，每代对应 key 的一个生命周期的开始与结束。 当你第一次创建一个 key 时，会生成第 0 代，后续的修改操作都是在往第 0 代中追加修改版本号。 当你把 key 删除后，它就会生成新的第 1 代，一个 key 不断经历创建、删除的过程，它就会生成多个代。 generation 结构详细信息如下： type generation struct { ver int64 //表示此key的修改次数 created revision //表示generation结构创建时的版本号 revs []revision //每次修改key时的revision追加到此数组 } generation 结构中包含此 key 的修改次数、generation 创建时的版本号、对此 key 的修改版本号记录列表。 你需要注意的是版本号（revision）并不是一个简单的整数，而是一个结构体。revision 结构及含义如下： type revision struct { main int64 // 一个全局递增的主版本号，随put/txn/delete事务递增，一个事务内的key main版本号是一致的 sub int64 // 一个事务内的子版本号，从0开始随事务内put/delete操作递增 } revision 包含 main 和 sub 两个字段： main 是全局递增的版本号，它是个 etcd 逻辑时钟，随着 put/txn/delete 等事务递增。 sub 是一个事务内的子版本号，从 0 开始随事务内的 put/delete 操作递增。 比如启动一个空集群，全局版本号默认为 1，执行下面的 txn 事务，它包含两次 put、一次 get 操作，那么按照我们上面介绍的原理，全局版本号随读写事务自增，因此是 main 为 2，sub 随事务内的 put/delete 操作递增，因此 key hello 的 revison 为{2,0}，key world 的 revision 为{2,1}。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:2:1","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"相关操作 treeIndex 模块具体实现的方法如下所示： // server/storage/mvcc/index.go 25 行 type index interface { Get(key []byte, atRev int64) (rev, created revision, ver int64, err error) Range(key, end []byte, atRev int64) ([][]byte, []revision) Revisions(key, end []byte, atRev int64, limit int) ([]revision, int) CountRevisions(key, end []byte, atRev int64) int Put(key []byte, rev revision) Tombstone(key []byte, rev revision) error RangeSince(key, end []byte, rev int64) []revision Compact(rev int64) map[revision]struct{} Keep(rev int64) map[revision]struct{} Equal(b index) bool Insert(ki *keyIndex) KeyIndex(ki *keyIndex) *keyIndex } 简单分析以下 Get 方法： // server/storage/mvcc/index.go 69 行 func (ti *treeIndex) Get(key []byte, atRev int64) (modified, created revision, ver int64, err error) { keyi := \u0026keyIndex{key: key} ti.RLock() defer ti.RUnlock() if keyi = ti.keyIndex(keyi); keyi == nil { return revision{}, revision{}, 0, ErrRevisionNotFound } return keyi.get(ti.lg, atRev) } 只是查询，所以加的是 ReadLock。 // server/storage/mvcc/index.go 137 行 func (ki *keyIndex) get(lg *zap.Logger, atRev int64) (modified, created revision, ver int64, err error) { if ki.isEmpty() { lg.Panic( \"'get' got an unexpected empty keyIndex\", zap.String(\"key\", string(ki.key)), ) } // 首先找到对应的 Generation g := ki.findGeneration(atRev) if g.isEmpty() { return revision{}, revision{}, 0, ErrRevisionNotFound } // 然后根据版本号大小关系找到对应位置 n := g.walk(func(rev revision) bool { return rev.main \u003e atRev }) if n != -1 { return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil } return revision{}, revision{}, 0, ErrRevisionNotFound } // server/storage/mvcc/index.go 280 行 func (ki *keyIndex) findGeneration(rev int64) *generation { lastg := len(ki.generations) - 1 cg := lastg // 比较简单，就是一个 for 循环 for cg \u003e= 0 { if len(ki.generations[cg].revs) == 0 { cg-- continue } g := ki.generations[cg] if cg != lastg { if tomb := g.revs[len(g.revs)-1].main; tomb \u003c= rev { return nil } } // 找的是小于等于的版本号 if g.revs[0].main \u003c= rev { return \u0026ki.generations[cg] } cg-- } return nil } 这个for循环是倒着循环的，即从最新的 generations 开始遍历。这也算是一个小的优化吧，毕竟查询的时候就算是历史版本也是最近的版本，这样倒叙循环能省掉一些无效的遍历，会快一些。 找到对应的 generation 后就进入 walk 逻辑： /* n := g.walk(func(rev revision) bool { return rev.main \u003e atRev }) if n != -1 { return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil } */ func (g *generation) walk(f func(rev revision) bool) int { l := len(g.revs) for i := range g.revs { ok := f(g.revs[l-i-1]) if !ok { return l - i - 1 } } return -1 } walk 就是行走的意思，可以理解为遍历每个元素，都执行这个操作。 这也是 Go 中常用的一种写法。 实现也很简单，就是一个for循环，找到对应的位置并返回。 这就是 treeIndex 模块的 Get 方法实现： 通过第一次 for 循环找到对应 generation 再通过第二次 for 循环找到对应 revision 然后就是根据 revision 为 key，去 blotdb 中查询对应的 value 了。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:2:2","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"3. MVCC 模块 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:3:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"1. put 一个 put 命令流程如下图所示： 共分为以下几个步骤： 1）查询 keyIndex keyIndex 中存储了 key 的创建版本号、修改的次数等信息，这些信息在事务中发挥着重要作用，因此会存储在 boltdb 的 value 中。 2）写入 boltdb 3）更新 treeIndex 4）持久化 为了提升性能，具体实现为异步批量操作 为了提升 etcd 的写吞吐量、性能，一般情况下（默认堆积的写事务数大于 1 万才在写事务结束时同步持久化），数据持久化由 Backend 的异步 goroutine 完成，它通过事务批量提交，定时将 boltdb 页缓存中的脏数据提交到持久化存储磁盘中。 源码如下： // server/storage/mvcc/kvstore_txn.go 108 行 func (tw *storeTxnWrite) Put(key, value []byte, lease lease.LeaseID) int64 { tw.put(key, value, lease) return tw.beginRev + 1 } // server/storage/mvcc/kvstore_txn.go 182 行 func (tw *storeTxnWrite) put(key, value []byte, leaseID lease.LeaseID) { rev := tw.beginRev + 1 c := rev oldLease := lease.NoLease // 1.查询keyIndex _, created, ver, err := tw.s.kvindex.Get(key, rev) if err == nil { c = created.main oldLease = tw.s.le.GetLease(lease.LeaseItem{Key: string(key)}) } ibytes := newRevBytes() idxRev := revision{main: rev, sub: int64(len(tw.changes))} revToBytes(idxRev, ibytes) ver = ver + 1 kv := mvccpb.KeyValue{ Key: key, Value: value, CreateRevision: c, ModRevision: rev, Version: ver, Lease: int64(leaseID), } d, err := kv.Marshal() if err != nil { tw.storeTxnRead.s.lg.Fatal( \"failed to marshal mvccpb.KeyValue\", zap.Error(err), ) } // 2.写blotdb tw.tx.UnsafeSeqPut(schema.Key, ibytes, d) // 3.更新keyIndex tw.s.kvindex.Put(key, idxRev) tw.changes = append(tw.changes, kv) // lease 相关更新 // 若存在旧lease则移除 if oldLease != lease.NoLease { if tw.s.le == nil { panic(\"no lessor to detach lease\") } err = tw.s.le.Detach(oldLease, []lease.LeaseItem{{Key: string(key)}}) if err != nil { tw.storeTxnRead.s.lg.Error( \"failed to detach old lease from a key\", zap.Error(err), ) } } // 若本次指定了 lease则关联上 if leaseID != lease.NoLease { if tw.s.le == nil { panic(\"no lessor to attach lease\") } err = tw.s.le.Attach(leaseID, []lease.LeaseItem{{Key: string(key)}}) if err != nil { panic(\"unexpected error from lease Attach\") } } } 具体逻辑和前面分析的一致，不过这里需要注意的是 Lease 相关的处理。PUT 时会移除旧的 Lease 和 key 的关联。这就意味着如果想要一直让 key 关联 lease 的话需要每次 PUT 都指定Lease才行。 这和 Redis 的 TTL 还是有很大的不同 然后发现一个问题，如果更新的时候提交一个相同的 leaseID，岂不是会先 Detach 然后又 Attach 上去？可以说是白给了。 于是提了个 PR，现在已经合并进主干了。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:3:1","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"2. get 具体流程如下： 1）查询版本号 2）查询 blotdb 具体如下： // server/storage/mvcc/kvstore_txn.go 61行 func (tr *storeTxnRead) Range(ctx context.Context, key, end []byte, ro RangeOptions) (r *RangeResult, err error) { return tr.rangeKeys(ctx, key, end, tr.Rev(), ro) } // server/storage/mvcc/kvstore_txn.go 127行 func (tr *storeTxnRead) rangeKeys(ctx context.Context, key, end []byte, curRev int64, ro RangeOptions) (*RangeResult, error) { rev := ro.Rev if rev \u003e curRev { return \u0026RangeResult{KVs: nil, Count: -1, Rev: curRev}, ErrFutureRev } // 若没指定或指定了错误的版本号就会默认查最新的一个版本 if rev \u003c= 0 { rev = curRev } // 1.查找 revisions // 这里如果当前查询的版本号比compactMainRev小说明这个版本已经被回收了 直接返回错误 if rev \u003c tr.s.compactMainRev { return \u0026RangeResult{KVs: nil, Count: -1, Rev: 0}, ErrCompacted } if ro.Count { total := tr.s.kvindex.CountRevisions(key, end, rev) tr.trace.Step(\"count revisions from in-memory index tree\") return \u0026RangeResult{KVs: nil, Count: total, Rev: curRev}, nil } // 否则就查询比当前版本号大的所有版本号 revpairs, total := tr.s.kvindex.Revisions(key, end, rev, int(ro.Limit)) tr.trace.Step(\"range keys from in-memory index tree\") if len(revpairs) == 0 { return \u0026RangeResult{KVs: nil, Count: total, Rev: curRev}, nil } limit := int(ro.Limit) if limit \u003c= 0 || limit \u003e len(revpairs) { limit = len(revpairs) } kvs := make([]mvccpb.KeyValue, limit) revBytes := newRevBytes() // 2.查询 blotdb // 然后根据上面查到的版本号循环去blotdb中查找对应value for i, revpair := range revpairs[:len(kvs)] { select { case \u003c-ctx.Done(): return nil, ctx.Err() default: } revToBytes(revpair, revBytes) _, vs := tr.tx.UnsafeRange(schema.Key, revBytes, nil, 0) if len(vs) != 1 { tr.s.lg.Fatal( \"range failed to find revision pair\", zap.Int64(\"revision-main\", revpair.main), zap.Int64(\"revision-sub\", revpair.sub), ) } if err := kvs[i].Unmarshal(vs[0]); err != nil { tr.s.lg.Fatal( \"failed to unmarshal mvccpb.KeyValue\", zap.Error(err), ) } } tr.trace.Step(\"range keys from bolt db\") return \u0026RangeResult{KVs: kvs, Count: total, Rev: curRev}, nil } 根据源码可以知道，当我们没有指定 Revision 时，etcd 会默认查询最新版本的数据。 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:3:2","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"3. del 当执行 del 命令时 etcd 实现的是延期删除模式，原理与 key 更新类似。 与更新 key 不一样之处在于： 一方面，生成的 boltdb key 版本号{4,0,t}追加了删除标识（tombstone, 简写 t），boltdb value 变成只含用户 key 的 KeyValue 结构体。 另一方面 treeIndex 模块也会给此 key hello 对应的 keyIndex 对象，追加一个空的 generation 对象，表示此索引对应的 key 被删除了。 当你再次查询 hello 的时候，treeIndex 模块根据 key hello 查找到 keyindex 对象后，若发现其存在空的 generation 对象，并且查询的版本号大于等于被删除时的版本号，则会返回空。 那么 key 打上删除标记后有哪些用途呢？什么时候会真正删除它呢？ 一方面删除 key 时会生成 events，Watch 模块根据 key 的删除标识，会生成对应的 Delete 事件。 另一方面，当你重启 etcd，遍历 boltdb 中的 key 构建 treeIndex 内存树时，你需要知道哪些 key 是已经被删除的，并为对应的 key 索引生成 tombstone 标识。 而真正删除 treeIndex 中的索引对象、boltdb 中的 key 是通过压缩 (compactor) 组件异步完成。 正因为 etcd 的删除 key 操作是基于以上延期删除原理实现的，因此只要压缩组件未回收历史版本，我们就能从 etcd 中找回误删的数据。 具体如下： // server/storage/mvcc/kvstore_txn.go 101行 func (tw *storeTxnWrite) DeleteRange(key, end []byte) (int64, int64) { if n := tw.deleteRange(key, end); n != 0 || len(tw.changes) \u003e 0 { return n, tw.beginRev + 1 } return 0, tw.beginRev } // server/storage/mvcc/kvstore_txn.go 247行 func (tw *storeTxnWrite) deleteRange(key, end []byte) int64 { rrev := tw.beginRev if len(tw.changes) \u003e 0 { rrev++ } // 1.先在 keyIndex 中找到 blotdb 中对应的key keys, _ := tw.s.kvindex.Range(key, end, rrev) if len(keys) == 0 { return 0 } // 2. 循环删除 for _, key := range keys { tw.delete(key) } return int64(len(keys)) } 具体 blotdb 删除逻辑如下： // server/storage/mvcc/kvstore_txn.go 262行 func (tw *storeTxnWrite) delete(key []byte) { ibytes := newRevBytes() idxRev := revision{main: tw.beginRev + 1, sub: int64(len(tw.changes))} revToBytes(idxRev, ibytes) // 1.标记删除 blotdb // 在 blotdb 的 key上追加tombstone标识(标记删除) ibytes = appendMarkTombstone(tw.storeTxnRead.s.lg, ibytes) kv := mvccpb.KeyValue{Key: key} d, err := kv.Marshal() if err != nil { tw.storeTxnRead.s.lg.Fatal( \"failed to marshal mvccpb.KeyValue\", zap.Error(err), ) } // 因为是标记删除,所以这里调用的是 put而不是delete tw.tx.UnsafeSeqPut(schema.Key, ibytes, d) // 2.处理keyIndex err = tw.s.kvindex.Tombstone(key, idxRev) if err != nil { tw.storeTxnRead.s.lg.Fatal( \"failed to tombstone an existing key\", zap.String(\"key\", string(key)), zap.Error(err), ) } tw.changes = append(tw.changes, kv) // 3.如果还有关联的 lease,则移除关联 item := lease.LeaseItem{Key: string(key)} leaseID := tw.s.le.GetLease(item) if leaseID != lease.NoLease { err = tw.s.le.Detach(leaseID, []lease.LeaseItem{item}) if err != nil { tw.storeTxnRead.s.lg.Error( \"failed to detach old lease from a key\", zap.Error(err), ) } } } 对 keyIndex 的处理如下： // server/storage/mvcc/index.go 165 行 func (ti *treeIndex) Tombstone(key []byte, rev revision) error { keyi := \u0026keyIndex{key: key} ti.Lock() defer ti.Unlock() // 如果 key 不存在，返回一个错误 item := ti.tree.Get(keyi) if item == nil { return ErrRevisionNotFound } ki := item.(*keyIndex) return ki.tombstone(ti.lg, rev.main, rev.sub) } 具体逻辑如下： // server/storage/mvcc/key_index.go 119行 func (ki *keyIndex) tombstone(lg *zap.Logger, main int64, sub int64) error { if ki.isEmpty() { lg.Panic( \"'tombstone' got an unexpected empty keyIndex\", zap.String(\"key\", string(ki.key)), ) } if ki.generations[len(ki.generations)-1].isEmpty() { return ErrRevisionNotFound } // 首先是把当前删除也作为一个版本号写进入 ki.put(lg, main, sub) // 然后新增了一个 generation，后续的操作就会记录到这个新的 generation 里 ki.generations = append(ki.generations, generation{}) // 这个是用于 prometheus 测量数据用的，标记着 etcd 中的 key的数量 // 虽然是标记删除但还是把这个计数-1了，等后续这个key被再次创建的时候又会+1 keysGauge.Dec() return nil } ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:3:3","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"4. 小结 1）blotdb 中以 revision 作为 key，以存储多版本数据。 2）treeIndex 模块中构建 BTree 结构的 keyIndex 以关联 key 和 revisions 的关系，加快查询速度。 3）当你未带版本号查询 key 时，etcd 返回的是 key 最新版本数据。 4）删除一个数据时，etcd 并未真正删除它，而是基于 lazy delete 实现的异步删除，真正删除 key 是通过 etcd 的压缩组件去异步实现的。 具体为 del 时会在 keyIndex 中追加一个空的 generation 若查询时发送有空的 generation 且查询版本号大于 keyIndex 中的版本号则说明该 key 已经被删除了，当前查询会返回空数据 ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:4:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["etcd"],"content":"5. 参考 https://github.com/etcd-io/etcd ","date":"2022-01-14","objectID":"/posts/etcd/12-mvcc-analyze/:5:0","tags":["etcd"],"title":"etcd教程(十二)---etcd mvcc 源码分析","uri":"/posts/etcd/12-mvcc-analyze/"},{"categories":["Golang"],"content":"context包结构分析及其基本使用介绍","date":"2022-01-07","objectID":"/posts/go/swagger/","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"本文主要记录了如何在 Go 中使用 swaggo 根据注释自动生成 API 文档，以及如何使用条件编译来降低二进制文件大小。 之前也用过其他的API文档工具，但是最大的问题还是文档和代码是分离的。总是出现文档和代码不同步的情况。 于是最终采用的 swaggo，根据注释自动生成 API 文档，注释即文档。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:0:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"1. 概述 ","date":"2022-01-07","objectID":"/posts/go/swagger/:1:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"关于Swaggo 或许你使用过 Swagger, 而 swaggo 就是代替了你手动编写 yaml 的部分。只要通过一个命令 就可以将注释转换成文档，这让我们可以更加专注于代码。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:1:1","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"接入流程 接入流程主要分为以下几个步骤： 0）main 文件中添加注释-配置Server，服务信息 1）controller 中添加注释-配置接口，接口信息 2）swag init 生成 docs 目录 3）配置 handler 访问 4）访问测试 ","date":"2022-01-07","objectID":"/posts/go/swagger/:1:2","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"2. 演示 以 gin 框架为例，演示一下具体操作流程。 完整 Demo 见 Github ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"配置服务信息 main.go 中引入以下两个包： import \"github.com/swaggo/gin-swagger\" // gin-swagger middleware import \"github.com/swaggo/files\" // swagger embed files 并在 main 方法上添加以下注释描述 server： 具体可以添加哪些注释参考 通用API信息 // 添加注释以描述 server 信息 // @title Swagger Example API // @version 1.0 // @description This is a sample server celler server. // @termsOfService http://swagger.io/terms/ // @contact.name API Support // @contact.url http://www.swagger.io/support // @contact.email support@swagger.io // @license.name Apache 2.0 // @license.url http://www.apache.org/licenses/LICENSE-2.0.html // @host localhost:8080 // @BasePath /api/v1 // @securityDefinitions.basic BasicAuth func main() { r := gin.Default() c := controller.NewController() v1 := r.Group(\"/api/v1\") { accounts := v1.Group(\"/accounts\") { accounts.GET(\":id\", c.ShowAccount) } //... } r.Run(\":8080\") } //... ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:1","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"配置接口文档 在每个接口的 controller 上添加注释以描述接口，就像下面这样： 具体可以添加哪些注释参考 API操作 // ShowAccount godoc // @Summary Show an account // @Description get string by ID // @Tags accounts // @Accept json // @Produce json // @Param id path int true \"Account ID\" // @Success 200 {object} model.Account // @Failure 400 {object} httputil.HTTPError // @Failure 404 {object} httputil.HTTPError // @Failure 500 {object} httputil.HTTPError // @Router /accounts/{id} [get] func (c *Controller) ShowAccount(ctx *gin.Context) { id := ctx.Param(\"id\") aid, err := strconv.Atoi(id) if err != nil { httputil.NewError(ctx, http.StatusBadRequest, err) return } account, err := model.AccountOne(aid) if err != nil { httputil.NewError(ctx, http.StatusNotFound, err) return } ctx.JSON(http.StatusOK, account) } ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:2","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"swag init 需要先安装 swag，使用如下命令下载swag： $ go get -u github.com/swaggo/swag/cmd/swag # 1.16 及以上版本 $ go install github.com/swaggo/swag/cmd/swag@latest 在包含main.go文件的项目根目录运行swag init。这将会解析注释并生成需要的文件（docs文件夹和docs/docs.go）。 swag init 注：swag init 默认会找当前目录下的 main.go 文件，如果不叫 main.go 也可以手动指定文件位置。 # -o 指定输出目录。 swag init -g cmd/api/api.go -o cmd/api/docs 需要注意的是：swag init 的时候需要在项目根目录下执行，否则无法检测到所有文件中的注释。 比如在 /xxx 目录下执行 swag init 就只能检测到 xxx 目录下的，如果还有和 xxx 目录同级或者更上层的目录中的代码都检测不到。 init 之后会生成一个 docs 文件夹，这里面就是接口描述文件，生成后还需要将其导入到 main.go 中。 在 main.go 中导入刚才生成的 docs 包 在 main.go 中导入刚才生成的 docs 包 在 main.go 中导入刚才生成的 docs 包 重要的事情说三遍 加上之前导入的 gin-swag 相关的两个包，一共新导入了三个包。 _ \"xx/cmd/api/docs\" // main 文件中导入 docs 包 ginSwagger \"github.com/swaggo/gin-swagger\" \"github.com/swaggo/gin-swagger/swaggerFiles\" ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:3","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"配置文档handler 最后则是在 router 中增加 swagger 的 handler 了。 在 main.go 或其他地方增加一个 handler. engine := gin.New() engine.GET(\"swagger/*any\", ginSwagger.WrapHandler(swaggerFiles.Handler)) ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:4","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"访问测试 项目运行起来后访问ip:port/swagger/index.html 即可看到 API 文档。 如果注释有更新，需要重新生成 docs 并重启服务才会生效。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:2:5","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"3. 注释语法 具体语法见 官方文档，这里主要列出几个特殊的点。 Controller 注释支持很多字段，这里主要记录常用的。 tags：给 API 按照 tag 分组，便于管理。 accept、produce：API 接收和响应的 MMIE 类型。 param：接口请求参数，重要 Syntax：param name,param type,data type,is mandatory?,comment attribute(optional) response、success、failure：API 响应内容，如果成功失败返回值不一样也可以通过 success、failure 分别描述。 Syntax：return code,{param type},data type,comment header：响应头 Syntax： return code,{param type},data type,comment router：接口路由 Syntax：path,[httpMethod] 主要详细记录一下 param 和 response 该怎么写。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:3:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"param 语法：param name,param type,data type,is mandatory?,comment attribute(optional) 参数类型： query path header body formData 数据类型： string (string) integer (int, uint, uint32, uint64) number (float32) boolean (bool) user defined struct 示例 // @Param Authorization header string true \"JWT\" // @Param req body listModel true \"相关信息\" // @Param amount query string true \"订单金额(元)\" type listModel struct { AnswerId string `form:\"answerId\"` Page int `form:\"page\"` } 以上示例就用到了 3 种参数： header 中的 Authorization body 中的 listModel queryString 中的 amount 由于是可以用结构体的，所以一般都建议使用结构体，这样比较简洁。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:3:1","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"resp 语法：return code,{param type},data type,comment 返回值类型和数据类型和 param 都是一致的。 示例： // @Success 200 {object} respmodel.WxPayMp // @Failure 400 {object} srv.Result 由于成功失败返回结构不同，所以分别处理。如果相同就直接用 response 即可。 不过大部分项目中应该会定义一个通用返回结构，比如这样的： type Result struct { Code int `json:\"code\"` Data interface{} `json:\"data\"` Msg string `json:\"msg\"` } 不同的接口，只需要替换里面的 Data 字段即可。 对于这种情况，如果每次都指定返回值是 Result 结构，就无法看到具体的响应了。但是肯定也不可能为每个接口重新生成一个对应的返回结构。 好在 swaggo 提供了 响应对象中的模型组合，可以自行组合结构体，以处理这种通用返回结果的情况。 对于固定返回结构来说这个就很方便，可以为每个接口指定对应的 data 字段内容。 具体如下： // JSONResult的data字段类型将被proto.Order类型替换 // @success 200 {object} jsonresult.JSONResult{data=proto.Order} \"desc\" type JSONResult struct { Code int `json:\"code\" ` Message string `json:\"message\"` Data interface{} `json:\"data\"` } type Order struct { //in `proto` package ... } 还支持对象数组和原始类型作为嵌套响应 // @success 200 {object} jsonresult.JSONResult{data=[]proto.Order} \"desc\" // @success 200 {object} jsonresult.JSONResult{data=string} \"desc\" // @success 200 {object} jsonresult.JSONResult{data=[]string} \"desc\" 替换多个字段的类型。如果某字段不存在，将添加该字段。 // @success 200 {object} jsonresult.JSONResult{data1=string,data2=[]string,data3=proto.Order,data4=[]proto.Order} \"desc\" 对于上面的情况就可以这样处理了： // @Success 200 {object} srv.Result{data=respmodel.WxPayMp} // @Failure 400 {object} srv.Result 对于不同的接口，只需要替换其中的 data 字段即可。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:3:2","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"4. FAQ ","date":"2022-01-07","objectID":"/posts/go/swagger/:4:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"无法解析外部依赖 就是在项目引入了另外一个独立项目的数据结构体，在接口上配置 // @Success 200 {object} ginfw.BaseHttpResponse 发现 swag init 无法正常生成想要的 swagger yaml 文件。 swag init 命令执行错误 cannot find type definition 解决方案 增加--parseDependency --parseInternal 两个参数同时在main.go 中导入我们依赖的包。 swag init –parseInternal depends on –parseDependency 注：因为依赖的是外部包的内部结构,导致无法扫描到，同理，直接引入 go 内置的包也无法直接解析，也是需要这样处理一下。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:4:1","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"无法引入内部依赖 有时候一个项目里依赖也无法解析: // @Success 200 {object} srv.Result{data=respmodel.AnswerItem} swag init 时报错： cannot find type definition: respmodel.AnswerItem 尝试了--parseInternal depends on --parseDependency也没有效果，最后发现： 这个是因为同一个包里面定义了几个重名的结构体导致的，改名即可解决问题。 比如这里是在 user 的 respmodel 中定义了 AnswerItem，然后在 admin 的 respmodel 中也定义了 AnswerItem，二者结构不同只是名字相同。 因为几个项目是在一个仓库里，导致初始化时不知道用哪个了。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:4:2","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"构建速度 在添加了--parseDependency 后生成速度明显减低，此时可以添加--parseDepth 参数指定数据结构深度来加快生成速度。 相关参数含义如下： 参数名 含义解释 parseInternal 解析内部依赖包，默认值: false parseDependency 解析外部依赖包，默认值: false parseDepth 解析依赖包深度，默认值:100 注：parseDepth 这个参数非常有用，如果你知道需要解析的数据结构深度，建议使用这个参数，swag 命令执行时间会大大缩短。 ","date":"2022-01-07","objectID":"/posts/go/swagger/:4:3","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"5. 优化 ","date":"2022-01-07","objectID":"/posts/go/swagger/:5:0","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"swag fmt swaggo 提供了 swag fmt 工具，可以针对Swag的注释自动格式化，就像go fmt，让注释看起来更统一。 示例： swag fmt 排除目录（不扫描）示例： swag fmt -d ./ --exclude ./internal 指定 main.go 文件示例： swag fmt -g cmd/api/api.go ","date":"2022-01-07","objectID":"/posts/go/swagger/:5:1","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["Golang"],"content":"条件编译 swaggo 是直接 build 到二进制里的，会极大增加二进制文件的大小，一般在生产环境不需要将 docs 编译进去。 可以利用 go 提供的条件编译来实现是否编译文档。 在main.go声明swagHandler,并在该参数不为空时才加入路由： package main //... var swagHandler gin.HandlerFunc func main(){ // ... if swagHandler != nil { r.GET(\"/swagger/*any\", swagHandler) } //... } 同时,我们将该参数在另外加了build tag的包中初始化。 条件编译只需要在对应文件首行加入 go:build xxx 即可，这样只有编译时指定 xxx tag 才会把该文件编译进去。 //go:build doc package main import ( ginSwagger \"github.com/swaggo/gin-swagger\" \"github.com/swaggo/gin-swagger/swaggerFiles\" _ \"i-go/gin/swagger/docs\" ) func init() { swagHandler = ginSwagger.WrapHandler(swaggerFiles.Handler) } 之后我们就可以使用go build -tags \"doc\"来打包带文档的包，直接go build来打包不带文档的包。 注：go 1.16 之前旧版条件编译语法为 //+build 1.16之后增加了新版语法 go:build 完整 Demo 见 Github ","date":"2022-01-07","objectID":"/posts/go/swagger/:5:2","tags":["Golang"],"title":"Go语言之使用 swaggo 一键生成 API 文档","uri":"/posts/go/swagger/"},{"categories":["etcd"],"content":"etcd 事务特性","date":"2021-12-24","objectID":"/posts/etcd/11-txn/","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"本文主要记录了 etcd 事务API 以及 ACID 特性的大致实现。 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:0:0","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"1. 事务 API ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:1:0","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"概述 以 Alice 向 Bob 转账为例： Alice 给 Bob 转账 100 元，Alice 账号减少 100，Bob 账号增加 100，这涉及到多个 key 的原子更新。 在 etcd v2 的时候， etcd 提供了CAS（Compare and swap），然而其只支持单 key，不支持多 key，因此无法满足类似转账场景的需求。严格意义上说 CAS 称不上事务，无法实现事务的各个隔离级别。 etcd v3 为了解决多 key 的原子操作问题，提供了全新迷你事务 API，同时基于 MVCC 版本号，它可以实现各种隔离级别的事务。它的基本结构如下： client.Txn(ctx).If(cmp1, cmp2, ...).Then(op1, op2, ...,).Else(op1, op2, …) 事务 API 由 If 语句、Then 语句、Else 语句组成 它的基本原理是，在 If 语句中，你可以添加一系列的条件表达式，若条件表达式全部通过检查，则执行 Then 语句的 get/put/delete 等操作，否则执行 Else 的 get/put/delete 等操作。 If 语句中的支持项如下： 1）key 的最近一次修改版本号 mod_revision，简称 mod，可以用于检查 key 最近一次被修改时的版本号是否符合你的预期。 比如当你查询到 Alice 账号资金为 100 元时，它的 mod_revision 是 v1，当你发起转账操作时，你得确保 Alice 账号上的 100 元未被挪用，这就可以通过 mod(\"Alice\") = \"v1\" 条件表达式来保障转账安全性。 2）key 的创建版本号 create_revision，简称 create,可以用于检测 key 是否已存在。 比如在分布式锁场景里，只有分布式锁 key(lock) 不存在的时候，你才能发起 put 操作创建锁，这时你可以通过 create(\"lock\") = \"0\"来判断，因为一个 key 不存在的话它的 create_revision 版本号就是 0。 3） key 的修改次数 version；可以用于检查 key 的修改次数是否符合预期。 比如你期望 key 在修改次数小于 3 时，才能发起某些操作时，可以通过 version(“key”) \u003c “3\"来判断。 4）key 的值，可以用于检查 key 的 value 值是否符合预期。 比如期望 Alice 的账号资金为 200, value(\"Alice\") = \"200\"。 If 语句通过以上 MVCC 版本号、value 值、各种比较运算符 (等于、大于、小于、不等于)，实现了灵活的比较的功能，满足你各类业务场景诉求。 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:1:1","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"示例 下面是使用 etcdctl 的 txn 事务命令，基于以上介绍的特性，初步实现的一个 Alice 向 Bob 转账 100 元的事务： # 指定使用 etcd v3 api $ export ETCDCTL_API=3 // -i 交互式事务 $ etcdctl txn -i compares: //对应If语句 value(\"Alice\") = \"200\" //判断Alice账号资金是否为200 success requests (get, put, del): //对应Then语句 put Alice 100 //Alice账号初始资金200减100 put Bob 300 //Bob账号初始资金200加100 failure requests (get, put, del): //对应Else语句 get Alice get Bob SUCCESS //If语句检测通过 OK // Then 中的语句1执行成功 OK // Then 中的语句1执行成功 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:1:2","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"2. ACID 特性 ACID 是衡量事务的四个特性，由原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）组成。 其他数据库的 ACID 实现可以看这篇文章：MySQL教程(十)—MySQL ACID 实现原理。 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:2:0","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"原子性与持久性 事务的原子性（Atomicity）是指在一个事务中，所有请求要么同时成功，要么同时失败。 比如在我们的转账案例中，是绝对无法容忍 Alice 账号扣款成功，但是 Bob 账号资金到账失败的场景。 持久性（Durability）是指事务一旦提交，其所做的修改会永久保存在数据库。 软件系统在运行过程中会遇到各种各样的软硬件故障，如果 etcd 在执行上面事务过程中，刚执行完扣款命令（put Alice 100）就突然 crash 了，它是如何保证转账事务的原子性与持久性的呢？ T1 时间点只是将修改写入到了内存，并未持久化。 crash 后事务并未成功执行和持久化任意数据到磁盘上。在节点重启时，etcd server 会重放 WAL 中的已提交日志条目，再次执行以上转账事务。 T2 时间点则是写入内存完成后，持久化到磁盘时 crash。我们知道 consistent index 字段值是和 key-value 数据在一个 boltdb 事务里同时持久化到磁盘中的，所以持久化失败后者两个值也没能更新成功，那么当节点重启，etcd server 重放 WAL 中已提交日志条目时，同样会再次应用转账事务到状态机中，因此事务的原子性和持久化依然能得到保证。 会不会部分数据提交成功，部分数据提交失败呢？ ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:2:1","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"一致性 分布式系统中多副本数据一致性，它是指各个副本之间的数据是否一致，比如 Redis 的主备是异步复制的，那么它的一致性是最终一致性的。 CAP 原理中的一致性是指可线性化。核心原理是虽然整个系统是由多副本组成，但是通过线性化能力支持，对 client 而言就如一个副本，应用程序无需关心系统有多少个副本。 一致性哈希，它是一种分布式系统中的数据分片算法，具备良好的分散性、平衡性。 事务中的一致性，它是指事务变更前后，数据库必须满足若干恒等条件的状态约束 一致性往往是由数据库和业务程序两方面来保障的。 在本例中，转账系统内的各账号资金总额，在转账前后应该一致，同时各账号资产不能小于 0。 一方面，业务程序在转账逻辑里面，需检查转账者资产大于等于转账金额。在事务提交时，通过账号资产的版本号，确保双方账号资产未被其他事务修改。 另一方面，etcd 会通过 WAL 日志和 consistent index、boltdb 事务特性，去确保事务的原子性，因此不会有部分成功部分失败的操作，导致资金凭空消失、新增。 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:2:2","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"隔离性 常见的事务隔离级别有以下四种： 未提交读（Read UnCommitted），也就是一个 client 能读取到未提交的事务。 已提交读（Read Committed），指的是只能读取到已经提交的事务数据，但是存在不可重复读的问题。 可重复读（Repeated Read），它是指在一个事务中，同一个读操作 get Alice/Bob 在事务的任意时刻都能得到同样的结果，其他修改事务提交后也不会影响你本事务所看到的结果。 串行化（Serializable），它是最高的事务隔离级别，读写相互阻塞，通过牺牲并发能力、串行化来解决事务并发更新过程中的隔离问题。 为了优化性能，在基于 MVCC 机制实现的各个数据库系统中，提供了一个名为“可串行化的快照隔离”级别，相比悲观锁而言，它是一种乐观并发控制，通过快照技术实现的类似串行化的效果，事务提交时能检查是否冲突。 未提交读 由于 etcd 是批量提交写事务的，而读事务又是快照读，因此当 MVCC 写事务完成时，它需要更新 buffer，这样下一个读请求到达时，才能从 buffer 中获取到最新数据。所以不会出现问题。 已提交读、可重复读 比未提交读隔离级别更高的是已提交读，它是指在事务中能读取到已提交数据，但是存在不可重复读的问题。已提交读，也就是说你每次读操作，若未增加任何版本号限制，默认都是当前读，etcd 会返回最新已提交的事务结果给你。 那么如何实现可重复读呢？ 你可以通过 MVCC 快照读，或者参考 etcd 的事务框架 STM 实现，它在事务中维护一个读缓存，优先从读缓存中查找，不存在则从 etcd 查询并更新到缓存中，这样事务中后续读请求都可从缓存中查找，确保了可重复读。 串行化快照隔离 串行化快照隔离是最严格的事务隔离级别，它是指在在事务刚开始时，首先获取 etcd 当前的版本号 rev，事务中后续发出的读请求都带上这个版本号 rev，告诉 etcd 你需要获取那个时间点的快照数据，etcd 的 MVCC 机制就能确保事务中能读取到同一时刻的数据。 同时，它还要确保事务提交时，你读写的数据都是最新的，未被其他人修改，也就是要增加冲突检测机制。 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:2:3","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"示例 到此可以发现之前的 demo 其实存在一些问题，它缺少了完整事务的冲突检测机制。 修改版如下： 首先你可通过一个事务获取 Alice 和 Bob 账号的上资金和版本号，用以判断 Alice 是否有足够的金额转账给 Bob 和事务提交时做冲突检测: $ etcdctl txn -i -w=json compares: success requests (get, put, del): get Alice get Bob failure requests (get, put, del): { \"kvs\":[ { \"key\":\"QWxpY2U=\", \"create_revision\":2, \"mod_revision\":2, \"version\":1, \"value\":\"MjAw\" } ], ...... \"kvs\":[ { \"key\":\"Qm9i\", \"create_revision\":3, \"mod_revision\":3, \"version\":1, \"value\":\"MzAw\" } ], } 其次发起资金转账操作，Alice 账号减去 100，Bob 账号增加 100。为了保证转账事务的准确性、一致性，提交事务的时候需检查 Alice 和 Bob 账号最新修改版本号与读取资金时的一致 (compares 操作中增加版本号检测)，以保证其他事务未修改两个账号的资金。 $ etcdctl txn -i compares: mod(\"Alice\") = \"2\" mod(\"Bob\") = \"3\" success requests (get, put, del): put Alice 100 put Bob 300 failure requests (get, put, del): get Alice get Bob SUCCESS OK OK ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:2:4","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"3. 小结 1）事务 API 的基本结构，它由 If、Then、Else 语句组成。 2）其中 If 支持多个比较规则，它是用于事务提交时的冲突检测，比较的对象支持 key 的 mod_revision、create_revision、version、value 值。 3）etcd 事务的 ACID 特性 原子性，持久性：主要依靠 WAL + consistent index + blotdb，crash 后会根据 wal 重放保证数据不丢失 隔离性：主要依靠 MVCC 一致性：事务追求的最终目标，一致性的实现既需要数据库层面的保障，也需要应用层面的保障 ","date":"2021-12-24","objectID":"/posts/etcd/11-txn/:3:0","tags":["etcd"],"title":"etcd教程(十一)---etcd 事务初体验","uri":"/posts/etcd/11-txn/"},{"categories":["etcd"],"content":"通过源码分析 etcd lease 的原理与具体实现","date":"2021-12-17","objectID":"/posts/etcd/10-lease/","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"本文主要记录了 etcd 中 Lease 的大致原理，包括 v2版本到v3版本的优化点，最后通过分析源码了解了具体实现。 ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:0:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"1. 概述 以下源码基于 etcd v3.5.1. Lease 顾名思义，client 和 etcd server 之间存在一个约定，内容是 etcd server 保证在约定的有效期内（TTL），不会删除你关联到此 Lease 上的 key-value。 若你未在有效期内续租，那么 etcd server 就会删除 Lease 和其关联的 key-value。 可以简单理解为 key 的有效期。 Lease，是基于主动型上报模式提供的一种活性检测机制。 Lease 整体架构如下图所示： etcd 在启动时会创建 Lessor 模块，而 Lessor 模块启动的时候，它会启动一个常驻 goroutine 以执行以下两个任务，如上图所示： 一个是RevokeExpiredLease 任务，定时检查是否有过期 Lease，发起撤销过期的 Lease 操作。 一个是CheckpointScheduledLease，定时触发更新 Lease 的剩余到期时间的操作。 // server/etcdserver/server.go 299 行 func NewServer(cfg config.ServerConfig) (srv *EtcdServer, err error) { // ... // etcd server 启动时会启动 Lessor 模块 srv.lessor = lease.NewLessor(srv.Logger(), srv.be, srv.cluster, lease.LessorConfig{ MinLeaseTTL: int64(math.Ceil(minTTL.Seconds())), CheckpointInterval: cfg.LeaseCheckpointInterval, CheckpointPersist: cfg.LeaseCheckpointPersist, ExpiredLeasesRetryInterval: srv.Cfg.ReqTimeout(), }) // 同时还指定了 Checkpointer 方法，如果有开启 LeaseCheckpoint 设置的话 if srv.Cfg.EnableLeaseCheckpoint { // setting checkpointer enables lease checkpoint feature. srv.lessor.SetCheckpointer(func(ctx context.Context, cp *pb.LeaseCheckpointRequest) { srv.raftRequestOnce(ctx, pb.InternalRaftRequest{LeaseCheckpoint: cp}) }) } // ... } // server/lease/lessor.go 204 行 func NewLessor(lg *zap.Logger, b backend.Backend, cluster cluster, cfg LessorConfig) Lessor { return newLessor(lg, b, cluster, cfg) } func newLessor(lg *zap.Logger, b backend.Backend, cluster cluster, cfg LessorConfig) *lessor { checkpointInterval := cfg.CheckpointInterval expiredLeaseRetryInterval := cfg.ExpiredLeasesRetryInterval if checkpointInterval == 0 { checkpointInterval = defaultLeaseCheckpointInterval } if expiredLeaseRetryInterval == 0 { expiredLeaseRetryInterval = defaultExpiredleaseRetryInterval } l := \u0026lessor{ leaseMap: make(map[LeaseID]*Lease), itemMap: make(map[LeaseItem]LeaseID), leaseExpiredNotifier: newLeaseExpiredNotifier(), leaseCheckpointHeap: make(LeaseQueue, 0), b: b, minLeaseTTL: cfg.MinLeaseTTL, checkpointInterval: checkpointInterval, expiredLeaseRetryInterval: expiredLeaseRetryInterval, checkpointPersist: cfg.CheckpointPersist, // expiredC is a small buffered chan to avoid unnecessary blocking. expiredC: make(chan []*Lease, 16), stopC: make(chan struct{}), doneC: make(chan struct{}), lg: lg, cluster: cluster, } // 从 blotdb 中加载lease数据并在内存中重建 l.initAndRecover() // 这就是图中的两个常驻任务 go l.runLoop() return l } // // server/lease/lessor.go 611 行 func (le *lessor) runLoop() { defer close(le.doneC) for { le.revokeExpiredLeases() // 移除已经过期的 lease le.checkpointScheduledLeases() // 更新lease的剩余到期时间，并将数据给follower节点 select { case \u003c-time.After(500 * time.Millisecond): case \u003c-le.stopC: return } } } Lessor 模块提供了 Grant、Revoke、LeaseTimeToLive、LeaseKeepAlive API 给 client 使用，各接口作用如下: Grant：表示创建一个 TTL 为你指定秒数的 Lease，Lessor 会将 Lease 信息持久化存储在 boltdb 中； Revoke：表示撤销 Lease 并删除其关联的数据； LeaseTimeToLive：表示获取一个 Lease 的有效期、剩余时间； LeaseKeepAlive：表示为 Lease 续期。 ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:1:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"2. key 如何关联 Lease 大致分为两步： 1）创建 Lease 2）将 key 与 lease 关联 具体流程如下： ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:2:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"2.1 创建 Lease client 可通过 clientv3 库的 Lease API 发起 RPC 调用来创建 Lease，例如： # 创建一个TTL为600秒的lease，etcd server返回LeaseID $ etcdctl lease grant 600 lease 326975935f48f814 granted with TTL(600s) # 查看lease的TTL、剩余时间 $ etcdctl lease timetolive 326975935f48f814 lease 326975935f48f814 granted with TTL(600s)， remaining(590s) 当 Lease server 收到 client 的创建一个有效期 600 秒的 Lease 请求后，会通过 Raft 模块完成日志同步，随后 Apply 模块通过 Lessor 模块的 Grant 接口执行日志条目内容。 首先 Lessor 的 Grant 接口会把 Lease 保存到内存的 ItemMap 数据结构中，然后它需要持久化 Lease，将 Lease 数据保存到 boltdb 的 Lease bucket 中，返回一个唯一的 LeaseID 给 client。 // server/lease/lessor.go 272行 func (le *lessor) Grant(id LeaseID, ttl int64) (*Lease, error) { l := \u0026Lease{ ID: id, ttl: ttl, itemSet: make(map[LeaseItem]struct{}), revokec: make(chan struct{}), } le.mu.Lock() defer le.mu.Unlock() if _, ok := le.leaseMap[id]; ok { return nil, ErrLeaseExists } if l.ttl \u003c le.minLeaseTTL { l.ttl = le.minLeaseTTL } if le.isPrimary() { l.refresh(0) } else { l.forever() } // 将新建的 lease 存入 lessor 模块中，一个 map 结构 le.leaseMap[id] = l // 将 lease 持久化到 blotdb中。 l.persistTo(le.b) leaseTotalTTLs.Observe(float64(l.ttl)) leaseGranted.Inc() if le.isPrimary() { item := \u0026LeaseWithTime{id: l.ID, time: l.expiry} le.leaseExpiredNotifier.RegisterOrUpdate(item) le.scheduleCheckpointIfNeeded(l) } return l, nil } ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:2:1","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"2.2 将 key 与 lease 关联 KV 模块的 API 接口提供了一个\"–lease\"参数，你可以通过写入 key 时带上该参数，将 key node 关联到对应的 LeaseID 上。 $ etcdctl put node healthy --lease 326975935f48f818 OK $ etcdctl get node -w=json | python -m json.tool { \"kvs\":[ { \"create_revision\":24， \"key\":\"bm9kZQ==\"， \"Lease\":3632563850270275608， \"mod_revision\":24， \"value\":\"aGVhbHRoeQ==\"， \"version\":1 } ] } 当你通过 put 等命令新增一个指定了\"–lease\"的 key 时，MVCC 模块它会通过 Lessor 模块的 Attach 方法，将 key 关联到 Lease 的 key 内存集合 ItemSet 中，为了保证持久化，在写入 blotdb 时会将 key 关联的 lease 信息一并写入。 // server/lease/lessor.go 546行 func (le *lessor) Attach(id LeaseID, items []LeaseItem) error { le.mu.Lock() defer le.mu.Unlock() l := le.leaseMap[id] if l == nil { return ErrLeaseNotFound } l.mu.Lock() for _, it := range items { l.itemSet[it] = struct{}{} le.itemMap[it] = id } l.mu.Unlock() return nil } // 有 Attach 自然就会有 Detach // server/lease/lessor.go 573行 func (le *lessor) Detach(id LeaseID, items []LeaseItem) error { le.mu.Lock() defer le.mu.Unlock() l := le.leaseMap[id] if l == nil { return ErrLeaseNotFound } l.mu.Lock() for _, it := range items { delete(l.itemSet, it) delete(le.itemMap, it) } l.mu.Unlock() return nil } 其中 LeaseItem 其实就是我们提供的 key ： type LeaseItem struct { Key string } ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:2:2","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"3. Lease 续期 在正常情况下，你的节点存活时，需要定期发送 KeepAlive 请求给 etcd 续期健康状态的 Lease，否则你的 Lease 和关联的数据就会被删除。 那么如何高效的实现这个需求呢？ 续期性能受多方面影响： 首先是 TTL，TTL 过长会导致节点异常后，无法及时从 etcd 中删除，影响服务可用性，而过短，则要求 client 频繁发送续期请求。 其次是 Lease 数，如果 Lease 成千上万个，那么 etcd 可能无法支撑如此大规模的 Lease 数，导致高负载。 v2 版本 在早期 v2 版本中，没有 Lease 概念，TTL 属性是在 key 上面，为了保证 key 不删除，即便你的 TTL 相同，client 也需要为每个 TTL、key 创建一个 HTTP/1.x 连接，定时发送续期请求给 etcd server。 很显然，v2 老版本这种设计，因不支持连接多路复用、相同 TTL 无法复用导致性能较差，无法支撑较大规模的 Lease 场景。 v3版本 etcd v3 版本为了解决以上问题，提出了 Lease 特性，TTL 属性转移到了 Lease 上， 同时协议从 HTTP/1.x 优化成 gRPC 协议。 一方面不同 key 若 TTL 相同，可复用同一个 Lease， 显著减少了 Lease 数。 另一方面，通过 gRPC HTTP/2 实现了多路复用，流式传输，同一连接可支持为多个 Lease 续期，大大减少了连接数。 通过以上两个优化，实现 Lease 性能大幅提升，满足了各个业务场景诉求。 // server/lease/lessor.go 391 行 func (le *lessor) Renew(id LeaseID) (int64, error) { // ... 省略无关代码 // 重置 remaining TTL字段，如果存在的话，这个和 checkPoint有关 clearRemainingTTL := le.cp != nil \u0026\u0026 l.remainingTTL \u003e 0 if clearRemainingTTL { // 可以看到，就是将 Remaining_TTL 字段设置为0了 le.cp(context.Background(), \u0026pb.LeaseCheckpointRequest{Checkpoints: []*pb.LeaseCheckpoint{{ID: int64(l.ID), Remaining_TTL: 0}}}) } l.refresh(0) // refresh lease 的过期时间 item := \u0026LeaseWithTime{id: l.ID, time: l.expiry} le.leaseExpiredNotifier.RegisterOrUpdate(item) leaseRenewed.Inc() return l.ttl, nil } ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:3:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"4. 如何高效淘汰过期 Lease 淘汰过期 Lease 的工作由 Lessor 模块的一个异步 goroutine 负责。它会定时从最小堆中取出已过期的 Lease，执行删除 Lease 和其关联的 key 列表数据的 RevokeExpiredLease 任务。 etcd Lessor 主循环每隔 500ms 执行一次撤销 Lease 检查（RevokeExpiredLease），每次轮询堆顶的元素，若已过期则加入到待淘汰列表，直到堆顶的 Lease 过期时间大于当前，则结束本轮轮询。 优化前 etcd 早期的时候，淘汰 Lease 非常暴力。etcd 会直接遍历所有 Lease，逐个检查 Lease 是否过期，过期则从 Lease 关联的 key 集合中，取出 key 列表，删除它们，时间复杂度是 O(N)。 优化后 目前 etcd 是基于最小堆来管理 Lease，实现快速淘汰过期的 Lease。 每次新增 Lease、续期的时候，它会插入、更新一个对象到最小堆中，对象含有 LeaseID 和其到期时间 unixnano，对象之间按到期时间升序排序。 使用堆优化后后，插入、更新、删除，它的时间复杂度是 O(Log N)，查询堆顶对象是否过期时间复杂度仅为 O(1)，性能大大提升，可支撑大规模场景下 Lease 的高效淘汰。 获取到待过期的 LeaseID 后，Leader 是如何通知其他 Follower 节点淘汰它们呢？ Lessor 模块会将已确认过期的 LeaseID，保存在一个名为 expiredC 的 channel 中，而 etcd server 的主循环会定期从 channel 中获取 LeaseID，发起 revoke 请求，通过 Raft Log 传递给 Follower 节点。 各个节点收到 revoke Lease 请求后，获取关联到此 Lease 上的 key 列表，从 boltdb 中删除 key，从 Lessor 的 Lease map 内存中删除此 Lease 对象，最后还需要从 boltdb 的 Lease bucket 中删除这个 Lease。 // server/lease/lessor.go 628行 // 这就是之前提到的 lessor 模块的两个定时任务中的一个 func (le *lessor) revokeExpiredLeases() { var ls []*Lease // 限制每次定时任务最多只能移除多少个 lease // 主要是防止同时过期的 lease 太多，阻塞后续的任务 revokeLimit := leaseRevokeRate / 2 le.mu.RLock() if le.isPrimary() { // 找到已经过期的 lease ls = le.findExpiredLeases(revokeLimit) } le.mu.RUnlock() if len(ls) != 0 { select { case \u003c-le.stopC: return // 然后发送到 expiredC chan 中，具体的处理逻辑由 etcd server 主循环负责 case le.expiredC \u003c- ls: default: // the receiver of expiredC is probably busy handling // other stuff // let's try this next time after 500ms } } } ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:4:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"5. checkpoint 机制 为了降低 Lease 特性的实现复杂度，检查 Lease 是否过期、维护最小堆、针对过期的 Lease 发起 revoke 操作，都是 Leader 节点负责的，它类似于 Lease 的仲裁者，通过以上清晰的权责划分。 那么当 Leader 因重启、crash、磁盘 IO 等异常不可用时，Follower 节点就会发起 Leader 选举，新 Leader 要完成以上职责，必须重建 Lease 过期最小堆等管理数据结构，那么以上重建可能会触发什么问题呢？ 当你的集群发生 Leader 切换后，新的 Leader 基于 Lease map 信息，按 Lease 过期时间构建一个最小堆时，etcd 早期版本为了优化性能，并未持久化存储 Lease 剩余 TTL 信息，因此重建的时候就会自动给所有 Lease 自动续期了。 如果较频繁出现 Leader 切换，切换时间小于 Lease 的 TTL，这会导致 Lease 永远无法删除，大量 key 堆积，db 大小超过配额等异常。 为了解决这个问题，etcd 引入了检查点机制，也就是 Lessor 模块的**checkpointScheduledLease **定时任务。 一方面，etcd 启动的时候，Leader 节点后台会运行此异步任务，定期批量地将 Lease 剩余的 TTL 基于 Raft Log 同步给 Follower 节点，Follower 节点收到 CheckPoint 请求后，更新内存数据结构 LeaseMap 的剩余 TTL 信息。 另一方面，当 Leader 节点收到 KeepAlive 请求的时候，它也会通过 checkpoint 机制把此 Lease 的剩余 TTL 重置，并同步给 Follower 节点，尽量确保续期后集群各个节点的 Lease 剩余 TTL 一致性。 最后你要注意的是，此特性对性能有一定影响，目前仍然是试验特性。你可以通过 experimental-enable-lease-checkpoint 参数开启。 // server/lease/lessor.go 826行 type Lease struct { ID LeaseID ttl int64 // time to live of the lease in seconds remainingTTL int64 // remaining time to live in seconds, if zero valued it is considered unset and the full ttl should be used // expiryMu protects concurrent accesses to expiry expiryMu sync.RWMutex // expiry is time when lease should expire. no expiration when expiry.IsZero() is true expiry time.Time // mu protects concurrent accesses to itemSet mu sync.RWMutex itemSet map[LeaseItem]struct{} revokec chan struct{} } Lease 中的 remainingTTL 字段就是用于完成这个功能的。 在 Grant 创建 Lease 时，该字段是没有赋值的： func (le *lessor) Grant(id LeaseID, ttl int64) (*Lease, error) { l := \u0026Lease{ ID: id, ttl: ttl, itemSet: make(map[LeaseItem]struct{}), revokec: make(chan struct{}), } } 即，新建的 Lease remainingTTL 字段都为0。 至于具体的逻辑自然是在checkpointScheduledLease 这个定时任务中了： // server/lease/lessor.go 655 行 func (le *lessor) checkpointScheduledLeases() { var cps []*pb.LeaseCheckpoint // 和另一个定时任务一样，同样是加了限制，防止该任务消耗太多时间 for i := 0; i \u003c leaseCheckpointRate/2; i++ { le.mu.Lock() if le.isPrimary() { // 寻找需要执行 checkPoint 的 Lease，同样是限制了最大个数 cps = le.findDueScheduledCheckpoints(maxLeaseCheckpointBatchSize) } le.mu.Unlock() if len(cps) != 0 { // cp 是一个方法，调用该方法进行 checkPoint 检查 le.cp(context.Background(), \u0026pb.LeaseCheckpointRequest{Checkpoints: cps}) } // 上面查询时限制了最大查询数，如果最终个数少于指定值，则说明没有更多结果了，退出循环 // 比如限制要找1000个，结果返回值也是1000个，说明后续可能还有没找到的Lease，但是如果只找到了100个，那肯定是找完了。 if len(cps) \u003c maxLeaseCheckpointBatchSize { return } } } 至于是怎么找的，其实就是循环从 heap 中取出来的： func (le *lessor) findDueScheduledCheckpoints(checkpointLimit int) []*pb.LeaseCheckpoint { if le.cp == nil { return nil } now := time.Now() cps := []*pb.LeaseCheckpoint{} for le.leaseCheckpointHeap.Len() \u003e 0 \u0026\u0026 len(cps) \u003c checkpointLimit { lt := le.leaseCheckpointHeap[0] // 这是一个最小堆，第一个元素的time值就是最小的，如果第一个元素的checkpoint time 都没到，则说明该找的都找到了，直接返回 if lt.time.After(now) /* lt.time: next checkpoint time */ { return cps } heap.Pop(\u0026le.leaseCheckpointHeap) var l *Lease var ok bool if l, ok = le.leaseMap[lt.id]; !ok { continue } // 如果这个lease都过期了则不检测，由expireCheck任务处理 if !now.Before(l.expiry) { continue } remainingTTL := int64(math.Ceil(l.expiry.Sub(now).Seconds())) if remainingTTL \u003e= l.ttl { continue } if le.lg != nil { le.lg.Debug(\"Checkpointing lease\", zap.Int64(\"leaseID\", int64(lt.id)), zap.Int64(\"remainingTTL\", remainingTTL), ) } cps = append(cps, \u0026pb.LeaseCheckpoint{ID: int64(lt.id), Remaining_TTL: remainingTTL}) } return cps } 继续追踪下去，看下找到只会 etcd 是如何处理的， // cp 是一个方法，调用该方法进行 checkPoint 检查 le.cp(context.Background(), \u0026pb.LeaseCheckpointRequest{Checkpoints: cps}) 如果够仔细的话，可以发送cp 字段其实是在 NewServer 方法中赋值的： // server/etcdserver/server.go 299 行 func NewServer(cfg config.ServerConfig) (srv *EtcdServer, err error) { // 因为该功能对性能有一定影响，目前还是试验性功能，需要通过参数指定开启 if srv.Cfg.EnableLeaseCheckpoint { // setting checkpointer enables lease checkpoint feature. srv.lessor.SetCheckpointer(func(ctx context.Context, cp *pb.LeaseCheckpointRequest) { srv.raftRequestOnce(ctx, pb.InternalRaftRequest{LeaseCheckpoint: cp}) }) } } 具体逻辑如下： // server/etcdserver/v3_server.go 593行 func (s *EtcdServer) raftRequestOnce(ctx co","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:5:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"6. 小结 1）Lease 的核心是 TTL，当 Lease 的 TTL 过期时，它会自动删除其关联的 key-value 数据。 2）v3 版本通过引入 Lease 的概念，将 TTL 和 Key 解耦，支持多 key 共用一个 Lease 来提升性能。同时协议从 HTTP/1.x 优化成 gRPC 协议，支持多路连接复用，显著降低了 server 连接数等资源开销。 3）Lease 过期通过最小堆来提升效率。 4）通过 Checkpoint 机制想 follower 同步 Lease 信息来解决 Leader 异常情况下 TTL 自动被续期，可能导致 Lease 永不淘汰的问题而诞生。 作者能力实在是有限，文中很有可能会有一些错误的理解。所以当你发现了一些违和的地方，也请不吝指教，谢谢你！ 再次感谢你能看到这里！ ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:6:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"7. 参考 https://github.com/etcd-io/etcd etcd 实战课 ","date":"2021-12-17","objectID":"/posts/etcd/10-lease/:7:0","tags":["etcd"],"title":"etcd教程(十)---lease 机制源码分析","uri":"/posts/etcd/10-lease/"},{"categories":["etcd"],"content":"raft 算法在 etcd 中的具体实现","date":"2021-12-04","objectID":"/posts/etcd/09-raft/","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"本文主要记录了 raft 算法在 etcd 中的具体实现，主要分为 Leader 选举、日志复制、安全性三大部分。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:0:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"1. 概述 在上一篇文章 Raft 算法概述 中记录的 raft 算法的大致原理，本期主要看一下 etcd 中是如何实现 raft 算法的。 首先 Raft 算法主要将具体实现拆分成了 3 个小问题： 1）Leader 选举 2）日志复制 3）安全性 然后共识算法一般和复制状态机一起使用： ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:1:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"2. Leader 选举 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:2:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"节点状态 首先在 Raft 协议中它定义了集群中的如下节点状态，任何时刻，每个节点肯定处于其中一个状态： Follower，跟随者， 同步从 Leader 收到的日志，etcd 启动的时候默认为此状态； Candidate，竞选者，可以发起 Leader 选举； Leader，集群领导者， 唯一性，拥有同步日志的特权，需定时广播心跳给 Follower 节点，以维持领导者身份。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:2:1","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"term Raft 协议将时间划分成一个个任期（Term），任期用连续的整数表示，每个任期从一次选举开始，赢得选举的节点在该任期内充当 Leader 的职责，随着时间的消逝，集群可能会发生新的选举，任期号也会单调递增。 通过任期号，可以比较各个节点的数据新旧、识别过期的 Leader 等，它在 Raft 算法中充当逻辑时钟，发挥着重要作用。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:2:2","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"选举流程 当 Follower 节点接收 Leader 节点心跳消息超时后，它会转变成 Candidate 节点，并可发起竞选 Leader 投票，若获得集群多数节点的支持后，它就可转变成 Leader 节点。 etcd 默认心跳间隔时间（heartbeat-interval）是 100ms， 默认竞选超时时间（election timeout）是 1000ms， 注意：你需要根据实际部署环境、业务场景适当调优，否则就很可能会频繁发生 Leader 选举切换，导致服务稳定性下降。 以下图为例： 当 Leader 节点异常后，Follower 节点会接收 Leader 的心跳消息超时，当超时时间大于竞选超时时间后，它们会进入 Candidate 状态。 进入 Candidate 状态的节点，会立即发起选举流程，自增任期号，投票给自己，并向其他节点发送竞选 Leader 投票消息（MsgVote）。 C 节点收到 Follower B 节点竞选 Leader 消息后，这时候可能会出现如下两种情况： 1） C 节点判断 B 节点的数据至少和自己一样新、B 节点任期号大于 C 当前任期号、并且 C 未投票给其他候选者，就可投票给 B。这时 B 节点获得了集群多数节点支持，于是成为了新的 Leader。 2）恰好 C 也心跳超时超过竞选时间了，它也发起了选举，并投票给了自己，那么它将拒绝投票给 B，这时谁也无法获取集群多数派支持，只能等待竞选超时，开启新一轮选举。 Raft 为了优化选票被瓜分导致选举失败的问题，引入了随机数，每个节点等待发起选举的时间点不一致，优雅的解决了潜在的竞选活锁，同时易于理解。 通过随机数优雅的避免了总是出现情况2。 如果现有 Leader 发现了新的 Leader 任期号，那么它就需要转换到 Follower 节点。A 节点 crash 后，再次启动成为 Follower，假设因为网络问题无法连通 B、C 节点，这时候根据状态图，我们知道它将不停自增任期号，发起选举。等 A 节点网络异常恢复后，那么现有 Leader 收到了新的任期号，就会触发新一轮 Leader 选举，影响服务的可用性。 那如何避免以上场景中的无效的选举呢？ 在 etcd 3.4 中，etcd 引入了一个 PreVote 参数（默认 false），可以用来启用 PreCandidate 状态解决此问题。 Follower 在转换成 Candidate 状态前，先进入 PreCandidate 状态，不自增任期号， 发起预投票。若获得集群多数节点认可，确定有概率成为 Leader 才能进入 Candidate 状态，发起选举流程。 因 A 节点数据落后较多，预投票请求无法获得多数节点认可，因此它就不会进入 Candidate 状态，导致集群重新选举。 这就是 Raft Leader 选举核心原理，使用心跳机制维持 Leader 身份、触发 Leader 选举，etcd 基于它实现了高可用，只要集群一半以上节点存活、可相互通信，Leader 宕机后，就能快速选举出新的 Leader，继续对外提供服务。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:2:3","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"3. 日志复制 具体流程如下图所示： 1）首先 Leader 收到 client 的请求后，etcdserver 的 KVServer 模块会向 Raft 模块提交一个 put hello 为 world 提案消息（流程图中的序号 2 流程）， 它的消息类型是 MsgProp。 2）Leader 的 Raft 模块获取到 MsgProp 提案消息后，为此提案生成一个日志条目，追加到未持久化、不稳定的 Raft 日志中，随后会遍历集群 Follower 列表和进度信息，为每个 Follower 生成追加（MsgApp）类型的 RPC 消息，此消息中包含待复制给 Follower 的日志条目。 3）etcdserver 模块通过 channel 从 Raft 模块获取到 Ready 结构后（流程图中的序号 3 流程），因 B 节点是 Leader，它首先会通过基于 HTTP 协议的网络模块将追加日志条目消息（MsgApp）广播给 Follower，并同时将待持久化的日志条目持久化到 WAL 文件中（流程图中的序号 4 流程），最后将日志条目追加到稳定的 Raft 日志存储中（流程图中的序号 5 流程）。 4）各个 Follower 收到追加日志条目（MsgApp）消息，并通过安全检查后，它会持久化消息到 WAL 日志中，并将消息追加到 Raft 日志存储，随后会向 Leader 回复一个应答追加日志条目（MsgAppResp）的消息，告知 Leader 当前已复制的日志最大索引（流程图中的序号 6 流程）。 5）Leader 收到应答追加日志条目（MsgAppResp）消息后，会将 Follower 回复的已复制日志最大索引更新到跟踪 Follower 进展的 Match Index 字段。 6）最后 Leader 根据 Follower 的 MatchIndex 信息，计算出一个位置，如果这个位置已经被一半以上节点持久化，那么这个位置之前的日志条目都可以被标记为已提交。 7）最后各个节点的 etcdserver 模块，可通过 channel 从 Raft 模块获取到已提交的日志条目（流程图中的序号 7 流程），应用日志条目内容到存储状态机（流程图中的序号 8 流程），返回结果给 client。 通过以上流程，Leader 就完成了同步日志条目给 Follower 的任务，一个日志条目被确定为已提交的前提是，它需要被 Leader 同步到一半以上节点上。以上就是 etcd Raft 日志复制的核心原理。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:3:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"raft 日志 下图是 Raft 日志复制过程中的日志细节图： 在日志图中，最上方的是日志条目序号 / 索引，日志由有序号标识的一个个条目组成，每个日志条目内容保存了 Leader 任期号和提案内容。最开始的时候，A 节点是 Leader，任期号为 1，A 节点 crash 后，B 节点通过选举成为新的 Leader， 任期号为 2。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:3:1","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"FAQ Leader 是如何知道从哪个索引位置发送日志条目给 Follower，以及 Follower 已复制的日志最大索引是多少呢？ Leader 会维护两个核心字段来追踪各个 Follower 的进度信息，一个字段是 NextIndex， 它表示 Leader 发送给 Follower 节点的下一个日志条目索引。一个字段是 MatchIndex， 它表示 Follower 节点已复制的最大日志条目的索引，比如上面的日志图 1 中 C 节点的已复制最大日志条目索引为 5，A 节点为 4。 日志条目什么时候才会追加到稳定的 Raft 日志中呢？Raft 模块负责持久化吗？ 上层应用通过 Raft 模块的输出接口（如 Ready 结构），获取到待持久化的日志条目和待发送给 Peer 节点的消息后（如上面的 MsgApp 日志消息），需持久化日志条目到自定义的 WAL 模块，通过自定义的网络模块将消息发送给 Peer 节点。 日志条目持久化到稳定存储中后，这时候你就可以将日志条目追加到稳定的 Raft 日志中。即便这个日志是内存存储，节点重启时也不会丢失任何日志条目，因为 WAL 模块已持久化此日志条目，可通过它重建 Raft 日志。 etcd Raft 模块提供了一个内置的内存存储（MemoryStorage）模块实现，etcd 使用的就是它，Raft 日志条目保存在内存中。网络模块并未提供内置的实现，etcd 基于 HTTP 协议实现了 peer 节点间的网络通信，并根据消息类型，支持选择 pipeline、stream 等模式发送，显著提高了网络吞吐量、降低了延时。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:3:2","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"4. 安全性 假设当前raft日志条目如下图所示： Leader B 在应用日志指令 put hello 为 world 到状态机，并返回给 client 成功后，突然 crash 了，那么 Follower A 和 C 是否都有资格选举成为 Leader 呢？ 从日志图 2 中我们可以看到，如果 A 成为了 Leader 那么就会导致数据丢失，因为它并未含有刚刚 client 已经写入成功的 put hello 为 world 指令。 Raft 算法如何确保面对这类问题时不丢数据和各节点数据一致性呢？ Raft 通过给选举和日志复制增加一系列规则，来实现 Raft 算法的安全性。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:4:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"选举规则 当节点收到选举投票的时候，需检查候选者的最后一条日志中的任期号： 若小于自己则拒绝投票。 如果任期号相同，日志却比自己短，也拒绝为其投票。 这样能保证投票的节点数据至少比当前节点数据新。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:4:1","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"日志复制规则 在日志图 2 中，Leader B 返回给 client 成功后若突然 crash 了，此时可能还并未将 6 号日志条目已提交的消息通知到 Follower A 和 C，那么如何确保 6 号日志条目不被新 Leader 删除呢？ 同时在 etcd 集群运行过程中，Leader 节点若频繁发生 crash 后，可能会导致 Follower 节点与 Leader 节点日志条目冲突，如何保证各个节点的同 Raft 日志位置含有同样的日志条目？ 以上各类异常场景的安全性是通过 Raft 算法中的 Leader 完全特性和只附加原则、日志匹配等安全机制来保证的。 Leader 完全特性：是指如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有 Leader 中。 只附加原则：Leader 只能追加日志条目，不能删除已持久化的日志条目。因此 Follower C 成为新 Leader 后，会将前任的 6 号日志条目复制到 A 节点。 日志匹配特性：Leader 在发送追加日志 RPC 消息时，会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。Follower 节点会检查相同索引位置的任期号是否与 Leader 一致，一致才能追加。 它本质上是一种归纳法，一开始日志空满足匹配特性，随后每增加一个日志条目时，都要求上一个日志条目信息与 Leader 一致，那么最终整个日志集肯定是一致的。 通过以上的 Leader 选举限制、Leader 完全特性、只附加原则、日志匹配等安全特性，Raft 就实现了一个可严格通过数学反证法、归纳法证明的高可用、一致性算法，为 etcd 的安全性保驾护航。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:4:2","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["etcd"],"content":"5. 小结 Raft 算法它将一个复杂问题拆分成三个子问题，分别是 Leader 选举、日志复制和安全性。 1）Raft 通过心跳机制、随机化等实现了 Leader 选举，只要集群半数以上节点存活可相互通信，etcd 就可对外提供高可用服务。 2）一个日志条目只有被 Leader 同步到一半以上节点上，此日志条目才能称之为成功复制、已提交。 3）Raft 的安全性，通过对 Leader 选举和日志复制增加一系列规则，保证了整个集群的一致性、完整性。 几个关键点： 1）Leader 选举 每个节点刚启动的时候都是默认为 Follower 状态 Leader 会定时给 Follower 发送心跳消息，如果超时后 Follower 就会切换到 Candidate 状态并发起新的一轮选举 etcd 默认心跳间隔时间（heartbeat-interval）是 100ms， 默认竞选超时时间（election timeout）是 1000ms 为了保证各个节点不会同时发起选举，出现选票被瓜分的情况，默认给每个节点的超时时间上增加了一个随机数，比如Node1心跳超时可能是101ms，Node2可能是102ms 为了避免无效选举，增加了 PreCandidate 状态。 基本和 Raft Paper 一致，增加了 PreCandidate 状态避免无效选举来提升稳定性。 2）日志复制 Leader 收到写请求后，生成一个提案并提交给 Raft 模块 Leader 的Raft 模块为次提案生成一个日志条目，并追加到 Raft 日志中，此处有 WAL持久化。 Leader 将新的日志发送给 Follower，Leader 会维护两个核心字段来追踪各个 Follower 的进度信息，一个字段是 NextIndex， 它表示 Leader 发送给 Follower 节点的下一个日志条目索引。一个字段是 MatchIndex， 它表示 Follower 节点已复制的最大日志条目的索引。 Follower 收到日志后先进行安全检测，通过检测后将该日志写入自己的 Raft 日志中，并回复 Leader 当前已复制的日志最大索引。此处也有WAL持久化。 最后 Leader 根据 Follower 的 MatchIndex 信息，找出已经被半数以上的节点同步的位置，这个位置之前的所有日志条目都可以提交了。 Leader 通过消息告诉 Follower 那些日志条目可以执行提交了 Follower 根据 Leader 的信息从Raft模块中取出对应日志条目内容，并应用到状态机中。 可以看到日志复制是一个异步的过程，其中如何判断一条日志已经被同步到超过半数的节点上是通过 MatchIndex 来判断的。这样就很巧妙，不需要为每条日志分别记录同步状态。 3）安全性 选举规则：不会投票给数据比自己还旧的节点，避免 Leader 选举后数据丢失。 Leader 完全特性：如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有 Leader 中。 日志只附加规则：Leader 只能追加日志条目，不能删除已持久化的日志条目，保证数据不会丢失 日志匹配特性：追加日志时，引入了一致性检查。它本质上是一种归纳法，一开始日志空满足匹配特性，随后每增加一个日志条目时，都要求上一个日志条目信息与 Leader 一致，那么最终整个日志集肯定是一致的。 保证任何情况下都不会出现数据异常。 挖个坑，后续简单分析下源码 相关思想： 1）使用 PreCandidate 来避免无效选举。 其实参数检测也是差不多这种思想，明显不能完成的请求可以直接返回了，而不是所有前置条件都弄好了，最后发现有什么格式错误这种情况。 比如下单的时候不会先生成订单什么的，然后发现你账户余额不足，最后返回了错误。 肯定是能先判断的就先判断了，避免生成这些无效的订单。 2）使用MatchIndex 来判断是否是否已经通过给超过半数节点了。 类似的，消息通知这种系统的时候，肯定也是每条消息上记录了发送时间，然后为每个用户记录一个上次读取消息的时间，这样根据这两个时间就能判断出哪些消息已经哪些未读，而不是每条消息上都记录一个已读未读的状态。 3）使用日志只附加原则+日志匹配特性来保证数据不出错 这个就是数学归纳法。 比如财务对账系统中，最开始左右账户余额都为0，此时是能对上的，那么只要保证数据不被篡改（etcd 中是用日志只附加原则来限制，真实系统中根据场景具体限制），那么后续任意时间段的数据都是能对上的。 4）raft 将共识问题拆分为 3 个子问题。 问题拆解，这也是最重要的一点。这种思想其实无处不在，关键是如何拆分，个人认为要拆解到无法拆解为止，即保证任务的原子性。 比如计算机计算加减乘除这个任务，看起来要实现加减乘除4个运算，那么可以拆分成4个子任务。 实际上计算机中的加减乘除都是转换为加法和位移运算完成的。 那么实现加减乘除这个任务最终其实就是实现一个加法，然后和位移运算进行组合，从而实现减乘除。 所以编程的时候也可以尽量保证代码的原子性，比如 Go 语言里推崇的小接口,一个接口只包含一个方法： 比如常见的 Reader 接口就实现了一个 Read方法； Writer 接口也只实现 Write 方法 如果需要一个 ReaderWriter 接口那么只需要将 Reader 接口和 Writer 接口组合起来即可。 Etcd 里实现的 Raft 算法库也是很精简的，只有 Raft 算法，将所有的业务逻辑都抽象出来了，而不是和 Raft 算法库耦合在一起。 通过这种方法可以提升代码的复用性。 ","date":"2021-12-04","objectID":"/posts/etcd/09-raft/:5:0","tags":["etcd"],"title":"etcd教程(九)---Raft 算法具体实现","uri":"/posts/etcd/09-raft/"},{"categories":["Distributed"],"content":"Raft 算法概述","date":"2021-11-26","objectID":"/posts/distributed/raft/","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"本文主要记录 raft 算法的大致实现，包括 leader 选举、日志复制和安全性等三个子问题。 本文主要是一些简单的总结，便于新手对 raft 算法有个大致理解，强烈推荐有基础的朋友去看一下 raft 论文,文末有相关资料链接。 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:0:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"1. 背景 什么是 Raft？ Raft 协议是一种共识算法（consensus algorithm）。 Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; 为什么需要 Raft ？ 回答该问题之前可以思考一下另一个问题：为什么需要共识算法？ 为了解决单点问题，软件系统工程师引入了数据复制技术，实现多副本。而多副本间的数据复制就会出现一致性问题。所以需要共识算法来解决该问题。 共识算法的祖师爷是 Paxos， 但是由于它过于复杂，难于理解，工程实践上也较难落地，导致在工程界落地较慢。 Raft 算法正是为了可理解性、易实现而诞生的。 The first drawback is that Paxos is exceptionally difficult to understand The second problem with Paxos is that it does not provide a good foundation for building practical implementations. ","date":"2021-11-26","objectID":"/posts/distributed/raft/:1:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"2. Raft 算法 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:2:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"可理解性设计 为了达到易于理解的目标，raft做了很多努力，其中最主要是两件事情： 问题分解 状态简化 问题分解是将复杂的问题划分为数个可以被独立解释、理解、解决的子问题。在raft，子问题包括，leader election， log replication，safety，membership changes。 而状态简化更好理解，就是对算法做出一些限制，减少需要考虑的状态数，使得算法更加清晰，更少的不确定性（比如，保证新选举出来的 leader 会包含所有 commited log entry ） ","date":"2021-11-26","objectID":"/posts/distributed/raft/:2:1","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"Raft 简介 raft 会先选举出 leader，leader 完全负责 replicated log 的管理。leader 负责接受所有客户端更新请求，然后复制到 follower 节点，并在“安全”的时候执行这些请求。如果 leader 故障，followes 会重新选举出新的 leader。 通过 leader，raft 将一致性问题分解成三个相当独立的子问题： Leader Election：当集群启动或者 leader 失效时必须选出一个新的l eader。 Log Replication：leader 必须接收客户端提交的日志，并将其复制到集群中的其他节点，强制其他节点的日志与 leader 一样。 Safety：最关键的安全点就是图3.2中的 State Machine Safety Property。如果任何一个 server 已经在它的状态机apply了一条日志，其他的 server 不可能在相同的 index 处 apply 其他不同的日志条目。后面将会讲述 raft 如何实现这一点。 下面两张图包含了 raft 的核心部分： ","date":"2021-11-26","objectID":"/posts/distributed/raft/:2:2","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"3. 子问题 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:3:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"1. Leader election 在 raft 中，一个节点任一时刻都会处于以下三个状态之一： Leader leader 处理所有来自客户端的请求(如果客户端访问 follower，会把请求重定向到 leader) Follower follower 是消极的，他们不会主动发出请求而仅仅对来自 leader 和 candidate 的请求作出回应。 Candidate Candidate 状态用来选举出一个 leader。 在正常情况下会只有一个 leader，其他节点都是 follower。 Raft 使用心跳机制来触发 leader 选举，具体状态转换流程如图： 可以看到： 所有节点启动时都是follower状态； 在一段时间内如果没有收到来自 leader 的心跳，从 follower 切换到 candidate，且 term+1并发起选举； 如果收到 majority 的投票（含自己的一票）则切换到 leader 状态； 如果发现其他节点 term 比自己更新，则主动切换到 follower。 Term Raft 将时间划分为任意长度的 term，用连续整数编号。每一个 term都从选举开始，一个或多个 candidate 想要成为 leader，如果一个 candidate 赢得选举，它将会在剩余的 term 中作为 leader。在一些情况下选票可能会被瓜分，导致没有 leader 产生，这个 term 将会以没有 leader 结束，一个新的 term 将会很快产生。Raft 确保每个 term 至多有一个 leader。 term 在 Raft 中起到了逻辑时钟的作用，它可以帮助 server 检测过期信息比如过期的 leader。每一个 server 都存储有 current term 字段，会自动随时间增加。当 server 间通信的时候，会交换 current term，如果一个节点的 current term 比另一个小，它会自动将其更新为较大者。如果c andidate 或者 leader 发现了自己的 term 过期了，它会立刻转为 follower 状态。如果一个节点收到了一个含有过期的 term 的请求，它会拒绝该请求。 election timeout 可能会出现的一种情况是，所有 follower 节点，检测到超时后都同时发起选举，因为都会默认投票给自己，这就会导致最终没有节点可能获取到超过半数的选票，最终选举失败，然后选举超时后又开始下一轮选举，进入死循环。 Raft 使用随机选举超时来确保选票被瓜分的情况很少出现。election timeout 的值会在一个固定区间内随机的选取(比如150-300ms)。这使得在大部分情况下仅有一个 server 会检测到超时，它将会在其他节点发现超时前发起选举，则有很大概率赢得选举。 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:3:1","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"2. Log Replication 当有了 leader，系统就可以对外提供服务了。每一个客户端的写请求都包含着一个待状态机执行的命令，leader 会将这个命令作为新的一条日志追加到自己的日志中，然后并行向其他 server 发出AppendEntries RPC 来复制日志。 当日志被安全的复制之后，leader可以将日志 apply 到自己的状态机，并将执行结果返回给客户端。如果 follower 宕机或运行很慢，甚至丢包，leader 会无限的重试RPC (即使已经将结果报告给了客户端)，直到所有的 follower 最终都存储了相同的日志。 Replicated State Machine 共识算法的实现一般是基于复制状态机（Replicated state machines）.replicated state machine 用于解决分布式系统中的各种容错问题。 If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state. 简单来说：相同的初始状态 + 相同的输入 = 相同的结束状态。 通常使用 replicated log 来实现 Replicated state machine ，如下图所示： 每一个 server 都有一个日志保存了一系列的指令，state machine 会顺序执行这些指令。每一个日志都以相同顺序保存着相同的指令，因此每一个 state machine 处理相同的指令，state machine 是一样的，所以最终会达到相同的状态及输出。 共识算法的任务则是保证 replicated log 的一致。server 中的一致性模块接收客户端传来的指令并添加到自己的日志中，它也可以和其他 server 中的一致性模块沟通来确保每一条 log 都能有相同的内容和顺序，即使其中一些 server 宕机。 一旦指令被正确复制，就可以称作committed。每一个 server 中的状态机按日志顺序处理committed 指令，并将输出返回客户端。 请求完整流程 当系统（leader）收到一个来自客户端的写请求，到返回给客户端，整个过程从leader的视角来看会经历以下步骤： 1）leader append log entry 2）leader issue AppendEntries RPC in parallel 3）leader wait for majority response 4）leader apply entry to state machine 5）leader reply to client 6）leader notify follower apply log 可以看到日志的提交过程有点类似两阶段提交(2PC)，不过与2PC的区别在于，leader只需要大多数（majority）节点的回复即可，这样只要超过一半节点处于工作状态则系统就是可用的。 在上面的流程中，leader只需要日志被复制到大多数节点即可向客户端返回，一旦向客户端返回成功消息，那么系统就必须保证log（其实是log所包含的command）在任何异常的情况下都不会发生回滚。这里有两个词： commit(committed)：指日志被复制到了大多数节点后日志的状态 apply(applied)：指节点将日志应用到状态机，真正影响到节点状态 日志按下图的方式进行组织： 每条日志储存了一条命令和 leader 接收到该指令时的 term 序号。日志中的 term 序号可以用来检测不一致的情况，每一条日志也拥有一个整数索引用于定位。 从上图可以看到，五个节点的日志并不完全一致，raft算法为了保证高可用，并不是强一致性，而是最终一致性，leader会不断尝试给follower发log entries，直到所有节点的log entries都相同。 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:3:2","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"3. Safety 衡量一个分布式算法，有许多属性，如 safety：nothing bad happens, liveness： something good eventually happens. 在任何系统模型下，都需要满足safety属性，即在任何情况下，系统都不能出现不可逆的错误，也不能向客户端返回错误的内容。比如，raft保证被复制到大多数节点的日志不会被回滚，那么就是safety属性。而raft最终会让所有节点状态一致，这属于liveness属性。 raft 会保证以下属性： Election safety 选举安全性，即任一任期内最多一个leader被选出。这一点非常重要，在一个复制集中任何时刻只能有一个leader。系统中同时有多余一个leader，被称之为脑裂（brain split），这是非常严重的问题，会导致数据的覆盖丢失。在raft中，两点保证了这个属性： 一个节点某一任期内最多只能投一票； 只有获得majority投票的节点才会成为leader。 因此，某一任期内一定只有一个leader。 Leader Append-Only leader 不允许覆盖或删除日志条目，只能在后面进行追加。 这个限制比较简单容易实现。 Log Matching log匹配特性， 就是说如果两个节点上的某个log entry的log index相同且term相同，那么在该index之前的所有log entry应该都是相同的。 依赖于以下两点： 首先，leader 在某一 term 的任一位置只会创建一个 log entry，且 log entry 是 append-only； 其次，consistency check。leader在AppendEntries中包含最新log entry之前的一个log 的term和index，如果follower在对应的term index找不到日志，那么就会告知leader不一致。 在没有异常的情况下，log matching是很容易满足的，但如果出现了node crash，情况就会变得复杂=，比如下图： 上图的a-f是某个follower可能存在的六个状态 leader、follower都可能crash，那么follower维护的日志与leader相比可能出现以下情况 比leader日志少，如上图中的ab 比leader日志多，如上图中的cd 某些位置比leader多，某些日志比leader少，如ef（多少是针对某一任期而言） 当出现了leader与follower不一致的情况，leader强制让follower保持和自己一致。 为了使得follower的日志和leader的日志一致，leader必须找到自己和follower最后一致的日志索引，然后删掉在那之后follower的日志，并将leader在那之后的日志全部发送给follower。所有的这些操作都发生在AppendEntries RPC的一致性检查中。 leader持有针对每一个follower的nextIndex索引，代表下一条要发送给对应follower的日志索引。当leader刚上任时，它会初始化所有的nextIndex值为最后一条日志的下一个索引，如图中的11。如果follower的日志和leader的不一致，下一次AppendEntries的一致性检查就会失败。在遭到拒绝后， leader就会降低该follower的nextIndex并进行重试。最终nextIndex会到达leader和follower一致的位置。这条AppendEntries RPC会执行成功，并覆盖follower在这之后原有的日志，之后follower的日志会保持和leader一致，直到这个term结束。 Leader Completeness 在任何基于leader的一致性算法中，leader必须最终存有全部committed日志。 在一些一致性算法（如Viewstamped Replication），节点 即使不包含全部 committed 日志也能被选举为 leader，这些算法通过其他的机制来定位缺失的日志，并将其转移给新的 leader。然而这增加了系统的复杂度，raft 使用了更加简单的方法来确保所有 committed 的日志存在于每个新选举出来的 leader，不需要转移日志。因此日志只需要从 leader 流向 follower 即可，而且不需要重写自己的日志。 Raft 使用投票过程来确保选举成为 leader 的 candidate 一定包含全部committed 的日志。 具体如下： 1）选举时，各个节点只会投票给 commited 日志大于等于自己的节点； 2）Candidate 必须获得超过半数的选票才能赢得选举； 3）Leader 复制日志时也需要复制给超过半数的节点。 这也就意味着，每次选举出来的 leader 一定包含最新的 committed 日志。 State Machine Safety 如果一条日志成功复制到大多数节点上，leader就知道可以commit了。如果leader在commit之前崩溃了，新的leader将会尝试完成复制这条日志。然而一个leader不可能立刻推导出之前term的entry已经commit了。 上图是一个较为复杂的情况： 在时刻(a), s1是leader，在term2提交的日志只复制到了s1 s2两个节点就crash了。 在时刻(b), s5成为了term 3的leader，日志只复制到了s5，然后crash。 然后在(c)时刻，s1又成为了term 4的leader，开始复制日志，于是把term2的日志复制到了s3，此刻，可以看出term2对应的日志已经被复制到了majority，因此是committed，可以被状态机应用。 不幸的是，接下来（d）时刻，s1又crash了，s5重新当选，然后将term3的日志复制到所有节点，这就出现了一种奇怪的现象：被复制到大多数节点（或者说可能已经应用）的日志（term 2 的日志）被回滚。 究其根本，是因为term4时的leader s1在（C）时刻提交了之前term2任期的日志。为了杜绝这种情况的发生，raft 做了以下限制： 某个leader选举成功之后，不会直接提交前任leader时期的日志，而是通过提交当前任期的日志的时候“顺手”把之前的日志也提交了，具体怎么实现了，在log matching部分有详细介绍。 为了避免leader在整个任期中都没有收到客户端请求，导致日志一直没有被提交的情况，leader 会在在任期开始的时候发立即尝试复制、提交一条空的log。 因此，在上图中，不会出现（C）时刻的情况，即term4任期的leader s1不会复制term2的日志到s3。而是如同(e)描述的情况，通过复制-提交 term4的日志顺便提交term2的日志。如果term4的日志提交成功，那么term2的日志也一定提交成功，此时即使s1 crash，s5也不会重新当选。 ","date":"2021-11-26","objectID":"/posts/distributed/raft/:3:3","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"4. 小结 raft将共识问题分解成三个相对独立的问题，leader election，log replication 以及 safety。 流程是先选举出leader，然后leader负责复制、提交log（log中包含command），最后通过 safety 中的各种限制保证了 raft 不会出现或者能够应对各种异常情况。 leader election约束： 同一任期内最多只能投一票； 只会投票给日志和自己一样，或者比自己新的节点 log replication约束： 一个log被复制到大多数节点，就是committed，保证不会回滚 leader一定包含最新的committed log，因此leader只会追加日志，不会删除覆盖日志 不同节点，某个位置上日志相同，那么这个位置之前的所有日志一定是相同的 Raft never commits log entries from previous terms by counting replicas. ","date":"2021-11-26","objectID":"/posts/distributed/raft/:4:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Distributed"],"content":"5. 相关资料 # raft 论文 中英文 https://raft.github.io/raft.pdf https://github.com/maemual/raft-zh_cn # raft 动画 中英文 http://thesecretlivesofdata.com/raft/ http://kailing.pub/raft/index.html ","date":"2021-11-26","objectID":"/posts/distributed/raft/:5:0","tags":["Distributed"],"title":"Raft 算法概述","uri":"/posts/distributed/raft/"},{"categories":["Redis"],"content":"Redis Scan 命令原理解析与踩坑","date":"2021-11-12","objectID":"/posts/redis/redis-scan/","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"主要分析了 Redis Scan 命令基本使用和具体实现，包括 Count 参数与 Scan 总耗时的关系，以及核心的逆二进制迭代算法分析。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:0:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"1. 概述 由于 Redis 是单线程在处理用户的命令，而 Keys 命令会一次性遍历所有 Key，于是在 命令执行过程中，无法执行其他命令。这就导致如果 Redis 中的 key 比较多，那么 Keys 命令执行时间就会比较长，从而阻塞 Redis。 所以很多教程都推荐使用 Scan 命令来代替 Keys，因为 Scan 可以限制每次遍历的 key 数量。 Keys 的缺点： 1）没有limit，我们只能一次性获取所有符合条件的key，如果结果有上百万条，那么等待你的就是“无穷无尽”的字符串输出。 2）keys命令是遍历算法，时间复杂度是O(N)。如我们刚才所说，这个命令非常容易导致Redis服务卡顿。因此，我们要尽量避免在生产环境使用该命令。 相比于keys命令，Scan命令有两个比较明显的优势： 1）Scan命令的时间复杂度虽然也是O(N)，但它是分次进行的，不会阻塞线程。 2）Scan命令提供了 count 参数，可以控制每次遍历的集合数。 可以理解为 Scan 是渐进式的 Keys。 Scan 命令语法如下： SCAN cursor [MATCH pattern] [COUNT count] cursor - 游标。 pattern - 匹配的模式。 count - 指定每次遍历多少个集合。 可以简单理解为每次遍历多少个元素 根据测试，推荐 Count大小为 1W。 Scan 返回值为数组，会返回一个游标+一系列的 Key 大致用法如下： SCAN命令是基于游标的，每次调用后，都会返回一个游标，用于下一次迭代。当游标返回0时，表示迭代结束。 第一次 Scan 时指定游标为 0，表示开启新的一轮迭代，然后 Scan 命令返回一个新的游标，作为第二次 Scan 时的游标值继续迭代，一直到 Scan 返回游标为0，表示本轮迭代结束。 通过这个就可以看出，Scan 完成一次迭代，需要和 Redis 进行多次交互。 Scan 命令注意事项： 返回的结果可能会有重复，需要客户端去重复，这点非常重要; 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的; 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零; ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:1:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"2. Scan 踩坑 使用时遇到一个 特殊场景，跨区域远程连接 Redis 并进行模糊查询，扫描所有指定前缀的 Key。 最开始也没多想，直接就是开始 Scan，然后 Count 参数指定的是 1000。 Redis 中大概几百万 Key。 最后发现这个接口需要几十上百秒才返回。 什么原因呢？ Scan 命令中的 Count 指定一次扫描多少 Key，这里指定为 1000，几百万Key就需要几千次迭代，即和 Redis 交互几千次，然后因为是远程连接，网络延迟比较大，所以耗时特别长。 最后将 Count 参数调大后，减少了交互次数，就好多了。 Count 参数越大，Redis 阻塞时间也会越长，需要取舍。 极限一点，Count 参数和总 Key 数一致时，Scan 命令就和 Keys 效果一样了。 Count 大小和 Scan 总耗时的关系如下图： 图源：keydb 可以发现 Count 越大，总耗时就越短，不过越后面提升就越不明显了。 所以推荐的 Count 大小为 1W 左右。 如果不考虑 Redis 的阻塞，其实 Keys 比 Scan 会快很多，毕竟一次性处理，省去了多余的交互。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:2:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"3. Scan原理 Redis使用了Hash表作为底层实现，原因不外乎高效且实现简单。类似于HashMap那样数组+链表的结构。其中第一维的数组大小为2n(n\u003e=0)。每次扩容数组长度扩大一倍。 Scan命令就是对这个一维数组进行遍历。每次返回的游标值也都是这个数组的索引。Count 参数表示遍历多少个数组的元素，将这些元素下挂接的符合条件的结果都返回。因为每个元素下挂接的链表大小不同，所以每次返回的结果数量也就不同。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"演示 关于 Scan 命令的遍历顺序，我们可以用一个小栗子来具体看一下： 127.0.0.1:6379\u003e keys * 1) \"db_number\" 2) \"key1\" 3) \"myKey\" 127.0.0.1:6379\u003e scan 0 MATCH * COUNT 1 1) \"2\" 2) 1) \"db_number\" 127.0.0.1:6379\u003e scan 2 MATCH * COUNT 1 1) \"1\" 2) 1) \"myKey\" 127.0.0.1:6379\u003e scan 1 MATCH * COUNT 1 1) \"3\" 2) 1) \"key1\" 127.0.0.1:6379\u003e scan 3 MATCH * COUNT 1 1) \"0\" 2) (empty list or set) 如上所示，SCAN命令的遍历顺序是：0-\u003e2-\u003e1-\u003e3 这个顺序看起来有些奇怪，我们把它转换成二进制：00-\u003e10-\u003e01-\u003e11 可以看到每次这个序列是高位加1的。 普通二进制的加法，是从右往左相加、进位。而这个序列是从左往右相加、进位的。 相关源码： v = rev(v); v++; v = rev(v); 将游标倒置，加一后，再倒置，也就是我们所说的“高位加1”的操作。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:1","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"相关源码 先贴一下代码： unsigned long dictScan(dict *d, unsigned long v, dictScanFunction *fn, void *privdata) { dictht *t0, *t1; const dictEntry *de; unsigned long m0, m1; if (dictSize(d) == 0) return 0; if (!dictIsRehashing(d)) {//没有在做rehash，所以只有第一个表有数据的 t0 = \u0026(d-\u003eht[0]); m0 = t0-\u003esizemask; //槽位大小-1,因为大小总是2^N,所以sizemask的二进制总是后面都为1, //比如16个slot的字典，sizemask为00001111 /* Emit entries at cursor */ de = t0-\u003etable[v \u0026 m0];//找到当前这个槽位，然后处理数据 while (de) { fn(privdata, de);//将这个slot的链表数据全部入队，准备返回给客户端。 de = de-\u003enext; } } else { t0 = \u0026d-\u003eht[0]; t1 = \u0026d-\u003eht[1]; /* Make sure t0 is the smaller and t1 is the bigger table */ if (t0-\u003esize \u003e t1-\u003esize) {//将地位设置为 t0 = \u0026d-\u003eht[1]; t1 = \u0026d-\u003eht[0]; } m0 = t0-\u003esizemask; m1 = t1-\u003esizemask; /* Emit entries at cursor */ de = t0-\u003etable[v \u0026 m0];//处理小一点的表。 while (de) { fn(privdata, de); de = de-\u003enext; } /* Iterate over indices in larger table that are the expansion * of the index pointed to by the cursor in the smaller table */ do {//扫描大点的表里面的槽位，注意这里是个循环，会将小表没有覆盖的slot全部扫描一次的 /* Emit entries at cursor */ de = t1-\u003etable[v \u0026 m1]; while (de) { fn(privdata, de); de = de-\u003enext; } /* Increment bits not covered by the smaller mask */ //下面的意思是，还需要扩展小点的表，将其后缀固定，然后看高位可以怎么扩充。 //其实就是想扫描一下小表里面的元素可能会扩充到哪些地方，需要将那些地方处理一遍。 //后面的(v \u0026 m0)是保留v在小表里面的后缀。 //((v | m0) + 1) \u0026 ~m0) 是想给v的扩展部分的二进制位不断的加1，来造成高位不断增加的效果。 v = (((v | m0) + 1) \u0026 ~m0) | (v \u0026 m0); /* Continue while bits covered by mask difference is non-zero */ } while (v \u0026 (m0 ^ m1));//终止条件是 v的高位区别位没有1了，其实就是说到头了。 } /* Set unmasked bits so incrementing the reversed cursor * operates on the masked bits of the smaller table */ v |= ~m0; //按位取反，其实相当于v |= m0-1 , ~m0也就是11110000, //这里相当于将v的不相干的高位全部置为1，待会再进行翻转二进制位，然后加1，然后再转回来 /* Increment the reverse cursor */ v = rev(v); v++; v = rev(v); //下面将v的每一位倒过来再加1，再倒回去，这是什么意思呢， //其实就是要将有效二进制位里面的高位第一个0位设置置为1，因为现在是0嘛 return v; } ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:2","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"reverse binary iteration Redis Scan 命令最终使用的是 reverse binary iteration 算法，大概可以翻译为 逆二进制迭代，具体算法细节可以看一下这个Github 相关讨论 这个算法简单来说就是： 依次从高位（有效位）开始，不断尝试将当前高位设置为1，然后变动更高位为不同组合，以此来扫描整个字典数组。 其最大的优势在于，从高位扫描的时候，如果槽位是2^N个,扫描的临近的2个元素都是与2^(N-1)相关的就是说同模的，比如槽位8时，0%4 == 4%4， 1%4 == 5%4 ， 因此想到其实hash的时候，跟模是很相关的。 比如当整个字典大小只有4的时候，一个元素计算出的整数为5， 那么计算他的hash值需要模4，也就是hash(n) == 5%4 == 1 , 元素存放在第1个槽位中。当字典扩容的时候，字典大小变为8， 此时计算hash的时候为5%8 == 5 ， 该元素从1号slot迁移到了5号，1和5是对应的，我们称之为同模或者对应。 同模的槽位的元素最容易出现合并或者拆分了。因此在迭代的时候只要及时的扫描这些相关的槽位，这样就不会造成大面积的重复扫描。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:3","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"3 种情况 迭代哈希表时，有以下三种情况： 从迭代开始到结束，哈希表不 Rehash； 从迭代开始到结束，哈希表Rehash，但每次迭代，哈希表要么不开始 Rehash，要么已经结束 Rehash； 从一次迭代开始到结束，哈希表在一次或多次迭代中 Rehash。 即再 Rehash 过程中，执行 Scan 命令，这时数据可能只迁移了一部分。 因此，游标的实现需要兼顾以上三种情况。上述三种情况下游标实现的要求如下： 第一种情况比较简单。假设redis的hash表大小为4，第一个游标为0，读取第一个bucket的数据，然后游标返回2，下次读取bucket 2 ，依次遍历。 第二种情况更复杂。假设redis的hash表大小为4，如果rehash后大小变成8。如果如上返回游标(即返回2)，则显示下图： 假设bucket 0读取后返回到cursor 2，当客户端再次Scan cursor 2时，hash表已经被rehash，大小翻倍到8，redis计算一个key bucket如下： hash(key)\u0026(size-1) 即如果大小为4，hash(key)\u002611，如果大小为8，hash(key)\u0026111。所以当size从4扩大到8时，2 号bucket中的原始数据会被分散到2 (010) 和 6 (110) 这两个 bucket中。 从二进制来看，size为4时，在hash(key)之后，取低两位，即hash(key)\u002611，如果size为8，bucket位置为hash(key) \u0026 111，即取低三个位。 所以依旧不会出现漏掉数据的情况。 第三种情况，如果返回游标2时正在进行rehash，则Hash表1的bucket 2中的一些数据可能已经rehash到了的Hash表2 的bucket[2]或bucket[6]，那么必须完全遍历 哈希表2的 bucket 2 和 6，否则可能会丢失数据。 Redis 全局有两个Hash表，扩容时会渐进式的将表1的数据迁移到表2，查询时程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找。 详细信息可以查看：Redis教程(四)—全局数据结构 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:4","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"游标计算 具体游标计算代码如下： Scan 命令中的游标，其实就是 Redis 内部的 bucket。 v |= ~m0; // 将游标v的unmarsked 比特都置为1 v = rev(v);// 反转v v++; //这个是关键，加1，对一个数加1，其实就是将这个数的低位的连续1变为0，然后将最低的一个0变为1，其实就是将最低的一个0变为1 v = rev(v);//再次反转，即得到下一个游标值 代码逻辑非常简单，计算过程如下： 图源：developpaper 大小为 4 时，游标状态转换为 0-2-1-3。 当大小为 8 时，游标状态转换为 0-4-2-6-1-5-3-7。 可以看出，当size由小变大时，所有原来的游标都能在大hashTable中找到对应的位置，并且顺序一致，不会重复读取，也不会被遗漏。 总结一下：redis在rehash 扩容的时候，不会重复或者漏掉数据。但缩容，可能会造成重复但不会漏掉数据。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:5","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"缩容处理 之所以会出现重复数据，其实就是为了保证缩容后数据不丢。 假设当前 hash 大小为 8： 1）第一次先遍历了 0 号槽，返回游标为 4； 2）准备遍历 4 号槽，然后此时发生了缩容，4 号槽的元素也进到 0 号槽了。 3）但是0 号槽之前已经被遍历过了，此时会丢数据吗？ 答案就在源码中： do {//扫描大点的表里面的槽位，注意这里是个循环，会将小表没有覆盖的slot全部扫描一次的 /* Emit entries at cursor */ de = t1-\u003etable[v \u0026 m1]; while (de) { fn(privdata, de); de = de-\u003enext; } /* Increment bits not covered by the smaller mask */ //下面的意思是，还需要扩展小点的表，将其后缀固定，然后看高位可以怎么扩充。 //其实就是想扫描一下小表里面的元素可能会扩充到哪些地方，需要将那些地方处理一遍。 //后面的(v \u0026 m0)是保留v在小表里面的后缀。 //((v | m0) + 1) \u0026 ~m0) 是想给v的扩展部分的二进制位不断的加1，来造成高位不断增加的效果。 v = (((v | m0) + 1) \u0026 ~m0) | (v \u0026 m0); /* Continue while bits covered by mask difference is non-zero */ } while (v \u0026 (m0 ^ m1));//终止条件是 v的高位区别位没有1了，其实就是说到头了。 具体计算方法： v = (((v | m0) + 1) \u0026 ~m0) | (v \u0026 m0); 右边的下半部分是v，左边的上半部分是v。 (v\u0026m0) 取出v的低位，例如size=4时v\u002600000011 左半边(v|m0) + 1 将V 的低位设置为1，然后+1 将进位到v 的高位，再次\u0026m0，V 的高位将被取出。 假设游标返回2并且正在rehashing，大小从4变为8，那么M0 = 00000011 v = 00000010 根据公式计算的下一个光标是 ((00000010 | 00000011) +1) \u0026 (11111111100) | (00000010 \u0026 00000011) = (00000100) \u0026 (11111111100) | (00000000010) = (000000000110) 正好是 6。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:3:6","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"4. 小结 Scan Count 参数限制的是遍历的 bucket 数，而不是限制的返回的元素个数 由于不同 bucket 中的元素个数不同，其中满足条件的个数也不同，所以每次 Scan 返回元素也不一定相同 Count 越大，Scan 总耗时越短，但是单次耗时越大，即阻塞Redis 时间边长 推荐 Count 大小为 1W左右 当 Count = Redis Key 总数时，Scan 和 Keys 效果一致 Scan 采用 逆二进制迭代法来计算游标，主要为了兼容Rehash的情况 Scan 为了兼容缩容后不漏掉数据，会出现重复遍历。 即客户端需要做去重处理 核心就是 逆二进制迭代法，比较复杂，而且算法作者也没有具体证明，为什么这样就能实现，只是测试发现没有问题，各种情况都能兼容。 具体算法细节可以看一下这个Github 相关讨论 antirez: Hello @pietern! I’m starting to re-evaluate the idea of an iterator for Redis, and the first item in this task is definitely to understand better your pull request and implementation. I don’t understand exactly the implementation with the reversed bits counter… I wonder if there is a way to make that more intuitive… so investing some more time into this, and if I fail I’ll just merge your code trying to augment it with more comments… Hard to explain but awesome. pietern： Although I don’t have a formal proof for these guarantees, I’m reasonably confident they hold. I worked through every hash table state (stable, grow, shrink) and it appears to work everywhere by means of the reverse binary iteration (for lack of a better word). 所以只能说这个算法很巧妙。就像卡马克快速逆平方根算法： float Q_rsqrt( float number ) { long i; float x2, y; const float threehalfs = 1.5F ; x2 = number * 0.5F ; y = number ; i = * ( long * ) \u0026y; // evil floating point bit level hacking i = 0x5f3759df - ( i \u003e\u003e 1 ); // what the fuck? y = * ( float * ) \u0026i; y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration // y = y * ( threehalfs - ( x2 * y * y ) ); // 2nd iteration, this can be removed return y ; } 其中的这个0x5f3759df数就很巧妙。 ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:4:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Redis"],"content":"5. 参考 http://antirez.com/news/63 https://developpaper.com/redis-scan-command-principle/ https://www.cnblogs.com/thrillerz/p/4527510.html https://www.jianshu.com/p/abe5d8ae4852 https://zhuanlan.zhihu.com/p/46353221 https://docs.keydb.dev/blog/2020/08/10/blog-post/ ","date":"2021-11-12","objectID":"/posts/redis/redis-scan/:5:0","tags":["Redis"],"title":"Redis Scan 原理解析与踩坑","uri":"/posts/redis/redis-scan/"},{"categories":["Golang"],"content":"sync.Mutex 源码分析","date":"2021-11-05","objectID":"/posts/go/sync-mutex/","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"Go 语言在 sync 包中提供了用于同步的一些基本原语,sync.Mutex 就是其中最常用的一个。 本文基于 Go 1.17.1 ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:0:0","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"1. 基本结构 Go 语言的 sync.Mutex由两个字段 state 和 sema 组成。其中 state 表示当前互斥锁的状态，而 sema 是用于控制锁状态的信号量。 // sync/mutex.go 25行 type Mutex struct { state int32 sema uint32 } 上述两个字段加起来只占 8 字节空间的结构体表示了 Go 语言中的互斥锁。 ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:1:0","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"状态 互斥锁的状态比较复杂，如下图所示，最低三位分别表示 mutexLocked、mutexWoken 和 mutexStarving，剩下的位置用来表示当前有多少个 Goroutine 在等待互斥锁的释放： int32 中的不同位分别表示了不同的状态： mutexLocked — 表示互斥锁的锁定状态； mutexWoken — 表示从正常模式被从唤醒； mutexStarving — 当前的互斥锁进入饥饿状态； waitersCount — 当前互斥锁上等待的 Goroutine 个数 在默认情况下，互斥锁的所有状态位都是 0，即默认为未锁定状态。 同时也表明 Mutex 是不需要初始化的 源码中也提供了相关常量 // sync/mutex.go 36 const ( mutexLocked = 1 \u003c\u003c iota // 1 0001 含义：用最后一位表示当前对象锁的状态，0-未锁住 1-已锁住 mutexWoken // 2 0010 含义：用倒数第二位表示当前对象是否被唤醒 0-唤醒 1-未唤醒 mutexStarving // 4 0100 含义：用倒数第三位表示当前对象是否为饥饿模式，0为正常模式，1为饥饿模式。 mutexWaiterShift = iota // 3，从倒数第四位往前的bit位表示在排队等待的goroutine数 starvationThresholdNs = 1e6 // 1ms 切换到饥饿模式的阈值 ) ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:1:1","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"正常模式和饥饿模式 Mutex 有两种模式： 正常模式； 饥饿模式。 在正常模式下，锁的等待者会按照先进先出的顺序获取锁。但是刚被唤起的 Goroutine 与新创建的 Goroutine 竞争时，大概率会获取不到锁，为了减少这种情况的出现，一旦 Goroutine 超过 1ms 没有获取到锁，它就会将当前互斥锁切换饥饿模式，防止部分 Goroutine 被饿死。 引入饥饿模式的目的是保证互斥锁的公平性。 说明 Mutex 是公平锁。 在饥饿模式中，互斥锁会直接交给等待队列最前面的 Goroutine。新的 Goroutine 在该状态下不能获取锁、也不会进入自旋状态，它们只会在队列的末尾等待。如果一个 Goroutine 获得了互斥锁并且它在队列的末尾或者它等待的时间少于 1ms，那么当前的互斥锁就会切换回正常模式。 与饥饿模式相比，正常模式下的互斥锁能够提供更好地性能，饥饿模式的能避免 Goroutine 由于陷入等待无法获取锁而造成的高尾延时。 这里贴一下源码中的注释 Mutex fairness. Mutex can be in 2 modes of operations: normal and starvation. In normal mode waiters are queued in FIFO order, but a woken up waiter does not own the mutex and competes with new arriving goroutines over the ownership. New arriving goroutines have an advantage -- they are already running on CPU and there can be lots of them, so a woken up waiter has good chances of losing. In such case it is queued at front of the wait queue. If a waiter fails to acquire the mutex for more than 1ms, it switches mutex to the starvation mode. In starvation mode ownership of the mutex is directly handed off from the unlocking goroutine to the waiter at the front of the queue. New arriving goroutines don't try to acquire the mutex even if it appears to be unlocked, and don't try to spin. Instead they queue themselves at the tail of the wait queue. If a waiter receives ownership of the mutex and sees that either (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms, it switches mutex back to normal operation mode. Normal mode has considerably better performance as a goroutine can acquire a mutex several times in a row even if there are blocked waiters. Starvation mode is important to prevent pathological cases of tail latency. ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:1:2","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"2. 加解锁过程 在sync包中 中定义了 Locker 接口： // sync/mutex.go 31 行 type Locker interface { Lock() Unlock() } Mutex 实现了 Locker 接口。除了互斥锁 Mutex 之外读写锁 RWMutex，也实现了 Locker 接口。 ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:2:0","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"Lock 互斥锁的加锁是靠 Mutex.Lock 方法完成的，以下代码进行了简化，省略了 race 相关代码，只保留主干部分： // sync/mutex.go 72 行 func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } 可以看到，整个加锁过程分为 Fast path 和 Slow Path。 Fast path if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { return } 如果 m.state 为 0,说明当前锁为未锁定状态，将其设置为 1。 这也是最简单的部分，直接通过一个 CAS 操作，尝试获取锁。 Slow Path 如果互斥锁的状态不是 0 时就会进入 Slow Path。尝试通过自旋（Spinnig）等方式等待锁的释放，该方法的主体是一个非常大 for 循环，这里将它分成几个部分介绍获取锁的过程： 1）判断当前 Goroutine 能否进入自旋； 2）通过自旋等待互斥锁的释放； 3）计算互斥锁的最新状态； 4）更新互斥锁的状态并获取锁； func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false awoke := false iter := 0 old := m.state for { if old\u0026(mutexLocked|mutexStarving) == mutexLocked \u0026\u0026 runtime_canSpin(iter) { if !awoke \u0026\u0026 old\u0026mutexWoken == 0 \u0026\u0026 old\u003e\u003emutexWaiterShift != 0 \u0026\u0026 atomic.CompareAndSwapInt32(\u0026m.state, old, old|mutexWoken) { awoke = true } runtime_doSpin() iter++ old = m.state continue } new := old if old\u0026mutexStarving == 0 { new |= mutexLocked } if old\u0026(mutexLocked|mutexStarving) != 0 { new += 1 \u003c\u003c mutexWaiterShift } if starving \u0026\u0026 old\u0026mutexLocked != 0 { new |= mutexStarving } if awoke { if new\u0026mutexWoken == 0 { throw(\"sync: inconsistent mutex state\") } new \u0026^= mutexWoken } if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { if old\u0026(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026m.sema, queueLifo, 1) starving = starving || runtime_nanotime()-waitStartTime \u003e starvationThresholdNs old = m.state if old\u0026mutexStarving != 0 { if old\u0026(mutexLocked|mutexWoken) != 0 || old\u003e\u003emutexWaiterShift == 0 { throw(\"sync: inconsistent mutex state\") } delta := int32(mutexLocked - 1\u003c\u003cmutexWaiterShift) if !starving || old\u003e\u003emutexWaiterShift == 1 { delta -= mutexStarving } atomic.AddInt32(\u0026m.state, delta) break } awoke = true iter = 0 } else { old = m.state } } } 1）判断当前 Goroutine 能否进入自旋； 自旋是一种多线程同步机制，当前的进程在进入自旋的过程中会一直保持 CPU 的占用，持续检查某个条件是否为真。在多核的 CPU 上，自旋可以避免 Goroutine 的切换，使用恰当会对性能带来很大的增益，但是使用的不恰当就会拖慢整个程序，所以 Goroutine 进入自旋的条件非常苛刻： 1）互斥锁只有在普通模式才能进入自旋； 2）runtime.sync_runtime_canSpin需要返回 true 运行在多 CPU 的机器上； 当前 Goroutine 为了获取该锁进入自旋的次数小于四次； 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空； // runtime/proc.go 6364 行 func sync_runtime_canSpin(i int) bool { if i \u003e= active_spin || ncpu \u003c= 1 || gomaxprocs \u003c= int32(sched.npidle+sched.nmspinning)+1 { return false } if p := getg().m.p.ptr(); !runqempty(p) { return false } return true } 2）通过自旋等待互斥锁的释放； 一旦当前 Goroutine 能够进入自旋就会调用runtime.sync_runtime_doSpin和runtime.procyield执行 30 次的 PAUSE 指令，该指令只会占用 CPU 并消耗 CPU 时间： // runtime/proc.go 6381 行 func sync_runtime_doSpin() { procyield(active_spin_cnt) } // runtime/asm_386.s 574 行 TEXT runtime·procyield(SB),NOSPLIT,$0-0 MOVL cycles+0(FP), AX again: PAUSE SUBL $1, AX JNZ again RET 3）计算互斥锁的最新状态； 处理了自旋相关的特殊逻辑之后，互斥锁会根据上下文计算当前互斥锁最新的状态。几个不同的条件分别会更新 state 字段中存储的不同信息 — mutexLocked、mutexStarving、mutexWoken 和 mutexWaiterShift： new := old if old\u0026mutexStarving == 0 { new |= mutexLocked } if old\u0026(mutexLocked|mutexStarving) != 0 { new += 1 \u003c\u003c mutexWaiterShift } if starving \u0026\u0026 old\u0026mutexLocked != 0 { new |= mutexStarving } if awoke { if new\u0026mutexWoken == 0 { throw(\"sync: inconsistent mutex state\") } new \u0026^= mutexWoken } 4）更新互斥锁的状态并获取锁； 计算了新的互斥锁状态之后，会使用 CAS 函数更新状态 if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { if old\u0026(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026m.sema, queueLifo, 1) starving = starving || runtime_nanotime()-waitStartTime \u003e starvationThresholdNs old = m.state if old\u0026mutexStarving != 0 { if old\u0026(mutexLocked|mutexWoken) != 0 || old\u003e\u003emutexWaiterShift == 0 { throw(\"sync: inconsistent mutex state\") } delta := int32(mutexLocked - 1\u003c\u003cmutexWaiterShift) if !starving || old\u003e\u003emutexWaiterShif","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:2:1","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"Unlock 相比之下互斥锁的解锁过程就比较简单,同样分为 Fast path 和 Slow path。 func (m *Mutex) Unlock() { // Fast path: drop lock bit. new := atomic.AddInt32(\u0026m.state, -mutexLocked) if new != 0 { // Outlined slow path to allow inlining the fast path. // To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock. m.unlockSlow(new) } } Fast path 该过程会先使用atomic.AddInt32函数快速解锁，这时会发生下面的两种情况： 如果该函数返回的新状态等于 0，当前 Goroutine 就成功解锁了互斥锁； 如果该函数返回的新状态不等于 0，则进入 Slow path。 Slow path func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)\u0026mutexLocked == 0 { throw(\"sync: unlock of unlocked mutex\") } if new\u0026mutexStarving == 0 { old := new for { if old\u003e\u003emutexWaiterShift == 0 || old\u0026(mutexLocked|mutexWoken|mutexStarving) != 0 { return } new = (old - 1\u003c\u003cmutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { runtime_Semrelease(\u0026m.sema, false, 1) return } old = m.state } } else { runtime_Semrelease(\u0026m.sema, true, 1) } } 先校验锁状态的合法性 — 如果当前互斥锁已经被解锁过了会直接抛出异常 “sync: unlock of unlocked mutex” 中止当前程序。 if (new+mutexLocked)\u0026mutexLocked == 0 { throw(\"sync: unlock of unlocked mutex\") } 然后根据当前锁模式分别处理 if new\u0026mutexStarving == 0 { // 正常模式 }else{ // 饥饿模式 } 在正常模式下，上述代码会使用如下所示的处理过程： 如果互斥锁不存在等待者或者互斥锁的 mutexLocked、mutexStarving、mutexWoken 状态不都为 0，那么当前方法可以直接返回，不需要唤醒其他等待者； 如果互斥锁存在等待者，会通过runtime_Semrelease唤醒等待者并移交锁的所有权； 在饥饿模式下，上述代码会直接调用runtime_Semrelease将当前锁交给下一个正在尝试获取锁的等待者，等待者被唤醒后会得到锁，在这时互斥锁还不会退出饥饿状态； // runtime/sema.go 65行 func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int) { semrelease1(addr, handoff, skipframes) } func semrelease1(addr *uint32, handoff bool, skipframes int) { root := semroot(addr) atomic.Xadd(addr, 1) if atomic.Load(\u0026root.nwait) == 0 { return } // Harder case: search for a waiter and wake it. lockWithRank(\u0026root.lock, lockRankRoot) if atomic.Load(\u0026root.nwait) == 0 { unlock(\u0026root.lock) return } s, t0 := root.dequeue(addr) if s != nil { atomic.Xadd(\u0026root.nwait, -1) } unlock(\u0026root.lock) if s != nil { // May be slow or even yield, so unlock first acquiretime := s.acquiretime if acquiretime != 0 { mutexevent(t0-acquiretime, 3+skipframes) } if s.ticket != 0 { throw(\"corrupted semaphore ticket\") } if handoff \u0026\u0026 cansemacquire(addr) { s.ticket = 1 } readyWithTime(s, 5+skipframes) if s.ticket == 1 \u0026\u0026 getg().m.locks == 0 { goyield() } } } ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:2:2","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"3. 小结 互斥锁的加锁过程比较复杂，它涉及自旋、信号量以及调度等概念： 如果互斥锁处于初始化状态，会通过置位 mutexLocked 加锁； 如果互斥锁处于 mutexLocked 状态并且在普通模式下工作，会进入自旋，执行 30 次 PAUSE 指令消耗 CPU 时间等待锁的释放； 如果当前 Goroutine 等待锁的时间超过了 1ms，互斥锁就会切换到饥饿模式； 互斥锁在正常情况下会通过sync_runtime_SemacquireMutex将尝试获取锁的 Goroutine 切换至休眠状态，等待锁的持有者唤醒； 如果当前 Goroutine 是互斥锁上的最后一个等待的协程或者等待的时间小于 1ms，那么它会将互斥锁切换回正常模式； 互斥锁的解锁过程与之相比就比较简单： 当互斥锁已经被解锁时，调用 Mutex.Lock 会直接抛出异常； 当互斥锁处于饥饿模式时，将锁的所有权交给队列中的下一个等待者，等待者会负责设置 mutexLocked 标志位； 当互斥锁处于普通模式时，如果没有 Goroutine 等待锁的释放或者已经有被唤醒的 Goroutine 获得了锁，会直接返回；在其他情况下会通过sync.runtime_Semrelease 唤醒对应的 Goroutine； ","date":"2021-11-05","objectID":"/posts/go/sync-mutex/:3:0","tags":["Golang"],"title":"Go语言之sync.Mutex 源码分析","uri":"/posts/go/sync-mutex/"},{"categories":["Golang"],"content":"context包结构分析及其基本使用介绍","date":"2021-10-22","objectID":"/posts/go/context/","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"本文主要简单介绍了Go语言(golang)中的context包。给出了context 的基本用法和使用建议，并从源码层面对其底层结构和具体实现原理进行分析。 ","date":"2021-10-22","objectID":"/posts/go/context/:0:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"1. 概述 以下分析基于 Go 1.17.1 ","date":"2021-10-22","objectID":"/posts/go/context/:1:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"1.1 什么是 Context 上下文 context.Context在Go 语言中用来设置截止日期、同步信号，传递请求相关值的结构体。上下文与 Goroutine 有比较密切的关系，是 Go 语言中独特的设计，在其他编程语言中我们很少见到类似的概念。 主要用于超时控制和多Goroutine间的数据传递。 注：这里的数据传递主要指全局数据，如 链路追踪里的 traceId 之类的数据，并不是普通的参数传递(也非常不推荐用来传递参数)。 ","date":"2021-10-22","objectID":"/posts/go/context/:1:1","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"1.2 设计原理 因为context.Context主要作用就是进行超时控制，然后外部程序监听到超时后就可以停止执行任务，取消 Goroutine。 网上有很多用 Context 来取消 Goroutine 的字眼，初学者(比如笔者)可能误会，以为 Context 可以直接取消 Goroutine。 实际，Context 只是完成了一个信号的传递，具体的取消逻辑需要由程序自己监听这个信号，然后手动处理。 Go 语言中的 Context 通过构建一颗 Context 树，从而将没有层级的 Goroutine 关联起来。如下图所示： 所有 Context 都依赖于 BackgroundCtx 或者 TODOCtx，其实这二者都是一个 emptyCtx，只是语义上不一样。 在超时或者手动取消的时候信号都会从最顶层的 Goroutine 一层一层传递到最下层。这样该 Context 关联的所有 Goroutine 都能收到信号，然后进入自定义的退出逻辑。 比如这里手动取消了 ctxB1，然后 ctxB1 的两个子ctx(C1和C2)也会收到取消信号，这样3个Goroutine都能收到取消信号进行退出了。 ","date":"2021-10-22","objectID":"/posts/go/context/:1:2","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"1.3 使用场景 最常见的就是 后台 HTTP/RPC Server。 在 Go 的 server 里，通常每来一个请求都会启动若干个 goroutine 同时工作：有些去数据库拿数据，有些调用下游接口获取相关数据,具体如下图： 而客户端一般不会无限制的等待，都会被请求设定超时时间，比如100ms。 比如这里GoroutineA消耗80ms，GoroutineB3消耗30ms，已经超时了，那么后续的GoroutineCDEF都没必要执行了，客户端已经超时返回了，服务端就算计算出结果也没有任何意义了。 所以这里就可以使用 Context 来在多个 Goroutine 之间进行超时信号传递。 同时引入超时控制后有两个好处： 1）客户端可以快速返回，提升用户体验 2）服务端可以减少无效的计算 ","date":"2021-10-22","objectID":"/posts/go/context/:1:3","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"2. Demo 演示 相关代码见 Github ","date":"2021-10-22","objectID":"/posts/go/context/:2:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"2.1 WithCancel 返回一个可以手动取消的 Context，可以手动调用 cancel() 方法以取消该 context。 // 启动一个 worker goroutine 一直产生随机数，知道找到满足条件的数时，手动调用 cancel 取消 ctx，让 worker goroutine 退出 func main() { rand.Seed(time.Now().UnixNano()) ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*100) // defer cancel() // 一般推荐 defer 中调用cancel() ret := make(chan int) go RandWithCancel(ctx, ret) for r := range ret { // 当找到满足条件的数时就退出 if r ==20 { fmt.Println(\"find r:\", r) break } } cancel() // 这里测试就手动调用cancel() 取消context time.Sleep(time.Second) // sleep 等待 worker goroutine 退出 } func RandWithCancel(ctx context.Context, ret chan int) { defer close(ret) timer := time.NewTimer(time.Millisecond) for { select { case \u003c-ctx.Done(): fmt.Println(\"ctx cancel\") timer.Stop() return case \u003c-timer.C: r := rand.Intn(100) ret \u003c- r timer.Reset(time.Millisecond) } } } ","date":"2021-10-22","objectID":"/posts/go/context/:2:1","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"2.2 WithDeadline \u0026 WithTimeout 可以自定义超时时间，时间到了自动取消context。 其实 WithTimeout就是对 WithDeadline 的一个封装： func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } // 启动一个 worker goroutine 一直产生随机数，直到 ctx 超时后退出 func main() { rand.Seed(time.Now().UnixNano()) // ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*100) ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(time.Millisecond*100)) // defer cancel() // 一般推荐 defer 中调用cancel() ret := make(chan int) go RandWithTimeout(ctx, ret) for r := range ret { // 当找到满足条件的数时就退出 if r == 20 { fmt.Println(\"find r:\", r) break } } cancel() // 这里测试就手动调用cancel() 取消context time.Sleep(time.Second) // sleep 等待 worker goroutine 退出 } func RandWithTimeout(ctx context.Context, ret chan int) { defer close(ret) timer := time.NewTimer(time.Millisecond) for { select { case \u003c-ctx.Done(): fmt.Println(\"ctx cancel\") timer.Stop() return case \u003c-timer.C: r := rand.Intn(100) ret \u003c- r timer.Reset(time.Millisecond) } } } 在这个案例中，因为限制了超时时间，所以并不是每次都能找到满足条件的 r 值。 ","date":"2021-10-22","objectID":"/posts/go/context/:2:2","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"2.3 WithValue 可以传递数据的context，携带关键信息，为全链路提供线索，比如接入elk等系统，需要来一个trace_id，那WithValue就非常适合做这个事。 // 通过 ctx 进行超时控制的同时，在 ctx 中存放 traceId 进行链路追踪。 func main() { withTimeout, cancel := context.WithTimeout(context.Background(), time.Millisecond*1) defer cancel() ctx := context.WithValue(withTimeout, \"traceId\", \"id12345\") r := f1(ctx) fmt.Println(\"r:\", r) } func f1(ctx context.Context) int { fmt.Println(\"f1 traceId:\", fromCtx(ctx)) var ret = make(chan int, 1) go f2(ctx, ret) r1 := rand.Intn(10) fmt.Println(\"r1:\", r1) select { case \u003c-ctx.Done(): return r1 case r2 := \u003c-ret: return r1 + r2 } } func f2(ctx context.Context, ret chan int) { fmt.Println(\"f2 traceId:\", fromCtx(ctx)) // sleep 模拟耗时逻辑 time.Sleep(time.Millisecond * 10) r2 := rand.Intn(10) fmt.Println(\"r2:\", r2) ret \u003c- r2 } func fromCtx(ctx context.Context) string { return ctx.Value(\"traceId\").(string) } 为了进行超时控制，本就需要在多个 goroutine 之前传递 ctx，所以把 traceId 这种信息存放到 ctx 中是非常方便的。 ","date":"2021-10-22","objectID":"/posts/go/context/:2:3","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"3. 源码分析 Context 在 Go 1.7 版本引入标准库中，主要内容可以概括为： 1 个接口 Context 4 种实现 emptyCtx cancelCtx timerCtx valueCtx 6 个方法 Background TODO WithCancel WithDeadline WithTimeout WithValue ","date":"2021-10-22","objectID":"/posts/go/context/:3:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"1 个接口 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u003c-chan struct{} Err() error Value(key interface{}) interface{} } Deadline() ：返回一个time.Time，表示当前Context应该结束的时间，ok则表示有结束时间 Done()：返回一个只读chan，如果可以从该 chan 中读取到数据，则说明 ctx 被取消了 Err()：返回 Context 被取消的原因 Value(key)：返回key对应的value，是协程安全的 同时包中也定义了提供 cancel 功能需要实现的接口。这个主要是后文会提到的“取消信号、超时信号”需要去实现。 // A canceler is a context type that can be canceled directly. The // implementations are *cancelCtx and *timerCtx. type canceler interface { cancel(removeFromParent bool, err error) Done() \u003c-chan struct{} } ","date":"2021-10-22","objectID":"/posts/go/context/:3:1","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"4 种实现 为了更方便的创建 Context，包里定义了 Background 来作为所有 Context 的根，它是一个 emptyCtx 的实例。 emptyCtx 这也是最简单的一个 ctx type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } 过空方法实现了 context.Context 接口，它没有任何功能。 Background 和 TODO 这两个方法都会返回预先初始化好的私有变量 background 和 todo，它们会在同一个 Go 程序中被复用： var ( background = new(emptyCtx) todo = new(emptyCtx) ) func Background() Context { return background } func TODO() Context { return todo } 从源代码来看，context.Background 和 context.TODO和也只是互为别名，没有太大的差别，只是在使用和语义上稍有不同： context.Background 是上下文的默认值，所有其他的上下文都应该从它衍生出来； context.TODO 应该仅在不确定应该使用哪种上下文时使用； 在多数情况下，如果当前函数没有上下文作为入参，我们都会使用 context.Background 作为起始的上下文向下传递。 cancelCtx 这是一个带 cancel 功能的 context。 type cancelCtx struct { // 直接嵌入了一个 Context，那么可以把 cancelCtx 看做是一个 Context Context mu sync.Mutex // protects following fields done atomic.Value // of chan struct{}, created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call } 同时 cancelCtx 还实现了 canceler 接口，提供了 cancel 方法，可以手动取消： type canceler interface { cancel(removeFromParent bool, err error) Done() \u003c-chan struct{} } 实现了上面定义的两个方法的 Context，就表明该 Context 是可取消的。 创建 cancelCtx 的方法如下： func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { if parent == nil { panic(\"cannot create context from nil parent\") } c := newCancelCtx(parent) propagateCancel(parent, \u0026c) return \u0026c, func() { c.cancel(true, Canceled) } } func newCancelCtx(parent Context) cancelCtx { return cancelCtx{Context: parent} } 这是一个暴露给用户的方法，传入一个父 Context（这通常是一个 background，作为根节点），返回新建的 context，并通过闭包的形式，返回了一个 cancel 方法。 newCancelCtx将传入的上下文包装成私有结构体context.cancelCtx。 propagateCancel则会构建父子上下文之间的关联，形成树结构，当父上下文被取消时，子上下文也会被取消： func propagateCancel(parent Context, child canceler) { // 1.如果 parent ctx 是不可取消的 ctx，则直接返回 不进行关联 done := parent.Done() if done == nil { return // parent is never canceled } // 2.接着判断一下 父ctx 是否已经被取消 select { case \u003c-done: // 2.1 如果 父ctx 已经被取消了，那就没必要关联了 // 然后这里也要顺便把子ctx给取消了，因为父ctx取消了 子ctx就应该被取消 // 这里是因为还没有关联上，所以需要手动触发取消 // parent is already canceled child.cancel(false, parent.Err()) return default: } // 3. 从父 ctx 中提取出 cancelCtx 并将子ctx加入到父ctx 的 children 里面 if p, ok := parentCancelCtx(parent); ok { p.mu.Lock() // double check 一下，确认父 ctx 是否被取消 if p.err != nil { // 取消了就直接把当前这个子ctx给取消了 // parent has already been canceled child.cancel(false, p.err) } else { // 否则就添加到 children 里面 if p.children == nil { p.children = make(map[canceler]struct{}) } p.children[child] = struct{}{} } p.mu.Unlock() } else { // 如果没有找到可取消的父 context。新启动一个协程监控父节点或子节点取消信号 atomic.AddInt32(\u0026goroutines, +1) go func() { select { case \u003c-parent.Done(): child.cancel(false, parent.Err()) case \u003c-child.Done(): } }() } } 上述函数总共与父上下文相关的三种不同的情况： 1）当 parent.Done() == nil，也就是 parent 不会触发取消事件时，当前函数会直接返回； 2）当 child 的继承链包含可以取消的上下文时，会判断 parent 是否已经触发了取消信号； 如果已经被取消，child 会立刻被取消； 如果没有被取消，child 会被加入 parent 的 children 列表中，等待 parent 释放取消信号； 3）当父上下文是开发者自定义的类型、实现了 context.Context 接口并在 Done() 方法中返回了非空的管道时； 运行一个新的 Goroutine 同时监听 parent.Done() 和 child.Done() 两个 Channel； 在 parent.Done() 关闭时调用 child.cancel 取消子上下文； propagateCancel 的作用是在 parent 和 child 之间同步取消和结束的信号，保证在 parent 被取消时，child 也会收到对应的信号，不会出现状态不一致的情况。 func parentCancelCtx(parent Context) (*cancelCtx, bool) { done := parent.Done() // 如果 done 为 nil 说明这个ctx是不可取消的 // 如果 done == closedchan 说明这个ctx不是标准的 cancelCtx，可能是自定义的 if done == closedchan || done == nil { return nil, false } // 然后调用 value 方法从ctx中提取出 cancelCtx p, ok := parent.Value(\u0026cancelCtxKey).(*cancelCtx) if !ok { return nil, false } // 最后再判断一下cancelCtx 里存的 done 和 父ctx里的done是否一致 // 如果不一致说明parent不是一个 cancelCtx pdone, _ := p.done.Load().(chan struct{}) if pdone != done { return nil, false } return p, true } cancelCtx 的 done 方法肯定会返回一个 chan struct{} func (c *cancelCtx) Done() \u003c-chan struct{} { d := c.done.Load() if d != ","date":"2021-10-22","objectID":"/posts/go/context/:3:2","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"4. 使用建议 在官方博客里，对于使用 context 提出了几点建议： Do not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. The Context should be the first parameter, typically named ctx. Do not pass a nil Context, even if a function permits it. Pass context.TODO if you are unsure about which Context to use. Use context Values only for request-scoped data that transits processes and APIs, not for passing optional parameters to functions. The same Context may be passed to functions running in different goroutines; Contexts are safe for simultaneous use by multiple goroutines. 翻译过来就是： 不要将 Context 塞到结构体里。直接将 Context 类型作为函数的第一参数，而且一般都命名为 ctx。 不要向函数传入一个 nil 的 context，如果你实在不知道传什么，标准库给你准备好了一个 context：todo。 不要把本应该作为函数参数的类型塞到 context 中，context 存储的应该是一些共同的数据。例如：登陆的 session、cookie 等。 同一个 context 可能会被传递到多个 goroutine，别担心，context 是并发安全的。 ","date":"2021-10-22","objectID":"/posts/go/context/:4:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"5. 参考 https://pkg.go.dev/context https://faiface.github.io/post/context-should-go-away-go2/ https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-context/ https://blog.csdn.net/qq_36183935/article/details/81137834 https://blog.csdn.net/u011957758/article/details/82948750 https://www.jianshu.com/p/e5df3cd0708b https://zhuanlan.zhihu.com/p/68792989 ","date":"2021-10-22","objectID":"/posts/go/context/:5:0","tags":["Golang"],"title":"Go语言之 Context 实战与源码分析","uri":"/posts/go/context/"},{"categories":["Golang"],"content":"sync.pool 库源码分析及其使用场景介绍","date":"2021-10-15","objectID":"/posts/go/sync-pool/","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"本文主要介绍了Go语言(golang)中的sync.pool包。给出了 sync.pool 的基本用法，以及各大框架中的使用案例。并从源码层面对其底层结构和具体实现原理进行分析。 以下分析基于 Go 1.17.1 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:0:0","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"1. 概述 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:1:0","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"大致理念 sync.Pool 是 sync 包下的一个组件，可以作为保存临时取还对象的一个“池子”，可以缓存暂时不用的对象，下次需要时直接使用（无需重新分配）。 因为频繁的内存分配和回收会对性能产生影响，通过复用临时对象就可以避免改问题。 下面是 2018 年的时候，《Go 夜读》上关于 sync.Pool 的分享，关于适用场景： 当多个 goroutine 都需要创建同⼀个对象的时候，如果 goroutine 数过多，导致对象的创建数⽬剧增，进⽽导致 GC 压⼒增大。形成 “并发⼤－占⽤内存⼤－GC 缓慢－处理并发能⼒降低－并发更⼤”这样的恶性循环。 在这个时候，需要有⼀个对象池，每个 goroutine 不再⾃⼰单独创建对象，⽽是从对象池中获取出⼀个对象（如果池中已经有的话）。 因此关键思想就是对象的复用，避免重复创建、销毁，下面我们来看看如何使用。 所以，sync.pool 的作用一句话描述就是，复用临时对象，以避免频繁的内存分配和回收，从而减少 GC 压力。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:1:1","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"基本使用 首先，sync.Pool 是协程安全的，这对于使用者来说是极其方便的。使用前，设置好对象的 New 函数，用于在 Pool 里没有缓存的对象时，创建一个。之后，在程序的任何地方、任何时候仅通过 Get()、Put() 方法就可以取、还对象了。 以下为基本使用 demo： 完整代码见 Github package main import ( \"fmt\" \"sync\" ) type Gopher struct { Name string Remark [1024]byte } func (s *Gopher) Reset() { s.Name = \"\" s.Remark = [1024]byte{} } var gopherPool = sync.Pool{ New: func() interface{} { return new(Gopher) }, } func main() { g := gopherPool.Get().(*Gopher) fmt.Println(\"首次从 pool 里获取：\", g.Name) g.Name = \"first\" fmt.Printf(\"设置 p.Name = %s\\n\", g.Name) gopherPool.Put(g) fmt.Println(\"Pool 里已有一个对象：\u0026{first}，调用 Get: \", gopherPool.Get().(*Gopher).Name) fmt.Println(\"Pool 没有对象了，调用 Get: \", gopherPool.Get().(*Gopher).Name) } 运行结果： 首次从 pool 里获取： 设置 p.Name = first Pool 里已有一个对象：\u0026{first}，调用 Get: first Pool 没有对象了，调用 Get: 首先，需要初始化 Pool，唯一需要的就是设置好 New 函数。当调用 Get 方法时，如果池子里缓存了对象，就直接返回缓存的对象。如果没有存货，则调用 New 函数创建一个新的对象。 另外，我们发现 Get 方法取出来的对象和上次 Put 进去的对象实际上是同一个，Pool 没有做任何“清空”的处理。但我们不应当对此有任何假设，因为在实际的并发使用场景中，无法保证这种顺序，最好的做法是在 Put 前，将对象清空。 Benchmark var defaultGopher, _ = json.Marshal(Gopher{Name: \"17x\"}) func BenchmarkUnmarshal(b *testing.B) { var g *Gopher for n := 0; n \u003c b.N; n++ { g = new(Gopher) json.Unmarshal(defaultGopher, g) } } func BenchmarkUnmarshalWithPool(b *testing.B) { var g *Gopher for n := 0; n \u003c b.N; n++ { g = gopherPool.Get().(*Gopher) json.Unmarshal(defaultGopher, g) g.Reset() // 重置后在放进去 gopherPool.Put(g) } } 运行结果： BenchmarkUnmarshal-6 9518 124806 ns/op 1280 B/op 6 allocs/op BenchmarkUnmarshalWithPool-6 10000 124350 ns/op 128 B/op 5 allocs/op 功能比较简单，其中 json 反序列化占用了大量时间，因此两种方式最终的执行时间几乎没什么变化。但是内存占用差了一个数量级，使用了 sync.Pool 后，内存占用仅为未使用的 128/1280=1/10，对 GC 的影响就很大了。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:1:2","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"使用案例 fmt 包 这部分主要看 fmt.Printf 如何使用： func Printf(format string, a ...interface{}) (n int, err error) { return Fprintf(os.Stdout, format, a...) } 继续看 Fprintf： func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) { p := newPrinter() p.doPrintf(format, a) n, err = w.Write(p.buf) p.free() return } Fprintf 函数的参数是一个 io.Writer，Printf 传的是 os.Stdout，相当于直接输出到标准输出。这里的 newPrinter 用的就是 Pool： var ppFree = sync.Pool{ New: func() interface{} { return new(pp) }, } func newPrinter() *pp { p := ppFree.Get().(*pp) p.panicking = false p.erroring = false p.wrapErrs = false p.fmt.init(\u0026p.buf) return p } 回到 Fprintf 函数，拿到 pp 指针后，会做一些 format 的操作，并且将 p.buf 里面的内容写入 w。最后，调用 free 函数，将 pp 指针归还到 Pool 中： func (p *pp) free() { if cap(p.buf) \u003e 64\u003c\u003c10 { return } p.buf = p.buf[:0] p.arg = nil p.value = reflect.Value{} p.wrappedErr = nil ppFree.Put(p) } 归还到 Pool 前还进行了字段清零，这样，通过 Get 拿到缓存的对象时，就可以安全地使用了。 gin 框架 gin 框架会给每个请求分配一个 Context 用以进行追踪，这就是典型的 sync.pool 使用场景： func New() *Engine { engine := \u0026Engine{ } engine.pool.New = func() interface{} { return engine.allocateContext() } return engine } func (engine *Engine) allocateContext() *Context { return \u0026Context{engine: engine} } func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } 从 pool 中获取 Context 对象，用完后又还回去，注意 还回去之前这里也调用了 reset() 方法进行字段清空。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:1:3","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"正确姿势 根据以上几个案例，可以看出正确使用姿势就是： 1）设置 New 方法 2）使用时直接 Get 3）使用完成后先进行字段清空,然后在 Put 回去。 一定要进行 Reset，不然会出现意想不到的问题。分享一个类似的坑 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:1:4","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"2. 源码分析 一下分析基于 Go 1.17.1 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:0","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"Pool 结构体 type Pool struct { noCopy noCopy local unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal localSize uintptr // size of the local array victim unsafe.Pointer // local from previous cycle victimSize uintptr // size of victims array New func() interface{} } 字段详解： noCopy对象，实现了sync.Locker接口，使得内嵌了 noCopy 的对象在进行 go vet 静态检查的时候，可以检查出是否被复制。 具体见 Go issues 8005 说明 Pool 对象也是不允许复制的。 local 字段存储指向 [P]poolLocal 数组（严格来说，它是一个切片）的指针。localSize 则表示 local 数组的大小。 访问时，根据 P 的 id 去访问对应下标的 local[pid] 通过这样的设计，多个 goroutine 使用同一个 Pool 时，减少了竞争，提升了性能。 有点类似于降低锁粒度，分段锁的思想。 victim 和 victimSize 则会在在一轮 GC 到来时，分别“接管” local 和 localSize。 victim cache 是一种提高缓存性能的硬件技术; victim 的机制用于减少 GC 后冷启动导致的性能抖动，让分配对象更平滑; sync.Pool 引入的意图在于降低 GC 压力的同时提高命中率。 New就是我们指定的新建对象的方法。 Victim Cache 是一种提高缓存性能的硬件技术，主要用于提升缓存命令率。 所谓受害者缓存（Victim Cache），是一个与直接匹配或低相联缓存并用的、容量很小的全相联缓存。当一个数据块被逐出缓存时，并不直接丢弃，而是暂先进入受害者缓存。如果受害者缓存已满，就替换掉其中一项。当进行缓存标签匹配时，在与索引指向标签匹配的同时，并行查看受害者缓存，如果在受害者缓存发现匹配，就将其此数据块与缓存中的不匹配数据块做交换，同时返回给处理器。 local local 具体结构如下： type poolLocal struct { poolLocalInternal // 将 poolLocal 补齐至128字节(即两个cache line)的倍数，防止 false sharing, // 伪共享，仅占位用，防止在 cache line 上分配多个 poolLocalInternal pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte } type poolLocalInternal struct { private interface{} // Can be used only by the respective P. shared poolChain // Local P can pushHead/popHead; any P can popTail. } poolLocalInternal对象 中包含一个 private 和 shared。其中 private 只有当前 p 能用，shared 则是其他 p 都可以用。 cpu cache \u0026 false sharing cpu cache 现代 cpu 中，cache 都划分成以 cache line (cache block) 为单位，在 x86_64 体系下一般都是 64 字节，cache line 是操作的最小单元。 程序即使只想读内存中的 1 个字节数据，也要同时把附近 63 节字加载到 cache 中，如果读取超个 64 字节，那么就要加载到多个 cache line 中。 这样，访问后续 63 字节数据时就可以直接从 cache line 中读取，性能有很大提升。 false sharing 伪共享的非标准定义为：缓存系统中是以缓存行（cache line）为单位存储的，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会令整个 cache line 失效，无意中影响彼此的性能，这就是伪共享。 简单来说，如果没有 pad 字段，那么当需要访问 0 号索引的 poolLocal 时，CPU 同时会把 0 号和 1 号索引同时加载到 cpu cache。在只修改 0 号索引的情况下，会让 1 号索引的 poolLocal 失效。这样，当其他线程想要读取 1 号索引时，发生 cache miss，还得重新再加载，对性能有损。增加一个 pad，补齐缓存行，让相关的字段能独立地加载到缓存行就不会出现 false sharding 了。 poolChain poolChain 是一个双端队列的实现 type poolChain struct { head *poolChainElt tail *poolChainElt } type poolChainElt struct { poolDequeue next, prev *poolChainElt } type poolDequeue struct { headTail uint64 vals []eface } poolDequeue 被实现为单生产者、多消费者的固定大小的无锁（atomic 实现） Ring 式队列（底层存储使用数组，使用两个指针标记 head、tail）。生产者可以从 head 插入、head 删除，而消费者仅可从 tail 删除。 headTail 指向队列的头和尾，通过位运算将 head 和 tail 存入 headTail 变量中。 我们用一幅图来完整地描述 Pool 结构体： 图源：码农桃花源 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:1","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"大致流程 分析完 Pool 结构体之后，先提前说明一下大致的流程，后续分析时便于理解。 存储 为每个 P 开辟了一个 Local 用于数据，降低竞争。 Local 中包含 private 和 shared。 private ：只有当前 P 能使用 shared：所有 P 共享，当 private 没有时优先去当前 P 的 local.shared 中取，如果还没有就去其他 P 中 local.shared 中窃取一个来用。 Get 优先从当前P 的 local.private 中取，没有则从当前 P 的 local.shared 中取，还没有则去其他 P 中 local.shared 中窃取一个。 Put 优先存放到当前 P 的 local.private，local.private 已经有值了就往 shared 中放。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:2","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"Get func (p *Pool) Get() interface{} { // ... l, pid := p.pin() x := l.private l.private = nil if x == nil { x, _ = l.shared.popHead() if x == nil { x = p.getSlow(pid) } } runtime_procUnpin() // ... if x == nil \u0026\u0026 p.New != nil { x = p.New() } return x } 流程如下： 首先，调用 p.pin() 函数将当前的 goroutine 和 P 绑定，禁止被抢占，返回当前 P 对应的 poolLocal，以及 pid。 然后直接取 l.private，赋值给 x，并置 l.private 为 nil。 判断 x 是否为空，若为空，则尝试从 l.shared 的头部 pop 一个对象出来，同时赋值给 x。 如果 x 仍然为空，则调用 getSlow 尝试从其他 P 的 shared 双端队列尾部“偷”一个对象出来。 Pool 的相关操作做完了，调用 runtime_procUnpin() 解除禁止抢占。 最后如果还是没有取到缓存的对象，那就直接调用预先设置好的 New 函数，创建一个出来。 pin 首先，调用 p.pin() 函数将当前的 goroutine 和 P 绑定，禁止被抢占，返回当前 P 对应的 poolLocal，以及 pid。 func (p *Pool) pin() (*poolLocal, int) { // pin 具体逻辑由 runtime_procPin 实现 pid := runtime_procPin() // 原子操作取出 p.localSize 和 p.local s := runtime_LoadAcquintptr(\u0026p.localSize) // load-acquire l := p.local // load-consume // 因为是把 pid 做下标从 pool.local 中取得 p 对应的 local 的， // 所以如果 pid 小于 pool.local size 的时候才有可能取到对应的 local if uintptr(pid) \u003c s { return indexLocal(l, pid), pid } // 正常情况下会一直满足该条件， // 只有刚开始 pool.local 还没创建或者动态调整了 P 的数量这两种情况 // 会进入到下面的逻辑 去创建 pool.local return p.pinSlow() } pinSlow pinSlow 主要是完成 pool.local 的创建。 func (p *Pool) pinSlow() (*poolLocal, int) { // 这里先取消绑定，然后加锁，最后有绑定上 runtime_procUnpin() allPoolsMu.Lock() defer allPoolsMu.Unlock() pid := runtime_procPin() // doubleCheck 因为在执行上述命令过程中 pinSlow 可能已经被其他的线程调用，因此这时候需要再次对 pid 进行检查 s := p.localSize l := p.local if uintptr(pid) \u003c s { return indexLocal(l, pid), pid } if p.local == nil { allPools = append(allPools, p) } // 根据当前 P 的数量创建 pool.local并更新pool.localSize size := runtime.GOMAXPROCS(0) local := make([]poolLocal, size) atomic.StorePointer(\u0026p.local, unsafe.Pointer(\u0026local[0])) // store-release runtime_StoreReluintptr(\u0026p.localSize, uintptr(size)) // store-release // 最后根据 pid 返回当前 P 对应的 local return \u0026local[pid], pid } popHead 然后回到 Get 方法 x := l.private l.private = nil if x == nil { x, _ = l.shared.popHead() if x == nil { x = p.getSlow(pid) } } 优先从 local.private 中取，如果没有就调用poolChain.popHead()去 local.shared 中取一个。 func (c *poolChain) popHead() (interface{}, bool) { d := c.head for d != nil { if val, ok := d.popHead(); ok { return val, ok } d = loadPoolChainElt(\u0026d.prev) } return nil, false } popHead 函数只会被 producer 调用。首先拿到头节点：c.head，如果头节点不为空的话，尝试调用头节点的 poolDequeue.popHead 方法。 func (d *poolDequeue) popHead() (interface{}, bool) { var slot *eface for { ptrs := atomic.LoadUint64(\u0026d.headTail) head, tail := d.unpack(ptrs) // 收尾相连则说明队列是空的 if tail == head { return nil, false } // head 位置是队头的前一个位置，所以此处要先退一位。 // 在读出 slot 的 value 之前就把 head 值减 1，取消对这个 slot 的控制 head-- ptrs2 := d.pack(head, tail) // 通过 CAS 操作更新头部的位置 这样当前头部第一个元素就算是被移除了 if atomic.CompareAndSwapUint64(\u0026d.headTail, ptrs, ptrs2) { slot = \u0026d.vals[head\u0026uint32(len(d.vals)-1)] break } } // 类型转换与 nil 判断 val := *(*interface{})(unsafe.Pointer(slot)) if val == dequeueNil(nil) { val = nil } // 然后把这个 slot 置空，因为现在这个 slot 已经是队列的 head 了，置空便于前一个 head 被回收。 *slot = eface{} return val, true } 此函数会删掉并且返回 queue 的头节点。但如果 queue 为空的话，返回 false。这里的 queue 存储的实际上就是 Pool 里缓存的对象。 整个函数的核心是一个无限循环，这是 Go 中常用的无锁化编程形式。 首先调用 unpack 函数分离出 head 和 tail 指针，如果 head 和 tail 相等，即首尾相等，那么这个队列就是空的，直接就返回 nil，false。 否则，将 head 指针后移一位，即 head 值减 1，然后调用 pack 打包 head 和 tail 指针。使用 CAS 更新 headTail 的值，并且把 vals 相应索引处的元素赋值给 slot。 因为 vals 长度实际是只能是 2 的 n 次幂，因此 len(d.vals)-1 实际上得到的值的低 n 位是全 1，它再与 head 进行与运算，实际就是取 head 低 n 位的值作为下标。 得到相应 slot 的元素后，经过类型转换并判断是否是 dequeueNil，如果是，说明没取到缓存的对象，返回 nil。 type dequeueNil *struct{} 最后，返回 val 之前，将 slot “归零”，移除和上一个 head 的关联，便于回收上一个 Head。 *slot = eface{} 结束后回到 poolChain.popHead()，如果调用 poolDequeue.popHead() 拿到了缓存的对象，就直接返回。否则，将 d 重新指向 d.prev，继续尝试获取缓存的对象。 getSlow 如果在 shared 里没有获取到缓存对象，则继续调用 Pool.getSlow()，尝试从其他 P 的 poolLocal 偷取： func (p *Pool) getSlow(pid int) interface{} { size := runtime_LoadAcquintptr(\u0026p.localSize) // load-acquire locals := p.local // load-consume // 尝试从其他 p 中窃取一个对象 for i := 0; i \u003c int(size); i++ { // (pid+i+1)%int(size) 保证每次都可以从 当前pid+1 这个位置开始尝试窃取。 l := indexLocal(locals, (pid+i+1)%int(size)) // 如果能取到就直接返回 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:3","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"Put func (p *Pool) Put(x interface{}) { if x == nil { return } // ... l, _ := p.pin() if l.private == nil { l.private = x x = nil } if x != nil { l.shared.pushHead(x) } runtime_procUnpin() // ... } 流程也比较简单： 先绑定 g 和 P，然后尝试将 x 赋值给 private 字段。 如果失败，就调用 pushHead 方法尝试将其放入 shared 字段所维护的双端队列中。 pushHead p.pin() 和之前是一样的，就不分析了，主要看一下 pushHead() func (c *poolChain) pushHead(val interface{}) { d := c.head if d == nil { // 第一次写入，队列为空则进行初始化 默认长度为8 const initSize = 8 // Must be a power of 2 d = new(poolChainElt) d.vals = make([]eface, initSize) c.head = d storePoolChainElt(\u0026c.tail, d) } // 存储元素 存储成功直接返回 if d.pushHead(val) { return } // The current dequeue is full. Allocate a new one of twice // the size. // 存储失败说明队列满了 进行扩容 newSize := len(d.vals) * 2 if newSize \u003e= dequeueLimit { // 限制一下，不能无限扩容 newSize = dequeueLimit } // 扩容逻辑也比较简单，就是首尾相连，构成链表 d2 := \u0026poolChainElt{prev: d} d2.vals = make([]eface, newSize) c.head = d2 storePoolChainElt(\u0026d.next, d2) d2.pushHead(val) } 如果 c.head 为空，就要创建一个 poolChainElt，作为首结点，当然也是尾节点。它管理的双端队列的长度，初始为 8，放满之后，再创建一个 poolChainElt 节点时，双端队列的长度就要翻倍。当然，有一个最大长度限制（2^30）： const dequeueBits = 32 const dequeueLimit = (1 \u003c\u003c dequeueBits) / 4 调用 poolDequeue.pushHead 尝试将对象放到 poolDeque 里去： func (d *poolDequeue) pushHead(val interface{}) bool { ptrs := atomic.LoadUint64(\u0026d.headTail) head, tail := d.unpack(ptrs) // 首先判断队列是否已满： 也就是将尾部指针加上 d.vals 的长度，再取低 31 位，看它是否和 head 相等 if (tail+uint32(len(d.vals)))\u0026(1\u003c\u003cdequeueBits-1) == head { return false } slot := \u0026d.vals[head\u0026uint32(len(d.vals)-1)] // Check if the head slot has been released by popTail. typ := atomic.LoadPointer(\u0026slot.typ) if typ != nil { // Another goroutine is still cleaning up the tail, so // the queue is actually still full. return false } // slot占位，将val存入vals中 if val == nil { val = dequeueNil(nil) } *(*interface{})(unsafe.Pointer(slot)) = val // head 增加 1 atomic.AddUint64(\u0026d.headTail, 1\u003c\u003cdequeueBits) return true } 首先判断队列是否已满： if (tail+uint32(len(d.vals)))\u0026(1\u003c\u003cdequeueBits-1) == head { return false } 队列没满，通过 head 指针找到即将填充的 slot 位置：取 head 指针的低 31 位。 slot := \u0026d.vals[head\u0026uint32(len(d.vals)-1)] // Check if the head slot has been released by popTail. typ := atomic.LoadPointer(\u0026slot.typ) if typ != nil { // Another goroutine is still cleaning up the tail, so // the queue is actually still full. return false } 这里还判断了当前 slot 是否正在被 popTail 释放，popTail 相关语句如下： // 最后也是将这个 slot 置空 // 先清空 val 再清空 typ 操作顺序和 pushHead 正好相反 slot.val = nil atomic.StorePointer(\u0026slot.typ, nil) 所以如果 slot.typ==nil 就说明这个 slot 正在被 popTail 释放，说明队列其实还是满的，直接返回 false，走后续的扩容逻辑。 最后，将 val 赋值到 slot，并将 head 指针值加 1。 // slot占位，将val存入vals中 *(*interface{})(unsafe.Pointer(slot)) = val 这里的实现比较巧妙，slot 是 eface 类型，即空接口，将 slot 转为 interface{} 类型，这样 val 能以 interface{} 赋值给 slot 让 slot.typ 和 slot.val 指向其内存块，于是 slot.typ 和 slot.val 均不为空。 pack/unpack 最后我们再来看一下 pack 和 unpack 函数，它们实际上是一组绑定、解绑 head 和 tail 指针的两个函数。 func (d *poolDequeue) pack(head, tail uint32) uint64 { const mask = 1\u003c\u003cdequeueBits - 1 return (uint64(head) \u003c\u003c dequeueBits) | uint64(tail\u0026mask) } mask 的低 31 位为全 1，其他位为 0，它和 tail 相与，就是只看 tail 的低 31 位。而 head 向左移 32 位之后，低 32 位为全 0。最后把两部分“或”起来，head 和 tail 就“绑定”在一起了。 func (d *poolDequeue) unpack(ptrs uint64) (head, tail uint32) { const mask = 1\u003c\u003cdequeueBits - 1 head = uint32((ptrs \u003e\u003e dequeueBits) \u0026 mask) tail = uint32(ptrs \u0026 mask) return } unpack 则是相反的逻辑，取出 head 指针的方法就是将 ptrs 右移 32 位，再与 mask 相与，同样只看 head 的低 31 位。而 tail 实际上更简单，直接将 ptrs 与 mask 相与就可以了。 小结 Put 和 Get 类似，首先尝试将放到当前 p 的 local.private 上，已经有了就放到 local.shared。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:4","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"GC 对于 Pool 而言，并不能无限扩展，否则对象占用内存太多了，会引起内存溢出。 sync.pool 选择了在 GC 时进行清理。 在 pool.go 文件的 init 函数里，注册了 GC 发生时，如何清理 Pool 的函数： // sync/pool.go func init() { runtime_registerPoolCleanup(poolCleanup) } 编译器在编译时将其注册到运行时： // src/runtime/mgc.go // Hooks for other packages var poolcleanup func() // 利用编译器标志将 sync 包中的清理注册到运行时 //go:linkname sync_runtime_registerPoolCleanup sync.runtime_registerPoolCleanup func sync_runtime_registerPoolCleanup(f func()) { poolcleanup = f } 具体的 poolCleanup() 函数如下： func poolCleanup() { for _, p := range oldPools { p.victim = nil p.victimSize = 0 } for _, p := range allPools { p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 } oldPools, allPools = allPools, nil } 整体看起来，比较简洁。主要是将 local 和 victim 作交换，这样也就不致于让 GC 把所有的 Pool 都清空了，有 victim 在“兜底”。 初始状态下，oldPools 和 allPools 均为 nil。 第 1 次调用 Get，由于 p.local 为 nil，将会在 pinSlow 中创建 p.local，然后将 p 放入 allPools，此时 allPools 长度为 1，oldPools 为 nil。 对象使用完毕，第 1 次调用 Put 放回对象。 第 1 次GC STW 阶段，allPools 中所有 p.local 将值赋值给 victim 并置为 nil。allPools 赋值给 oldPools，最后 allPools 为 nil，oldPools 长度为 1。 第 2 次调用 Get，由于 p.local 为 nil，此时会从 p.victim 里面尝试取对象。 对象使用完毕，第 2 次调用 Put 放回对象，但由于 p.local 为 nil，重新创建 p.local，并将对象放回，此时 allPools 长度为 1，oldPools 长度为 1。 第 2 次 GC STW 阶段，oldPools 中所有 p.victim 置 nil，前一次的 cache 在本次 GC 时被回收，allPools 所有 p.local 将值赋值给 victim 并置为nil，最后 allPools 为 nil，oldPools 长度为 1。 简单来说就是清理时，先清理 oldPools 的 local.victim,然后把 allPools 中的 local 赋值给 victim，最后再把 allPools 赋值给 oldPools，把 allPools 置空。 GC 时只会清理 oldPools，allPools 只会先把数据转移到 victim，然后把 allPools 变成 oldPools，如果从 oldPools 中读取出来的数据进行 Put 也会直接放到 allPools 中，相当于要两次 GC 都没有被访问到并且Put回来才会被移除。 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:2:5","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"3. 小结 1）关键思想是对象的复用，避免重复创建、销毁。将暂时不用的对象缓存起来，待下次需要的时候直接使用，不用再次经过内存分配，复用对象的内存，减轻 GC 的压力。 2）sync.Pool 是协程安全的，使用起来非常方便。设置好 New 函数后，调用 Get 获取，调用 Put 归还对象。 3）不要对 Get 得到的对象有任何假设，更好的做法是归还对象时，将对象“清空”。 4）Pool 里对象的生命周期受 GC 影响，不适合于做连接池，因为连接池需要自己管理对象的生命周期。 一些设计思想或者相关知识点： lock free：无锁编程是很多编程语言里逃离不了的话题。sync.Pool的无锁是在poolDequeue和poolChain层面实现的。 原子操作代替锁：poolDequeue对一些关键变量采用了CAS操作，比如poolDequeue.headTail，既可完整保证并发又能降低相比锁而言的开销。 cacheline false sharing 问题 noCopy 禁止复制 分段锁，降低锁粒度 victim cache 常见优化手段：复用 ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:3:0","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Golang"],"content":"4. 参考 https://golang.org/src/sync/pool.go https://en.wikipedia.org/wiki/False_sharing https://en.wikipedia.org/wiki/Victim_cache https://zhuanlan.zhihu.com/p/110140126 https://medium.com/swlh/go-the-idea-behind-sync-pool-32da5089df72 https://zhuanlan.zhihu.com/p/133638023 https://www.jianshu.com/p/dc4b5562aad2 https://colobu.com/2019/10/08/how-is-sync-Pool-improved-in-Go-1-13/ ","date":"2021-10-15","objectID":"/posts/go/sync-pool/:4:0","tags":["Golang"],"title":"Go语言之 sync.pool 源码分析","uri":"/posts/go/sync-pool/"},{"categories":["Kafka"],"content":"Kafka 中的高水位（High Watermark）和 Leader Epoch 机制","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"本文解释了 Kafka 中的高水位和 Leader Epoch 机制。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 系列相关代码见 Github 高水位（High Watermark）是 Kafka 中非常重要的概念，而 Leader Epoch 是社区在 0.11 版本中新推出的，主要是为了弥补高水位机制的一些缺陷。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"2. 高水位（High Watermark） 水位一词多用于流式处理领域，比如，Spark Streaming 或 Flink 框架中都有水位的概念。教科书中关于水位的经典定义通常是这样的： 在时刻 T，任意创建时间（Event Time）为 T’，且 T’≤T 的所有事件都已经到达或被观测到，那么 T 就被定义为水位。 具体如下图所示： 图中标注“Completed”的蓝色部分代表已完成的工作，标注“In-Flight”的红色部分代表正在进行中的工作，两者的边界就是水位线。 在 Kafka 的世界中，水位的概念有一点不同，它是用消息位移来表征的。 另外 Kafka 中只有高水位没有低水位的说法，所以下文主要围绕 高水位展开。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"高水位的作用 在 Kafka 中，高水位的作用主要有 2 个。 1）定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。 2）帮助 Kafka 完成副本同步。 我们假设这是某个分区 Leader 副本的高水位图。首先，请你注意图中的“已提交消息”和“未提交消息”。 在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。消费者只能消费已提交消息，即图中位移小于 8 的所有消息。 注意，这里我们不讨论 Kafka 事务，因为事务机制会影响消费者所能看到的消息的范围，它不只是简单依赖高水位来判断。它依靠一个名为 LSO（Log Stable Offset）的位移值来判断事务型消费者的可见性。 图中还有一个日志末端位移的概念，即 Log End Offset，简写是 LEO。它表示副本写入下一条消息的位移值。 注意，数字 15 所在的方框是虚线，这就说明，这个副本当前只有 15 条消息，位移值是从 0 到 14，下一条新消息的位移是 15 **高水位和 LEO 是副本对象的两个重要属性。**Kafka 所有副本都有对应的高水位和 LEO 值，而不仅仅是 Leader 副本。只不过 Leader 副本比较特殊，Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。换句话说，分区的高水位就是其 Leader 副本的高水位。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"高水位更新机制 实际上，除了保存一组高水位值和 LEO 值=之外，在 Leader 副本所在的 Broker 上，还保存了其他 Follower 副本（也称为远程副本）的 LEO 值。 在这张图中，我们可以看到，Broker 0 上保存了某分区的 Leader 副本和所有 Follower 副本的 LEO 值，而 Broker 1 上仅仅保存了该分区的某个 Follower 副本。 为什么要在 Broker 0 上保存这些远程副本呢？ 其实，它们的主要作用是，帮助 Leader 副本确定其高水位，也就是分区高水位。 更新机制如下表： 更新对象 更新时机 Broker 1 上的 Follow 副本 LEO Follower 副本从 Leader 副本拉取消息，写入到本地磁盘后，会更新其 LEO 值。 Broker 0 上Leader 副本 LEO Leader副本接收到生产者发送的消息，写入到本地磁盘后，会更新其LEO值。 Broker 0 上远程副本 LEO Follower副本从eader副本拉取消息时，会告诉L eader副本从哪个位移处开始拉取。L eader副本会使用这个位移值来更新远程副本的L EO。 Broker 1 上Follower副本高水位 Follower副本成功更新完LEO之后，会比较其LEO值与Leader副本发来的高水位值，并用两者的较小值去更新它自己的高水位。 Broker 0上Leader副本高水位 主要有两个更新时机: 一个是Leader副本更新其LEO之后;另一个是更新完远程副本LEO之后。具体的算法是:取 Leader副本和所有与Leader同步的远程副本LEO中的最小值 Leader 副本 处理生产者请求的逻辑如下： 1）写入消息到本地磁盘。 2）更新分区高水位值。 获取 Leader 副本所在 Broker 端保存的所有远程副本 LEO 值（LEO-1，LEO-2，……，LEO-n）。 获取 Leader 副本高水位值：currentHW。 更新 currentHW = max{currentHW, min（LEO-1, LEO-2, ……，LEO-n）}。 处理 Follower 副本拉取消息的逻辑如下： 1）读取磁盘（或页缓存）中的消息数据。 2）使用 Follower 副本发送请求中的位移值更新远程副本 LEO 值。 3）更新分区高水位值（具体步骤与处理生产者请求的步骤相同）。 Follower 副本 从 Leader 拉取消息的处理逻辑如下： 1）写入消息到本地磁盘。 2）更新 LEO 值。 3）更新高水位值。 获取 Leader 发送的高水位值：currentHW。 获取步骤 2 中更新过的 LEO 值：currentLEO。 更新高水位为 min(currentHW, currentLEO)。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"副本同步机制解析 首先是初始状态。下面这张图中的 remote LEO 就是刚才的远程副本的 LEO 值。在初始状态时，所有值都是 0。 当生产者给主题分区发送一条消息后，状态变更为： 此时，Leader 副本成功将消息写入了本地磁盘，故 LEO 值被更新为 1。 Follower 再次尝试从 Leader 拉取消息。和之前不同的是，这次有消息可以拉取了，因此状态进一步变更为： 这时，Follower 副本也成功地更新 LEO 为 1。此时，Leader 和 Follower 副本的 LEO 都是 1，但各自的高水位依然是 0，还没有被更新。它们需要在下一轮的拉取中被更新，如下图所示： 在新一轮的拉取请求中，由于位移值是 0 的消息已经拉取成功，因此 Follower 副本这次请求拉取的是位移值 =1 的消息。Leader 副本接收到此请求后，更新远程副本 LEO 为 1，然后更新 Leader 高水位为 1。做完这些之后，它会将当前已更新过的高水位值 1 发送给 Follower 副本。Follower 副本接收到以后，也将自己的高水位值更新成 1。至此，一次完整的消息同步周期就结束了。事实上，Kafka 就是利用这样的机制，实现了 Leader 和 Follower 副本之间的同步。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"消息丢失问题 从刚才的分析中，我们知道，Follower 副本的高水位更新需要一轮额外的拉取请求才能实现。如果把上面那个例子扩展到多个 Follower 副本，情况可能更糟，也许需要多轮拉取请求。也就是说，Leader 副本高水位更新和 Follower 副本高水位更新在时间上是存在错配的。这种错配是很多“数据丢失”或“数据不一致”问题的根源。 开始时，副本 A 和副本 B 都处于正常状态，A 是 Leader 副本。某个使用了默认 acks 设置的生产者程序向 A 发送了两条消息，A 全部写入成功，此时 Kafka 会通知生产者说两条消息全部发送成功。 现在我们假设 Leader 和 Follower 都写入了这两条消息，而且 Leader 副本的高水位也已经更新了，但 Follower 副本高水位还未更新——这是可能出现的。还记得吧，Follower 端高水位的更新与 Leader 端有时间错配。倘若此时副本 B 所在的 Broker 宕机，当它重启回来后，副本 B 会执行日志截断操作，将 LEO 值调整为之前的高水位值，也就是 1。这就是说，位移值为 1 的那条消息被副本 B 从磁盘中删除，此时副本 B 的底层磁盘文件中只保存有 1 条消息，即位移值为 0 的那条消息。 当执行完截断操作后，副本 B 开始从 A 拉取消息，执行正常的消息同步。如果就在这个节骨眼上，副本 A 所在的 Broker 宕机了，那么 Kafka 就别无选择，只能让副本 B 成为新的 Leader，此时，当 A 回来后，需要执行相同的日志截断操作，即将高水位调整为与 B 相同的值，也就是 1。这样操作之后，位移值为 1 的那条消息就从这两个副本中被永远地抹掉了。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:2:4","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"3. Leader Epoch 机制 社区在 0.11 版本正式引入了 Leader Epoch 概念，来规避因高水位更新错配导致的各种不一致问题。 所谓 Leader Epoch，我们大致可以认为是 Leader 版本。它由两部分数据组成。 1）Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的 Leader 被认为是过期 Leader，不能再行使 Leader 权力。 2）起始位移（Start Offset）。Leader 副本在该 Epoch 值上写入的首条消息的位移。 场景和之前大致是类似的，只不过引用 Leader Epoch 机制后，Follower 副本 B 重启回来后，需要向 A 发送一个特殊的请求去获取 Leader 的 LEO 值。在这个例子中，该值为 2。当获知到 Leader LEO=2 后，B 发现该 LEO 值不比它自己的 LEO 值小，而且缓存中也没有保存任何起始位移值 \u003e 2 的 Epoch 条目，因此 B 无需执行任何日志截断操作。这是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。 现在，副本 A 宕机了，B 成为 Leader。同样地，当 A 重启回来后，执行与 B 相同的逻辑判断，发现也不用执行日志截断，至此位移值为 1 的那条消息在两个副本中均得到保留。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。 这样，通过 Leader Epoch 机制，Kafka 完美地规避了这种数据丢失场景。 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"4. 小结 本文主要介绍了 Kafka 的高水位机制以及 Leader Epoch 机制。 高水位的作用： 1）定义消息可见性 在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。消费者只能消费已提交消息。 2）帮助 Kafka 完成副本同步 高水位在界定 Kafka 消息对外可见性以及实现副本机制等方面起到了非常重要的作用，但其设计上的缺陷给 Kafka 留下了很多数据丢失或数据不一致的潜在风险。 为此，社区引入了 Leader Epoch 机制，尝试规避掉这类风险。 Leader Epoch，我们大致可以认为是Leader版本。它由两部分数据组成： 1）Epoch,一个单调增加的版本号。每当副本领导权发生变更时，都会增加该 版本号。 2）起始位移Leader副本在该Epoch值上写入的首条消息的位移。 Kafka 系列相关代码见 Github ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["Kafka"],"content":"5. 参考 《Kafka 核心技术与实战》 《Apache Kafka实战》 https://www.cnblogs.com/youngchaolin/p/12641463.html https://juejin.cn/post/6979110739416088607 ","date":"2021-09-30","objectID":"/posts/kafka/12-hw-leader-epoch/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(十二)---Kafka 中的高水位和 Leader Epoch 机制","uri":"/posts/kafka/12-hw-leader-epoch/"},{"categories":["MySQL"],"content":"MySQL ACID 原子性（atomicity）、一致性（consistency）、隔离性（isolation）、持久性（durability） 特性实现原理简单分析","date":"2021-09-19","objectID":"/posts/mysql/10-acid/","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"本文主要记录了 MySQL ACID 的具体实现原理，包括原子性（atomicity）、一致性（consistency）、隔离性（isolation）、持久性（durability）。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:0:0","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"1. 概述 本文主要讲述数据库事务和 ACID 具体是什么，以及 MySQL 是如何实现它们的。 结论 ACID 4 个特性中： 一致性（consistency）是目的； 原子性（atomicity）、隔离性（isolation）、持久性（durability）是手段。 即通过原子性、隔离性、持久性来保证一致性。 因此数据库必须实现 AID 三大特性才有可能实现一致性(C)。 MySQL ACID 特性实现原理： 原子性：主要基于 undo log 持久性：主要基于 redo log 隔离性：InnoDB 默认的隔离级别是 RR，RR 的实现主要基于锁机制（包含 next-key lock）、MVCC（包括数据的隐藏列、基于 undo log 的版本链、ReadView） 一致性：事务追求的最终目标，一致性的实现既需要数据库层面的保障，也需要应用层面的保障 数据库事务和 ACID 特性是个什么关系呢？ 首先引用 维基百科 的解释： 数据库事务：在数据库管理系统中，事务是单个逻辑或工作单元，有时由多个操作组成。在数据库中以一致模式完成的任何逻辑计算都称为事务。 ACID：ACID是数据库事务的一组属性，旨在在出现错误、电源故障和其他事故的情况下保证数据的有效性。在数据库的上下文中，满足 ACID 属性的一系列数据库操作（可以将其视为对数据的单个逻辑操作）称为事务。 事务就是在数据库中以一致模式完成的任何逻辑计算，而满足 ACID 属性的一系列数据库操作也可以称作事务。 即：满足 ACID 特性的操作就是事务。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:1:0","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"2. ACID 具体实现 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:2:0","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"2.1 原子性（atomicity） 事务通常由多个语句组成，原子性保证每个事务都被视为一个单独的单元,要么完全成功，要么完全失败。即一个事务（transaction）中的所有操作，要么全部执行成功，要么全部不执行。 简单来说：原子性的结果就是没有中间状态，如果有中间状态则一致性就不会得到满足。 仔细想一想没有中间结果是不可能实现的，所以 MySQL 采用了曲线救国的方式：即执行失败后，可以回滚，保证不会出现部分成功的情况，通过 undolog 实现该特性。 事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 undo log 属于逻辑日志，它记录的是 sql 执行相关的信息。当发生回滚时，InnoDB 会根据 undo log 的内容做与之前相反的工作：对于每个 insert，回滚时会执行 delete；对于每个 delete，回滚时会执行insert；对于每个 update，回滚时会执行一个相反的 update，把数据改回去。 虽然从结果上看是保证原子性了，但是和真正的原子性还是有差距的，应为这样实现其实是有中间结果，在并发场景下其他事务可能会看到中间结果。 为了保证中间结果对外不可见，需要通过隔离性来保证并发场景下的正确性 通过隔离性保证其他事务不会看到本事务的执行结果； 当然 read uncommitted 级别则没有这个功能，所以一般不用该隔离级别。 MySQL 原子性实现原理 通过 undolog 在失败时回滚保证在结果上是原子性的， 即没有中间状态。 通过隔离性保证了在其他并发事务看来是原子性的，即中间状态对外不可见。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:2:1","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"2.2 隔离性（isolation） 隔离性，指一个事务内部的操作及使用的数据对正在进行的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 正是它保证了原子操作的过程中，中间状态对其它事务不可见。 隔离性有原子性的部分，但是更多的还是关注并发事务。 Mysql 隔离级别有以下四种（级别由低到高）： ReadUncommitted 读未提交 ReadCommitted 读已提交 RepeatableRead 可重复度 Serializable 串行化 隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们主要考虑最简单的读操作和写操作(加锁读等特殊读操作会特殊说明)，那么隔离性的探讨，主要可以分为两个方面： (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性 (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 锁机制 按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。 表锁在操作数据时会锁定整张表，并发性能较差； 行锁则只锁定需要操作的数据，并发性能好。 但是由于加锁本身需要消耗资源(获得锁、检查锁、释放锁等都需要消耗资源)，因此在锁定数据较多情况下使用表锁可以节省大量资源。 MVCC机制 MVCC最大的优点是读不加锁，因此读写不冲突，并发性能好。InnoDB实现MVCC，多个版本的数据可以共存，主要基于以下技术及数据结构： 1）隐藏列：InnoDB中每行数据都有隐藏列，隐藏列中包含了本行数据的事务id、指向undo log的指针等。 2）基于undo log的版本链：前面说到每行数据的隐藏列中包含了指向undo log的指针，而每条undo log也会指向更早版本的undo log，从而形成一条版本链。 3）ReadView：通过隐藏列和版本链，MySQL可以将数据恢复到指定版本；但是具体要恢复到哪个版本，则需要根据ReadView来确定。 所谓ReadView，是指事务（记做事务A）在某一时刻给整个事务系统（trx_sys）打快照，之后再进行读操作时，会将读取到的数据中的事务id与trx_sys快照比较，从而判断数据对该ReadView是否可见，即对事务A是否可见。 TODO 隔离性是最复杂的，内容比较多，等后续展开细说。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:2:2","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"2.3 持久性（durability） 持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 InnoDB 通过 redo log 重做日志保证了事务的持久性。 事务开始之后就产生 redo log，redo log 的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo log 文件中。 参数innodb_flush_log_at_trx_commit可设置在事务 commit 的时候必须要写入 redo log 文件。 redo log 具体原理： 当数据修改时，除了修改 Buffer Pool 中的数据，还会在 redo log 记录这次操作； 当事务提交时，会调用 fsync 接口对 redo log 进行刷盘。 redo log 采用的是 WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到 Buffer Pool 。 如果 MySQL 宕机，重启时可以读取 redo log 中的数据，对数据库进行恢复，保证了数据不会因 MySQL 宕机而丢失，从而满足了持久性要求。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:2:3","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"2.4 一致性（consistency） 一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。 数据库的完整性约束包括但不限于：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）。 可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:2:4","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"3. 小结 下面总结一下ACID特性及其实现原理： 原子性：语句要么全执行，要么全不执行，即没有中间状态，是事务最核心的特性，事务本身就是以原子性来定义的；实现主要基于 undo log； 持久性：保证事务提交后不会因为宕机等原因导致数据丢失；实现主要基于 redo log； 隔离性：保证事务执行尽可能不受其他事务影响；InnoDB 默认的隔离级别是 RR，RR 的实现主要基于锁机制（包含 next-key lock）、MVCC（包括数据的隐藏列、基于 undo log 的版本链、ReadView）； 一致性：事务追求的最终目标，一致性的实现既需要数据库层面的保障，也需要应用层面的保障。 ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:3:0","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["MySQL"],"content":"4. 参考 《MySQL技术内幕 InnoDB存储引擎》 《高性能MySQL》 https://blog.csdn.net/weixin_44015043/article/details/105217603 https://www.cnblogs.com/feixiablog/articles/8301798.html https://www.cnblogs.com/jianzh5/p/11643151.html https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html#auto_id_11 https://www.cnblogs.com/xinysu/p/6555082.html ","date":"2021-09-19","objectID":"/posts/mysql/10-acid/:4:0","tags":["MySQL"],"title":"MySQL教程(十)---MySQL ACID 实现原理","uri":"/posts/mysql/10-acid/"},{"categories":["Kafka"],"content":"Kafka 消费者组和 Rebalance 及如何避免无效 Rebalance","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"本文主要讲述了 Kafka 的消费者组（Consumer Group）和 消费者组的 Rebalance 及如何避免无效 Rebalance。 Kakfa 相关代码见 Github ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"1. 传统消息模型 传统消息模型一般分为消息队列模型和发布订阅模型： 1）消息队列模型的缺陷在于消息一旦被消费，就会从队列中被删除，而且只能被下游的一个 Consumer 消费。这种模型的伸缩性（scalability）很差，因为下游的多个 Consumer 都要“抢”这个共享消息队列的消息。 2）发布 / 订阅模型倒是允许消息被多个 Consumer 消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果。 Kafka 的 Consumer Group 机制正好避开这两种模型的缺陷，又兼具它们的优点。 Kafka 仅仅使用 Consumer Group 这一种机制，却同时实现了传统消息引擎系统的两大模型： 如果所有实例都属于同一个 Group，那么它实现的就是消息队列模型； 如果所有实例分别属于不同的 Group，那么它实现的就是发布 / 订阅模型。 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"2. Consumer Group Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。 组内可以有多个消费者或消费者实例（Consumer Instance），它们共享一个公共的 ID，这个 ID 被称为 Group ID。组内的所有消费者协调在一起来消费订阅主题（Subscribed Topics）的所有分区（Partition）。当然，每个分区只能由同一个消费者组内的一个 Consumer 实例来消费。 Consumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 在实际场景中，使用进程更为常见一些。 Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。 Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。 这个分区当然也可以被其他的 Group 消费。 理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数，这样能最大限度地实现高伸缩性。 注意：Consumer Group 中的实例是以 分区 为单位进行消费的，如果实例数大于分区数就会导致有的实例无法消费到任何消息。 假如有 6 个分区，Consumer Group 中却有 8 个实例，那么有两个实例将不会被分配任何分区，它们永远处于空闲状态。 因此，在实际使用过程中一般不推荐设置大于总分区数的 Consumer 实例。 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"3. Rebalance ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"Rebalance 流程 Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。 比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。 那么 Consumer Group 何时进行 Rebalance 呢？Rebalance 的触发条件有 3 个。 1）组成员数发生变更。比如有新的 Consumer 实例加入组或者离开组，亦或是有 Consumer 实例崩溃被“踢出”组。 2）订阅主题数发生变更。Consumer Group 可以使用正则表达式的方式订阅主题，比如 consumer.subscribe(Pattern.compile(“t.*c”)) 就表明该 Group 订阅所有以字母 t 开头、字母 c 结尾的主题。在 Consumer Group 的运行过程中，你新创建了一个满足这样条件的主题，那么该 Group 就会发生 Rebalance。 3）订阅主题的分区数发生变更。Kafka 当前只能允许增加一个主题的分区数。当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。 假设目前某个 Consumer Group 下有两个 Consumer，比如 A 和 B，当第三个成员 C 加入时，Kafka 会触发 Rebalance，并根据默认的分配策略重新为 A、B 和 C 分配分区，如下图所示： ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"Rebalance 的弊端 在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。 其次，目前 Rebalance 的设计是所有 Consumer 实例共同参与，全部重新分配所有分区。 其实更高效的做法是尽量减少分配方案的变动。例如实例 A 之前负责消费分区 1、2、3，那么 Rebalance 之后，如果可能的话，最好还是让实例 A 继续消费分区 1、2、3，而不是被重新分配其他的分区。这样的话，实例 A 连接这些分区所在 Broker 的 TCP 连接就可以继续用，不用重新创建连接其他 Broker 的 Socket 资源。 最后，Rebalance 实在是太慢了。 曾经，有个国外用户的 Group 内有几百个 Consumer 实例，成功 Rebalance 一次要几个小时！这完全是不能忍受的。最悲剧的是，目前社区对此无能为力，至少现在还没有特别好的解决方案。 因此一些大数据框架都使用的 standalone consumer。 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"避免无效 Rebalance 要避免 Rebalance，还是要从 Rebalance 发生的时机入手： 1）组成员数量发生变化 2）订阅主题数量发生变化 3）订阅主题的分区数发生变化 后面两个通常都是运维的主动操作，所以它们引发的 Rebalance 大都是不可避免的。所以我们主要说说因为组成员数量变化而引发的 Rebalance 该如何避免。 如果 Consumer Group 下的 Consumer 实例数量发生变化，就一定会引发 Rebalance。这是 Rebalance 发生的最常见的原因。 除了手动启动或停止之外，Consumer 实例可能会被 Coordinator 错误地认为“已停止”从而被“踢出”Group。如果是这个原因导致的 Rebalance，我们就不能不管了。 Coordinator 即协调者，它专门为 Consumer Group 服务，负责为 Group 执行 Rebalance 以及提供位移管理和组成员管理等。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经“死”了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。 Consumer 端有个参数，叫 session.timeout.ms，就是被用来表征此事的。该参数的默认值是 10 秒，即如果 Coordinator 在 10 秒之内没有收到 Group 下某 Consumer 实例的心跳，它就会认为这个 Consumer 实例已经挂了。可以这么说，session.timeout.ms 决定了 Consumer 存活性的时间间隔。 Consumer 还提供了一个允许你控制发送心跳请求频率的参数，就是 heartbeat.interval.ms。这个值设置得越小，Consumer 实例发送心跳请求的频率就越高。频繁地发送心跳请求会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启 Rebalance，因为，目前 Coordinator 通知各个 Consumer 实例开启 Rebalance 的方法，就是将 REBALANCE_NEEDED 标志封装进心跳请求的响应体中。 Consumer 端还有一个参数，用于控制 Consumer 实际消费能力对 Rebalance 的影响，即 max.poll.interval.ms 参数。它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起“离开组”的请求，Coordinator 也会开启新一轮 Rebalance。 第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被“踢出”Group 而引发的。因此，你需要仔细地设置 session.timeout.ms 和 heartbeat.interval.ms 的值。我在这里给出一些推荐数值，你可以“无脑”地应用在你的生产环境中。 设置 session.timeout.ms = 6s。 设置 heartbeat.interval.ms = 2s。 要保证 Consumer 实例在被判定为“dead”之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms \u003e= 3 * heartbeat.interval.ms。 第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。max.poll.interval.ms 参数值的设置显得尤为关键。如果要避免非预期的 Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。 最后 Consumer 端的因为 GC 设置不合理导致程序频发 Full GC 也可能引发非预期 Rebalance。 小结，调整以下 4 个参数以避免无效 Rebalance： session.timeout.ms heartbeat.interval.ms max.poll.interval.ms GC 参数 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:3:3","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"4. Go 客户端 // sarama 库中消费者组为一个接口 sarama.ConsumerGroup 所有实现该接口的类型都能当做消费者组使用。 // MyConsumerGroupHandler 实现 sarama.ConsumerGroup 接口，作为自定义ConsumerGroup type MyConsumerGroupHandler struct { name string count int64 } // Setup 执行在 获得新 session 后 的第一步, 在 ConsumeClaim() 之前 func (MyConsumerGroupHandler) Setup(_ sarama.ConsumerGroupSession) error { return nil } // Cleanup 执行在 session 结束前, 当所有 ConsumeClaim goroutines 都退出时 func (MyConsumerGroupHandler) Cleanup(_ sarama.ConsumerGroupSession) error { return nil } // ConsumeClaim 具体的消费逻辑 func (h MyConsumerGroupHandler) ConsumeClaim(sess sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error { for msg := range claim.Messages() { fmt.Printf(\"[consumer] name:%s topic:%q partition:%d offset:%d\\n\", h.name, msg.Topic, msg.Partition, msg.Offset) // 标记消息已被消费 内部会更新 consumer offset sess.MarkMessage(msg, \"\") h.count++ if h.count%10000 == 0 { fmt.Printf(\"name:%s 消费数:%v\\n\", h.name, h.count) } } return nil } func ConsumerGroup(topic, group, name string) { config := sarama.NewConfig() config.Consumer.Return.Errors = true ctx, cancel := context.WithCancel(context.Background()) defer cancel() cg, err := sarama.NewConsumerGroup([]string{conf.HOST}, group, config) if err != nil { log.Fatal(\"NewConsumerGroup err: \", err) } defer cg.Close() var wg sync.WaitGroup wg.Add(1) go func() { defer wg.Done() handler := MyConsumerGroupHandler{name: name} for { fmt.Println(\"running: \", name) /* ![important] 应该在一个无限循环中不停地调用 Consume() 因为每次 Rebalance 后需要再次执行 Consume() 来恢复连接 Consume 开始才发起 Join Group 请求 如果当前消费者加入后成为了 消费者组 leader,则还会进行 Rebalance 过程，从新分配 组内每个消费组需要消费的 topic 和 partition，最后 Sync Group 后才开始消费 具体信息见 https://github.com/lixd/kafka-go-example/issues/4 */ err = cg.Consume(ctx, []string{topic}, handler) if err != nil { log.Println(\"Consume err: \", err) } // 如果 context 被 cancel 了，那么退出 if ctx.Err() != nil { return } } }() wg.Wait() } 注意点： 1）需要实现 sarama.ConsumerGroup 接口 2）具体消费逻辑在 ConsumeClaim 方法中实现，消费完成后通过sess.MarkMessage标记消息已经被消费 3）需要在 for 循环中调用 Consume 方法，因为 Rebalance 的存在会导致连接终端或者被分配到其他 Broker 上去，需要重新建立连接，具体见 issues#4 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"5. 小结 Kakfa 相关代码见 Github Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。 建议消费者组中的实例数等于 Topic 的分区数。 Rebalance 弊端： 影响 Consumer 端 TPS 很慢 效率不高 非必要 Rebalance： 因为Consumer没能及时发送心跳请求，导致被踢出”Group而引发的。 Consumer消费时间过长导致的。 减少 Rebalance 的 4个参数： session.timeout.ms heartbeat.interval.ms max.poll.interval.ms GC 参数 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"6. 参考 https://github.com/Shopify/sarama、 《Kafka 核心技术与实战》 ","date":"2021-09-10","objectID":"/posts/kafka/11-consumer-group-rebalance/:6:0","tags":["Kafka"],"title":"Kafka(Go)教程(十一)---Consumer Group \u0026 Rebalance","uri":"/posts/kafka/11-consumer-group-rebalance/"},{"categories":["Kafka"],"content":"Kafka是如何实现精确一次（exactly once）语义的？","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"本文主要讲述了Kafka 消息交付可靠性保障以及精确处理一次语义的实现，具体包括幂等生产者和事务生产者。 Kakfa 相关代码见 Github ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"1. 概述 所谓的消息交付可靠性保障，是指 Kafka 对 Producer 和 Consumer 要处理的消息提供什么样的承诺。常见的承诺有以下三种： 最多一次（at most once）：消息可能会丢失，但绝不会被重复发送。 至少一次（at least once）：消息不会丢失，但有可能被重复发送。 精确一次（exactly once）：消息不会丢失，也不会被重复发送。 目前，Kafka 默认提供的交付可靠性保障是第二种，即至少一次。 这样虽然不出丢失消息，但是会导致消息重复发送。 Kafka 也可以提供最多一次交付保障，只需要让 Producer 禁止重试即可。 这样一来肯定不会重复发送，但是可能会丢失消息。 无论是至少一次还是最多一次，都不如精确一次来得有吸引力。大部分用户还是希望消息只会被交付一次，这样的话，消息既不会丢失，也不会被重复处理。 Kafka 分别通过 幂等性（Idempotence）和事务（Transaction）这两种机制实现了 精确一次（exactly once）语义。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"2. 幂等性（Idempotence） 幂等这个词原是数学领域中的概念，指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。 幂等性最大的优势在于我们可以安全地重试任何幂等性操作，反正它们也不会破坏我们的系统状态。 在 Kafka 中，Producer 默认不是幂等性的，但我们可以创建幂等性 Producer。它其实是 0.11.0.0 版本引入的新功能。指定 Producer 幂等性的方法很简单，仅需要设置一个参数即可，即 props.put(“enable.idempotence”, ture)，或 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG， true)。 enable.idempotence 被设置成 true 后，Producer 自动升级成幂等性 Producer，其他所有的代码逻辑都不需要改变。Kafka 自动帮你做消息的重复去重。 底层具体的原理很简单，就是经典的用空间去换时间的优化思路，即在 Broker 端多保存一些字段。当 Producer 发送了具有相同字段值的消息后，Broker 能够自动知晓这些消息已经重复了，于是可以在后台默默地把它们“丢弃”掉。 当然，实际的实现原理并没有这么简单，但你大致可以这么理解。 Kafka 为了实现幂等性，它在底层设计架构中引入了 ProducerID 和 SequenceNumber。 Producer 需要做的只有两件事： 1）初始化时像向 Broker 申请一个 ProducerID 2）为每条消息绑定一个 SequenceNumber Kafka Broker 收到消息后会以 ProducerID 为单位存储 SequenceNumber，也就是说即时 Producer 重复发送了， Broker 端也会将其过滤掉。 实现比较简单，同样的限制也比较大： 首先，它只能保证单分区上的幂等性。即一个幂等性 Producer 能够保证某个主题的一个分区上不出现重复消息，它无法实现多个分区的幂等性。 因为 SequenceNumber 是以 Topic + Partition 为单位单调递增的，如果一条消息被发送到了多个分区必然会分配到不同的 SequenceNumber ,导致重复问题。 其次，它只能实现单会话上的幂等性。不能实现跨会话的幂等性。当你重启 Producer 进程之后，这种幂等性保证就丧失了。 重启 Producer 后会分配一个新的 ProducerID，相当于之前保存的 SequenceNumber 就丢失了。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"3. 事务（Transaction） Kafka 的事务概念类似于我们熟知的数据库提供的事务。 Kafka 自 0.11 版本开始也提供了对事务的支持，目前主要是在 read committed 隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。 事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。 设置事务型 Producer 的方法也很简单，满足两个要求即可： 和幂等性 Producer 一样，开启 enable.idempotence = true。 设置 Producer 端参数 transactional. id。最好为其设置一个有意义的名字。 此外，你还需要在 Producer 代码中做一些调整，如这段代码所示： producer.initTransactions(); try { producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction(); } catch (KafkaException e) { producer.abortTransaction(); } 和普通 Producer 代码相比，事务型 Producer 的显著特点是调用了一些事务 API，如 initTransaction、beginTransaction、commitTransaction 和 abortTransaction，它们分别对应事务的初始化、事务开始、事务提交以及事务终止。 这段代码能够保证 Record1 和 Record2 被当作一个事务统一提交到 Kafka，要么它们全部提交成功，要么全部写入失败。 实际上即使写入失败，Kafka 也会把它们写入到底层的日志中，也就是说 Consumer 还是会看到这些消息。因此在 Consumer 端，读取事务型 Producer 发送的消息也是需要一些变更的。修改起来也很简单，设置 isolation.level 参数的值即可。当前这个参数有两个取值： read_uncommitted：这是默认值，表明 Consumer 能够读取到 Kafka 写入的任何消息，不论事务型 Producer 提交事务还是终止事务，其写入的消息都可以读取。 很显然，如果你用了事务型 Producer，那么对应的 Consumer 就不要使用这个值。 read_committed：表明 Consumer 只会读取事务型 Producer 成功提交事务写入的消息。 当然了，它也能看到非事务型 Producer 写入的所有消息。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"4. Go 客户端 Go 客户端 sarama 暂时并没有实现 事务功能。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"幂等性 Producer sarama 中使用幂等性 Producer 设置和 Java 客户端也差不多： config := sarama.NewConfig() config.Producer.Idempotent = true // 1.开启幂等性 config.Producer.RequiredAcks = sarama.WaitForAll // 开启幂等性后 acks 必须设置为 -1 即所有 isr 列表中的 broker 都ack后才ok config.Net.MaxOpenRequests = 1 // 并发请求数也只能为1 // 上述的几个额外配置完全可以由 sarama 内置,或者直接提供一个方法即可，全部需要调用者手动配置感觉体验不是很好 除了开启 Idempotent 之外还必须配置其他几个配置项，具体条件如下： if c.Producer.Idempotent { if !c.Version.IsAtLeast(V0_11_0_0) { return ConfigurationError(\"Idempotent producer requires Version \u003e= V0_11_0_0\") } if c.Producer.Retry.Max == 0 { return ConfigurationError(\"Idempotent producer requires Producer.Retry.Max \u003e= 1\") } if c.Producer.RequiredAcks != WaitForAll { return ConfigurationError(\"Idempotent producer requires Producer.RequiredAcks to be WaitForAll\") } if c.Net.MaxOpenRequests \u003e 1 { return ConfigurationError(\"Idempotent producer requires Net.MaxOpenRequests to be 1\") } } Kafka 版本必须大于 V0_11_0_0。 Retry 次数必须大于0 ，不然发送失败后不能 Retry 就保证不了 Exactly Once 了。 默认值为3，不需要手动设置。 Acks 必须为 -1，即所有 ISR 中的 Broker 都返回才算成功。 最后并发请求数必须为1，不能并发请求。 Producer 完整代码如下： func Producer(topic string, limit int) { config := sarama.NewConfig() config.Producer.Return.Successes = true config.Producer.Return.Errors = true config.Producer.Idempotent = true // 开启幂等性 config.Producer.RequiredAcks = sarama.WaitForAll // 开启幂等性后 acks 必须设置为 -1 即所有 isr 列表中的 broker 都ack后才ok config.Net.MaxOpenRequests = 1 // 开启幂等性后 并发请求数也只能为1 // 上述的几个额外配置完全可以由 sarama 内置,或者直接提供一个方法即可，全部需要调用者手动配置感觉体验不是很好 producer, err := sarama.NewSyncProducer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } defer producer.Close() for i := 0; i \u003c limit; i++ { str := strconv.Itoa(int(time.Now().UnixNano())) msg := \u0026sarama.ProducerMessage{Topic: topic, Key: sarama.StringEncoder(str), Value: sarama.StringEncoder(str)} partition, offset, err := producer.SendMessage(msg) if err != nil { log.Println(\"SendMessage err: \", err) return } log.Printf(\"[Producer] partitionid: %d; offset:%d, value: %s\\n\", partition, offset, str) } } 注意：幂等性 Producer 保证的是同一条消息不会因为 Kafka 的原因发送多次，并不是说会过滤掉重复消息。一直发送相同内容的消息，依旧会正常发送成功。 即：Producer 中的去重并不依赖根据消息内容。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:4:1","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"源码分析 通过源码简单分析一下 samara 中的 幂等性 Producer 是如何实现的。 大致原理：发送消息后会在 Producer 中记录一个字段用于去重。 具体 Producer 逻辑在上文Kafka(Go)教程(六)—sarama 客户端 producer 源码分析有详细介绍，这里也按照这个顺序进行分析。 newAsyncProducer 首先在创建 Producer 的时候会调用newTransactionManager创建一个TransactionManager,这个就是用于保证 幂等性的。 func newAsyncProducer(client Client) (AsyncProducer, error) { txnmgr, err := newTransactionManager(client.Config(), client) if err != nil { return nil, err } p := \u0026asyncProducer{ client: client, conf: client.Config(), errors: make(chan *ProducerError), input: make(chan *ProducerMessage), successes: make(chan *ProducerMessage), retries: make(chan *ProducerMessage), brokers: make(map[*Broker]*brokerProducer), brokerRefs: make(map[*brokerProducer]int), txnmgr: txnmgr, } return p, nil } transactionManager 结构如下： type transactionManager struct { producerID int64 producerEpoch int16 sequenceNumbers map[string]int32 mutex sync.Mutex } 其中 producerID+producerEpoch用于区分 Producer，sequenceNumbers则是用于消息去重。 进入newTransactionManager看下具体实现： func newTransactionManager(conf *Config, client Client) (*transactionManager, error) { txnmgr := \u0026transactionManager{ producerID: noProducerID, producerEpoch: noProducerEpoch, } // 如果开启了 幂等性 特殊处理 if conf.Producer.Idempotent { initProducerIDResponse, err := client.InitProducerID() if err != nil { return nil, err } txnmgr.producerID = initProducerIDResponse.ProducerID txnmgr.producerEpoch = initProducerIDResponse.ProducerEpoch txnmgr.sequenceNumbers = make(map[string]int32) txnmgr.mutex = sync.Mutex{} Logger.Printf(\"Obtained a ProducerId: %d and ProducerEpoch: %d\\n\", txnmgr.producerID, txnmgr.producerEpoch) } return txnmgr, nil } 可以看到 如果开启了 幂等性 则会调用client.InitProducerID()像 Kafka Broker 发送请求，申请一个 ProducerID，用以区分不同 Producer。 具体生成规则由 Kafka Broker端实现。 dispatch func (pp *partitionProducer) dispatch() { // msg.retries == 0 保证了每条消息只会生成一次序号 if pp.parent.conf.Producer.Idempotent \u0026\u0026 msg.retries == 0 \u0026\u0026 msg.flags == 0 { msg.sequenceNumber, msg.producerEpoch = pp.parent.txnmgr.getAndIncrementSequenceNumber(msg.Topic, msg.Partition) msg.hasSequence = true } pp.brokerProducer.input \u003c- msg } } func (t *transactionManager) getAndIncrementSequenceNumber(topic string, partition int32) (int32, int16) { key := fmt.Sprintf(\"%s-%d\", topic, partition) // 序号是以分区为单位的 这也是为什么只能保证指定分区下的 ExactlyOnce t.mutex.Lock() // 加锁保证并发安全 defer t.mutex.Unlock() sequence := t.sequenceNumbers[key] t.sequenceNumbers[key] = sequence + 1 // seq 是递增的 return sequence, t.producerEpoch } fmt.Sprintf 拼接字符串效率特别低，不建议使用。 在把消息交给 brokerProducer 之前给消息分配了一个 sequenceNumber,通过这个序号来保证了同一条消息只会发送一次，以实现 Exactly Once 语义。 add 最后到了 brokerProducer 的 add 方法，堆积消息批量发送。 func (ps *produceSet) add(msg *ProducerMessage) error { var size int set := partitions[msg.Partition] if set == nil { if ps.parent.conf.Version.IsAtLeast(V0_11_0_0) { batch := \u0026RecordBatch{ FirstTimestamp: timestamp, Version: 2, Codec: ps.parent.conf.Producer.Compression, CompressionLevel: ps.parent.conf.Producer.CompressionLevel, ProducerID: ps.producerID, ProducerEpoch: ps.producerEpoch, } // 如果是幂等性 Producer 则在这里把消息的 seq 赋值给 batch.FirstSequence // 由于只有 set == nil 时才会执行该逻辑 所以 batch.FirstSequence 只会被第一条消息赋值 if ps.parent.conf.Producer.Idempotent { batch.FirstSequence = msg.sequenceNumber } set = \u0026partitionSet{recordsToSend: newDefaultRecords(batch)} size = recordBatchOverhead } else { set = \u0026partitionSet{recordsToSend: newLegacyRecords(new(MessageSet))} } partitions[msg.Partition] = set } if ps.parent.conf.Version.IsAtLeast(V0_11_0_0) { // 最后判断当前消息的 sequenceNumber 是否小于 batch 的 FirstSequence 这里的 set.recordsToSend.RecordBatch 就是上面的 batch // 所以正常情况下 msg.sequenceNumber 应该是大于等于 FirstSequence 的 if ps.parent.conf.Producer.Idempotent \u0026\u0026 msg.sequenceNumber \u003c set.recordsToSend.RecordBatch.FirstSequence { return errors.New(\"assertion failed: message out of sequence added to a batch\") } } return nil } 在初始化 Set 时将当前 msg 的 seq 赋值给了 Set，而 Seq 是单调递增的，所以后续 msg 的 seq 应该都大于 Set 的 FirstSequence。 小结 Producer 逻辑基本分析完了，和前面分析的一样，Producer 需要做的只有两件事： 1）初始化时像向 Broker 申请一个 ProducerID 2）为每条消息绑定一","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:4:2","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"5. 小结 Kakfa 相关代码见 Github 幂等性 Producer 和事务型 Producer 都是 Kafka 社区力图为 Kafka 实现精确一次处理语义所提供的工具，只是它们的作用范围是不同的。 幂等性 Producer 只能保证单分区、单会话上的消息幂等性； 而事务能够保证跨分区、跨会话间的幂等性。 从交付语义上来看，自然是事务型 Producer 能做的更多。天下没有免费的午餐。比起幂等性 Producer，事务型 Producer 的性能要更差，在实际使用过程中，我们需要仔细评估引入事务的开销，切不可无脑地启用事务。 最后还是建议实际使用时在 Consumer 端也要进行去重，防止重复消费，这样比较稳妥。 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"6. 参考 https://github.com/Shopify/sarama https://www.cnblogs.com/smartloli/p/11922639.html 《Kafka 核心技术与实战》 ","date":"2021-09-03","objectID":"/posts/kafka/10-exactly-once-impl/:6:0","tags":["Kafka"],"title":"Kafka(Go)教程(十)---Kafka 是如何实现精确一次（exactly once）语义的？","uri":"/posts/kafka/10-exactly-once-impl/"},{"categories":["Kafka"],"content":"Kafka 如何避免消息丢失","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"本文主要从 Producer、Broker、Consumer 等 3 个方面分析了 Kafka 应该如何配置才能避免消息丢失。 Kakfa 相关代码见 Github ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"1. 概述 在使用 MQ 的时候最大的问题就是消息丢失，常见的丢失情况如下： 1）Producer 端丢失 2）Broker 端丢失 3）Consumer 端丢失 一条消息从生产到消费一共要经过以下 3 个流程： 1）Producer 发送到 Broker 2）Broker 保存消息(持久化) 3）Consumer 消费消息 3 个步骤分别对应了上述的 3 种消息丢失场景。 接下来以 Kafka 为例分析该如何避免这些问题。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"2. Kafka 消息持久化保障 一句话概括，Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。 其他 MQ 也类似。 第一个核心要素是已提交的消息。 什么是已提交的消息？当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为“已提交”消息了。 那为什么是若干个 Broker 呢？这取决于你对“已提交”的定义。你可以选择只要有一个 Broker 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。 第二个核心要素就是有限度的持久化保证。 也就是说 Kafka 不可能保证在任何情况下都做到不丢失消息。 举个极端点的例子，如果地球都不存在了，Kafka 还能保存任何消息吗？显然不能！ 有限度其实就是说 Kafka 不丢消息是有前提条件的。假如你的消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"3. 具体场景分析 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"3.1 Producer 端丢失 Producer 端丢消息更多是因为消息根本没有提交到 Kafka。 目前 Kafka Producer 是异步发送消息的，也就是说如果你调用的是 producer.send(msg) 这个 API，那么它通常会立即返回，但此时你不能认为消息发送已成功完成。 这种发送方式有个有趣的名字，叫“fire and forget”，翻译一下就是“发射后不管”。如果出现消息丢失，我们是无法知晓的。这个发送方式挺不靠谱,非常不建议使用。 导致消息没有发送成功的因素也有很多： 1）例如网络抖动，导致消息压根就没有发送到 Broker 端； 2）或者消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等。 Kafka 不认为消息是已提交的，因此也就没有 Kafka 丢失消息这一说了。 解决方案也很简单：Producer 永远要使用带有回调通知的发送 API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。 通过回调，一旦出现消息提交失败的情况，你就可以有针对性地进行处理。 举例来说： 如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了； 如果是消息不合格造成的，那么可以调整消息格式后再次发送。 总之，处理发送失败的责任在 Producer 端而非 Broker 端。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"3.2 Broker 端丢失 Broker 丢失消息是由 Kafka 自身原因造成的。Kafka 为了提高吞吐量和性能，采用异步批量的刷盘策略，也就是按照一定的消息量和间隔时间进行刷盘。 Broker 端丢失消息才真的是因为 Kafka 造成的。 Kafka 收到消息后会先存储在也缓存中(Page Cache)中，之后由操作系统根据自己的策略进行刷盘或者通过 fsync 命令强制刷盘。如果系统挂掉，在 PageCache 中的数据就会丢失。 Kafka没有提供同步刷盘的方式，也就是说单个 Broker 丢失消息是必定会出现的。 为了解决单个 broker 数据丢失问题，Kafka 通过 producer 和 broker 协同处理单个 broker 丢失参数的情况： acks=0，producer 不等待 broker 的响应，效率最高，但是消息很可能会丢。 acks=1，leader broker 收到消息后，不等待其他 follower 的响应，即返回 ack。也可以理解为 ack 数为1。 此时，如果 follower 还没有收到 leader 同步的消息 leader 就挂了，那么消息会丢失。 acks=-1(-1等效于all)，leader broker 收到消息后，挂起，等待所有 ISR 列表中的 follower 返回结果后，再返回 ack。 这种配置下，如果 Leader 刚收到消息就断电，producer 可以知道消息没有被发送成功，将会重新发送。 如果在 follower 收到数据以后，成功返回 ack，leader 断电，数据将存在于原来的 follower 中。在重新选举以后，新的leader 会持有该部分数据。 在配置为 all 或者 -1 的时候，只要 Producer 收到 Broker 的响应就可以理解为消息已经持久化了。 虽然可能只是刚写入了 PageCache，但是刷盘也就是迟早的事，除非刚好刷盘之前多个 Broker 同时挂了，那确实是没办法了。 建议根据实际情况设置： 如果要严格保证消息不丢失，请设置为 all 或 -1； 如果允许存在丢失，建议设置为 1； 一般不建议设为 0，除非无所谓消息丢不丢失。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"3.3 Consumer 端丢失 Consumer 端丢失数据主要体现在 Consumer 端要消费的消息不见了。 出现该情况的唯一原因就是：Consumer 没有正确消费消息，就把位移提交了，导致 Kafka 认为该消息已经被消费了，从而导致消息丢失。 可以看出这其实也不是 Kafka 的问题，毕竟 Kafka 也不知道究竟消费没有，只能以 Consumer 提交的位移为依据。 场景1：获取到消息后直接提交位移了，然后再处理消息。 这样在提交位移后，处理完消息前，如果程序挂掉，这部分消息就算是丢失了。 场景2：多线程并发消费消息，且开启了自动提交，导致消费完成之前程序就自动提交了位移，如果程序挂掉也会出现消息丢失。 解决方案也很简单：确定消费完成后才提交消息，如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:3:3","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"4. 最佳实践 以下为一些常见的 Kafka 无消息丢失的配置： 避免 Producer 端丢失 1）不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。 2）设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries \u003e 0 的 Producer 能够自动重试消息发送，避免消息丢失。 避免 Broker 端丢失 3）设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。 4）设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。 5）设置 replication.factor \u003e= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。 6）设置 min.insync.replicas \u003e 1。这依然是 Broker 端参数，控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。 7）确保 replication.factor \u003e min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。 避免 Consumer 端丢失 8）确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"5. 小结 Kakfa 相关代码见 Github 消息生命周期中的 3 个地方都可能会出现消息丢失情况： 1）Producer 端：通过回调确保消息成功发送到 Kafka 了 2）Broker 端：通过多 Broker 以及 Producer 端设置 acks=all 降低消息丢失概率 3）Consumer 端：一定要在消息处理完成后再提交位移 需要应用程序和 Kafka 一起配合才能保证消息不丢失。 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"6. 参考 https://kafka.apache.org/documentation/#configuration 《Kafka 核心技术与实战》 ","date":"2021-08-27","objectID":"/posts/kafka/09-avoid-msg-lost/:6:0","tags":["Kafka"],"title":"Kafka(Go)教程(九)---如何避免消息丢失?","uri":"/posts/kafka/09-avoid-msg-lost/"},{"categories":["Kafka"],"content":"Kafka 生产者压缩算法及 Go 客户端 sarama 实战和源码分析","date":"2021-08-21","objectID":"/posts/kafka/08-compression/","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"本文主要分析了 Kafka 生产者压缩与解压逻辑以及注事事项点，最终通过 Go 客户端 sarama 演示了 压缩算法的具体使用，并通过源码分析了 Go 客户端 sarama 的具体实现。 Kakfa 相关代码见 Github ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"1. 概述 压缩（compression）它秉承了用时间去换空间的经典 trade-off 思想，具体来说就是用 CPU 时间去换磁盘空间或网络 I/O 传输量，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"2. 具体流程 一句话概括为：Producer 端压缩、Broker 端保持、Consumer 端解压缩。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"2.1 压缩 在 Kafka 中，压缩可能发生在两个地方：Producer 端和 Broker 端。 Producer 程序中配置 compression.type 参数即表示启用指定类型的压缩算法。比如下面这段程序代码展示了如何构建一个开启 GZIP 的 Producer 对象： Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"acks\", \"all\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 开启GZIP压缩 props.put(\"compression.type\", \"gzip\"); Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); 这里比较关键的代码行是 props.put(“compression.type”, “gzip”)，它表明该 Producer 的压缩算法使用的是 GZIP。这样 Producer 启动后生产的每个消息集合都是经 GZIP 压缩过的，故而能很好地节省网络传输带宽以及 Kafka Broker 端的磁盘占用。 除了在 Producer 端进行压缩外有两种例外情况就可能让 Broker 重新压缩消息： 1）Broker 端指定了和 Producer 端不同的压缩算法。 比如 Producer 使用 GZIP，而 Broker 只能接收使用 Snappy 算法压缩的消息。这种情况下 Broker 接收到 GZIP 压缩消息后，只能解压缩然后使用 Snappy 重新压缩一遍。 Broker 端也有一个参数叫 compression.type ，默认情况下为 Producer，表示 Broker 端会“尊重” Producer 端使用的压缩算法。 如果设置了不同的 compression.type 值，可能会发生预料之外的压缩 / 解压缩操作，通常表现为 Broker 端 CPU 使用率飙升。 2）Broker 端发生了消息格式转换。 Kafka 的消息层次分为两层：消息集合（message set）以及消息（message）。一个消息集合中包含若干条日志项（record item），而日志项才是真正封装消息的地方。Kafka 底层的消息日志由一系列消息集合日志项组成。Kafka 通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。 目前 Kafka 共有两大类消息格式，社区分别称之为 V1 版本和 V2 版本。V2 版本是 Kafka 0.11.0.0 中正式引入的。 V2 版本优化点： 1）在 V1 版本中，每条消息都需要执行 CRC 校验，比较浪费 CPU ，在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层。 2）V1 版本中保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而 V2 版本的做法是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。 在一个生产环境中，Kafka 集群中同时保存多种版本的消息格式非常常见。为了兼容老版本的格式，Broker 端会对新版本消息执行向老版本格式的转换。这个过程中会涉及消息的解压缩和重新压缩。一般情况下这种消息格式转换对性能是有很大影响的，除了这里的压缩之外，它还让 Kafka 丧失了引以为豪的 Zero Copy 特性。 Zero Copy 的前提就是应用程序不需要对数据进行处理。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"2.2 解压 通常来说解压缩发生在 Consumer 程序中，也就是说 Producer 发送压缩消息到 Broker 后，Broker 照单全收并原样保存起来。当 Consumer 程序请求这部分消息时，Broker 依然原样发送出去，当消息到达 Consumer 端后，由 Consumer 自行解压缩还原成之前的消息。 为了让 Consumer 知道使用哪种压缩算法，Kafka 会将启用了哪种压缩算法封装进消息集合中，这样当 Consumer 读取到消息集合时，它自然就知道了这些消息使用的是哪种压缩算法。 Broker 端也会进行解压缩。每个压缩过的消息集合在 Broker 端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。而且这种校验是非常重要的，也不能直接去掉。 这种解压缩对 Broker 端性能是有一定影响的，特别是对 CPU 的使用率而言。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"3. 压缩算法 先比较一下各个压缩算法的优劣，这样我们才能有针对性地配置适合我们业务的压缩策略。 看一个压缩算法的优劣，有两个重要的指标：一个指标是压缩比，另一个指标就是压缩 / 解压缩吞吐量。 下面这张表是 Facebook Zstandard提供的一份压缩算法 benchmark 比较结果： Compressor name Ratio Compression Decompress. zstd 1.4.5 -1 2.884 500 MB/s 1660 MB/s zlib 1.2.11 -1 2.743 90 MB/s 400 MB/s brotli 1.0.7 -0 2.703 400 MB/s 450 MB/s zstd 1.4.5 –fast=1 2.434 570 MB/s 2200 MB/s zstd 1.4.5 –fast=3 2.312 640 MB/s 2300 MB/s quicklz 1.5.0 -1 2.238 560 MB/s 710 MB/s zstd 1.4.5 –fast=5 2.178 700 MB/s 2420 MB/s lzo1x 2.10 -1 2.106 690 MB/s 820 MB/s lz4 1.9.2 2.101 740 MB/s 4530 MB/s zstd 1.4.5 –fast=7 2.096 750 MB/s 2480 MB/s lzf 3.6 -1 2.077 410 MB/s 860 MB/s snappy 1.1.8 2.073 560 MB/s 1790 MB/s 从表中我们可以发现 zstd 算法有着最高的压缩比，而在吞吐量上的表现只能说中规中矩。反观 LZ4 算法，它在吞吐量方面则是毫无疑问的执牛耳者。 在实际使用中，GZIP、Snappy、LZ4 甚至是 zstd 的表现各有千秋。但对于 Kafka 而言，它们的性能测试结果却出奇得一致，即： 在吞吐量方面：LZ4 \u003e Snappy \u003e zstd 和 GZIP； 而在压缩比方面，zstd \u003e LZ4 \u003e GZIP \u003e Snappy。 具体到物理资源，使用 Snappy 算法占用的网络带宽最多，zstd 最少，这是合理的，毕竟 zstd 就是要提供超高的压缩比； 在 CPU 使用率方面，各个算法表现得差不多，只是在压缩时 Snappy 算法使用的 CPU 较多一些，而在解压缩时 GZIP 算法则可能使用更多的 CPU。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"4. Go 压缩实战 不同语言客户端的 解压和压缩可能会有差异，但是大致流程应该不会变化。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"4.1 Demo Go 里面使用压缩也是在 Producer 端通过 config 进行配置,相关配置如下： type Config struct{ Producer struct { Compression CompressionCodec CompressionLevel int } } type CompressionCodec int8 通过Config.Producer.Compression指定压缩算法，通过Config.Producer.CompressionLevel指定压缩等级。 压缩算法是通过枚举值指定的，内置算法列表如下： const ( // CompressionNone no compression CompressionNone CompressionCodec = iota // CompressionGZIP compression using GZIP CompressionGZIP // CompressionSnappy compression using snappy CompressionSnappy // CompressionLZ4 compression using LZ4 CompressionLZ4 // CompressionZSTD compression using ZSTD CompressionZSTD ) 具体使用哪种算法还要根据 Kafka 版本决定，不同 Kafka 版本的这些算法的支持不一样。 完整代码如下： var defaultMsg = strings.Repeat(\"Golang\", 1000) func Producer(topic string, limit int) { config := sarama.NewConfig() // config.Producer.Compression = sarama.CompressionGZIP // config.Producer.CompressionLevel = gzip.BestCompression config.Producer.Return.Successes = true config.Producer.Return.Errors = true // 这个默认值就是 true 可以不用手动 赋值 producer, err := sarama.NewSyncProducer([]string{kafka.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } defer producer.Close() for i := 0; i \u003c limit; i++ { msg := \u0026sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(defaultMsg)} partition, offset, err := producer.SendMessage(msg) if err != nil { log.Println(\"SendMessage err: \", err) return } log.Printf(\"[Producer] partitionid: %d; offset:%d\\n\", partition, offset) } } 发送了 1000 条消息测试一下压缩效果： 每条消息为 6000 byte，1000 条差不多 6M 压缩前： I have no name!@cd04519bf1df:/bitnami/kafka/data/compression-0$ ls -lhS total 6.0M -rw-r--r-- 1 1001 root 10M Aug 22 02:32 00000000000000000000.index -rw-r--r-- 1 1001 root 10M Aug 22 02:32 00000000000000000000.timeindex -rw-r--r-- 1 1001 root 6.0M Aug 22 02:32 00000000000000000000.log -rw-r--r-- 1 1001 root 8 Aug 22 02:28 leader-epoch-checkpoint 果然是 6 M 和预期一致。 压缩后： I have no name!@cd04519bf1df:/bitnami/kafka/data/compression-0$ ls -lhS total 140K -rw-r--r-- 1 1001 root 10M Aug 22 02:34 00000000000000000000.index -rw-r--r-- 1 1001 root 10M Aug 22 02:34 00000000000000000000.timeindex -rw-r--r-- 1 1001 root 125K Aug 22 02:34 00000000000000000000.log -rw-r--r-- 1 1001 root 8 Aug 22 02:33 leader-epoch-checkpoint 125 K，说明压缩是生效的。 这么高的压缩比是因为发送的内容全是重复字符串导致的，使用真实数据应该没这么高压缩比。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:4:1","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"4.2 源码分析 在之前文章Kafka(Go)教程(六)—sarama 客户端 producer 源码分析 中分析了 Producer 的具体流程，其中消息会经过 TopicProducer、PartitionProducer 最终通过 BrokerProudcer 到达 Kafka。 由于压缩是发生在 Producer 的，所以肯定是在最终通过 BrokerProudcer 到达 Kafka 之前就需要进行压缩，有了大致方向于是开始翻源码。 最终在 BrokerProudcer 的 buildRequest 方法中找到了答案： 为了便于阅读，省略了无关代码 buildRequest 在发生到 Kafka 之前对消息进行一次格式化。 // produce_set.go 132 行 func (ps *produceSet) buildRequest() *ProduceRequest { req := \u0026ProduceRequest{ RequiredAcks: ps.parent.conf.Producer.RequiredAcks, Timeout: int32(ps.parent.conf.Producer.Timeout / time.Millisecond), } for topic, partitionSets := range ps.msgs { for partition, set := range partitionSets { if ps.parent.conf.Producer.Compression == CompressionNone { req.AddSet(topic, partition, set.recordsToSend.MsgSet) } else { payload, err := encode(set.recordsToSend.MsgSet, ps.parent.conf.MetricRegistry) if err != nil { Logger.Println(err) // if this happens, it's basically our fault. panic(err) } compMsg := \u0026Message{ Codec: ps.parent.conf.Producer.Compression, CompressionLevel: ps.parent.conf.Producer.CompressionLevel, Key: nil, Value: payload, Set: set.recordsToSend.MsgSet, // Provide the underlying message set for accurate metrics } } } } return req } 核心逻辑如下： if ps.parent.conf.Producer.Compression == CompressionNone { req.AddSet(topic, partition, set.recordsToSend.MsgSet) } else { payload, err := encode(set.recordsToSend.MsgSet, ps.parent.conf.MetricRegistry) } 前面提到过是按照 消息集合 为单位进行压缩的，这里的 MsgSet 就是那个消息集合。 然后为了 Consumer 能知道如何解压，把压缩算法和压缩等级信息附加在了消息上： compMsg := \u0026Message{ Codec: ps.parent.conf.Producer.Compression, CompressionLevel: ps.parent.conf.Producer.CompressionLevel, Key: nil, Value: payload, Set: set.recordsToSend.MsgSet, // Provide the underlying message set for accurate metrics } 如果不需要压缩则直接添加到 Set，如果否则就调用encode方法，合理猜测，压缩逻辑就在 encode 里面，继续追踪下去： encode // encoder_decoder.go 21 行 func encode(e encoder, metricRegistry metrics.Registry) ([]byte, error) { var prepEnc prepEncoder var realEnc realEncoder err := e.encode(\u0026prepEnc) if err != nil { return nil, err } if prepEnc.length \u003c 0 || prepEnc.length \u003e int(MaxRequestSize) { return nil, PacketEncodingError{fmt.Sprintf(\"invalid request size (%d)\", prepEnc.length)} } realEnc.raw = make([]byte, prepEnc.length) realEnc.registry = metricRegistry err = e.encode(\u0026realEnc) if err != nil { return nil, err } return realEnc.raw, nil } 可以看到这里面有prepEncoder和realEncoder 两个 encoder，其中 prepEncoder 只是走了一遍 encode 流程并没有真执行 encode： type prepEncoder struct { stack []pushEncoder length int } // primitives func (pe *prepEncoder) putInt8(in int8) { pe.length++ } func (pe *prepEncoder) putInt16(in int16) { pe.length += 2 } func (pe *prepEncoder) putInt32(in int32) { pe.length += 4 } func (pe *prepEncoder) putInt64(in int64) { pe.length += 8 } 主要是为了获取 length 进行下面这个校验： if prepEnc.length \u003c 0 || prepEnc.length \u003e int(MaxRequestSize) { return nil, PacketEncodingError{fmt.Sprintf(\"invalid request size (%d)\", prepEnc.length)} } 所以真正逻辑肯定在 realEncoder 里，继续跟进。 最后发现全是 interface 不是很好追踪，对于这种情况就需要面向实现跟进，比如之前是 MsgSet 调用的 encode 方法，这里就直接找到 MsgSet 的相关实现。 type MessageSet struct { PartialTrailingMessage bool // whether the set on the wire contained an incomplete trailing MessageBlock OverflowMessage bool // whether the set on the wire contained an overflow message Messages []*MessageBlock } func (ms *MessageSet) encode(pe packetEncoder) error { for i := range ms.Messages { err := ms.Messages[i].encode(pe) if err != nil { return err } } return nil } 又调用了ms.Messages[i].encode(pe)，Messages 切片里存的是 MessageBlock，所以继续跟进 MessageBlock： type MessageBlock struct { Offset int64 Msg *Message } func (msb *MessageBlock) encode(pe packetEncoder) error { pe.putInt64(msb.Offset) pe.push(\u0026lengthField{}) err := msb.Msg.encode(pe) if err != nil { return err } return pe.pop() } 又调用了msb.Msg.encode(pe)，继续跟进 Message： func (m *Message) encode(pe packetEncoder) error { var payload []byte payload, err = compress(m.Codec, m.CompressionLevel, m.Value) if err != nil { return err } m.compressedCache = payload // Keep in mind the co","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:4:2","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"4.3 小结 sarama 实现和官方逻辑大体一致： 1）producer 中配置压缩算法和压缩等级 2）发送消息前按照 MsgSet 为单位进行压缩，并将 压缩算法和压缩等级 附加到消息中便于 consumer 解压。 3）consumer 根据消息中附加的 压缩算法和压缩等级进行解压 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:4:3","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"5. 小结 Kakfa 相关代码见 Github 具体流程： Producer 端压缩、Broker 端保持、Consumer 端解压缩。 特殊情况： 1）Broker 端指定了和 Producer 端不同的压缩算法。 2）Broker 端发生了消息格式转换。 以上两种情况 Broker 端都需要解压后在重新压缩。除此之外 Broker 端为了进行消息校验，也需要进行一次解压。 压缩算法： 在吞吐量方面：LZ4 \u003e Snappy \u003e zstd 和 GZIP； 而在压缩比方面，zstd \u003e LZ4 \u003e GZIP \u003e Snappy。 使用建议： 由于压缩\u0026解压需要消耗 CPU，所以建议在以下情况才开启： 1）Producer 、Consumer CPU 充足 2）带宽资源有限 如果 CPU 资源有很多富余，而带宽受限，这种情况强烈建议开启 zstd 压缩，这样能极大地节省网络资源消耗。 Go 优化： 使用 sync.pool 复用对象，避免创建大量临时对象。 ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"6. 参考 https://github.com/Shopify/sarama 《Kafka 核心技术与实战》 https://github.com/facebook/zstd/ ","date":"2021-08-21","objectID":"/posts/kafka/08-compression/:6:0","tags":["Kafka"],"title":"Kafka(Go)教程(八)---生产者压缩算法详解及源码分析","uri":"/posts/kafka/08-compression/"},{"categories":["Kafka"],"content":"Kafka 生产者分区机制原理剖析及 Go 客户端 sarama 实战","date":"2021-08-20","objectID":"/posts/kafka/07-partition/","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"本文主要分析了 Kafka 消息分区（Partition）机制的原理，包括常见分区策略以及自定义分区策略。最后对 Go 客户端 Sarama 具体实现进行了演示和分析。 Kakfa 相关代码见 Github ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 有主题（Topic）的概念，它是承载真实数据的逻辑容器，而在主题之下还分为若干个分区，也就是说 Kafka 的消息组织方式实际上是三级结构：主题 - 分区 - 消息。 主题下的每条消息只会保存在某一个分区中，而不会在多个分区中被保存多份。官网上的这张图非常清晰地展示了 Kafka 的三级结构，如下所示： 你觉得为什么 Kafka 要做这样的设计？为什么使用分区的概念而不是直接使用多个主题呢？ 其实分区的作用就是提供负载均衡的能力，或者说对数据进行分区的主要原因，就是为了实现系统的高伸缩性（Scalability）。 不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的，这样每个节点的机器都能独立地执行各自分区的读写请求处理。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。 不同的分布式系统对分区的叫法也不尽相同。比如在 Kafka 中叫分区，在 MongoDB 和 Elasticsearch 中就叫分片 Shard，而在 HBase 中则叫 Region，在 Cassandra 中又被称作 vnode。 从表面看起来它们实现原理可能不尽相同，但对底层分区（Partitioning）的整体思想却从未改变。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"2. 分区策略 所谓分区策略是决定生产者将消息发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"自定义分区策略 如果要自定义分区策略，你需要显式地配置生产者端的参数 partitioner.class。 这个参数该怎么设定呢？方法很简单，在编写生产者程序时，你可以编写一个具体的类实现 org.apache.kafka.clients.producer.Partitioner 接口。这个接口也很简单，只定义了两个方法：partition() 和 close()，通常你只需要实现最重要的 partition 方法。我们来看看这个方法的方法签名： int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); 这里的 topic、key、keyBytes、value和valueBytes都属于消息数据，cluster则是集群信息（比如当前 Kafka 集群共有多少主题、多少 Broker 等）。 Kafka 给你这么多信息，就是希望让你能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。 只要你自己的实现类定义好了 partition 方法，同时设置 partitioner.class 参数为你自己实现类的 Full Qualified Name，那么生产者程序就会按照你的代码逻辑对消息进行分区。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"轮询策略 也称 Round-robin 策略，即顺序分配。 比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。 轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"随机策略 也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。 从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"按消息键保序策略 也称 Key-ordering 策略。 Kafka 允许为每条消息定义消息键，简称为 Key。这个 Key 的作用非常大，它可以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等；也可以用来表征消息元数据。 特别是在 Kafka 不支持时间戳的年代，在一些场景中，工程师们都是直接将消息创建时间封装进 Key 里面的。一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，如下图所示。 实现这个策略的 partition 方法同样简单，只需要下面两行代码即可： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return Math.abs(key.hashCode()) % partitions.size(); 前面提到的 Kafka 默认分区策略实际上同时实现了两种策略：如果指定了 Key，那么默认实现按消息键保序策略；如果没有指定 Key，则使用轮询策略。 注：Kafka 是不能保证全局消息顺序的，只能保证单个 Partition 下的顺序，所以在需要保证顺序的场景可以使用 Key-Ordering 策略将同一个用户的消息发送到同一分区，即可保证顺序。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:2:4","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"3. Go 自定义分区策略 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"1. Demo 前面讲的自定义分区策略是按照 Java客户端的逻辑，不同语言 客户端 可能实现上有所不同，这里用 Go Client sarama 写个 Demo 展示一下： 大致逻辑相同，只是具体实现不同 首先还是创建生产者时通过参数配置具体的分区策略 type Config struct { Producer struct { Partitioner PartitionerConstructor } } 可以看到 Config.Producer 里有一个 Partitioner 的参数，这就是分区策略配置项。 类型为 PartitionerConstructor，分区构造器，具体如下： type PartitionerConstructor func(topic string) Partitioner 这是一个 构造方法，该方法返回的 Partitioner 才是正在的 分区器。 type Partitioner interface { Partition(message *ProducerMessage, numPartitions int32) (int32, error) RequiresConsistency() bool } 是一个 接口类型，所以要定义自定义分区策略只需要实现该接口即可。 我们先实现一个自定义的 Partitioner type myPartitioner struct { partition int32 } // Partition 返回的是分区的位置或者索引，并不是具体的分区号。比如有十个分区[0,1，2,3...9] 这里返回 0 表示取数组中的第0个位置的分区。在 Go 客户端中是这样实现的，具体见下文源码分析 func (p *myPartitioner) Partition(message *sarama.ProducerMessage, numPartitions int32) (int32, error) { if p.partition \u003e= numPartitions { p.partition = 0 } ret := p.partition p.partition++ return ret, nil } // 该方法的作用在下文源码分析中有详细解释 func (p *myPartitioner) RequiresConsistency() bool { return false } 然后在实现一个构造方法即可 func NewMyPartitioner(topic string) sarama.Partitioner { return \u0026myPartitioner{} } 最后构造生产者时指定自定义的 分区策略 config := sarama.NewConfig() config.Producer.Partitioner = NewMyPartitioner // 这个就是我们自定义 Partitioner 的构造方法 这样就完成了自定义分区策略。 运行起来看一下效果： 2021/08/21 08:39:47 [Producer] partitionid: 0; offset:0, value: 1629506387755300000 2021/08/21 08:39:47 [Producer] partitionid: 1; offset:0, value: 1629506387969360200 2021/08/21 08:39:48 [Producer] partitionid: 0; offset:1, value: 1629506387969760600 2021/08/21 08:39:48 [Producer] partitionid: 1; offset:1, value: 1629506388011270100 可以看到确实是轮询的在往两个分区里发送。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"2. 源码分析 在上一篇文章Kafka(Go)教程(六)—sarama 客户端 producer 源码分析 中分析了 Producer 的具体流程，其中消息会经过 TopicProducer、PartitionProducer 最终通过 BrokerProudcer 到达 Kafka。 那么分区策略肯定是在到 PartitionProducer 之前执行了，于是找到对应源码： func (tp *topicProducer) dispatch() { for msg := range tp.input { if msg.retries == 0 { if err := tp.partitionMessage(msg); err != nil { tp.parent.returnError(msg, err) continue } } handler := tp.handlers[msg.Partition] if handler == nil { handler = tp.parent.newPartitionProducer(msg.Topic, msg.Partition) tp.handlers[msg.Partition] = handler } handler \u003c- msg } } 可以看到,tp.partitionMessage(msg)这里就是在对消息进行分区处理： func (tp *topicProducer) partitionMessage(msg *ProducerMessage) error { var partitions []int32 // 1.首先找到对应 Broker 的所有 partition err := tp.breaker.Run(func() (err error) { requiresConsistency := false if ep, ok := tp.partitioner.(DynamicConsistencyPartitioner); ok { requiresConsistency = ep.MessageRequiresConsistency(msg) } else { requiresConsistency = tp.partitioner.RequiresConsistency() } // 如果是指定了需要一致性就调用就直接查询该Broker对应的所有分区(即使该分区当前不可用) if requiresConsistency { partitions, err = tp.parent.client. Partitions(msg.Topic) } else { // 没有指定一致性则只会往当前可用的分区里发 partitions, err = tp.parent.client.WritablePartitions(msg.Topic) } return }) if err != nil { return err } numPartitions := int32(len(partitions)) // 3.最后调用配置的 partitioner 的 Partition 方法来确定分区 choice, err := tp.partitioner.Partition(msg, numPartitions) msg.Partition = partitions[choice] return nil } requiresConsistency 具体含义如下： // RequiresConsistency indicates to the user of the partitioner whether the // mapping of key-\u003epartition is consistent or not. Specifically, if a // partitioner requires consistency then it must be allowed to choose from all // partitions (even ones known to be unavailable), and its choice must be // respected by the caller. The obvious example is the HashPartitioner. 即：key-\u003epartition 的映射是否需要一致,如果强制指定需要一致,那么就算这个分区不可用了也会把消息发给该分区以保证一致性,未指定则只会把消息投递给可用分区.当使用 Key-Ordering 策略的时候需要设置为 true 才能保证同一个 Key 被投递到同一个分区. 然后分区的选择逻辑： choice, err := tp.partitioner.Partition(msg, numPartitions) msg.Partition = partitions[choice] 即 Partition 方法返回的只是一个 index，并不是具体的分区号。 到此分区逻辑就结束了，具体消息分发逻辑可以看上一篇文章Kafka(Go)教程(六)—sarama 客户端 producer 源码分析。 Kakfa 相关代码见 Github ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"4. 小结 Kafka的消息组织方式实际上是三级结构:主题-分区-消息。主题下的每条消息只会保存在某一一个分区中，而不会在多个分区中被保存多份。 分区是实现负载均衡以及高吞吐量的关键。 所谓分区策略，也就是决定生产者将消息发送到哪个分区的算法。Kafka为我们提供了默认分区策略，同时，它也支持你自定义分区策略。 比较常见的分区策略包括轮询策略、随机策略和按消息键保序策略。还有一种是基于地理位置的分区策略，但这种策略一-般只针对那些 大规模的Kafka集群，特别是跨城市、跨国家甚至是跨大洲的集群。 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"5. 参考 https://kafka.apache.org/documentation/#configuration 《Kafka 核心技术与实战》 ","date":"2021-08-20","objectID":"/posts/kafka/07-partition/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(七)---生产者分区机制原理剖析及实战","uri":"/posts/kafka/07-partition/"},{"categories":["Kafka"],"content":"Kafka Go sarama 客户端异步生产者源码分析","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"本文主要通过源码分析了 Kafka Go sarama 客户端生产者的实现原理，包括消息分发流程，消息打包处理，以及最终发送到 Kafka 等具体步骤，最后通过分析总结出的常见性能优化手段。 本文基于 sarama v1.29.1 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 系列相关代码见 Github 具体流程如下图： Sarama有两种类型的生产者，同步生产者和异步生产者。 To produce messages, use either the AsyncProducer or the SyncProducer. The AsyncProducer accepts messages on a channel and produces them asynchronously in the background as efficiently as possible; it is preferred in most cases. The SyncProducer provides a method which will block until Kafka acknowledges the message as produced. This can be useful but comes with two caveats: it will generally be less efficient, and the actual durability guarantees depend on the configured value of Producer.RequiredAcks. There are configurations where a message acknowledged by the SyncProducer can still sometimes be lost. 大致意思是异步生产者使用channel接收（生产成功或失败）的消息，并且也通过channel来发送消息，这样做通常是性能最高的。而同步生产者需要阻塞，直到收到了acks。但是这也带来了两个问题，一是性能变得更差了，而是可靠性是依靠参数acks来保证的。 异步生产者 Demo 如下： func Producer(topic string, limit int) { config := sarama.NewConfig() // 异步生产者不建议把 Errors 和 Successes 都开启，一般开启 Errors 就行 // 同步生产者就必须都开启，因为会同步返回发送成功或者失败 config.Producer.Return.Errors = true // 设定是否需要返回错误信息 config.Producer.Return.Successes = true // 设定是否需要返回成功信息 producer, err := sarama.NewAsyncProducer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } var ( wg sync.WaitGroup enqueued, timeout, successes, errors int ) // [!important] 异步生产者发送后必须把返回值从 Errors 或者 Successes 中读出来 不然会阻塞 sarama 内部处理逻辑 导致只能发出去一条消息 wg.Add(1) go func() { defer wg.Done() for range producer.Successes() { // log.Printf(\"[Producer] Success: key:%v msg:%+v \\n\", s.Key, s.Value) successes++ } }() wg.Add(1) go func() { defer wg.Done() for e := range producer.Errors() { log.Printf(\"[Producer] Errors：err:%v msg:%+v \\n\", e.Msg, e.Err) errors++ } }() // 异步发送 for i := 0; i \u003c limit; i++ { str := strconv.Itoa(int(time.Now().UnixNano())) msg := \u0026sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(str)} // 异步发送只是写入内存了就返回了，并没有真正发送出去 // sarama 库中用的是一个 channel 来接收，后台 goroutine 异步从该 channel 中取出消息并真正发送 // select + ctx 做超时控制,防止阻塞 producer.Input() \u003c- msg 也可能会阻塞 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*10) select { case producer.Input() \u003c- msg: enqueued++ case \u003c-ctx.Done(): timeout++ } cancel() if i%10000 == 0 \u0026\u0026 i != 0 { log.Printf(\"已发送消息数:%d 超时数:%d\\n\", i, timeout) } } // We are done producer.AsyncClose() wg.Wait() log.Printf(\"发送完毕 总发送条数:%d enqueued:%d timeout:%d successes: %d errors: %d\\n\", limit, enqueued, timeout, successes, errors) } 可以看到整个 API 使用起来还是非常简单的： 1）NewAsyncProducer() ：创建 一个 producer 对象 2）producer.Input() \u003c- msg ：发送消息 3）s = \u003c-producer.Successes()，e := \u003c-producer.Errors() ：异步获取成功或失败信息 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"2. 发送流程源码分析 为了便于阅读，省略了部分无关代码。 另外：由于同步生产者和异步生产者逻辑是一致的，只是在异步生产者基础上封装了一层，所以本文主要分析了异步生产者。 // 可以看到 同步生产者其实就是把异步生产者封装了一层 type syncProducer struct { producer *asyncProducer wg sync.WaitGroup } ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"NewAsyncProducer 首先是构建一个异步生产者对象 func NewAsyncProducer(addrs []string, conf *Config) (AsyncProducer, error) { client, err := NewClient(addrs, conf) if err != nil { return nil, err } return newAsyncProducer(client) } func newAsyncProducer(client Client) (AsyncProducer, error) { // ... p := \u0026asyncProducer{ client: client, conf: client.Config(), errors: make(chan *ProducerError), input: make(chan *ProducerMessage), successes: make(chan *ProducerMessage), retries: make(chan *ProducerMessage), brokers: make(map[*Broker]*brokerProducer), brokerRefs: make(map[*brokerProducer]int), txnmgr: txnmgr, } go withRecover(p.dispatcher) go withRecover(p.retryHandler) } 可以看到在 newAsyncProducer 最后开启了两个 goroutine，一个为 dispatcher，一个为 retryHandler 。 retryHandler 主要是处理重试逻辑，暂时先忽略。 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"dispatcher 主要根据 topic 将消息分发到对应的 channel。 func (p *asyncProducer) dispatcher() { handlers := make(map[string]chan\u003c- *ProducerMessage) // ... for msg := range p.input { // 拦截器逻辑 for _, interceptor := range p.conf.Producer.Interceptors { msg.safelyApplyInterceptor(interceptor) } // 找到这个Topic对应的Handler handler := handlers[msg.Topic] if handler == nil { // 如果没有这个Topic对应的Handler，那么创建一个 handler = p.newTopicProducer(msg.Topic) handlers[msg.Topic] = handler } // 然后把这条消息写进这个Handler中 handler \u003c- msg } } 具体逻辑：从 p.input 中取出消息并写入到 handler 中，如果 topic 对应的 handler 不存在，则调用 newTopicProducer() 创建。 这里的 handler 是一个 buffered channel 然后让我们来看下handler = p.newTopicProducer(msg.Topic)这一行的代码。 func (p *asyncProducer) newTopicProducer(topic string) chan\u003c- *ProducerMessage { input := make(chan *ProducerMessage, p.conf.ChannelBufferSize) tp := \u0026topicProducer{ parent: p, topic: topic, input: input, breaker: breaker.New(3, 1, 10*time.Second), handlers: make(map[int32]chan\u003c- *ProducerMessage), partitioner: p.conf.Producer.Partitioner(topic), } go withRecover(tp.dispatch) return input } 在这里创建了一个缓冲大小为ChannelBufferSize的channel，用于存放发送到这个主题的消息，然后创建了一个 topicProducer。 在这个时候你可以认为消息已经交付给各个 topic 对应的 topicProducer 了。 还有一个需要注意的是newTopicProducer 的这种写法，内部创建一个 chan 返回到外层，然后通过在内部新开一个 goroutine 来处理该 chan 里的消息，这种写法在后面还会遇到好几次。 相比之下在外部显示创建 chan 之后传递到该函数可能会更容易理解。 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"topicDispatch newTopicProducer的最后一行go withRecover(tp.dispatch)又启动了一个 goroutine 用于处理消息。也就是说，到了这一步，对于每一个Topic，都有一个协程来处理消息。 dispatch 具体如下： func (tp *topicProducer) dispatch() { for msg := range tp.input { handler := tp.handlers[msg.Partition] if handler == nil { handler = tp.parent.newPartitionProducer(msg.Topic, msg.Partition) tp.handlers[msg.Partition] = handler } handler \u003c- msg } } 可以看到又是同样的套路： 1）找到这条消息所在的分区对应的 channel，然后把消息丢进去 2）如果不存在则新建 chan ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"PartitionDispatch 新建的 chan 是通过 newPartitionProducer 返回的，和之前的newTopicProducer又是同样的套路,点进去看一下： func (p *asyncProducer) newPartitionProducer(topic string, partition int32) chan\u003c- *ProducerMessage { input := make(chan *ProducerMessage, p.conf.ChannelBufferSize) pp := \u0026partitionProducer{ parent: p, topic: topic, partition: partition, input: input, breaker: breaker.New(3, 1, 10*time.Second), retryState: make([]partitionRetryState, p.conf.Producer.Retry.Max+1), } go withRecover(pp.dispatch) return input } 果然是这样，有没有一种似曾相识的感觉。 TopicProducer是按照 Topic 进行分发，这里的 PartitionProducer 则是按照 partition 进行分发。 到这里可以认为消息已经交付给对应 topic 下的对应 partition 了。 每个 partition 都会有一个 goroutine 来处理分发给自己的消息。 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:4","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"PartitionProducer 到了这一步，我们再来看看消息到了每个 partition 所在的 channel 之后，是如何处理的。 其实在这一步中，主要是做一些错误处理之类的，然后把消息丢进brokerProducer。 可以理解为这一步是业务逻辑层到网络IO层的转变，在这之前我们只关心消息去到了哪个分区，而在这之后，我们需要找到这个分区所在的 broker 的地址，并使用之前已经建立好的 TCP 连接，发送这条消息。 具体 pp.dispatch 代码如下 func (pp *partitionProducer) dispatch() { // 找到这个主题和分区的leader所在的broker pp.leader, _ = pp.parent.client.Leader(pp.topic, pp.partition) if pp.leader != nil { // 根据 leader 信息创建一个 BrokerProducer 对象 pp.brokerProducer = pp.parent.getBrokerProducer(pp.leader) pp.parent.inFlight.Add(1) pp.brokerProducer.input \u003c- \u0026ProducerMessage{Topic: pp.topic, Partition: pp.partition, flags: syn} } // 然后把消息丢进brokerProducer中 for msg := range pp.input { pp.brokerProducer.input \u003c- msg } } 根据之前的套路我们知道，真正的逻辑肯定在pp.parent.getBrokerProducer(pp.leader) 这个方法里面。 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:5","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"BrokerProducer 到了这里，大概算是整个发送流程最后的一个步骤了。 让我们继续跟进pp.parent.getBrokerProducer(pp.leader)这行代码里面的内容。其实就是找到asyncProducer中的brokerProducer，如果不存在，则创建一个。 func (p *asyncProducer) getBrokerProducer(broker *Broker) *brokerProducer { p.brokerLock.Lock() defer p.brokerLock.Unlock() bp := p.brokers[broker] if bp == nil { bp = p.newBrokerProducer(broker) p.brokers[broker] = bp p.brokerRefs[bp] = 0 } p.brokerRefs[bp]++ return bp } 又调用了newBrokerProducer()，继续追踪下去： func (p *asyncProducer) newBrokerProducer(broker *Broker) *brokerProducer { var ( input = make(chan *ProducerMessage) bridge = make(chan *produceSet) responses = make(chan *brokerProducerResponse) ) bp := \u0026brokerProducer{ parent: p, broker: broker, input: input, output: bridge, responses: responses, stopchan: make(chan struct{}), buffer: newProduceSet(p), currentRetries: make(map[string]map[int32]error), } go withRecover(bp.run) // minimal bridge to make the network response `select`able go withRecover(func() { for set := range bridge { request := set.buildRequest() response, err := broker.Produce(request) responses \u003c- \u0026brokerProducerResponse{ set: set, err: err, res: response, } } close(responses) }) if p.conf.Producer.Retry.Max \u003c= 0 { bp.abandoned = make(chan struct{}) } return bp } 这里又启动了两个 goroutine，一个为 run，一个是匿名函数姑且称为 bridge。 bridge 看起来是真正的发送逻辑，那么 batch handle 逻辑应该是在 run 方法里了。 这里先分析 bridge 函数，run 在下一章分析。 buildRequest buildRequest 方法主要是构建一个标准的 Kafka Request 消息。 根据不同版本、是否配置压缩信息做了额外处理，这里先忽略，只看核心代码： func (ps *produceSet) buildRequest() *ProduceRequest { req := \u0026ProduceRequest{ RequiredAcks: ps.parent.conf.Producer.RequiredAcks, Timeout: int32(ps.parent.conf.Producer.Timeout / time.Millisecond), } for topic, partitionSets := range ps.msgs { for partition, set := range partitionSets { rb := set.recordsToSend.RecordBatch if len(rb.Records) \u003e 0 { rb.LastOffsetDelta = int32(len(rb.Records) - 1) for i, record := range rb.Records { record.OffsetDelta = int64(i) } } req.AddBatch(topic, partition, rb) continue } } } 首先是构建一个 req 对象，然后遍历 ps.msg 中的消息，根据 topic 和 partition 分别写入到 req 中。 Produce func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error) { var ( response *ProduceResponse err error ) if request.RequiredAcks == NoResponse { err = b.sendAndReceive(request, nil) } else { response = new(ProduceResponse) err = b.sendAndReceive(request, response) } if err != nil { return nil, err } return response, nil } 最终调用了sendAndReceive()方法将消息发送出去。 如果我们设置了需要 Acks，就会传一个 response 进去接收返回值；如果没设置，那么消息发出去之后，就不管了。 func (b *Broker) sendAndReceive(req protocolBody, res protocolBody) error { promise, err := b.send(req, res != nil, responseHeaderVersion) if err != nil { return err } select { case buf := \u003c-promise.packets: return versionedDecode(buf, res, req.version()) case err = \u003c-promise.errors: return err } } func (b *Broker) send(rb protocolBody, promiseResponse bool, responseHeaderVersion int16) (*responsePromise, error) { req := \u0026request{correlationID: b.correlationID, clientID: b.conf.ClientID, body: rb} buf, err := encode(req, b.conf.MetricRegistry) if err != nil { return nil, err } bytes, err := b.write(buf) } 最终通过bytes, err := b.write(buf) 发送出去。 func (b *Broker) write(buf []byte) (n int, err error) { if err := b.conn.SetWriteDeadline(time.Now().Add(b.conf.Net.WriteTimeout)); err != nil { return 0, err } // 这里就是 net 包中的逻辑了。。 return b.conn.Write(buf) } 至此，Sarama生产者相关的内容就介绍完毕了。 还有一个比较重要的，消息打包批量发送的逻辑，比较多再下一章讲。 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:2:6","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"3. 消息打包源码分析 在之前 BrokerProducer 逻辑中启动了两个 goroutine，其中 bridge 从 chan 中取消息并真正发送出去。 那么这个 chan 里的消息是哪里来的呢? 其实这就是另一个 goroutine 的工作了。 func (p *asyncProducer) newBrokerProducer(broker *Broker) *brokerProducer { var ( input = make(chan *ProducerMessage) bridge = make(chan *produceSet) responses = make(chan *brokerProducerResponse) ) bp := \u0026brokerProducer{ parent: p, broker: broker, input: input, output: bridge, responses: responses, stopchan: make(chan struct{}), buffer: newProduceSet(p), currentRetries: make(map[string]map[int32]error), } go withRecover(bp.run) // minimal bridge to make the network response `select`able go withRecover(func() { for set := range bridge { request := set.buildRequest() response, err := broker.Produce(request) responses \u003c- \u0026brokerProducerResponse{ set: set, err: err, res: response, } } close(responses) }) if p.conf.Producer.Retry.Max \u003c= 0 { bp.abandoned = make(chan struct{}) } return bp } ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"run func (bp *brokerProducer) run() { var output chan\u003c- *produceSet for { select { case msg, ok := \u003c-bp.input: // 1. 检查 buffer 空间是否足够存放当前 msg if bp.buffer.wouldOverflow(msg) { if err := bp.waitForSpace(msg, false); err != nil { bp.parent.retryMessage(msg, err) continue } } // 2. 将 msg 存入 buffer if err := bp.buffer.add(msg); err != nil { bp.parent.returnError(msg, err) continue } // 3. 如果间隔时间到了，也会将消息发出去 case \u003c-bp.timer: bp.timerFired = true // 4. 将 buffer 里的数据发送到 局部变量 output chan 里 case output \u003c- bp.buffer: bp.rollOver() case response, ok := \u003c-bp.responses: if ok { bp.handleResponse(response) } } // 5.如果发送时间到了 或者消息大小或者条数达到阈值 则表示可以发送了 将 bp.output chan 赋值给局部变量 output if bp.timerFired || bp.buffer.readyToFlush() { output = bp.output } else { output = nil } } } 1）首先检测 buffer 空间 2）将 msg 写入 buffer 3）后面的 3 4 5 步都是在发送消息，或者为发送消息做准备 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"wouldOverflow if bp.buffer.wouldOverflow(msg) { if err := bp.waitForSpace(msg, false); err != nil { bp.parent.retryMessage(msg, err) continue } } 在 add 之前先调用bp.buffer.wouldOverflow(msg) 方法检查 buffer 是否存在足够空间以存放当前消息。 wouldOverflow 比较简单，就是判断当前消息大小或者消息数量是否超过设定值： func (ps *produceSet) wouldOverflow(msg *ProducerMessage) bool { switch { case ps.bufferBytes+msg.byteSize(version) \u003e= int(MaxRequestSize-(10*1024)): return true case ps.msgs[msg.Topic] != nil \u0026\u0026 ps.msgs[msg.Topic][msg.Partition] != nil \u0026\u0026 ps.msgs[msg.Topic][msg.Partition].bufferBytes+msg.byteSize(version) \u003e= ps.parent.conf.Producer.MaxMessageBytes: return true case ps.parent.conf.Producer.Flush.MaxMessages \u003e 0 \u0026\u0026 ps.bufferCount \u003e= ps.parent.conf.Producer.Flush.MaxMessages: return true default: return false } } 如果不够就要调用bp.waitForSpace() 等待 buffer 腾出空间，其实就是把 buffer 里的消息发到 output chan。 这个 output chan 就是前面匿名函数里的 bridge。 func (bp *brokerProducer) waitForSpace(msg *ProducerMessage, forceRollover bool) error { for { select { case response := \u003c-bp.responses: bp.handleResponse(response) if reason := bp.needsRetry(msg); reason != nil { return reason } else if !bp.buffer.wouldOverflow(msg) \u0026\u0026 !forceRollover { return nil } case bp.output \u003c- bp.buffer: bp.rollOver() return nil } } } ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"add 接下来是调用bp.buffer.add()把消息添加到 buffer，功能比较简单，把待发送的消息添加到 buffer 中。 func (ps *produceSet) add(msg *ProducerMessage) error { // 1.消息编码 key, err = msg.Key.Encode() val, err = msg.Value.Encode() // 2.添加消息到 set.msgs 数组 set.msgs = append(set.msgs, msg) // 3.添加到set.recordsToSend msgToSend := \u0026Message{Codec: CompressionNone, Key: key, Value: val} if ps.parent.conf.Version.IsAtLeast(V0_10_0_0) { msgToSend.Timestamp = timestamp msgToSend.Version = 1 } set.recordsToSend.MsgSet.addMessage(msgToSend) // 4. 增加 buffer 大小和 buffer 中的消息条数 ps.bufferBytes += size ps.bufferCount++ } set.recordsToSend.MsgSet.addMessage也很简单： func (ms *MessageSet) addMessage(msg *Message) { block := new(MessageBlock) block.Msg = msg ms.Messages = append(ms.Messages, block) } ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:3:3","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"定时发送 因为异步发送者除了消息数或者消息大小达到阈值会触发一次发送之外，到了一定时间也会触发一次发送，具体逻辑也在这个 run 方法里，这个地方比较有意思。 func (bp *brokerProducer) run() { var output chan\u003c- *produceSet for { select { case msg, ok := \u003c-bp.input: // 1.时间到了就将 bp.timerFired 设置为 true case \u003c-bp.timer: bp.timerFired = true // 3.直接把 buffer 里的消息往局部变量 output 里发 case output \u003c- bp.buffer: bp.rollOver() } // 2.如果时间到了，或者 buffer 里的消息达到阈值后都会触发真正的发送逻辑，这里实现比较有意思，需要发送的时候就把 bp.output 也就是存放真正需要发送的批量消息的 chan 赋值给局部变量 output，如果不需要发送就把局部变量 output 清空 if bp.timerFired || bp.buffer.readyToFlush() { output = bp.output } else { output = nil } } } 根据注释中的 1、2、3步骤看来，如果第二步需要发送就会给 output 赋值，这样下一轮 select 的时候case output \u003c- bp.buffer: 这个 case 就可能会执行到，就会把消息发给 output，实际上就是发送给了 bp.output. 如果第二步时不需要发消息，output 就被置空，select 时对应的 case 就不会被执行。 正常写法一般是在启动一个 goroutine 来处理定时发送的功能，但是这样两个 goroutine 之间就会存在竞争，会影响性能。这样处理省去了加解锁过程，性能会高一些，但是随之而来的是代码复杂度的提升。 作者能力实在是有限，文中很有可能会有一些错误的理解。所以当你发现了一些违和的地方，也请不吝指教，谢谢你！ 再次感谢你能看到这里！ ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:3:4","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"4. 小结 1）具体流程：见开篇图 2）常见优化手段：批量处理。 异步消费者 Go 实现中做了消息批量发送这个优化，当累积了足够的消息后一次性发送，减少网络请求次数以提升性能。 类似于 Redis Pipeline，将多次命令一次性发送，以减少 RTT。 当然为了避免在消息少的时候很久都凑不齐足够消息，导致的无法发送，一般还会设定一个定时发送阈值，每隔一段时间也会发送一次。 这是一种常见的优化手段，比如 IO 相关的地方肯定会有什么 bufferio 之类的库，在写时先写 buffer，buffer 满了再一次性写入到磁盘。读取也是同样的，先读到 buffer 里，然后应用程序再从 buffer 里一行行读出去。 3）代码复杂度和性能取舍 在分析 Sarama Proudcer 的最后一段可以看到是有一个骚操作的，这种操作可以提升性能，但是随之而来的就是代码复杂度的提升。 最近在看 Go runtime 里面也有很多骚操作。这种底层库、中间件这样写没什么问题，但是平常我们的业务代码就尽量别搞骚炒作了。 “The performance improvement does not materialize from the air,it comes with code complexity increase.“一dvyokov 性能不会凭空提升，随之而来的一定是代码复杂度的增加。 Kafka 系列相关代码见 Github ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"5. 参考 https://github.com/Shopify/sarama https://cs50mu.github.io/post/2021/01/22/source-code-of-sarama-part-i/ https://www.jianshu.com/p/138e0ac2e1f0 https://juejin.cn/post/6866316565348876296 ","date":"2021-08-14","objectID":"/posts/kafka/06-sarama-producer/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(六)---sarama 客户端 producer 源码分析","uri":"/posts/kafka/06-sarama-producer/"},{"categories":["Kafka"],"content":"Kafka Go sarama 客户端同步异步生产者,独立消费者和消费者组的基本使用","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"本文主要讲解其中的 Producer API 和 Consumer API 在 Go Client sarama 中的使基本使用以及注意事项。 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"1. 概述 Kakfa 相关代码见 Github Kafka 有 5 个核心 API： Producer API Consumer API Stream API Connect API Admin API 在 Go sarama 客户端中暂时只实现了 Producer、Consumer、Admin 3 个API。 其中 Stream API 已经明确表示不会支持，Connect 未知。 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"2. Producer API Kafka 中生产者分为同步生产者和异步生产者。 顾名思义，同步生产者每条消息都会实时发送到 Kafka，而异步生产者则为了提升性能，会等待存了一批消息或者到了指定间隔时间才会一次性发送到 Kafka。 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"Async Producer sarama 中异步生产者使用 Demo 如下 func Producer(topic string, limit int) { config := sarama.NewConfig() // 异步生产者不建议把 Errors 和 Successes 都开启，一般开启 Errors 就行 // 同步生产者就必须都开启，因为会同步返回发送成功或者失败 config.Producer.Return.Errors = true // 设定是否需要返回错误信息 config.Producer.Return.Successes = true // 设定是否需要返回成功信息 producer, err := sarama.NewAsyncProducer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } var ( wg sync.WaitGroup enqueued, timeout, successes, errors int ) // [!important] 异步生产者发送后必须把返回值从 Errors 或者 Successes 中读出来 不然会阻塞 sarama 内部处理逻辑 导致只能发出去一条消息 wg.Add(1) go func() { defer wg.Done() for range producer.Successes() { // log.Printf(\"[Producer] Success: key:%v msg:%+v \\n\", s.Key, s.Value) successes++ } }() wg.Add(1) go func() { defer wg.Done() for e := range producer.Errors() { log.Printf(\"[Producer] Errors：err:%v msg:%+v \\n\", e.Msg, e.Err) errors++ } }() // 异步发送 for i := 0; i \u003c limit; i++ { str := strconv.Itoa(int(time.Now().UnixNano())) msg := \u0026sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(str)} // 异步发送只是写入内存了就返回了，并没有真正发送出去 // sarama 库中用的是一个 channel 来接收，后台 goroutine 异步从该 channel 中取出消息并真正发送 // select + ctx 做超时控制,防止阻塞 producer.Input() \u003c- msg 也可能会阻塞 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*10) select { case producer.Input() \u003c- msg: enqueued++ case \u003c-ctx.Done(): timeout++ } cancel() if i%10000 == 0 \u0026\u0026 i != 0 { log.Printf(\"已发送消息数:%d 超时数:%d\\n\", i, timeout) } } // We are done producer.AsyncClose() wg.Wait() log.Printf(\"发送完毕 总发送条数:%d enqueued:%d timeout:%d successes: %d errors: %d\\n\", limit, enqueued, timeout, successes, errors) } 注意点： 异步生产者只需要将消息发送到 chan 就会返回，同样的具体的响应包括 Success 或者 Errors 也是通过 chan 异步返回的。 必须把返回值从 Errors 或者 Successes 中读出来 不然会阻塞 producer.Input() ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"Sync Producer 同步生产者就更简单了： func Producer(topic string, limit int) { config := sarama.NewConfig() // 同步生产者必须同时开启 Return.Successes 和 Return.Errors // 因为同步生产者在发送之后就必须返回状态，所以需要两个都返回 config.Producer.Return.Successes = true config.Producer.Return.Errors = true // 这个默认值就是 true 可以不用手动 赋值 // 同步生产者和异步生产者逻辑是一致的，Success或者Errors都是通过channel返回的， // 只是同步生产者封装了一层，等channel返回之后才返回给调用者 // 具体见 sync_producer.go 文件72行 newSyncProducerFromAsyncProducer 方法 // 内部启动了两个 goroutine 分别处理Success Channel 和 Errors Channel // 同步生产者内部就是封装的异步生产者 // type syncProducer struct { // producer *asyncProducer // wg sync.WaitGroup // } producer, err := sarama.NewSyncProducer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } defer producer.Close() var successes, errors int for i := 0; i \u003c limit; i++ { str := strconv.Itoa(int(time.Now().UnixNano())) msg := \u0026sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(str)} partition, offset, err := producer.SendMessage(msg) // 发送逻辑也是封装的异步发送逻辑，可以理解为将异步封装成了同步 if err != nil { log.Printf(\"SendMessage:%d err:%v\\n \", i, err) errors++ continue } successes++ log.Printf(\"[Producer] partitionid: %d; offset:%d, value: %s\\n\", partition, offset, str) } log.Printf(\"发送完毕 总发送条数:%d successes: %d errors: %d\\n\", limit, successes, errors) } 注意点： 必须同时开启 Return.Successes 和 Return.Errors ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"3. Consumer API Kafka 中消费者分为独立消费者和消费者组。 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"StandaloneConsumer // SinglePartition 单分区消费 func SinglePartition(topic string) { config := sarama.NewConfig() consumer, err := sarama.NewConsumer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewConsumer err: \", err) } defer consumer.Close() // 参数1 指定消费哪个 topic // 参数2 分区 这里默认消费 0 号分区 kafka 中有分区的概念，类似于ES和MongoDB中的sharding，MySQL中的分表这种 // 参数3 offset 从哪儿开始消费起走，正常情况下每次消费完都会将这次的offset提交到kafka，然后下次可以接着消费， // 这里demo就从最新的开始消费，即该 consumer 启动之前产生的消息都无法被消费 // 如果改为 sarama.OffsetOldest 则会从最旧的消息开始消费，即每次重启 consumer 都会把该 topic 下的所有消息消费一次 partitionConsumer, err := consumer.ConsumePartition(topic, 0, sarama.OffsetOldest) if err != nil { log.Fatal(\"ConsumePartition err: \", err) } defer partitionConsumer.Close() // 会一直阻塞在这里 for message := range partitionConsumer.Messages() { log.Printf(\"[Consumer] partitionid: %d; offset:%d, value: %s\\n\", message.Partition, message.Offset, string(message.Value)) } } // Partitions 多分区消费 func Partitions(topic string) { config := sarama.NewConfig() consumer, err := sarama.NewConsumer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewConsumer err: \", err) } defer consumer.Close() // 先查询该 topic 有多少分区 partitions, err := consumer.Partitions(topic) if err != nil { log.Fatal(\"Partitions err: \", err) } var wg sync.WaitGroup wg.Add(len(partitions)) // 然后每个分区开一个 goroutine 来消费 for _, partitionId := range partitions { go consumeByPartition(consumer, topic, partitionId, \u0026wg) } wg.Wait() } func consumeByPartition(consumer sarama.Consumer, topic string, partitionId int32, wg *sync.WaitGroup) { defer wg.Done() partitionConsumer, err := consumer.ConsumePartition(topic, partitionId, sarama.OffsetOldest) if err != nil { log.Fatal(\"ConsumePartition err: \", err) } defer partitionConsumer.Close() for message := range partitionConsumer.Messages() { log.Printf(\"[Consumer] partitionid: %d; offset:%d, value: %s\\n\", message.Partition, message.Offset, string(message.Value)) } } 反复运行上面的 Demo 会发现，每次都会从第 1 条消息开始消费，一直到消费完全部消息。 这不是妥妥的重复消费吗? Kafka 和其他 MQ 最大的区别在于 Kafka 中的消息在消费后不会被删除，而是会一直保留，直到过期。 为了防止每次重启消费者都从第 1 条消息开始消费，我们需要在消费消息后将 offset 提交给 Kafka。这样重启后就可以接着上次的 Offset 继续消费了。 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"OffsetManager 在独立消费者中没有实现提交 Offset 的功能，所以我们需要借助 OffsetManager 来完成。 func OffsetManager(topic string) { config := sarama.NewConfig() // 配置开启自动提交 offset，这样 samara 库会定时帮我们把最新的 offset 信息提交给 kafka config.Consumer.Offsets.AutoCommit.Enable = true // 开启自动 commit offset config.Consumer.Offsets.AutoCommit.Interval = 1 * time.Second // 自动 commit时间间隔 client, err := sarama.NewClient([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewClient err: \", err) } defer client.Close() // offsetManager 用于管理每个 consumerGroup的 offset // 根据 groupID 来区分不同的 consumer，注意: 每次提交的 offset 信息也是和 groupID 关联的 offsetManager, err := sarama.NewOffsetManagerFromClient(\"myGroupID\", client) // 偏移量管理器 if err != nil { log.Println(\"NewOffsetManagerFromClient err:\", err) } defer offsetManager.Close() // 每个分区的 offset 也是分别管理的，demo 这里使用 0 分区，因为该 topic 只有 1 个分区 partitionOffsetManager, err := offsetManager.ManagePartition(topic, conf.DefaultPartition) // 对应分区的偏移量管理器 if err != nil { log.Println(\"ManagePartition err:\", err) } defer partitionOffsetManager.Close() // defer 在程序结束后在 commit 一次，防止自动提交间隔之间的信息被丢掉 defer offsetManager.Commit() consumer, err := sarama.NewConsumerFromClient(client) if err != nil { log.Println(\"NewConsumerFromClient err:\", err) } // 根据 kafka 中记录的上次消费的 offset 开始+1的位置接着消费 nextOffset, _ := partitionOffsetManager.NextOffset() // 取得下一消息的偏移量作为本次消费的起点 fmt.Println(\"nextOffset:\", nextOffset) pc, err := consumer.ConsumePartition(topic, conf.DefaultPartition, nextOffset) if err != nil { log.Println(\"ConsumePartition err:\", err) } defer pc.Close() for message := range pc.Messages() { value := string(message.Value) log.Printf(\"[Consumer] partitionid: %d; offset:%d, value: %s\\n\", message.Partition, message.Offset, value) // 每次消费后都更新一次 offset,这里更新的只是程序内存中的值，需要 commit 之后才能提交到 kafka partitionOffsetManager.MarkOffset(message.Offset+1, \"modified metadata\") // MarkOffset 更新最后消费的 offset } } 1）创建偏移量管理器 offsetManager, _ := sarama.NewOffsetManagerFromClient(\"myGroupID\", client) 2）创建对应分区的偏移量管理器 Kafka 中每个分区的偏移量是单独管理的 partitionOffsetManager, _ := offsetManager.ManagePartition(topic, kafka.DefaultPartition) 3）记录偏移量 这里记录的是下一条要取的消息，而不是取的最后一条消息，所以需要 +1 partitionOffsetManager.MarkOffset(message.Offset+1, \"modified metadata\") 4）提交偏移量 sarama 中默认会自动提交偏移量，但还是建议用 defer 在程序退出的时候手动提交一次。 defer offsetManager.Commit() ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"ConsumerGroup Kafka 消费者组中可以存在多个消费者，Kafka 会以 partition 为单位将消息分给各个消费者。每条消息只会被消费者组的一个消费者消费。 注意：是以分区为单位，如果消费者组中有两个消费者，但是订阅的 Topic 只有 1 个分区，那么注定有一个消费者永远消费不到任何消息。 消费者组的好处在于并发消费，Kafka 把分发逻辑已经实现了，我们只需要启动多个消费者即可。 如果只有一个消费者，我们需要手动获取消息后分发给多个 Goroutine，需要多写一段代码，而且 Offset 维护还比较麻烦。 // MyConsumerGroupHandler 实现 sarama.ConsumerGroup 接口，作为自定义ConsumerGroup type MyConsumerGroupHandler struct { name string count int64 } // Setup 执行在 获得新 session 后 的第一步, 在 ConsumeClaim() 之前 func (MyConsumerGroupHandler) Setup(_ sarama.ConsumerGroupSession) error { return nil } // Cleanup 执行在 session 结束前, 当所有 ConsumeClaim goroutines 都退出时 func (MyConsumerGroupHandler) Cleanup(_ sarama.ConsumerGroupSession) error { return nil } // ConsumeClaim 具体的消费逻辑 func (h MyConsumerGroupHandler) ConsumeClaim(sess sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error { for msg := range claim.Messages() { fmt.Printf(\"[consumer] name:%s topic:%q partition:%d offset:%d\\n\", h.name, msg.Topic, msg.Partition, msg.Offset) // 标记消息已被消费 内部会更新 consumer offset sess.MarkMessage(msg, \"\") h.count++ if h.count%10000 == 0 { fmt.Printf(\"name:%s 消费数:%v\\n\", h.name, h.count) } } return nil } func ConsumerGroup(topic, group, name string) { config := sarama.NewConfig() config.Consumer.Return.Errors = true ctx, cancel := context.WithCancel(context.Background()) defer cancel() cg, err := sarama.NewConsumerGroup([]string{conf.HOST}, group, config) if err != nil { log.Fatal(\"NewConsumerGroup err: \", err) } defer cg.Close() var wg sync.WaitGroup wg.Add(1) go func() { defer wg.Done() handler := MyConsumerGroupHandler{name: name} for { fmt.Println(\"running: \", name) /* ![important] 应该在一个无限循环中不停地调用 Consume() 因为每次 Rebalance 后需要再次执行 Consume() 来恢复连接 Consume 开始才发起 Join Group 请求 如果当前消费者加入后成为了 消费者组 leader,则还会进行 Rebalance 过程，从新分配 组内每个消费组需要消费的 topic 和 partition，最后 Sync Group 后才开始消费 具体信息见 https://github.com/lixd/kafka-go-example/issues/4 */ err = cg.Consume(ctx, []string{topic}, handler) if err != nil { log.Println(\"Consume err: \", err) } // 如果 context 被 cancel 了，那么退出 if ctx.Err() != nil { return } } }() wg.Wait() } 注意点： 主要是实现sarama.ConsumerGroup接口。Setup和Cleanup都是一些辅助性的工作，真正的逻辑在 ConsumeClaim方法中。 func (h MyConsumerGroupHandler) ConsumeClaim(sess sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error { for msg := range claim.Messages() { // 标记消息已被消费 内部会更新 consumer offset sess.MarkMessage(msg, \"\") } return nil } 需要调用sess.MarkMessage()方法更新 Offset。 Kakfa 相关代码见 Github ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:3:3","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"4. 小结 1）生产者 同步生产者 同步发送，效率低实时性高 异步生产者 异步发送，效率高 消息大小、数量达到阈值或间隔时间达到设定值时触发发送 异步生产者不会阻塞，而且会批量发送消息给 Kafka，性能上优于 同步生产者。 2）消费者 独立消费者 需要配合 OffsetManager 使用 消费者组 以分区为单位将消息分发给组里的各个消费者 若消费者数大于分区数，必定有消费者消费不到消息 ","date":"2021-08-13","objectID":"/posts/kafka/05-quick-start/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(五)---Producer-Consumer API 基本使用","uri":"/posts/kafka/05-quick-start/"},{"categories":["Kafka"],"content":"Kafka 线上部署与集群参数配置","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"本文主要记录了 Kafka 线上环境集群部署考虑因素以及非常非常重要的 Kafka 参数配置讲解。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 生产环境的一些注意事项，包括集群部署的考量和 集群配置参数的设置等，具体见下图： ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2. 集群部署 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2.1 操作系统 主要为以下 3 方面： I/O 模型的使用 Kafka 客户端底层使用了 Java 的 selector，selector 在 Linux 上的实现机制是 epoll，而在 Windows 平台上的实现机制是 select。 数据网络传输效率 在 Linux 部署 Kafka 能够享受到零拷贝技术所带来的快速数据传输特性。 社区支持度 主要支持 Linux ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2.2 磁盘 Kafka 使用磁盘的方式多是顺序读写操作，一定程度上规避了机械磁盘最大的劣势，即随机读写操作慢。 即 SDS 优势并不大 一方面 Kafka 自己实现了冗余机制来提供高可靠性；另一方面通过分区的概念，Kafka 也能在软件层面自行实现负载均衡。 RAID 也没必要上了 所以机械磁盘即可。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2.3 磁盘容量 规划磁盘容量时你需要考虑下面这几个元素： 新增消息数 消息留存时间 平均消息大小 备份数 是否启用压缩 假设每天需要向 Kafka 集群发送 1 亿条消息，每条消息保存两份以防止数据丢失，另外消息默认保存两周时间。现在假设消息的平均大小是 1KB。 那么每天的空间大小就等于 1 亿(消息数) * 1KB(消息大小) * 2(备份数) / 1000 / 1000 = 200GB。 一般情况下 Kafka 集群除了消息数据还有其他类型的数据，比如索引数据等，故我们再为这些数据预留出 10% 的磁盘空间，因此总的存储容量就是 220GB。 既然要保存两周，那么整体容量即为 220GB * 14，大约 3TB 左右。 Kafka 支持数据的压缩，假设压缩比是 0.75，那么最后你需要规划的存储空间就是 0.75 * 3 = 2.25TB。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2.4 带宽 假设 1 小时内处理 1TB 的业务数据，同时假设带宽是 1Gbps。 通常情况下你只能假设 Kafka 会用到 70% 的带宽资源，因为总要为其他应用或进程留一些资源。 另外不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2/3 的资源。 即单台服务器使用带宽 1G * 70% / 3 ≈ 240Mbps 这里的 2/3 其实是相当保守的，你可以结合你自己机器的使用情况酌情减少此值。 1 小时内处理 1TB 数据，即每秒需要处理 2336Mb，除以 240，约等于 10 台服务器。如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:4","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"2.5 小结 因素 考量点 建议 操作系统 操作系统 I/O 模型 Linux 磁盘类型 磁盘 I/O 性能 普通环境 机械硬盘 即可 磁盘容量 根据消息数、留存时间预估磁盘容量 实际使用中建议预留20%~30%磁盘空间 带宽 根据实际带宽资源和业务 SLA 预估服务器数量 对于千兆网络，建议每台服务器按照700Mbps来计算，避免大流量下的丢包 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:2:5","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"3. 参数配置 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"3.1 Broker 端参数 存储信息相关参数 log.dirs：这是非常重要的参数，指定了 Broker 需要使用的若干个文件目录路径。 该参数没有默认值，必须手动指定。 log.dir：注意这是 dir，结尾没有 s，说明它只能表示单个路径，它是补充上一个参数用的。 建议：只设置log.dirs 即可，不用设置 log.dir。而且更重要的是，在线上生产环境中一定要为 log.dirs 配置用逗号分隔的多个路径。 比如 /home/kafka1,/home/kafka2,/home/kafka3 这样 如果有条件的话你最好保证这些目录挂载到不同的物理磁盘上。这样做有两个好处： 提升读写性能：比起单块磁盘，多块物理磁盘同时读写数据有更高的吞吐量。 能够实现故障转移：即 Failover。这是 Kafka 1.1 版本新引入的强大功能，Kafka Broker 上任意一快磁盘坏掉后，上面的数据会自动地转移到其他正常的磁盘上，而且 Broker 还能正常工作。 Zookeeper 相关参数 zookeeper.connect：zk 的连接地址，这也是一个 CSV 格式的参数，比如我可以指定它的值为zk1:2181,zk2:2181,zk3:2181。 如果多个 Kafka 集群使用同一套 ZooKeeper 集群，可以使用 Zookeeper 的 chroot 功能。 只需要将参数这样设置：zk1:2181,zk2:2181,zk3:2181/kafka1和zk1:2181,zk2:2181,zk3:2181/kafka2。 切记 chroot 只需要写一次，而且是加到最后的。 Broker 连接相关参数 listeners：学名叫监听器，其实就是告诉外部连接者要通过什么协议访问指定主机名和端口开放的 Kafka 服务。 advertised.listeners：和 listeners 相比多了个 advertised。Advertised 的含义表示宣称的、公布的，就是说这组监听器是 Broker 用于对外发布的。 注意：host.name/port 为过期参数，不建议再使用了。 监听器它是若干个逗号分隔的三元组，每个三元组的格式为\u003c协议名称，主机名，端口号\u003e。 这里的协议名称可能是标准的名字，比如 PLAINTEXT 表示明文传输、SSL 表示使用 SSL 或 TLS 加密传输等；也可能是你自己定义的协议名字，比如CONTROLLER: //localhost:9092。 一旦你自己定义了协议名称，你必须还要指定listener.security.protocol.map参数告诉这个协议底层使用了哪种安全协议，比如指定listener.security.protocol.map=CONTROLLER:PLAINTEXT表示CONTROLLER这个自定义协议底层使用明文不加密传输数据。 注意：主机名真的就是填写主机名，不建议填写 IP。 Topic 管理相关参数 auto.create.topics.enable：是否允许自动创建 Topic 即：生产者往不存在的 Topic 发送消息时，是否允许自动创建该 Topic 测试环境可以开启(true)，生产环境建议关闭(false)。 unclean.leader.election.enable：是否允许 Unclean Leader 选举 即：是否允许落后 Leader 太多的副本参加 Leader 选举 如果数据落后的副本 选举为新 Leader 后可能会丢失数据 建议设置为 false auto.leader.rebalance.enable：是否允许定期进行 Leader 选举。 切换 Leader 代价是很大的，所有建议设置为 false 数据留存相关参数 log.retention.{hours|minutes|ms}：这是个“三兄弟”，都是控制一条消息数据被保存多长时间。从优先级上来说 ms 设置最高、minutes 次之、hours 最低。 虽然 ms 设置有最高的优先级，但是通常情况下我们还是设置 hours 级别的多一些，毕竟不需这么精确。 比如 log.retention.hours=168 表示默认保存 7 天的数据，自动删除 7 天前的数据。 log.retention.bytes：这是指定 Broker 为消息保存的总磁盘容量大小。 默认值为 -1 即：可以存储任意数据的消息，一般不用修改，除非是多租户场景。 message.max.bytes：控制 Broker 能够接收的最大消息大小。 默认的 1000012 太少了，还不到 1MB，实际场景中突破 1M 的消息还是比较多的 因此在线上环境中设置一个比较大的值还是比较保险的做法。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:3:1","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"3.2 Topic 级别参数 如果同时设置了 Topic 级别参数和全局 Broker 参数， Topic 级别参数会覆盖全局 Broker 参数的值。 retention.ms：规定了该 Topic 消息被保存的时长。默认是 7 天，即该 Topic 只保存最近 7 天的消息。一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。 retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。 max.message.bytes：它决定了 Kafka Broker 能够正常接收该 Topic 的最大消息大小。 Topic 级别参数有两种修改方式： 创建 Topic 时进行设置 动态设置 可以在创建时指定 bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic transaction --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880 也可以动态设置 bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name transaction --alter --add-config max.message.bytes=10485760 建议统一使用动态修改方式。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:3:2","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"3.3 JVM 参数 JVM 堆大小 建议设置成 6GB ，这是目前业界比较公认的一个合理值 垃圾回收器 如果 Broker 所在机器的 CPU 资源非常充裕，建议使用 CMS 收集器。启用方法是指定-XX:+UseCurrentMarkSweepGC。 否则，使用吞吐量收集器。开启方法是指定-XX:+UseParallelGC。 如果使用 Java 8，那么可以手动设置使用 G1 收集器 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:3:3","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"3.4 操作系统参数 文件描述符限制 通常情况下将它设置成一个超大的值是合理的做法，比如 ulimit -n 1000000 文件系统类型 根据官网的测试报告，ZFS \u003e XFS \u003e ext4 Swappiness 不建议直接将交换空间设置为 0 ，可以设置为一个较小值。 一旦设置成 0，当物理内存耗尽时，操作系统会触发 OOM killer 这个组件 如果设置成一个比较小的值，当开始使用 swap 空间时，你至少能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间 提交时间 或者说是 Flush 落盘时间。 向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。 这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。 因为 Kafka 在软件层面已经提供了多副本的冗余机制，所以这里稍微拉大提交间隔去换取性能还是一个合理的做法。 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:3:4","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"4. 参考 https://kafka.apache.org/documentation/#configuration 《Kafka 核心技术与实战》 ","date":"2021-08-06","objectID":"/posts/kafka/04-kafka-product-install-config/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(四)---Kafka 线上部署与集群参数配置","uri":"/posts/kafka/04-kafka-product-install-config/"},{"categories":["Kafka"],"content":"Kafka 入门教程,包括相关概念介绍:消息引擎、 Kafka 相关术语、基本定位及其版本选择等等","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"本文为 Kafka 入门教程,主要包括相关概念介绍如： 消息引擎、 Kafka 相关术语、角色定位及其版本选择等等。 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"1. 消息引擎 Kafka 系列相关代码见 Github Kafka 是什么呢？ 用一句话概括一下：Apache Kafka 是一款开源的消息引擎系统。 消息引擎系统 这个词可能比较陌生，国内一般用得多的是消息队列或者消息中间件。不过相比之下 消息引擎系统 这个称谓可能更合适。 消息队列给出了一个很不明确的暗示，仿佛 Kafka 是利用队列的方式构建的； 而消息中间件的提法有过度夸张“中间件”之嫌，让人搞不清楚这个中间件到底是做什么的。 消息引擎系统则更能提现出消息传递属性，就像引擎一样，具备某种能量转换传输的能力。 像 Kafka 这一类的系统国外有专属的名字叫 Messaging System，国内很多文献将其简单翻译成消息系统。 目前国内在翻译国外专有技术词汇方面做得确实够标准化，比如 Raft 算法和 Paxos 算法都属于 Consensus Algorithm 一族。但是国内一般称为 一致性算法，实际上 共识算法 比较准确。国外的 Consistency 被称为一致性、Consensus 也唤作一致性，甚至是 Coherence 都翻译成一致性。 消息系统有什么作用？脑子里想到的第一个词肯定是削峰填谷。 所谓的“削峰填谷”就是指缓冲上下游瞬时突发流量，使其更平滑。 比如下游系统处理能力为 10，上游压力则是在0~20动态变化的。假设第一秒上游压力为 20，第二秒为 0 ,如果没有消息系统直接把 20 的压力给到下游，由于处理不过来，有 10 的请求肯定会超时，然后第二秒没有压力下游也只能闲着。有消息系统后，直接将上游压力写入 消息系统，下游从消息系统中取出消息来处理，这样第一秒处理 10个，第二秒再处理 10 个，一直处于忙碌状态也不会出现超时的问题。 一个形象的比喻就是： 用消息系统之前就是抱着水管喝水，水管出水量不会因为你喝不过来就变小； 用消息系统之后就是拿瓶子喝水，每次倒多少水可以由自己控制。 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"2. Kafka 相关术语 Kafka 里的一些术语是其他 MQ 没有的，这里先提一下，后面遇到时能大致有个印象就行。 消息：Record。Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。 主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。 消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。 副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。 生产者：Producer。向主题发布新消息的应用程序。 消费者：Consumer。从主题订阅新消息的应用程序。 消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。 消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"3. Kafka 角色定位 Kafka 在设计之初就旨在提供三个方面的特性： 提供一套 API 实现生产者和消费者； 降低网络传输和磁盘存储开销； 实现高伸缩性架构。 Apache Kafka 真的只是消息引擎吗？ 目前 Kafka 官方文档写的是 Apache Kafka is an event streaming platform。即 Apache Kafka 不仅是消息引擎系统，还是一个流处理平台（Streaming Platform）。 Apache Kafka 从一个优秀的消息引擎系统起家，逐渐演变成现在分布式的流处理平台。 Kafka 社区于 0.10.0.0 版本正式推出了流处理组件 Kafka Streams，也正是从这个版本开始，Kafka 正式“变身”为分布式的流处理平台，而不仅仅是消息引擎系统了。 作为流处理平台，Kafka 与其他主流大数据流式计算框架相比，优势在哪里呢？ 大概是 Kafka 自己对于流式计算的定位。官网上明确标识 Kafka Streams 是一个用于搭建实时流处理的客户端库而非是一个完整的功能系统。 对于中小企业来说，它们的流处理数据量并不巨大，逻辑也并不复杂，搭建重量级的完整性平台实在是“杀鸡焉用牛刀”，而这正是 Kafka 流处理组件的用武之地。 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"4. Kafka 类型选择 目前 有 3 种 Kafka 可供选择： Apache Kafka 一般日常提到的 Kafka 都指的是 Apache Kafka，这是最“正宗”的 Kafka，也称为 社区版 Kafka。 Confluent Kafka，Confluent 公司是 Kafka 的 3 个创始人 Jay Kreps、Naha Narkhede 和饶军离开 LinkedIn 创办的，它主要从事商业化 Kafka 工具开发，并在此基础上发布了 Confluent Kafka。Confluent Kafka 提供了一些 Apache Kafka 没有的高级特性，比如跨数据中心备份、Schema 注册中心以及集群监控工具等。 Cloudera/Hortonworks Kafka：Cloudera 提供的 CDH 和 Hortonworks 提供的 HDP 是非常著名的大数据平台，里面集成了目前主流的大数据框架，当然也包括 Apache Kafka，因此我把这两款产品中的 Kafka 称为 CDH Kafka 和 HDP Kafka。 特点比较 Apache Kafka 它现在依然是开发人数最多、版本迭代速度最快的 Kafka，且社区比较活跃，对开发者友好。劣势在于它仅仅提供最最基础的组件，没有提供任何监控框架或工具。 如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。 Confluent Kafka 目前分为免费版和企业版两种。前者和 Apache Kafka 非常相像，除了常规的组件之外，免费版还包含 Schema 注册中心和 REST proxy 两大功能。后者用开放 HTTP 接口的方式允许你通过网络访问 Kafka 的各种功能，这两个都是 Apache Kafka 所没有的。 如果你需要用到 Kafka 的一些高级特性，那么推荐你使用 Confluent Kafka。 CDH/HDP Kafka，这些大数据平台天然集成了 Apache Kafka，通过便捷化的界面操作将 Kafka 的安装、运维、管理、监控全部统一在控制台中。使用起来非常方便，不过这样做的结果是直接降低了你对 Kafka 集群的掌控程度。 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:4:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"5. Kafka 演进历史 目前 Kafka 已经迭代到了 2.8.0 版本。再下载 Kafka 时会发现 Kafka 的版本号大概是这样的: Scala 2.12 - kafka_2.12-2.8.0.gz Scala 2.13 - kafka_2.13-2.8.0.gz 前面的版本号是编译 Kafka 源代码的 Scala 编译器版本,如 2.12 或者 2.13。 Kafka 服务器端的代码完全由 Scala 语言编写 后面 2.8.0 才是 Kafka 的版本号。由 3 个部分构成，即“大版本号 - 小版本号 - Patch 号“。 版本演进历史 版本号 说明 0.7 只有基础消息队列功能，无副本；打死也不使用 0.8 增加了副本机制，新的producer API；建议使用0.8.2.2版本；不建议使用0.8.2.0之后的producer API 0.9 增加权限和认证，新的consumer API，Kafka Connect功能；不建议使用consumer API； 0.10 引入Kafka Streams功能，bug修复；建议版本0.10.2.2；建议使用新版consumer API 0.11 producer API幂等，事物API，消息格式重构；建议版本0.11.0.3；谨慎对待消息格式变化 1.0、2.0 Kafka Streams改进；建议版本2.0； 2.4 消费者增量 Rebalance 支持 2.6 显著性能优化 2.8 替换掉 Zookeeper，元数据也存到 Kafka 最后还有个建议，不论你用的是哪个版本，都请尽量保持服务器端版本和客户端版本一致，否则你将损失很多 Kafka 为你提供的性能优化收益。 还有就是在生产环境不要贸然升级到最新版本，新版本多多少少都存在一些小问题，至少要在测试环境确认没问题后再升级。 Kafka 系列相关代码见 Github ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:5:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"6. 参考 https://kafka.apache.org/documentation 《Kafka 核心技术与实战》 ","date":"2021-08-06","objectID":"/posts/kafka/03-kafka-introduction/:6:0","tags":["Kafka"],"title":"Kafka(Go)教程(三)---Kafka 相关概念介绍","uri":"/posts/kafka/03-kafka-introduction/"},{"categories":["Kafka"],"content":"Kafka 的 Golang 客户端 sarama 基本使用","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"本文记录了 Kafka Golang 客户端(sarama)基本使用。 ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 系列相关代码见 Github Kafka 的 Golang 客户端比较少，不像 Java 由官方维护，Golang 的都是社区在维护。 这里选取的是 sarama,社区活跃度还行，不过封装度比较低，比较接近原生，不过有好处也有坏处吧，如果对 Kafka 比较熟悉使用起来还是不错的。 ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"2. Demo 这是一个简单的 Hello World 示例，主要用于演示如何使用 Kafka 和 测试上文中部署的 kafka 能否正常工作。 和其他 MQ 一样， Kafka 中同样分为 producer 和 consumer。 ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"2.1 producer package sync import ( \"log\" \"strconv\" \"time\" \"github.com/Shopify/sarama\" \"kafka-go-example/conf\" ) func Produce(topic string, limit int) { config := sarama.NewConfig() config.Producer.Return.Successes = true config.Producer.Return.Errors = true producer, err := sarama.NewSyncProducer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewSyncProducer err:\", err) } defer producer.Close() for i := 0; i \u003c limit; i++ { str := strconv.Itoa(int(time.Now().UnixNano())) msg := \u0026sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(str)} partition, offset, err := producer.SendMessage(msg) if err != nil { log.Println(\"SendMessage err: \", err) return } log.Printf(\"[Producer] partitionid: %d; offset:%d, value: %s\\n\", partition, offset, str) } } ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"2.2 consumer func Consume(topic string) { config := sarama.NewConfig() consumer, err := sarama.NewConsumer([]string{conf.HOST}, config) if err != nil { log.Fatal(\"NewConsumer err: \", err) } defer consumer.Close() partitionConsumer, err := consumer.ConsumePartition(topic, 0, sarama.OffsetNewest) if err != nil { log.Fatal(\"ConsumePartition err: \", err) } defer partitionConsumer.Close() for message := range partitionConsumer.Messages() { log.Printf(\"[Consumer] partitionid: %d; offset:%d, value: %s\\n\", message.Partition, message.Offset, string(message.Value)) } } ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"2.3 Test 先启动 consumer lixd@17x:~/17x/projects/kafka-go-example/helloworld/consumer/cmd$ go run main.go 再启动 producer lixd@17x:~/17x/projects/kafka-go-example/helloworld/producer/cmd$ go run main.go 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7340, value: 1627699112413451557 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7341, value: 1627699112483251015 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7342, value: 1627699112518530847 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7343, value: 1627699112552429595 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7344, value: 1627699112586320615 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7345, value: 1627699112621294679 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7346, value: 1627699112656351458 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7347, value: 1627699112690306556 2021/07/31 10:38:32 [Producer] partitionid: 0; offset:7348, value: 1627699112724161792 可以看到 生产者启动就开始往 kafka 中发送消息了。 切换回 consumer 看是否能正常消费。 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7340, value: 1627699112413451557 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7341, value: 1627699112483251015 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7342, value: 1627699112518530847 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7343, value: 1627699112552429595 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7344, value: 1627699112586320615 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7345, value: 1627699112621294679 2021/07/31 10:38:32 [Consumer] partitionid: 0; offset:7346, value: 1627699112656351458 ok，一切正常。 ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"3. 小结 本文主要通过一个 HelloWorld Demo 测试了上文部署的 Kafka 能否正常工作。 同时也展示了 Kafka Golang 客户端的基本使用。 文中相关的名词、概念会在后续文章中给出详细解释。 kKfka 系列相关代码见 Github ","date":"2021-07-30","objectID":"/posts/kafka/02-hello-kafka/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(二)---hello Kafka","uri":"/posts/kafka/02-hello-kafka/"},{"categories":["Kafka"],"content":"通过docker-compose 快速安装 Kafka","date":"2021-07-30","objectID":"/posts/kafka/01-install/","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"本文记录了如何通过 docker-compose 快速启动 kafka，部署一套开发环境。 ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:0:0","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"1. 概述 Kafka 是由 Apache 软件基金会旗下的一个开源 消息引擎系统。 使用 docker-compose 来部署开发环境也比较方便，只需要提准备一个 yaml 文件即可。 Kafka 系列相关代码见 Github ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:1:0","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"2. docker-compose.yaml 完整的 docker-compose.yaml内容如下： 当前 Kafka 还依赖 Zookeeper，所以需要先启动一个 Zookeeper 。 这里用的是Kafka 2.8.0版本，其他版本不一定兼容。 version: \"3\" services: zookeeper: image: 'bitnami/zookeeper:latest' ports: - '2181:2181' environment: # 匿名登录--必须开启 - ALLOW_ANONYMOUS_LOGIN=yes #volumes: #- ./zookeeper:/bitnami/zookeeper # 该镜像具体配置参考 https://github.com/bitnami/bitnami-docker-kafka/blob/master/README.md kafka: image: 'bitnami/kafka:2.8.0' ports: - '9092:9092' - '9999:9999' environment: - KAFKA_BROKER_ID=1 - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 # 客户端访问地址，更换成自己的 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://123.57.236.125:9092 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 # 允许使用PLAINTEXT协议(镜像中默认为关闭,需要手动开启) - ALLOW_PLAINTEXT_LISTENER=yes # 关闭自动创建 topic 功能 - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false # 全局消息过期时间 6 小时(测试时可以设置短一点) - KAFKA_CFG_LOG_RETENTION_HOURS=6 # 开启JMX监控 - JMX_PORT=9999 #volumes: #- ./kafka:/bitnami/kafka depends_on: - zookeeper # Web 管理界面 另外也可以用exporter+prometheus+grafana的方式来监控 https://github.com/danielqsj/kafka_exporter kafka_manager: image: 'hlebalbau/kafka-manager:latest' ports: - \"9000:9000\" environment: ZK_HOSTS: \"zookeeper:2181\" APPLICATION_SECRET: letmein depends_on: - zookeeper - kafka ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:0","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"镜像 在 dockerhub 上 kafka 相关镜像有 wurstmeister/kafka 和 bitnami/kafka 这两个用的人比较多,大概看了下 bitnami/kafka 更新比较频繁所以就选这个了。 ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:1","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"监控 监控的话 hlebalbau/kafka-manager 这个比较好用，其他的都太久没更新了。 不过 kafka-manager 除了监控外更偏向于集群管理，误操作的话影响比较大，如果有 prometheus + grafana 监控体系的直接用 kafka_exporter 会舒服很多。 另外 滴滴 开源的 LogikM 看起来也不错。 ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:2","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"数据卷 如果有持久化需求可以放开 yaml 文件中的 volumes相关配置，并创建对应文件夹同时将文件夹权限调整为 777。 因为容器内部使用 uid=1001 的用户在运行程序，容器外部其他用户创建的文件夹对 1001 来说是没有权限的。 ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:3","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"目录结构 整体目录结构如下所示： kafka/ ├── kafka ├── docker-compose.yaml └── zookeeper 内部的 kafka 和 zookeeper 目录分别用于存放相关数据。 ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:4","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"启动 在 docker-compose.yaml 文件目录下使用以下命令即可一键启动： docker-compose up ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:2:5","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Kafka"],"content":"3. 测试 启动后浏览器直接访问localhost:9000即可进入 Web 监控界面。 Kafka 系列相关代码见 Github ","date":"2021-07-30","objectID":"/posts/kafka/01-install/:3:0","tags":["Kafka"],"title":"Kafka(Go)教程(一)---通过docker-compose 安装 Kafka","uri":"/posts/kafka/01-install/"},{"categories":["Network"],"content":"通过Wireshark抓包分析一次HTTP(S)请求究竟需要多少流量，和 HTTP 请求的执行流程","date":"2021-07-23","objectID":"/posts/network/06-http-flow/","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"本文主要通过 Wireshark 抓包分析了一次 HTTP(S) 请求究竟需要多少流量，同时也分析了一下整个 HTTP 请求的执行流程。 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:0:0","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"1. 背景 最近查询监控，观察到某个负载的带宽峰值在最高的时候都达到了近 30M，然后查了对应时间段的系统 QPS，发现确实是有一个明细的峰值，但是也不应该有这种多流量吧？ 根据QPS和接口响应数据大致计算下来，最多 5M 带宽就够了。 后续对接口响应进行了优化和精简，整体响应数据降低了 20% 左右，但是负载的带宽峰值还是没什么变化，有降低但是并不明显。 HTTP 响应除了包含我们业务数据的响应体之外还有状态行和响应头，这些都是要算流量的，所以多出的流量是不是在这些地方呢？ 对比了几个请求后发现，虽然这部分会消耗一定流量，但是并不多，还没有业务数据多呢。 这就很奇怪了，也找不到流量到底消耗到哪儿去了，于是想着通过抓包来分析一下。 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:1:0","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2. 抓包分析 没办法，HTTP 层找不到原因只能从更底层找了，这里使用的是Wireshark工具进行抓包分析。 具体的抓包结果如下图所示： 由于一台机器上网络请求较多，我加了筛选条件，仅显示客户端和服务端通信的网络请求，所以请求的序号是不连续的。 因为没有保存，导致有部分截图漏掉了，所以又抓了一次，下面是第二次抓包的结果。 按阶段拆分了一下，可以看到整个流程可以分为 5 个阶段： 1）TCP 3 次握手 2）TLS 握手 3）正常业务请求 4）Keep-Alive 保持连接 5）TCP 4 次挥手 TCP 三次握手，四次挥手详情见计算机网络(二)—TCP三次握手四次挥手 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:0","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2.1 TCP 三次握手 首先是 三次握手，毕竟 HTTP 也是基于 TCP 的，所以需要先建立 TCP 连接。 从图中可以看到，客户端发送两次请求，总共 66+54 字节，服务端一次请求共 66 字节。 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:1","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2.2 TLS 握手 由于是 HTTPS，所以还需要进行额外的一个 TLS 握手，具体步骤如下： 1）客户端提供【客户端随机数、可选算法套件、sessionId】等信息 2）服务端提供【服务端随机数、选用算法套件、sessionId】等信息 3）服务端提供证书 4）服务端与客户端互换算法需要的参数 5）客户端根据前面提到的随机数和参数生成master secret，确认开始使用指定算法加密，并将握手信息加密传输给服务端，用来校验握手信息、秘钥是否正确 6）服务端进行与（5）一样的操作 7）客户端、服务端可以用master secret进行加密通讯 从截图或者步骤中可以看到，服务端需要提供证书给客户端，所以肯定会耗费较多的流量。 客户端：571(步骤1)+54(ACK)+147(步骤4)+153(步骤5)+576(步骤5) 服务端：60(ACK)+1446(步骤2)+1446(步骤3)+220(步骤4)+60(ACK)+312(步骤6) ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:2","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2.3 业务请求 建立 TLS 连接后才能进行真正的业务请求，这里一共进行了 3 个请求。 客户端：189(业务数据)+54(ACK)+268(业务数据)+54(ACK)+683(业务数据)+54(ACK) 服务端：636(业务数据)+92+350(业务数据)+92+257(业务数据)+92 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:3","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2.4 Keep-Alive 为了防止连接被关闭，客户端会自动发送 Keep-Alive 请求来保持连接。 必须要在 HTTP Request 中开启 Keep-Alive 才行 每个心跳包：客户端 55 字节，服务端 66 字节。 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:4","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"2.5 TCP 四次挥手 最后则是 TCP 的四次挥手了。 客户端：54+54 服务端：60+60 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:2:5","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Network"],"content":"3. 结论 经过前面的分析，基本上已经可以确定流量到底是消耗在哪个阶段了，这里还是总结一下，具体流量消耗请求见下表： CS / 阶段 TCP 3 次握手 TLS 握手 正常业务请求 Keep-Alive 保持连接 TCP 4 次挥手 总计 Client 120 1510 1302 55 105 3092 Server 66 3544 1515 66 120 5311 从表格中可以看到，一共发起了 3 个 HTTP 请求，客户端消耗了 3092 字节，服务端则多达 5311 字节。 客户端真正发送的业务数据为：1140 字节，占比 36% 服务端真正的响应数据为 1243 字节，占比只有 23%。 不管是客户端还是服务端 TLS 握手消耗占比都超过了 50%，这也是为什么要保持连接，不然每请求一次都要走一遍 TLS 握手，那这个消耗可太大了。 理论上讲 HTTP 肯定是比 HTTPS 快的，但是 HTTPS 多了 TLS 层极大增强了安全性，相比之下还是安全更重要，这也是为什么现在大部分网站都上 HTTPS 了。 这也印证了为什么负载带宽这么高，且减少响应数据后带宽变化不大。毕竟真正消耗带宽的地方并不是业务数据。 ","date":"2021-07-23","objectID":"/posts/network/06-http-flow/:3:0","tags":["Network"],"title":"一次HTTP(S)请求究竟需要多少流量?Wireshark抓包分析","uri":"/posts/network/06-http-flow/"},{"categories":["Golang"],"content":"Singleflight基本使用及源码分析","date":"2021-07-16","objectID":"/posts/go/singleflight/","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"本文主要分析了 Golang 中的一个第三方库，防缓存击穿利器 singleflight，包括基本使用和源码分析。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:0:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"1. 缓存击穿 平时开发中为了提升性能，减轻DB压力，一般会给热点数据设置缓存，如 Redis，用户请求过来后先查询 Redis，有则直接返回，没有就会去查询数据库，然后再写入缓存。 大致流程如下图所示： 以上流程存在一个问题，cache miss 后查询DB和将数据再次写入缓存这两个步骤是需要一定时间的，这段时间内的后续请求也会出现 cache miss，然后走同样的逻辑。 这就是缓存击穿：某个热点数据缓存失效后，同一时间的大量请求直接被打到的了DB，会给DB造成极大压力，甚至直接打崩DB。 常见的解决方案是加锁，cache miss 后请求DB之前必须先获取分布式锁，取锁失败说明是有其他请求在查询DB了，当前请求只需要循环等待并查询Redis检测取锁成功的请求把数据回写到Redis没有，如果有的话当前请求就可以直接从缓存中取数据返回了。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:1:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"2. singleflight 虽然加锁能解决问题，但是太重了，而且逻辑比较复杂，又是加锁又是等待的。 相比之下 singleflight 就是一个轻量级的解决方案。 Demo如下： package main import ( \"errors\" \"fmt\" \"log\" \"strconv\" \"sync\" \"time\" \"golang.org/x/sync/singleflight\" ) var ( g singleflight.Group ErrCacheMiss = errors.New(\"cache miss\") ) func main() { var wg sync.WaitGroup wg.Add(10) // 模拟10个并发 for i := 0; i \u003c 10; i++ { go func() { defer wg.Done() data, err := load(\"key\") if err != nil { log.Print(err) return } log.Println(data) }() } wg.Wait() } // 获取数据 func load(key string) (string, error) { data, err := loadFromCache(key) if err != nil \u0026\u0026 err == ErrCacheMiss { // 利用 singleflight 来归并请求 v, err, _ := g.Do(key, func() (interface{}, error) { data, err := loadFromDB(key) if err != nil { return nil, err } setCache(key, data) return data, nil }) if err != nil { log.Println(err) return \"\", err } data = v.(string) } return data, nil } // getDataFromCache 模拟从cache中获取值 cache miss func loadFromCache(key string) (string, error) { return \"\", ErrCacheMiss } // setCache 写入缓存 func setCache(key, data string) {} // getDataFromDB 模拟从数据库中获取值 func loadFromDB(key string) (string, error) { fmt.Println(\"query db\") unix := strconv.Itoa(int(time.Now().UnixNano())) return unix, nil } 结果如下： query db 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 2021/07/17 11:04:13 1626491053454483100 可以看到 10 个请求都获取到了结果，并且只有一个请求去查询数据库，极大的减轻了DB压力。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:2:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"3. 源码分析 这个库的实现很简单，除去注释大概就 100 来行代码，但是功能很强大，值得学习。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"Group type Group struct { mu sync.Mutex // protects m m map[string]*call // lazily initialized } Group 结构体由一个互斥锁和一个 map 组成，可以看到注释 map 是懒加载的，所以 Group 只要声明就可以使用，不用进行额外的初始化零值就可以直接使用。 type call struct { wg sync.WaitGroup // 函数返回值和err信息 val interface{} err error // 是否调用了 forget 方法 forgotten bool // 记录这个 key 被分享了多少次 dups int chans []chan\u003c- Result } call 保存了当前调用对应的信息，map 的键就是我们调用 Do 方法传入的 key ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:1","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"Do func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { // 懒加载 g.m = make(map[string]*call) } // 先判断 key 是否已经存在 if c, ok := g.m[key]; ok { // 存在则说明有其他请求在同步执行，本次请求只需要等待即可 c.dups++ g.mu.Unlock() c.wg.Wait() // / 等待最先进来的那个请求执行完成，因为需要完成后才能获取到结果，这里用 wg 来阻塞，避免了手动写一个循环等待的逻辑 // 这里区分 panic 错误和 runtime 的错误，避免出现死锁，后面可以看到为什么这么做 if e, ok := c.err.(*panicError); ok { panic(e) } else if c.err == errGoexit { runtime.Goexit() } // 最后直接从 call 对象中取出数据并返回 return c.val, c.err, true } // 如果 key 不存在则会走到这里 new 一个 call 并执行 c := new(call) c.wg.Add(1) g.m[key] = c // 注意 这里在 Unlock 之前就把 call 写到 m 中了，所以 这部分逻辑只有第一次请求会执行 g.mu.Unlock() // 然后我们调用 doCall 去执行 g.doCall(c, key, fn) return c.val, c.err, c.dups \u003e 0 } ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:2","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"doCall 这个方法比较灵性，通过两个 defer 巧妙的区分了到底是发生了 panic 还是用户主动调用了 runtime.Goexit，具体信息见https://golang.org/cl/134395 func (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) { // 首先这两个 bool 用于标记是否正常返回或者触发了 recover normalReturn := false recovered := false defer func() { // 如果既没有正常执行完毕，又没有 recover 那就说明需要直接退出了 if !normalReturn \u0026\u0026 !recovered { c.err = errGoexit } c.wg.Done() // 这里 done 之后前面的所有 wait 都会返回了 g.mu.Lock() defer g.mu.Unlock() // forgotten 默认值就是 false，所以默认就会调用 delete 移除掉 m 中的 key if !c.forgotten { // 然后这里也很巧妙，前面先调用了 done，于是所有等待的请求都返回了，那么这个c也没有用了，所以直接 delete 把这个 key 删掉，让后续的请求能再次触发 doCall，而不是直接从 m 中获取结果返回。 delete(g.m, key) } if e, ok := c.err.(*panicError); ok { // 如果返回的是 panic 错误，为了避免 channel 死锁，我们需要确保这个 panic 无法被恢复 if len(c.chans) \u003e 0 { go panic(e) select {} // Keep this goroutine around so that it will appear in the crash dump. } else { panic(e) } } else if c.err == errGoexit { // 如果是exitError就直接退出 } else { // 这里就是正常逻辑了,往 channel 里写入数据 for _, ch := range c.chans { ch \u003c- Result{c.val, c.err, c.dups \u003e 0} } } }() func() { // 使用匿名函数，保证下面的 defer 能在上一个defer之前执行 defer func() { // 如果不是正常退出那肯定是 panic 了 if !normalReturn { // 如果 panic 了我们就 recover 掉，然后 new 一个 panic 的错误后面在上层重新 panic if r := recover(); r != nil { c.err = newPanicError(r) } } }() c.val, c.err = fn() // 如果我们传入的 fn 正常执行了 normalReturn 肯定会被修改为 true // 所以 defer 里可以通过这个标记来判定是否 panic 了 normalReturn = true }() // 如果 normalReturn 为 false 就表示，我们的 fn panic 了 // 如果执行到了这一步，也说明我们的 fn 也被 recover 住了，不是直接 runtime exit if !normalReturn { recovered = true } } 逻辑还是比较复杂，我们分开来看，简化后代码如下： func main() { defer func() { fmt.Println(\"defer 1\") }() func() { defer func() { fmt.Println(\"defer 2\") }() fmt.Println(\"fn\") }() fmt.Println(\"根据 normalReturn 标记给 recover 赋值\") } 第一个点就是匿名函数：使用匿名函数，保证 defer 2 能在上一个 defer 1 之前执行。 因为 defer 1里面需要用到 normalReturn 标记，而这个标记又是在 defer2 中 处理的。同时为了捕获 fn 里的 panic 又必须使用 defer 来处理，所以用了一个匿名函数。 Go 中的 defer 是先进后出的，所以必须用 匿名函数保证 defer2 和 defer1 不在一个 函数里，这样 defer 2就可以先执行了。 正常执行顺序为: fn–\u003edefer2–\u003e根据 normalReturn 标记给 recover 赋值–\u003edefer1 第二个就是用双重 defer 区分 panic 和 runtime.Goexit。 fn 正常执行后就会将 normalReturn 赋值为 true，然后 defer2 里根据 normalReturn 值判断 fn 是否 panic，如果 panic 了就进行 recover 捕获掉这个panic，然后把error替换为自定义的 panicError。 并且根据 normalReturn 的值来对 recovered 标记进行赋值。 最后第一个 defer 就可以根据 normalReturn + recovered 这两个标记和 err 是否为 panicError 来判断是 fn 里发生了 panic 还是说调用了 runtime.Goexit。 第三个点就是 map 的移除： c.wg.Done() g.mu.Lock() defer g.mu.Unlock() if !c.forgotten { delete(g.m, key) } 光看这里看不出具体细节，需要结合前面 Do 方法中的这段逻辑 if c, ok := g.m[key]; ok { c.dups++ g.mu.Unlock() c.wg.Wait() return c.val, c.err, true } 首先doCall 中调用了c.wg.Done(),然后 Do 中的阻塞在c.wg.Wait() 这里的大量请求就全部返回了，直接就 return 了。 然后 doCall 中再调用delete(g.m, key) 把 key 从 m 中移除掉。 通过这个done巧妙的让 Do 中的wait返回后直接把 key 移除掉，这样后续使用同样 key 的请求在执行c, ok := g.m[key]判断时就会重新调用 doCall 方法，再执行一次 fn 了。 如果不移除就会导致后续请求直接从 m 这里取到数据返回了，根本不会执行 fn 去db中查最新的数据，而且 m 中的数据也会越堆积越多。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:3","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"DoChan 和 do 唯一的区别是 go g.doCall(c, key, fn),但对起了一个 goroutine 来执行，并通过 channel 来返回数据，这样外部可以自定义超时逻辑，防止因为 fn 的阻塞，导致大量请求都被阻塞。 func (g *Group) DoChan(key string, fn func() (interface{}, error)) \u003c-chan Result { ch := make(chan Result, 1) g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.m[key]; ok { c.dups++ c.chans = append(c.chans, ch) g.mu.Unlock() return ch } c := \u0026call{chans: []chan\u003c- Result{ch}} c.wg.Add(1) g.m[key] = c g.mu.Unlock() go g.doCall(c, key, fn) return ch } ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:4","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"Forget 手动移除某个 key，让后续请求能走 doCall 的逻辑，而不是直接阻塞。 func (g *Group) Forget(key string) { g.mu.Lock() if c, ok := g.m[key]; ok { c.forgotten = true } delete(g.m, key) g.mu.Unlock() } ","date":"2021-07-16","objectID":"/posts/go/singleflight/:3:5","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"4. 注意事项 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:4:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"1. 阻塞 singleflight 内部使用 waitGroup 来让同一个 key 的除了第一个请求的后续所有请求都阻塞。直到第一个请求执行 fn 返回后，其他请求才会返回。 这意味着，如果 fn 执行需要很长时间，那么后面的所有请求都会被一直阻塞。 这时候我们可以使用 DoChan 结合 ctx + select 做超时控制 func loadChan(ctx context.Context,key string) (string, error) { data, err := loadFromCache(key) if err != nil \u0026\u0026 err == ErrCacheMiss { // 使用 DoChan 结合 select 做超时控制 result := g.DoChan(key, func() (interface{}, error) { data, err := loadFromDB(key) if err != nil { return nil, err } setCache(key, data) return data, nil }) select { case r := \u003c-result: return r.Val.(string), r.Err case \u003c-ctx.Done(): return \"\", ctx.Err() } } return data, nil } ","date":"2021-07-16","objectID":"/posts/go/singleflight/:4:1","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"2. 请求失败 singleflight 的实现为，如果第一个请求失败了，那么后续所有等待的请求都会返回同一个 error。 实际上可以根据下游能支撑的 rps 定时 forget 一下 key，让更多的请求能有机会走到后续逻辑。 go func() { time.Sleep(100 * time.Millisecond) g.Forget(key) }() 比如1秒内有100个请求过来，正常是第一个请求能执行queryDB，后续99个都会阻塞。 增加这个 Forget 之后，每 100ms 就能有一个请求执行 queryDB，相当于是多了几次尝试的机会，相对的也给DB造成了更大的压力，需要根据具体场景进去取舍。 ","date":"2021-07-16","objectID":"/posts/go/singleflight/:4:2","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["Golang"],"content":"5. 参考 https://pkg.go.dev/golang.org/x/sync/singleflight https://golang.org/cl/134395 https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-sync-primitives/#singleflight https://lailin.xyz/post/go-training-week5-singleflight.html ","date":"2021-07-16","objectID":"/posts/go/singleflight/:5:0","tags":["Golang"],"title":"Go语言之防缓存穿透利器Singleflight","uri":"/posts/go/singleflight/"},{"categories":["etcd"],"content":"`docker-compose`来搭建`etcd`，包括单节点和集群模式","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"本文主要记录了 etcd 一个写请求的完整执行流程。主要包括 Quota模块、KVServer模块、WAL模块、Apply模块和MVCC模块。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:0:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"1. 概述 以一个简单的写请求为例： # 设置环境变量 指定使用 v3 版本API 否则可能会出现找不到 put 命令的情况 $ export ETCDCTL_API=3 $ etcdctl put hello world ok 具体流程如下图所示： 1）首先 client 端通过负载均衡算法选择一个 etcd 节点，发起 gRPC 调用； 2）然后 etcd 节点收到请求后经过 gRPC 拦截器、Quota 模块后，进入 KVServer 模块； 3）KVServer 模块向 Raft 模块提交一个提案，提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”。 4）随后此提案通过 RaftHTTP 网络模块转发、经过集群多数节点持久化后，状态会变成已提交； 5）etcdserver 从 Raft 模块获取已提交的日志条目，传递给 Apply 模块 6）Apply 模块通过 MVCC 模块执行提案内容，更新状态机。 与读流程不一样的是写流程还涉及 Quota、WAL、Apply 三个模块。etcd 的 crash-safe 及幂等性也正是基于 WAL 和 Apply 流程的 consistent index 等实现的。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:1:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"2. Quota 模块 Quota 模块主要用于检查下当前 etcd db 大小加上你请求的 key-value 大小之和是否超过了配额（quota-backend-bytes）。 如果超过了配额，它会产生一个告警（Alarm）请求，告警类型是 NO SPACE，并通过 Raft 日志同步给其它节点，告知 db 无空间了，并将告警持久化存储到 db 中。 最终，无论是 API 层 gRPC 模块还是负责将 Raft 侧已提交的日志条目应用到状态机的 Apply 模块，都拒绝写入，集群只读。 常见的 “etcdserver: mvcc: database space exceeded\" 错误就是因为Quota 模块检测到 db 大小超限导致的。 一方面默认 db 配额仅为 2G，当你的业务数据、写入 QPS、Kubernetes 集群规模增大后，你的 etcd db 大小就可能会超过 2G。 另一方面 etcd 是个 MVCC 数据库，保存了 key 的历史版本，当你未配置压缩策略的时候，随着数据不断写入，db 大小会不断增大，导致超限。 解决办法 1）首先当然是调大配额，etcd 社区建议不超过 8G。 如果填的是个小于 0 的数，就会禁用配额功能，这可能会让db 大小处于失控，导致性能下降，不建议你禁用配额。 2）检查 etcd 的压缩（compact）配置是否开启、配置是否合理。 压缩时只会给旧版本Key打上空闲（Free）标记，后续新的数据写入的时候可复用这块空间，db大小并不会减小。 如果需要回收空间，减少 db 大小，得使用碎片整理（defrag）， 它会遍历旧的 db 文件数据，写入到一个新的 db 文件。但是它对服务性能有较大影响，不建议你在生产集群频繁使用。 调整后还需要手动发送一个取消告警（etcdctl alarm disarm）的命令，以消除所有告警，否则因为告警的存在，集群还是无法写入。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:2:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"3. KVServer 模块 通过流程二的配额检查后，请求就从 API 层转发到了流程三的 KVServer 模块的 put 方法。 KVServer 模块主要功能为 1）打包提案：将 put 写请求内容打包成一个提案消息，提交给 Raft 模块 2）请求限速、检查：不过在提交提案前，还有如下的一系列检查和限速，限速、鉴权和大包检查。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:3:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"Preflight Check 为了保证集群稳定性，避免雪崩，任何提交到 Raft 模块的请求，都会做一些简单的限速判断。 限速 如果 Raft 模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了 5000，那么它就返回一个\"etcdserver: too many requests\"错误给 client。 鉴权 然后它会尝试去获取请求中的鉴权信息，若使用了密码鉴权、请求中携带了 token，如果 token 无效，则返回\"auth: invalid auth token\"错误给 client。 大包检查 其次它会检查你写入的包大小是否超过默认的 1.5MB， 如果超过了会返回\"etcdserver: request is too large\"错误给给 client。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:3:1","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"Propose 通过检查后会生成一个唯一的 ID，将此请求关联到一个对应的消息通知 channel（用于接收结果），然后向 Raft 模块发起（Propose）一个提案（Proposal） 向 Raft 模块发起提案后，KVServer 模块会等待此 put 请求，等待写入结果通过消息通知 channel 返回或者超时。etcd 默认超时时间是 7 秒（5 秒磁盘 IO 延时 +2*1 秒竞选超时时间），如果一个请求超时未返回结果，则可能会出现你熟悉的 etcdserver: request timed out 错误。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:3:2","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"4. WAL 模块 Raft 模块收到提案后，如果当前节点是 Follower，它会转发给 Leader，只有 Leader 才能处理写请求。 Leader 收到提案后，通过 Raft 模块输出待转发给 Follower 节点的消息和待持久化的日志条目，日志条目则封装了我们上面所说的 put hello 提案内容。 etcdserver 从 Raft 模块获取到以上消息和日志条目后，作为 Leader，它会将 put 提案消息广播给集群各个节点，同时需要把集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到一个 WAL（Write Ahead Log）日志文件中，用于保证集群的一致性、可恢复性，也就是我们图中的流程五模块。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:4:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"WAL 日志结构 WAL 日志结构如下： WAL 文件它由多种类型的 WAL 记录顺序追加写入组成，每个记录由类型、数据、循环冗余校验码组成。不同类型的记录通过 Type 字段区分，Data 为对应记录内容，CRC 为循环校验码信息。 WAL 记录类型目前支持 5 种，分别是文件元数据记录、日志条目记录、状态信息记录、CRC 记录、快照记录： 1）文件元数据记录，包含节点 ID、集群 ID 信息，它在 WAL 文件创建的时候写入； 2）日志条目记录：包含 Raft 日志信息，如 put 提案内容； 3）状态信息记录，包含集群的任期号、节点投票信息等，一个日志文件中会有多条，以最后的记录为准； 4）CRC 记录，包含上一个 WAL 文件的最后的 CRC（循环冗余校验码）信息， 在创建、切割 WAL 文件时，作为第一条记录写入到新的 WAL 文件， 用于校验数据文件的完整性、准确性等； 5）快照记录，包含快照的任期号、日志索引信息，用于检查快照文件的准确性。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:4:1","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"WAL 持久化 首先会将 put 请求封装成一个 Raft 日志条目，Raft 日志条目的数据结构信息如下： type Entry struct { Term uint64 `protobuf:\"varint，2，opt，name=Term\" json:\"Term\"` Index uint64 `protobuf:\"varint，3，opt，name=Index\" json:\"Index\"` Type EntryType `protobuf:\"varint，1，opt，name=Type，enum=Raftpb.EntryType\" json:\"Type\"` Data []byte `protobuf:\"bytes，4，opt，name=Data\" json:\"Data，omitempty\"` } 它由以下字段组成： Term 是 Leader 任期号，随着 Leader 选举增加； Index 是日志条目的索引，单调递增增加； Type 是日志类型，比如是普通的命令日志（EntryNormal）还是集群配置变更日志（EntryConfChange）； Data 保存我们上面描述的 put 提案内容。 具体持久化过程如下： 1）它首先先将 Raft 日志条目内容（含任期号、索引、提案内容）序列化后保存到 WAL 记录的 Data 字段， 然后计算 Data 的 CRC 值，设置 Type 为 Entry Type， 以上信息就组成了一个完整的 WAL 记录。 2）最后计算 WAL 记录的长度，顺序先写入 WAL 长度（Len Field），然后写入记录内容，调用 fsync 持久化到磁盘，完成将日志条目保存到持久化存储中。 3）当一半以上节点持久化此日志条目后， Raft 模块就会通过 channel 告知 etcdserver 模块，put 提案已经被集群多数节点确认，提案状态为已提交，你可以执行此提案内容了。 4）于是进入流程六，etcdserver 模块从 channel 取出提案内容，添加到先进先出（FIFO）调度队列，随后通过 Apply 模块按入队顺序，异步、依次执行提案内容。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:4:2","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"5. Apply 模块 Apply 模块主要用于执行处于 已提交状态的提案，将其更新到状态机。 Apply 模块在执行提案内容前，首先会判断当前提案是否已经执行过了，如果执行了则直接返回，若未执行同时无 db 配额满告警，则进入到 MVCC 模块，开始与持久化存储模块打交道。 如果执行过程中 crash，重启后如何找回异常提案，再次执行的呢？ 主要依赖 WAL 日志，因为提交给 Apply 模块执行的提案已获得多数节点确认、持久化，etcd 重启时，会从 WAL 中解析出 Raft 日志条目内容，追加到 Raft 日志的存储中，并重放已提交的日志提案给 Apply 模块执行。 重启恢复时，如何确保幂等性，防止提案重复执行导致数据混乱呢? etcd 通过引入一个 consistent index 的字段，来存储系统当前已经执行过的日志条目索引，实现幂等性。 因为 Raft 日志条目中的索引（index）字段，而且是全局单调递增的，每个日志条目索引对应一个提案。 如果一个命令执行后，我们在 db 里面也记录下当前已经执行过的日志条目索引，就可以解决幂等性问题了。 当然还需要将执行命令和记录index这两个操作作为原子性事务提交，才能实现幂等。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:5:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"6. MVCC 模块 MVCC 主要由两部分组成，一个是内存索引模块 treeIndex，保存 key 的历史版本号信息，另一个是 boltdb 模块，用来持久化存储 key-value 数据。 MVCC 模块执行 put hello 为 world 命令时，它是如何构建内存索引和保存哪些数据到 db 呢？ ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:6:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"treeIndex MVCC 写事务在执行 put hello 为 world 的请求时，会基于 currentRevision 自增生成新的 revision 如{2,0}，然后从 treeIndex 模块中查询 key 的创建版本号、修改次数信息。这些信息将填充到 boltdb 的 value 中，同时将用户的 hello key 和 revision 等信息存储到 B-tree，也就是下面简易写事务图的流程一，整体架构图中的流程八。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:6:1","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"boltdb MVCC 写事务自增全局版本号后生成的 revision{2,0}，它就是 boltdb 的 key，通过它就可以往 boltdb 写数据了，进入了整体架构图中的流程九。 那么写入 boltdb 的 value 含有哪些信息呢？ 写入 boltdb 的 value， 并不是简单的\"world\"，如果只存一个用户 value，索引又是保存在易失的内存上，那重启 etcd 后，我们就丢失了用户的 key 名，无法构建 treeIndex 模块了。 因此为了构建索引和支持 Lease 等特性，etcd 会持久化以下信息: key 名称； key 创建时的版本号（create_revision）、最后一次修改时的版本号（mod_revision）、key 自身修改的次数（version）； value 值； 租约信息。 boltdb value 的值就是将含以上信息的结构体序列化成的二进制数据，然后通过 boltdb 提供的 put 接口，etcd 就快速完成了将你的数据写入 boltdb。 注意：在以上流程中，etcd 并未提交事务（commit），因此数据只更新在 boltdb 所管理的内存数据结构中。 事务提交的过程，包含 B+tree 的平衡、分裂，将 boltdb 的脏数据（dirty page）、元数据信息刷新到磁盘，因此事务提交的开销是昂贵的。如果我们每次更新都提交事务，etcd 写性能就会较差。 etcd 的解决方案是合并再合并： 首先 boltdb key 是版本号，put/delete 操作时，都会基于当前版本号递增生成新的版本号，因此属于顺序写入，可以调整 boltdb 的 bucket.FillPercent 参数，使每个 page 填充更多数据，减少 page 的分裂次数并降低 db 空间。 其次 etcd 通过合并多个写事务请求，通常情况下，是异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）， 从而大大提高吞吐量 但是这优化又引发了另外的一个问题， 因为事务未提交，读请求可能无法从 boltdb 获取到最新数据。 为了解决这个问题，etcd 引入了一个 bucket buffer 来保存暂未提交的事务数据。在更新 boltdb 的时候，etcd 也会同步数据到 bucket buffer。因此 etcd 处理读请求的时候会优先从 bucket buffer 里面读取，其次再从 boltdb 读，通过 bucket buffer 实现读写性能提升，同时保证数据一致性。 这里和 MySQL 很类似，更新时也是优先写入 Buffer。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:6:2","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"7. 小结 1） Quota 模块工作原理和我们熟悉的 database space exceeded 错误触发原因 2）WAL 模块的存储结构，它由一条条记录顺序写入组成，每个记录含有 Type、CRC、Data，每个提案被提交前都会被持久化到 WAL 文件中，以保证集群的一致性和可恢复性。 3）Apply 模块基于 consistent index 和事务实现了幂等性，保证了节点在异常情况下不会重复执行重放的提案。 4）MVCC 模块是如何维护索引版本号、重启后如何从 boltdb 模块中获取内存索引结构的。 5）etcd 通过异步、批量提交事务机制，以提升写 QPS 和吞吐量。 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:7:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"8. 参考 https://github.com/etcd-io/etcd https://etcd.io/docs/v3.4/learning/ 《etcd实战课》 ","date":"2021-07-09","objectID":"/posts/etcd/08-write-process/:8:0","tags":["etcd"],"title":"etcd教程(八)---写请求执行流程分析","uri":"/posts/etcd/08-write-process/"},{"categories":["etcd"],"content":"`etcd 读请求具体执行流程分析","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"本文主要分析了 etcd 中一个读请求的具体执行流程。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:0:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"1. 概述 下面是一张 etcd 的简要基础架构图 按照分层模型，etcd 可分为 Client 层、API 网络层、Raft 算法层、逻辑层和存储层。这些层的功能如下： Client 层：Client 层包括 client v2 和 v3 两个大版本 API 客户端库，提供了简洁易用的 API，同时支持负载均衡、节点间故障自动转移，可极大降低业务使用 etcd 复杂度，提升开发效率、服务可用性。 API 网络层：API 网络层主要包括 client 访问 server 和 server 节点之间的通信协议。一方面，client 访问 etcd server 的 API 分为 v2 和 v3 两个大版本。v2 API 使用 HTTP/1.x 协议，v3 API 使用 gRPC 协议。同时 v3 通过 etcd grpc-gateway 组件也支持 HTTP/1.x 协议，便于各种语言的服务调用。另一方面，server 之间通信协议，是指节点间通过 Raft 算法实现数据复制和 Leader 选举等功能时使用的 HTTP 协议。 Raft 算法层：Raft 算法层实现了 Leader 选举、日志复制、ReadIndex 等核心算法特性，用于保障 etcd 多个节点间的数据一致性、提升服务可用性等，是 etcd 的基石和亮点。 功能逻辑层：etcd 核心特性实现层，如典型的 KVServer 模块、MVCC 模块、Auth 鉴权模块、Lease 租约模块、Compactor 压缩模块等，其中 MVCC 模块主要由 treeIndex 模块和 boltdb 模块组成。 存储层：存储层包含预写日志 (WAL) 模块、快照 (Snapshot) 模块、boltdb 模块。其中 WAL 可保障 etcd crash 后数据不丢失，boltdb 则保存了集群元数据和用户写入的数据。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:1:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2. 读请求流程 具体流程如下图所示： 以下面的命令进行分析： # --endpoint=http://127.0.0.1:2379 用于指定后端的 etcd 地址 /usr/local/bin # etcdctl --endpoint=http://127.0.0.1:2379 put hello world ok /usr/local/bin # --endpoint=http://127.0.0.1:2379 get hello world ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.1 Client 1）首先，etcdctl 会对命令中的参数进行解析。 “get”是请求的方法，它是 KVServer 模块的 API； “hello”是我们查询的 key 名； “endpoints”是我们后端的 etcd 地址。 通常，生产环境下中需要配置多个 endpoints，这样在 etcd 节点出现故障后，client 就可以自动重连到其它正常的节点，从而保证请求的正常执行。 2）在解析完请求中的参数后，etcdctl 会创建一个 clientv3 库对象，使用 KVServer 模块的 API 来访问 etcd server。 etcd clientv3 库采用的负载均衡算法为 Round-robin。针对每一个请求，Round-robin 算法通过轮询的方式依次从 endpoint 列表中选择一个 endpoint 访问 (长连接)，使 etcd server 负载尽量均衡。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:1","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.2 KVServer 与 拦截器 client 发送 Range RPC 请求到了 server 后就进入了 KVServer 模块。 etcd 通过拦截器以非侵入式的方式实现了许多特性，例如：丰富的 metrics、日志、请求行为检查、所有请求的执行耗时及错误码、来源 IP 等。 拦截器提供了在执行一个请求前后的 hook 能力，除了 debug 日志、metrics 统计、对 etcd Learner 节点请求接口和参数限制等能力，etcd 还基于它实现了以下特性: 1）要求执行一个操作前集群必须有 Leader； 2）请求延时超过指定阈值的，打印包含来源 IP 的慢查询日志 (3.5 版本)。 server 收到 client 的 Range RPC 请求后，根据 ServiceName 和 RPC Method 将请求转发到对应的 handler 实现，handler 首先会将上面描述的一系列拦截器串联成一个拦截器再执行（具体实现见这里），在拦截器逻辑中，通过调用 KVServer 模块的 Range 接口获取数据。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:2","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.3 串行读与线性读 etcd 为了保证服务高可用，生产环境一般部署多个节点，多节点之间的数据由于延迟等关系可能会存在不一致的情况。 当 client 发起一个写请求后分为以下几个步骤： 1）Leader 收到写请求，它会将此请求持久化到 WAL 日志，并广播给各个节点； 只有 Leader 节点能处理写请求。 2）若一半以上节点持久化成功，则该请求对应的日志条目被标识为已提交； 3）etcdserver 模块异步从 Raft 模块获取已提交的日志条目，应用到状态机 (boltdb 等)。 可以看出在多节点 etcd 集群中，各个节点的状态机数据一致性存在差异。 根据业务场景对数据一致性差异的接受程度，etcd 中有两种读模式。 1）串行 (Serializable) 读：直接读状态机数据返回、无需通过 Raft 协议与集群进行交互，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。 2）线性读：需要经过 Raft 协议模块，反应的是集群共识，因此在延时和吞吐量上相比串行读略差一点，适用于对数据一致性要求高的场景。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:3","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.4 ReadIndex 在 etcd 3.1 时引入了 ReadIndex 机制，保证在串行读的时候，也能读到最新的数据。 具体流程如下： 1）当 Follower 节点 收到一个线性读请求时，它首先会从 Leader 获取集群最新的已提交的日志索引 (committed index) Leader 收到 ReadIndex 请求时，为防止脑裂等异常场景，会向 Follower 节点发送心跳确认，一半以上节点确认 Leader 身份后才能将已提交的索引 (committed index) 返回给请求节点。 2）Follower 节点拿到 read index 后会和状态机的 applied index进行比较，如果 read index 大于 applied index 则会等待，直到状态机已应用索引 (applied index) 大于等于 Leader 的已提交索引时 (committed Index)才会去通知读请求，数据已赶上 Leader，你可以去状态机中访问数据了 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:4","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.5 MVCC MVCC 即多版本并发控制 (Multiversion concurrency control) ，MVCC模块是为了解决 etcd v2 不支持保存 key 的历史版本、不支持多 key 事务等问题而产生的。 它核心由内存树形索引模块 (treeIndex) 和嵌入式的 KV 持久化存储库 boltdb 组成。 etcd MVCC 具体方案如下： 每次修改操作，生成一个新的版本号 (revision)，以版本号为 key， value 为用户 key-value 等信息组成的结构体存储到 blotdb。 读取时先从 treeIndex 中获取 key 的版本号，再以版本号作为 boltdb 的 key，从 boltdb 中获取其 value 信息。 treeIndex treeIndex 模块是基于 Google 开源的内存版 btree 库实现的，treeIndex 模块只会保存用户的 key 和相关版本号信息，用户 key 的 value 数据存储在 boltdb 里面，所以对内存要求相对较低。 buffer 在获取到版本号信息后，就可从 boltdb 模块中获取用户的 key-value 数据了。 etcd 出于数据一致性、性能等考虑，在访问 boltdb 前，首先会从一个内存读事务 buffer 中，二分查找你要访问 key 是否在 buffer 里面，若命中则直接返回。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:5","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"2.6 boltdb 若 buffer 未命中，此时就真正需要向 boltdb 模块查询数据了。 boltdb 通过 bucket 隔离集群元数据与用户数据。 boltdb 使用 B+ tree 来组织用户的 key-value 数据，获取 bucket key 对象后，通过 boltdb 的游标 Cursor 可快速在 B+ tree 找到 key hello 对应的 value 数据，返回给 client。 到这里，一个读请求之路执行完成。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:2:6","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"3. FAQ Q：readIndex 需要请求 leader，那为什么不直接让 leader 返回读请求的结果？ A：主要是性能因素，如果将所有读请求都转发到 Leader，会导致 Leader 负载升高，内存、cpu、网络带宽资源都很容易耗尽。特别是expensive request场景，会让 Leader 节点性能会急剧下降。read index 机制的引入，使得每个follower节点都可以处理读请求，极大扩展提升了写性能。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:3:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"4. 小结 一个读请求从 client 通过 Round-robin 负载均衡算法，选择一个 etcd server 节点，发出 gRPC 请求，经过 etcd server 的 KVServer 模块、线性读模块、MVCC 的 treeIndex 和 boltdb 模块紧密协作，完成了一个读请求。 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:4:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["etcd"],"content":"5. 参考 https://en.wikipedia.org/wiki/Serializability https://github.com/etcd-io/etcd https://etcd.io/docs/v3.4/learning/ 《etcd实战课》 ","date":"2021-07-02","objectID":"/posts/etcd/07-read-process/:5:0","tags":["etcd"],"title":"etcd教程(七)---读请求执行流程分析","uri":"/posts/etcd/07-read-process/"},{"categories":["Kubernetes"],"content":"Kubernetes Deployment 详解","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"本文主要讲解了 Kubernetes 中最常见的控制器 Deployment。 ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"1. 概述 看完之前对 Pod 相关文章后，你应该知道了Pod 这个看似复杂的 API 对象，实际上就是对容器的进一步抽象和封装而已 Kubernetes系列教程(七)—Pod 之(1) 为什么需要 Pod Kubernetes系列教程(七)—Pod 之(2) Pod 基本概念与生命周期 Deployment 是 Kubernetes 中最常见的控制器，实际上它是一个两层控制器。 首先，它通过 ReplicaSet 的个数来描述应用的版本； 然后，它再通过 ReplicaSet 的属性（比如 replicas 的值），来保证 Pod 的副本数量。 注：Deployment 控制 ReplicaSet（版本），ReplicaSet 控制 Pod（副本数）。这个两层控制关系一定要牢记。 Deployment 是 Kubernetes 编排能力的一种提现，通过 Deployment 我们可以让 Pod 稳定的维持在指定的数量，除此之外还有滚动更新、版本回滚等功能。 ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"2. Controller 前面介绍 Kubernetes 架构的时候，曾经提到过一个叫作 kube-controller-manager 的组件。实际上，这个组件，就是一系列控制器的集合。我们可以查看一下 Kubernetes 项目的 pkg/controller 目录： $ cd kubernetes/pkg/controller/ $ ls -d */ deployment/ job/ podautoscaler/ cloud/ disruption/ namespace/ replicaset/ serviceaccount/ volume/ cronjob/ garbagecollector/ nodelifecycle/ replication/ statefulset/ daemon/ ... 这个目录下面的每一个控制器，都以独有的方式负责某种编排功能。而我们的 Deployment，正是这些控制器中的一种。 实际上，这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。 // 伪代码如下 for { 实际状态 := 获取集群中对象X的实际状态（Actual State） 期望状态 := 获取集群中对象X的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } } 具体实现中，实际状态往往来自于 Kubernetes 集群本身。而期望状态，一般来自于用户提交的 YAML 文件。 ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"3. Deployment 回顾一下nginx的例子 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 这个 Deployment 定义的编排动作非常简单，即：确保携带了 app=nginx 标签的 Pod 的个数，永远等于 spec.replicas 指定的个数，即 2 个。 这就意味着，如果在这个集群中，携带 app=nginx 标签的 Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建。 接下来，以 Deployment 为例，我和你简单描述一下它对控制器模型的实现： 1）Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态； 2）Deployment 对象的 Replicas 字段的值就是期望状态； 3）Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod。 可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作**“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）**。 而调谐的最终结果，往往都是对被控制对象的某种写操作。比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。 这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。 其实，像 Deployment 这种控制器的设计原理，就是 用一种对象管理另一种对象”的“艺术”。 其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。 而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。 像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。 类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。 Kubernetes 使用的这个“控制器模式”，跟我们平常所说的“事件驱动”，有什么区别和联系呢？ 事件往往是一次性的，如果操作失败比较难处理，但是控制器是循环一直在尝试的，最终达到一致，更符合kubernetes 声明式API。 ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"4. ReplicaSet Deployment 看似简单，但实际上，它实现了 Kubernetes 项目中一个非常重要的功能：Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）。 举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。 而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。 --- apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 从这个 YAML 文件中，我们可以看到，一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的。不难发现，它的定义其实是 Deployment 的一个子集。 更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。 具体如图所示: ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义。 伸缩 $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"滚动更新 将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。 先将新版本的V2从0个扩容到1个Pod，接着将旧版本的V1 从3个缩容到2个，这样慢慢的最后V1缩为0个，V2扩到3个。 滚动更新好处就是，即使V2版本出现异常，此时也会有两个V1版本在运行，然后用户可以手动处理这种情况，比如停止更新或者回滚到V1版本 如上所示，Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet 的属性。而一个应用的版本，对应的正是一个 ReplicaSet； 这个版本应用的 Pod 数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。通过这样的多个 ReplicaSet 对象，Kubernetes 项目就实现了对多个“应用版本”的描述。 ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:4:1","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"回滚 首先，我需要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本 $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了。 $ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:4:2","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"5. 参考 深入剖析Kubernetes https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ ","date":"2021-06-22","objectID":"/posts/kubernetes/08-deployment/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(八)---Deployment","uri":"/posts/kubernetes/08-deployment/"},{"categories":["Kubernetes"],"content":"Kubernetes 项目 Pod 的概念及其生命周期","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"本文主要讲述了 Kubernetes 中 Pod 的基本概念及其生命周期（lifecycle）。包括 Pod 的启动、容器探针、健康检测、恢复机制即 Pod 的终止等等。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"1. Pod 基本概念 Pod 扮演的是传统部署环境里“虚拟机”的角色。所以凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。 这些属性的共同特征是，它们描述的是“机器”这个整体。 NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段。 apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd NodeName：Pod具体调度到的节点。 一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。 HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容 apiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \"10.1.2.3\" hostnames: - \"foo.remote\" - \"bar.remote\" ... 这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示： cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote 最后两行就是通过 hostnames 指定的。 如果要修改hostname 一定要通过HostAliases指定 凡是跟容器的 Linux Namespace 相关的属性和Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的。 这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。 比如下面的Pod定义： Pod 中的容器共享 PID Namespace，而且会共享宿主机的 Network、IPC 和 PID Namespace。 --- apiVersion: v1 kind: Pod metadata: name: nginx spec: # 共享PID Namespace shareProcessNamespace: true # 共享宿主机的 Network、IPC 和 PID Namespace hostNetwork: true hostIPC: true hostPID: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true Containers Pod 中最重要的字段当然是Containers了。 首先，是 ImagePullPolicy 字段，它定义了镜像拉取的策略，默认是 Always即每次创建 Pod 都重新拉取一次镜像。 其次，是 Lifecycle 字段，它定义的是 Container Lifecycle Hooks，在容器状态发生变化时触发一系列“钩子”。 --- apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler \u003e /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] postStart 容器启动后通过 echo 命令写入一段欢迎信息。 执行在 Docker 容器 ENTRYPOINT 执行之后，但不保证严格顺序，也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。 preStop 容器被杀死之前（比如，收到了 SIGKILL 信号）调用 nginx 退出命令，优雅停止。 而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。 节点上的 kubelet 将等待最多宽限期（在 Pod 上指定，或从命令行传递；默认为 30 秒）以关闭容器，然后强行终止进程（使用 SIGKILL）。请注意，此宽限期包括执行 preStop 勾子的时间。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"2. Pod 生命周期与阶段 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"1. Pod 生命周期 Pod 遵循一个预定义的生命周期，起始于 Pending 阶段，如果至少 其中有一个主要容器正常启动，则进入 Running，之后取决于 Pod 中是否有容器以 失败状态结束而进入 Succeeded 或者 Failed 阶段。 在 Pod 运行期间，kubelet 能够重启容器以处理一些失效场景。 在 Pod 内部，Kubernetes 跟踪不同容器的状态并确定使 Pod 重新变得健康所需要采取的动作。 在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。 Pod 对象的状态包含了一组 Pod 状况（Conditions）。 如果应用需要的话，你也可以向其中注入自定义的就绪性信息。 Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者 被终止。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"2. Pod 阶段 Pod 生命周期的变化，主要体现在 Pod API 对象的 Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况： Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。 Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。 Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。 Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。 更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。 比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"3. 容器探针 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"1. 健康检查 在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器镜像是否运行（来自 Docker 返回的信息）作为依据。 这种机制，是生产环境中保证应用健康存活的重要手段。 我们一起来看一个 Kubernetes 文档中的例子: --- apiVersion: v1 kind: Pod metadata: labels: test: liveness name: test-liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。 与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一条我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。 除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下： ... livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 ... livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 所以，你的 Pod 其实可以暴露一个健康检查 URL（比如 /healthz），或者直接让健康检查去检测应用的监听端口。这两种配置方法，在 Web 服务类的应用中非常常用。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"2. 恢复机制 Kubernetes 里的 Pod 恢复机制，也叫 restartPolicy。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。 但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。 事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。 这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。 而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。 除了 Always，它还有 OnFailure 和 Never 两种情况： Always：在任何情况下，只要容器不在运行状态，就自动重启容器； OnFailure: 只在容器 异常时才自动重启容器； Never: 从来不重启容器。 在实际使用时，我们需要根据应用运行的特性，合理设置这三种恢复策略。 而如果你要关心这个容器退出后的上下文环境，比如容器退出后的日志、文件和目录，就需要将 restartPolicy 设置为 Never。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"3. PodPreset PodPreset 是一种 K8s API 资源，用于在创建 Pod 时注入其他运行时需要的信息，这些信息包括 secrets、volume mounts、environment variables 等。 可以看做是 Pod 模板。 首先定义一个 PodPreset 对象，把想要的字段都加进去: --- apiVersion: settings.k8s.io/v1alpha1 kind: PodPreset metadata: name: allow-database spec: selector: matchLabels: role: frontend env: - name: DB_PORT value: \"6379\" volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} 通过matchLabels:role: frontend匹配到对应的Pod，然后k8s会自动把PodPreset对象里的预定义的字段添加进去，这里就是env、volumeMounts、volumes3个字段。 然后我们写一个简单的Pod --- apiVersion: v1 kind: Pod metadata: name: website labels: app: website role: frontend spec: containers: - name: website image: nginx ports: - containerPort: 80 其中的 Label role: frontend和PodPreset allow-database 匹配，所以会在创建Pod之前自动把预定义字段添加进去。 需要说明的是，PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:3:3","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"4. Pod 的终止 由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用 KILL 信号终止它们，导致这些进程没有机会 完成清理操作。 具体停止过程大致如下： 1）用户删除 Pod 2）Pod 进入 Terminating 状态; 与此同时，k8s 会从对应的 service 上移除该 Pod 对应的 endpoint 与此同时，针对有 preStop hook 的容器，kubelet 会调用每个容器的 preStop hook，假如 preStop hook 的运行时间超出了 grace period（默认30秒），kubelet 会发送 SIGTERM 并再等 2 秒; 与此同时，针对没有 preStop hook 的容器，kubelet 发送 SIGTERM 3）grace period 超出之后，kubelet 发送 SIGKILL 干掉尚未退出的容器 kubelet 向runtime发送信号，最终runtime会将信号发送给容器中的主进程。 所以在程序中监听该信号可以实现优雅关闭。 ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"5. 参考 https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/ https://kubernetes.io/docs/concepts/workloads/pods/ https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-initialization/ 深入剖析Kubernetes ","date":"2021-06-18","objectID":"/posts/kubernetes/07-pod-2-baselifecycle/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(2) Pod 基本概念与生命周期","uri":"/posts/kubernetes/07-pod-2-baselifecycle/"},{"categories":["Kubernetes"],"content":"Kubernetes 项目为什么要搞出一个 Pod 的概念来","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"本文主要解释了 Kubernetes 中为什么需要新增 Pod 的概念及 Kubernetes 中的 容器设计模式。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"1. Why 回答开篇的问题：为什么 Kubernetes 项目又突然搞出一个 Pod 来呢？ 答：为了更好的管理。 首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。 Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。 Pod 这个看似复杂的 API 对象，实际上就是对容器的进一步抽象和封装而已，其实是一组共享了某些资源的容器。 具体的说：Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"2. 环境共享 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"2.1 Network Namespace 共享 Network Namespace 在 Docker 里也可以实现，通过 docker run --net --volumes-from这样的命令就能实现嘛，比如： $ docker run --net=B --volumes-from=B --name=A image-A 不过这样的前提条件是：容器 B 就必须比容器 A 先启动。 这会导致一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。 所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。 在 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。 在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。 这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。 而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。 具体如下图所示： 图源：深入剖析Kubernetes ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"2.2 Volume 有了 Infra 这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。 这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。 --- apiVersion: v1 kind: Pod metadata: name: two-containers spec: restartPolicy: Never volumes: - name: shared-data hostPath: path: /data containers: - name: nginx-container image: nginx volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html - name: debian-container image: debian volumeMounts: - name: shared-data mountPath: /pod-data command: [\"/bin/sh\"] args: [\"-c\", \"echo Hello from the debian container \u003e /pod-data/index.html\"] 在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"3. 容器设计模式 Pod 这种超亲密关系容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。 即：优先将关联密切的容器运行在一个 Pod 中。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"例一：Java Web 应用 第一个最典型的例子是：WAR 包与 Web 服务器。 我们现在有一个 Java Web 应用的 WAR 包，它需要被放在 Tomcat 的 webapps 目录下运行起来。假如，你现在只能用 Docker 来做这件事情，那该如何处理这个组合关系呢？ 一种方法是，把 WAR 包直接放在 Tomcat 镜像的 webapps 目录下，做成一个新的镜像运行起来。可是，这时候，如果你要更新 WAR 包的内容，或者要升级 Tomcat 镜像，就要重新制作一个新的发布镜像，非常麻烦。 另一种方法是，你压根儿不管 WAR 包，永远只发布一个 Tomcat 容器。不过，这个容器的 webapps 目录，就必须声明一个 hostPath 类型的 Volume，从而把宿主机上的 WAR 包挂载进 Tomcat 容器当中运行起来。 不过，这样你就必须要解决一个问题，即：如何让每一台宿主机，都预先准备好这个存储有 WAR 包的目录呢？ 实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示： --- apiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: initContainers: - image: geektime/sample:v2 name: war command: [\"cp\", \"/sample.war\", \"/app\"] volumeMounts: - mountPath: /app name: app-volume containers: - image: geektime/tomcat:7.0 name: tomcat command: [\"sh\",\"-c\",\"/root/apache-tomcat-7.0.42-v2/bin/start.sh\"] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001 volumes: - name: app-volume emptyDir: {} 在这个 Pod 中，我们定义了两个容器，第一个容器使用的镜像是 war，这个镜像里只有一个 WAR 包（sample.war）放在根目录下。而第二个容器则使用的是一个标准的 Tomcat 镜像。 WAR 包容器的类型不再是一个普通容器，而是一个 Init Container 类型的容器。在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。 所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句\"cp /sample.war /app\"，把应用的 WAR 包拷贝到 /app 目录下，然后退出。而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。 所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。 实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"例二：日志收集 第二个例子，则是容器的日志收集。 比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。 这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。 然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。 这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。 这样，一个最基本的日志收集工作就完成了。 Pod 的另一个重要特性是，它的所有容器都共享同一个 Network Namespace。这就使得很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。 这里最典型的例子莫过于 Istio 这个微服务治理项目了。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"4. 小结 Docker 容器核心实现原理：Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统。 容器设计模式：优先考虑将关联密切的容器运行在一个 Pod 中。 容器的本质是进程； 容器镜像就是这个系统里的“.exe”安装包； Pod 就是进程组； Kubernetes 就是操作系统。 Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。 ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["Kubernetes"],"content":"5. 参考 https://kubernetes.io/docs/concepts/workloads/pods/ https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-initialization/ 深入剖析Kubernetes ","date":"2021-06-11","objectID":"/posts/kubernetes/07-pod-1-why/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(七)---Pod 之(1) 为什么需要 Pod","uri":"/posts/kubernetes/07-pod-1-why/"},{"categories":["gRPC"],"content":"gRPC LoadBalance on Kubernetes ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"本文主要介绍了 Kubernetes 环境中的 gRPC 负载均衡具体实现。 gRPC 系列相关代码见 Github ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"1. 概述 系统中多个服务间的调用用的是 gRPC 进行通信，最初没考虑到负载均衡的问题，因为用的是 Kubernetes，想的是直接用 K8s 的 Service 不就可以实现负载均衡吗。 但是真正测试的时候才发现，所有流量都进入到了某一个 Pod，这时才意识到负载均衡可能出现了问题。 因为 gRPC 是基于 HTTP/2 之上的，而 HTTP/2 被设计为一个长期存在的 TCP 连接，所有都通过该连接进行多路复用。 这样虽然减少了管理连接的开销，但是在负载均衡上又引出了新的问题。 由于我们无法在连接层面进行均衡，为了做 gRPC 负载均衡，我们需要从连接级均衡转向请求级均衡。 换句话说，我们需要打开一个到每个目的地的 HTTP/2 连接，并平衡这些连接之间的请求。 这就意味着我们需要一个 7 层负载均衡，而 K8s 的 Service 核心使用的是 kube proxy，这是一个 4 层负载均衡，所以不能满足我们的要求。 整理了一下大致有以下几种方案： 1）每次都重新建立连接，用完后关闭连接，直接从源头上解决问题。 ？？？这算什么方案哈哈 2）客户端负载均衡 3）服务端负载均衡 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"2. 客户端负载均衡 这也是比较容易实现的方案，具体为：NameResolver + load balancing policy+Headless-Service。 相关教程可以看上一篇文章gRPC系列教程(十二)—客户端负载均衡 1）当 gRPC 客户端想要与 gRPC 服务器进行交互时，它首先尝试通过向 resolver 发出名称解析请求来解析服务器名称，解析程序返回已解析IP地址的列表。 2）Kubernetes Headless-Service 在创建的时候会将该服务对应的每个 Pod IP 以 A 记录的形式存储。 3）常见的 gRPC 库都内置了几个负载均衡算法，比如 gRPC-Go 中内置了pick_first和round_robin两种算法。 pick_first：尝试连接到第一个地址，如果连接成功，则将其用于所有RPC，如果连接失败，则尝试下一个地址（并继续这样做，直到一个连接成功）。 round_robin：连接到它看到的所有地址，并依次向每个后端发送一个RPC。例如，第一个RPC将发送到backend-1，第二个RPC将发送到backend-2，第三个RPC将再次发送到backend-1。 所以建立连接时只需要提供一个服务名即可，gRPC Client 会根据 DNS resolver 返回的 IP 列表分别建立连接，请求时使用 round_robin 算法进行负载均衡，选择其中一个连接用来发起请求。 核心代码如下： svc := \"mygrpc:50051\" ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() conn, err := grpc.DialContext( ctx, fmt.Sprintf(\"%s:///%s\", \"dns\", svc), grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`), // 指定轮询负载均衡算法 grpc.WithInsecure(), grpc.WithBlock(), ) if err != nil { log.Fatal(err) } 主要是配置负载均衡算法： grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`) 网上很多比较旧的文章用的是grpc.WithBalancerName(\"\")，在新版中不推荐使用了。 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"存在的问题 当 Pod 扩缩容时 客户端可以感知到并更新连接吗？ Pod 缩容后，由于 gRPC 具有连接探活机制，会自动丢弃无效连接。 Pod 扩容后，没有感知机制，导致后续扩容的 Pod 无法被请求到。 gRPC 连接默认能永久存活，如果将该值降低能改善这个问题。 在服务端做以下设置 port := conf.GetPort() lis, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer(grpc.KeepaliveParams(keepalive.ServerParameters{ MaxConnectionAge: time.Minute, })) pb.RegisterVerifyServer(s, core.Verify) log.Println(\"Serving gRPC on 0.0.0.0\" + port) if err = s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } 这样每个连接只会使用一分钟，到期后会重新建立连接，相当于对扩容的感知只会延迟 1 分钟。 虽然能用，但是并比是那么完美，强迫症表示完成无法接受这个方案。 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"kuberesolver 为了解决以上问题，很容易想到直接在 client 端调用 Kubernetes API 监测 Service 对应的 endpoints 变化，然后动态更新连接信息。 搜了一下发现 Github 上已经有这个思路的解决方案了：kuberesolver。 // Import the module import \"github.com/sercand/kuberesolver/v3\" // Register kuberesolver to grpc before calling grpc.Dial kuberesolver.RegisterInCluster() // if schema is 'kubernetes' then grpc will use kuberesolver to resolve addresses cc, err := grpc.Dial(\"kubernetes:///service.namespace:portname\", opts...) 具体就是将 DNSresolver 替换成了自定义的 kuberesolver。 同时如果 Kubernetes 集群中使用了 RBAC 授权的话需要给 client 所在Pod赋予 endpoint 资源的 get 和 watch 权限。 具体授权过程如下： 需要分别创建ServiceAccount、Role、RoleBinding3 个实例， k8s 用的也是 RBAC 授权，所以应该比较好理解。 apiVersion: v1 kind: ServiceAccount metadata: namespace: vaptcha name: grpclb-sa kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: vaptcha name: grpclb-role rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"watch\"] kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: grpclb-rolebinding namespace: vaptcha subjects: - kind: ServiceAccount name: grpclb-sa namespace: vaptcha roleRef: kind: Role name: grpclb-role apiGroup: rbac.authorization.k8s.io 创建对象 $ kubectl apply -f svc-account.yaml serviceaccount/example-sa created $ kubectl apply -f role.yaml role.rbac.authorization.k8s.io/example-role created $ kubectl apply -f role-binding.yaml rolebinding.rbac.authorization.k8s.io/example-rolebinding created Pod 中指定权限:serviceAccountName: grpclb-sa apiVersion: v1 kind: Pod metadata: namespace: mynamespace name: sa-token-test spec: containers: - name: nginx image: nginx:1.7.9 serviceAccountName: grpclb-sa 因为 kuberesolver 是直接调用 Kubernetes API 获取 endpoint 所以不需要创建 Headless Service 了，创建普通 Service 也可以。 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"3. 服务端负载均衡 服务端负载均衡主要是在 Pod 之前增加一个 中间组件，一般为 7 层负载均衡。 client 请求中间组件，由中间组件再去请求后端的 Pod。 常见的组件比如 Linkerd，或者 ServiceMesh 如 istio 中的 envoy 也能实现同样的效果。 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"4. 小结 相比之下更加推荐使用 客户端负载均衡。 客户端负载均衡更加简单，服务直连性能更高。 服务端负载均衡所有请求都需要经过负载均衡组件，相当于是又引入了一个全局热点。 ServiceMesh 的话对基础设施、技术栈要求比较高，落地比较困难。 ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"5. 参考 https://grpc.io/blog/grpc-load-balancing/ https://en.wikipedia.org/wiki/Round-robin_DNS https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/ ","date":"2021-05-28","objectID":"/posts/grpc/13-loadbalance-on-k8s/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(十三)--- Kubernetes 环境下的 gRPC 负载均衡","uri":"/posts/grpc/13-loadbalance-on-k8s/"},{"categories":["gRPC"],"content":"gRPC LoadBalance","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"本文主要介绍了 gRPC 客户端负载均衡策略及其配置与使用，包括 Name Resolver、ServiceConfig 等。 gRPC 系列相关代码见 Github ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 负载均衡包括客户端负载均衡和服务端负载均衡两种方向。本文主要介绍的是客户端负载均衡。 gRPC 的客户端负载均衡主要分为两个部分： 1）Name Resolver 2）Load Balancing Policy ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"1. NameResolver 具体可以参考官方文档-Name Resolver或者gRPC系列教程(十一)—NameResolver 实战及原理分析 gRPC 中的默认 name-system 是DNS，同时在客户端以插件形式提供了自定义 name-system 的机制。 gRPC NameResolver 会根据 name-system 选择对应的解析器，用以解析用户提供的服务器名，最后返回具体地址列表（IP+端口号）。 例如：默认使用 DNS name-system，我们只需要提供服务器的域名即端口号，NameResolver 就会使用 DNS 解析出域名对应的IP列表并返回。 在本例中我们会自定义一个 NameResolver。 ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:1:1","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"1.2 Load Balancing Policy 具体可以参考官方文档-Load Balancing Policy 常见的 gRPC 库都内置了几个负载均衡算法，比如 gRPC-Go 中内置了pick_first和round_robin两种算法。 pick_first：尝试连接到第一个地址，如果连接成功，则将其用于所有RPC，如果连接失败，则尝试下一个地址（并继续这样做，直到一个连接成功）。 round_robin：连接到它看到的所有地址，并依次向每个后端发送一个RPC。例如，第一个RPC将发送到backend-1，第二个RPC将发送到backend-2，第三个RPC将再次发送到backend-1。 本例中我们会分别测试两种负载均衡策略的效果。 ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:1:2","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"2. Demo ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"2.1 Server package main import ( \"context\" \"fmt\" \"log\" \"net\" \"sync\" \"google.golang.org/grpc\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" ) var ( addrs = []string{\":50051\", \":50052\"} ) type ecServer struct { pb.UnimplementedEchoServer addr string } func (s *ecServer) UnaryEcho(ctx context.Context, req *pb.EchoRequest) (*pb.EchoResponse, error) { return \u0026pb.EchoResponse{Message: fmt.Sprintf(\"%s (from %s)\", req.Message, s.addr)}, nil } func startServer(addr string) { lis, err := net.Listen(\"tcp\", addr) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterEchoServer(s, \u0026ecServer{addr: addr}) log.Printf(\"serving on 0.0.0.0%s\\n\", addr) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } func main() { var wg sync.WaitGroup for _, addr := range addrs { wg.Add(1) go func(addr string) { defer wg.Done() startServer(addr) }(addr) } wg.Wait() } 主要通过一个 for 循环，在 50051 和 50052 这两个端口上启动了服务。 ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"2.2 Client package main import ( \"context\" \"fmt\" \"log\" \"time\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" \"google.golang.org/grpc/resolver\" ) const ( exampleScheme = \"example\" exampleServiceName = \"lb.example.grpc.lixueduan.com\" ) var addrs = []string{\"localhost:50051\", \"localhost:50052\"} func callUnaryEcho(c pb.EchoClient, message string) { ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.UnaryEcho(ctx, \u0026pb.EchoRequest{Message: message}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } fmt.Println(r.Message) } func makeRPCs(cc *grpc.ClientConn, n int) { hwc := pb.NewEchoClient(cc) for i := 0; i \u003c n; i++ { callUnaryEcho(hwc, \"this is examples/load_balancing\") } } func main() { // \"pick_first\" is the default, so there's no need to set the load balancer. pickfirstConn, err := grpc.Dial( fmt.Sprintf(\"%s:///%s\", exampleScheme, exampleServiceName), grpc.WithInsecure(), grpc.WithBlock(), ) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer pickfirstConn.Close() fmt.Println(\"--- calling helloworld.Greeter/SayHello with pick_first ---\") makeRPCs(pickfirstConn, 10) fmt.Println() // Make another ClientConn with round_robin policy. roundrobinConn, err := grpc.Dial( fmt.Sprintf(\"%s:///%s\", exampleScheme, exampleServiceName), grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`), // This sets the initial balancing policy. grpc.WithInsecure(), grpc.WithBlock(), ) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer roundrobinConn.Close() fmt.Println(\"--- calling helloworld.Greeter/SayHello with round_robin ---\") makeRPCs(roundrobinConn, 10) } 可以看到，在客户端是分别使用不同的负载均衡策略建立了两个连接，首先是默认的策略 pick_first，然后则是 round_robin，核心代码为： grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`) 同时由于是本地测试，不方便使用内置的 dns Resolver 所以自定义了一个 Name Resolver，相关代码如下： // Following is an example name resolver implementation. Read the name // resolution example to learn more about it. type exampleResolverBuilder struct{} func (*exampleResolverBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) { r := \u0026exampleResolver{ target: target, cc: cc, addrsStore: map[string][]string{ exampleServiceName: addrs, }, } r.start() return r, nil } func (*exampleResolverBuilder) Scheme() string { return exampleScheme } type exampleResolver struct { target resolver.Target cc resolver.ClientConn addrsStore map[string][]string } func (r *exampleResolver) start() { addrStrs := r.addrsStore[r.target.Endpoint] addrs := make([]resolver.Address, len(addrStrs)) for i, s := range addrStrs { addrs[i] = resolver.Address{Addr: s} } r.cc.UpdateState(resolver.State{Addresses: addrs}) } func (*exampleResolver) ResolveNow(o resolver.ResolveNowOptions) {} func (*exampleResolver) Close() {} ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"3. Test 分别运行服务端和客户端查看结果 lixd@17x:~/17x/projects/grpc-go-example/features/load_balancing/server$ go run main.go 2021/05/23 09:47:59 serving on 0.0.0.0:50052 2021/05/23 09:47:59 serving on 0.0.0.0:50051 lixd@17x:~/17x/projects/grpc-go-example/features/load_balancing/client$ go run main.go --- calling helloworld.Greeter/SayHello with pick_first --- this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50051) --- calling helloworld.Greeter/SayHello with round_robin --- this is examples/load_balancing (from :50052) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50052) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50052) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50052) this is examples/load_balancing (from :50051) this is examples/load_balancing (from :50052) this is examples/load_balancing (from :50051) 可以看到 pick_first 负载均衡策略时一直请求第一个服务 50051，round_robin 时则会交替请求，这也和负载均衡策略相符合。 ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"3. 小结 本文介绍的 负载均衡属于 客户端负载均衡，需要在客户端做较大改动，因为 gRPC-go 中已经实现了对应的代码，所以使用起来还是很简单的。 gRPC 内置负载均衡实现： 1）根据提供了服务名，使用对应 name resolver 解析获取到具体的 ip+端口号 列表 2）根据具体服务列表，分别建立连接 gRPC 内部也维护了一个连接池 3）根据负载均衡策略选取一个连接进行 rpc 请求 比如之前的例子，服务名为example:///lb.example.grpc.lixueduan.com，使用自定义的 name resolver 解析出来具体的服务列表为localhost:50051,localhost:50052. 然后调用 dial 建立连接时会分别与这两个服务建立连接。最后根据负载均衡策略选择一个连接来发起 rpc 请求。所以 pick_first会一直请求50051服务，而 round_robin 会交替请求 50051和50052。 ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"4. 参考 https://github.com/grpc/grpc/blob/master/doc/naming.md https://github.com/grpc/grpc/blob/master/doc/load-balancing.md ","date":"2021-05-22","objectID":"/posts/grpc/12-client-side-loadbalance/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(十二)---客户端负载均衡","uri":"/posts/grpc/12-client-side-loadbalance/"},{"categories":["gRPC"],"content":"gRPC NameResolver 核心原理详解","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"本文主要介绍了 gRPC 的 NameResolver 及其简单使用，同时从源码层面对其核心原理进行了分析。 gRPC 系列相关代码见 Github ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"1. 概述 具体可以参考官方文档-Name Resolver gRPC 中的默认 name-system 是 DNS，同时在客户端以插件形式提供了自定义 name-system 的机制。 gRPC NameResolver 会根据 name-system 选择对应的解析器，用以解析用户提供的服务器名，最后返回具体地址列表（IP+端口号）。 例如：默认使用 DNS name-system，我们只需要提供服务器的域名即端口号，NameResolver 就会使用 DNS 解析出域名对应的 IP 列表并返回。 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"2. Demo 首先用一个 Demo 来介绍一个 gRPC 的 NameResolver 如何使用。 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"2.1 Server 服务端代码比较简单，没有什么需要注意的地方。 package main import ( \"context\" \"fmt\" \"log\" \"net\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" ) const addr = \"localhost:50051\" type ecServer struct { pb.UnimplementedEchoServer addr string } func (s *ecServer) UnaryEcho(ctx context.Context, req *pb.EchoRequest) (*pb.EchoResponse, error) { return \u0026pb.EchoResponse{Message: fmt.Sprintf(\"%s (from %s)\", req.Message, s.addr)}, nil } func main() { lis, err := net.Listen(\"tcp\", addr) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterEchoServer(s, \u0026ecServer{addr: addr}) log.Printf(\"serving on %s\\n\", addr) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"2.2 Client 客户端需要注意的是，这里建立连接时使用我们自定义的 Scheme，而不是默认的 dns，所以需要有和这个自定义的 Scheme 对应的 Resolver 来解析才行。 package main import ( \"context\" \"fmt\" \"log\" \"time\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" \"google.golang.org/grpc/resolver\" ) const ( myScheme = \"17x\" myServiceName = \"resolver.17x.lixueduan.com\" backendAddr = \"localhost:50051\" ) func callUnaryEcho(c pb.EchoClient, message string) { ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.UnaryEcho(ctx, \u0026pb.EchoRequest{Message: message}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } fmt.Println(r.Message) } func makeRPCs(cc *grpc.ClientConn, n int) { hwc := pb.NewEchoClient(cc) for i := 0; i \u003c n; i++ { callUnaryEcho(hwc, \"this is examples/name_resolving\") } } func main() { passthroughConn, err := grpc.Dial( // passthrough 也是gRPC内置的一个scheme fmt.Sprintf(\"passthrough:///%s\", backendAddr), // Dial to \"passthrough:///localhost:50051\" grpc.WithInsecure(), grpc.WithBlock(), ) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer passthroughConn.Close() fmt.Printf(\"--- calling helloworld.Greeter/SayHello to \\\"passthrough:///%s\\\"\\n\", backendAddr) makeRPCs(passthroughConn, 10) fmt.Println() ctx, cancel := context.WithTimeout(context.Background(), time.Second*3) defer cancel() exampleConn, err := grpc.DialContext( ctx, fmt.Sprintf(\"%s:///%s\", myScheme, myServiceName), // Dial to \"17x:///resolver.17x.lixueduan.com\" grpc.WithInsecure(), grpc.WithBlock(), ) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer exampleConn.Close() fmt.Printf(\"--- calling helloworld.Greeter/SayHello to \\\"%s:///%s\\\"\\n\", myScheme, myServiceName) makeRPCs(exampleConn, 10) } 具体 Resolver 实现如下： // Following is an example name resolver. It includes a // ResolverBuilder(https://godoc.org/google.golang.org/grpc/resolver#Builder) // and a Resolver(https://godoc.org/google.golang.org/grpc/resolver#Resolver). // // A ResolverBuilder is registered for a scheme (in this example, \"example\" is // the scheme). When a ClientConn is created for this scheme, the // ResolverBuilder will be picked to build a Resolver. Note that a new Resolver // is built for each ClientConn. The Resolver will watch the updates for the // target, and send updates to the ClientConn. // exampleResolverBuilder is a // ResolverBuilder(https://godoc.org/google.golang.org/grpc/resolver#Builder). type exampleResolverBuilder struct{} func (*exampleResolverBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) { r := \u0026exampleResolver{ target: target, cc: cc, addrsStore: map[string][]string{ myServiceName: {backendAddr}, }, } r.ResolveNow(resolver.ResolveNowOptions{}) return r, nil } func (*exampleResolverBuilder) Scheme() string { return myScheme } // exampleResolver is a // Resolver(https://godoc.org/google.golang.org/grpc/resolver#Resolver). type exampleResolver struct { target resolver.Target cc resolver.ClientConn addrsStore map[string][]string } func (r *exampleResolver) ResolveNow(o resolver.ResolveNowOptions) { // 直接从map中取出对于的addrList addrStrs := r.addrsStore[r.target.Endpoint] addrs := make([]resolver.Address, len(addrStrs)) for i, s := range addrStrs { addrs[i] = resolver.Address{Addr: s} } r.cc.UpdateState(resolver.State{Addresses: addrs}) } func (*exampleResolver) Close() {} func init() { // Register the example ResolverBuilder. This is usually done in a package's // init() function. resolver.Register(\u0026exampleResolverBuilder{}) } resolver 包括 ResolverBuilder 和 Resolver两个部分。 分别需要实现Builder和Resolver接口 type Builder interface { Build(target Target, cc ClientConn, opts BuildOptions) (Resolver, error) Scheme() string } type Resolver interface { ResolveNow(ResolveNowOptions) Close() } Resolver 是整个功能最核心的代码，用于将服务名解析成对应实例。 Builder 则采用 Builder 模式在包初始化时创建并注册构造自定义 Resolver 实例。当客户端通过 Dial 方法对指定服务进行拨号时，grpc resolver 查找注册的 Builder 实例调用其 Build() 方法构建自定义 Resolver。 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"2.3 Test 分别启动服务端和客户端进行测试： lixd@17x:~/17x/projects/grpc-go-example/features/name_resolving/server$ go run main.go 2021/05/15 10:04:11 serving on localhost:50051 lixd@17x:~/17x/projects/grpc-go-example/features/name_resolving/client$ go run main.go --- calling helloworld.Greeter/SayHello to \"passthrough:///localhost:50051\" this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) --- calling helloworld.Greeter/SayHello to \"17x:///resolver.17x.lixueduan.com\" this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) this is examples/name_resolving (from localhost:50051) 一切正常，说明我们的自定义 Resolver 是可以运行的，那么接下来从源码层面来分析一下 gRPC 中 Resolver 具体是如何工作的。 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"3. 源码分析 以下分析基于 grpc-go v1.35.0 版本 首先客户端调用grpc.Dial()方法建立连接，会进入DialContext()方法。 // clientconn.go 103 行 func Dial(target string, opts ...DialOption) (*ClientConn, error) { return DialContext(context.Background(), target, opts...) } DialContext() 内容比较多，这里只关注 Resolver 相关的代码： 这一段是通过解析 target 获取 scheme，然后根据 scheme 找到对应的 resolverBuilder // clientconn.go 249行 // 首先解析target cc.parsedTarget = grpcutil.ParseTarget(cc.target, cc.dopts.copts.Dialer != nil) channelz.Infof(logger, cc.channelzID, \"parsed scheme: %q\", cc.parsedTarget.Scheme) // 然后根据scheme从全局Resolver列表中找到对应的resolverBuilder resolverBuilder := cc.getResolver(cc.parsedTarget.Scheme) if resolverBuilder == nil { // 如果指定的scheme找不到对应的resolverBuilder那就用defaultScheme // 默认协议为 `passthrough`，它会从用户解析的 target 中直接读取 endpoint 地址 channelz.Infof(logger, cc.channelzID, \"scheme %q not registered, fallback to default scheme\", cc.parsedTarget.Scheme) cc.parsedTarget = resolver.Target{ Scheme: resolver.GetDefaultScheme(), Endpoint: target, } resolverBuilder = cc.getResolver(cc.parsedTarget.Scheme) if resolverBuilder == nil { return nil, fmt.Errorf(\"could not get resolver for default scheme: %q\", cc.parsedTarget.Scheme) } } 具体获取 resolver 的逻辑如下： // // clientconn.go 1577行 func (cc *ClientConn) getResolver(scheme string) resolver.Builder { for _, rb := range cc.dopts.resolvers { if scheme == rb.Scheme() { return rb } } return resolver.Get(scheme) } // resolver.go 54行 func Get(scheme string) Builder { if b, ok := m[scheme]; ok { return b } return nil } 可以看到最终是去 m 这个 map 中获取的 resolverBuilder。 那么这个 map m 中的 resolverBuilder 是从哪儿来的呢？ 这个 resolver 就是客户端代码中的 init 方法注册进去的，全局 resolverBuild 都存放一个 map 中，key 为 scheme，value 为对应的 resolverBuilder。 func init() { // Register the example ResolverBuilder. This is usually done in a package's // init() function. resolver.Register(\u0026exampleResolverBuilder{}) } func Register(b Builder) { m[b.Scheme()] = b } 接下来就通过 resolverBuilder 构建一个 Resolver 实例。 // clientconn.go 312行 // Build the resolver. rWrapper, err := newCCResolverWrapper(cc, resolverBuilder) if err != nil { return nil, fmt.Errorf(\"failed to build resolver: %v\", err) } // resolver_conn_wapper.go 48行 // newCCResolverWrapper uses the resolver.Builder to build a Resolver and // returns a ccResolverWrapper object which wraps the newly built resolver. func newCCResolverWrapper(cc *ClientConn, rb resolver.Builder) (*ccResolverWrapper, error) { ccr := \u0026ccResolverWrapper{ cc: cc, done: grpcsync.NewEvent(), } var credsClone credentials.TransportCredentials if creds := cc.dopts.copts.TransportCredentials; creds != nil { credsClone = creds.Clone() } rbo := resolver.BuildOptions{ DisableServiceConfig: cc.dopts.disableServiceConfig, DialCreds: credsClone, CredsBundle: cc.dopts.copts.CredsBundle, Dialer: cc.dopts.copts.Dialer, } var err error ccr.resolverMu.Lock() defer ccr.resolverMu.Unlock() // 调用resolverBuilder的Build方法构建Resolver ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo) if err != nil { return nil, err } return ccr, nil } 接来下我们看一下 gRPC 内置的 ResolverBuilder 是 Build 方法是怎么实现的，就拿 DNSResolverBuilder 为例，代码如下： // dns_resolver.go 109行 func (b *dnsBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) { // 首先依旧是解析target，获取格式化后的 host + port host, port, err := parseTarget(target.Endpoint, defaultPort) if err != nil { return nil, err } // 对host进行IP格式化处理 如果是IP地址则直接调用cc.UpdateState更新连接信息后返回 不走后续的dns解析逻辑了 if ipAddr, ok := formatIP(host); ok { addr := []resolver.Address{{Addr: ipAddr + \":\" + port}} cc.UpdateState(resolver.State{Addresses: addr}) return deadResolver{}, nil } // 如果是域名则需要进行dns解析 ctx, cancel := context.WithCancel(context.Background()) d := \u0026dnsResolver{ host: host, port: port, ctx: ctx, cancel: cancel, cc: cc, rn: make(chan struct{}, 1), disableServiceConfig: opts.DisableServiceConfig, } // 根据 Authority 判定使用默认Resolver还是自定义Resolver if target.Authority == \"\" { d.resolver = defaultResolver } else { d.resolver, err = customAuthorityResolver(target.Authority) if err","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"4. 小结 1）客户端启动时，注册自定义的 resolver 。 一般在 init() 方法，构造自定义的 resolveBuilder，并将其注册到 grpc 内部的 resolveBuilder 表中（其实是一个全局 map，key 为协议名，value 为构造的 resolveBuilder）。 2）客户端启动时通过自定义 Dail() 方法构造 grpc.ClientConn 单例 grpc.DialContext() 方法内部解析 URI，分析协议类型，并从 resolveBuilder 表中查找协议对应的 resolverBuilder。 找到指定的 resolveBuilder 后，调用 resolveBuilder 的 Build() 方法，构建自定义 resolver，同时开启协程，通过此 resolver 更新被调服务实例列表。 Dial() 方法接收主调服务名和被调服务名，并根据自定义的协议名，基于这两个参数构造服务的 URI Dial() 方法内部使用构造的 URI，调用 grpc.DialContext() 方法对指定服务进行拨号 3）grpc 底层 LB 库对每个实例均创建一个 subConnection，最终根据相应的 LB 策略，选择合适的 subConnection 处理某次 RPC 请求。 到这里在回头看 Demo 中的自定义 Resolver 应该就没什么问题了。由于只是个 Demo 所以真的非常简单。直接在 Build 中通过 map 存储addr，然后 ResolveNow 时直接从 map 中取出来更新服务实例列表，连 watcher 都省略了。 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["gRPC"],"content":"5. 参考 https://github.com/grpc/grpc-go https://github.com/grpc/grpc/blob/master/doc/naming.md https://blog.csdn.net/ra681t58cjxsgckj31/article/details/104079070 ","date":"2021-05-14","objectID":"/posts/grpc/11-name-resolver/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(十一)---NameResolver 实战及原理分析","uri":"/posts/grpc/11-name-resolver/"},{"categories":["web"],"content":"常见的设备追踪方法之 Canvas 指纹（fingerprinting）","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"本文主要讲解了常见的设备追踪方法之 Canvas 指纹（fingerprinting）原理及其具体实现。 可以通过该Demo页进行测试，其中的 CRC32 值即当前设备的 Canvas 指纹。 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:0:0","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"1. 预备知识 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:1:0","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"1. 1. Canvas Canvas（画布）是HTML5中一种动态绘图的标签，可以使用其生成甚至处理高级图片。 Canvas的兼容情况：几乎已被所有主流浏览器支持，可以通过大部分的PC、平板、智能手机访问！ ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:1:1","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"1.2. PNG图片数据格式 PNG 图片主要关注头部和尾部。 header 前八个字节89 50 4E 47 0D 0A 1A 0A是PNG格式固定的文件头； 00 00 00 0D代表图片长宽的数据块长度为13，也是固定值； 49 48 44 52是固定值,代表IHDR； 第二行开始的00 00 01 D9为图片宽度，00 00 00 D6为图片高度； 由于数据块长度为13，所以08 02 00 00 00为剩余填充部分； 12 04 6F 34为头部信息的CRC32校验和 footer 倒数第二行的 B4 82 2C D4 为图片内容的CRC32校验和 具体位置为倒数第16到第12之前的这4个值 图片内容有丝毫不一致该CRC32校验和都会不同 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:1:2","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"2. 具体实现 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:2:0","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"2.1 前提 Canvas 指纹基于以下前提： 使用 Canvas 绘制同样的内容，在不同电脑、浏览器上会因为硬件不同得到不同的结果。 即使生成的图片肉眼看上去一样,实际细节上也有很大的差异。 具体原因： 在图片格式来看，不同浏览器使用了不同的图形处理引擎、不同的图片导出选项、不同的默认压缩级别等。 在像素级别来看，操作系统各自使用了不同的设置和算法来进行抗锯齿和子像素渲染操作。 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:2:1","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"2.2 具体实现 步骤如下： 1）使用 Canvas 绘制一个图片，并导出为 base64 格式数据。 默认导出图片为 PNG 格式 2）根据 base64 格式数据计算得到用户的 Canvas 指纹 1）直接对整个 base64 数据进行 hash，将hash值作为用户指纹 2）从 PNG 图片中取出图片内容 CRC32 校验码，省去了计算hash效率比较高。 Demo 如下，完整代码见Github \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ccanvas id=\"myCanvas\"\u003e\u003c/canvas\u003e \u003cdiv id=\"crc32\"\u003e\u003c/div\u003e \u003cbr\u003e \u003cscript\u003e function string2Hex(str) { let result = \"\" for (let i = 0; i \u003c str.length; i++) { let askii = str.charCodeAt(i) if (askii \u003c 0x0f) { // 小于16转为16进制后在前面补零 result += \"0\" } result += askii.toString(16).toLocaleUpperCase() } return result } function extractCRC32FromBase64(base64) { base64 = base64.replace('data:image/png;base64,', '') const bin = atob(base64) // PNG图片第29到第33位是PNG元数据的CRC32校验码 这里只和图片尺寸有关 // PNG图片倒数第16到第12位这四位就是该图片的CRC32校验码 const crcAskii = bin.slice(-16, -12) return string2Hex(crcAskii.toString()) } \u003c/script\u003e \u003cscript\u003e function getSimpleCanvasFingerprint() { const canvas = document.getElementById('myCanvas') const ctx = canvas.getContext('2d') const txt = 'qwertyuiop!@#$%^\u0026*()_+' ctx.textBaseline = 'top' ctx.font = '14px \\'Arial\\'' ctx.textBaseline = 'tencent' ctx.fillStyle = '#f60' ctx.fillRect(125, 1, 62, 20) ctx.fillStyle = '#069' ctx.fillText(txt, 2, 15) ctx.fillStyle = 'rgba(102, 204, 0, 0.7)' ctx.fillText(txt, 4, 17) // 将 canvas 内容转为base64编码 return canvas.toDataURL() } let b64 = getSimpleCanvasFingerprint() const crc32 = extractCRC32FromBase64(b64) document.getElementById(\"crc32\").innerHTML = \"CRC32: \" + crc32 \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 实际使用时不需要将 canvas 显示出来，创建一个隐藏的 canvas 即可。 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:2:2","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"3. FAQ 1）Canvas 绘制内容越复杂越容易出现差异，但是效率越低。 2）CRC32校验码为8位16进制数理论上有42个不同的值。 3）为了增加准确度可以采集更多信息，比如浏览器版本、语言、UA等等。 推荐一个第三方库fingerprint.js，采集了几十项指标，号称识别率99.5%。 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:3:0","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["web"],"content":"4. 参考 https://en.wikipedia.org/wiki/Canvas_fingerprinting https://privacycheck.sec.lrz.de/active/fp_c/fp_canvas.html https://browserleaks.com/canvas https://tjublesson.top/2020/03/13/CTF%E2%80%94%E2%80%94Misc%E4%B9%8BPNG%E7%9A%84CRC32%E6%A0%A1%E9%AA%8C/ https://blog.csdn.net/Blues1021/article/details/45007943 ","date":"2021-05-04","objectID":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/:4:0","tags":["web"],"title":"设备追踪之Canvas指纹","uri":"/posts/web/%E8%AE%BE%E5%A4%87%E8%BF%BD%E8%B8%AA%E4%B9%8Bcanvas%E6%8C%87%E7%BA%B9/"},{"categories":["Kubernetes"],"content":"通过ConfigMap与Secret管理配置文件","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"本文主要记录了 Kubernetes 中的几种 Projected Volume，包括 ConfigMap、Secret、Downward API、ServiceAccountToken等。 ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"1. 概述 在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。 这些特殊 Volume 的作用，是为容器提供预先定义好的数据。 所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。 到目前为止，Kubernetes 支持的 Projected Volume 一共有四种： 1）ConfigMap； 2）Secret； 3）Downward API； 4）ServiceAccountToken。 ConfigMap 顾名思义，是用于保存配置数据的键值对，可以用来保存单个属性，也可以保存配置文件。 Secret 可以为 Pod 提供密码、Token、私钥等敏感数据；对于一些非敏感数据，比如应用的配置信息，则可以使用 ConfigMap。 就是把数据存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret、ConfigMap 里保存的信息了。 Downward API，它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。 不过，需要注意的是，Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。 其实，Secret、ConfigMap，以及 Downward API 这三种 Projected Volume 定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，都建议使用 Volume 文件的方式获取这些信息，Volume 方式可以自动更新，不过可能会有一定延迟。 ServiceAccountToken 一种特殊的 Secret，是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。 另外，为了方便使用，Kubernetes 已经为你提供了一个默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它（k8s 默认会为每一个Pod 都挂载该Volume）。 ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"2. ConfigMap # 创建 $ kubectl create configmap # 删除 $ kubectl delete configmap ConfigMap名称 # 编辑 $ kubectl edit configmap ConfigMap名称 # 查看-列表 $ kubectl get configmap # 查看-详情 $ kubectl describe configmap ConfigMap名称 ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"1. 创建 可以使用 kubectl create configmap 从以下多种方式创建 ConfigMap。 1）yaml 描述文件：事先写好标准的configmap的yaml文件，然后kubectl create -f 创建 2）–from-file：通过指定目录或文件创建，将一个或多个配置文件创建为一个ConfigMap 3）–from-literal：通过直接在命令行中通过 key-value 字符串方式指定configmap参数创建 4）–from-env-file：从 env 文件读取配置信息并创建为一个ConfigMap yaml 描述文件 $ kubectl create -f configmap.yaml configmap/test-conf created $ kubectl get configmaps NAME DATA AGE cm1 1 8s $ kubectl describe configmap test-conf Name: cm1 Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Data ==== user: ---- name: \"17x\" github: \"https://github.com/lixd\" blog: \"https://www.lixueduan.com\" Events: \u003cnone\u003e configmap.yaml 如下: apiVersion: v1 kind: ConfigMap metadata: name: cm1 namespace: default data: user: |+ name: \"17x\" github: \"https://github.com/lixd\" blog: \"https://www.lixueduan.com\" –from-file 1）单文件 $ kubectl create configmap cm2 --from-file=conf-17x.yaml configmap/cm1 created $ kubectl get configmaps NAME DATA AGE cm2 1 17s $ kubectl get configmap cm2 -o go-template='{{.data}}' map[user:name: \"17x\" github: \"https://github.com/lixd\" blog: \"https://www.lixueduan.com\"] 配置文件如下 User: \"17x\" Github: \"https://github.com/lixd\" Blog: \"https://www.lixueduan.com\" 2）目录 # 从 conf 目录下多个文件读取配置信息 $ kubectl create configmap cm3 --from-file=conf configmap/cm3 created $ kubectl get configmaps NAME DATA AGE cm3 2 8s $ kubectl get configmap cm3 -o go-template='{{.data}}' map[conf1.yaml:User: \"17x\" Github: \"https://github.com/lixd\" Blog: \"https://www.lixueduan.com\" conf2.yaml:User: \"17x\" Github: \"https://github.com/lixd\" Blog: \"https://www.refersmoon.com\" –from-literal 每个 –from-literal 对应一个信息条目。 $ kubectl create configmap cm4 --from-literal=name=\"17x\" --from-literal=blog=\"https://www.lixueduan.com\" configmap \"cm4\" created $ kubectl get configmap cm4 -o go-template='{{.data}}' map[blog:https://www.lixueduan.com name:17x][root@iZ2ze9ebgot9h2acvk4uabZ configmap] –from-env-file $ kubectl create configmap cm5 --from-env-file=conf.env configmap/cm5 created $ kubectl get configmap cm5 -o go-template='{{.data}}' map[blog:https://www.lixueduan.com github:https://github.com/lixd name:17x] conf.env文件如下 语法为 key=value name=17x github=https://github.com/lixd blog=https://www.lixueduan.com ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"2. 使用 Pod 可以通过三种方式来使用 ConfigMap，分别为： 将 ConfigMap 中的数据设置为环境变量 使用 Volume 将 ConfigMap 作为文件或目录挂载 注意：ConfigMap 中的每一个Key都会单独挂载为一个文件 注意！！ ConfigMap 必须在 Pod 使用它之前创建 使用 envFrom 时，将会自动忽略无效的键 一个Pod 只能使用同一个命名空间的 ConfigMap 用作环境变量 $ kubectl create configmap cm1 --from-literal=first=\"hello world\" --from-literal=second=\"hello configmap\" $ kubectl create configmap env-cm --from-literal=log_level=INFO apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-container image: busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] env: - name: CUSTOM_FIRST valueFrom: configMapKeyRef: name: cm1 key: first - name: CUSTOM_SECOND valueFrom: configMapKeyRef: name: cm1 key: second envFrom: - configMapRef: name: env-cm restartPolicy: Never 通过 env单个指定或者通过envFrom直接加载整个 configmap 根据以上 yaml 文件创建 pod 并查看日志 $ kubectl apply -f cm.yaml # 查看日志 会打印出一堆环境变量 其中就有我们指定的 configmap $ kubectl logs test-pod log_level=INFO CUSTOM_FIRST=hello world CUSTOM_SECOND=hello configmap 使用 Volume 将 ConfigMap 作为文件或目录挂载 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - image: busybox name: app volumeMounts: - mountPath: /etc/foo name: foo readOnly: true args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 30000 volumes: - name: foo configMap: name: cm1 进入 pod 查看，会在前面指定的 /etc/foo 目录下创建 configmap中的文件 $ kubectl apply -f cm2.pod.yaml # 进入 pod 查看，会在前面指定的 /etc/foo 目录下创建 configmap中的文件 $ kubectl exec -it test-pod sh $ cd /etc/foo/ $ ls first second $ cat first hello world ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"3. Secret Secret 和 Configmap 类似，不过 Secret 是加密后的，一般用于存储敏感数据，如 比如密码，token，密钥等。 Secret有三种类型： 1）Opaque：base64 编码格式的 Secret，用来存储密码、密钥等；但数据也可以通过base64 –decode解码得到原始数据，所以加密性很弱。 2）Service Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount 目录中。 3）kubernetes.io/dockerconfigjson ： 用来存储私有docker registry的认证信息。 ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"1. 创建 Secret 同样有多种创建方式 1）yaml 描述文件：事先写好标准的secret的yaml文件，然后kubectl create -f 创建 2）–from-file：通过指定目录或文件创建，将一个或多个配置文件创建为一个Secret 3）–from-literal：通过直接在命令行中通过 key-value 字符串方式指定configmap参数创建 yaml 描述文件 $ kubectl apply -f secret.yaml secret/mysecret created # describe 或者 get 命令不会直接显示 secret 中的内容 $ kubectl get secret mysecret -o go-template='{{.data}}' map[hello:d29ybGQ= secret.yaml 内容如下： 注意：通过yaml创建Opaque类型的Secret值需要base64编码 $ echo -n 'world'|base64 d29ybGQ= #secret.yaml --- apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: hello: d29ybGQ= –from-file 直接从文件创建，默认 key 为文件名 $ cat hello.txt world # generic 表示创建普通 secret $ kubectl create secret generic s1 --from-file=hello.txt secret/s1 created $ kubectl get secrets NAME TYPE DATA AGE s1 Opaque 1 20s # describe 或者 get 命令不会直接显示 secret 中的内容 $ kubectl get secret s1 -o go-template='{{.data}}' map[hello.txt:d29ybGQK] –from-literal 字符串方式可以手动指定 key、value $ kubectl create secret generic s2 --from-literal=hello=world secret/s2 created $ kubectl get secret s1 -o go-template='{{.data}}' map[hello:d29ybGQ=] ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"2. 使用 同样有两种方式： 将 Secret 中的数据设置为环境变量 使用 Volume 将 Secret 作为文件或目录挂载 用作环境变量 和 configmap 类似，把 configMapKeyRef 替换成 secretKeyRef 即可,同时 secret 是单个的，所以也去掉了批量获取的 envFrom 字段。 # secret-env.yaml --- apiVersion: v1 kind: Pod metadata: name: test-secret-env-pod spec: containers: - name: test-container image: busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] env: - name: CUSTOM_HELLO valueFrom: secretKeyRef: name: s2 key: hello restartPolicy: Never 运行pod并查看打印出来的环境变量 $ kubectl apply -f secret-env.yaml pod/test-secret-env-pod created $ kubectl logs test-secret-env-pod CUSTOM_HELLO=world Volume 挂载方式 # secret.pod.yaml --- apiVersion: v1 kind: Pod metadata: name: test-secret-pod spec: containers: - name: test-secret image: busybox args: - sleep - \"3600\" volumeMounts: - name: config mountPath: \"/etc/foo\" readOnly: true volumes: - name: config projected: sources: - secret: name: mysecret 创建Pod后进入容器查看，可以看到在指定目录/etc/foo中存在我们指定的secret中的内容hello $ kubectl exec -it test-secret-pod sh / # ls bin dev etc home proc root sys tmp usr var $ cd etc/foo/ $ ls hello $ cat hello world ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"4. Downward API 它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。 不过，需要注意的是，Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。 举个例子 apiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels 在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。 ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["Kubernetes"],"content":"5. 参考 https://kubernetes.io/docs/concepts/configuration/configmap/ https://kubernetes.io/docs/concepts/configuration/secret/ 深入剖析Kubernetes ","date":"2021-04-23","objectID":"/posts/kubernetes/06-projected-volume-configmap-secret/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(六)---通过ConfigMap与Secret管理配置文件","uri":"/posts/kubernetes/06-projected-volume-configmap-secret/"},{"categories":["gRPC"],"content":"gRPC Benchmark Tools ghz","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"本文主要介绍了 gRPC 压测工具 ghz ，包括 ghz 的安装、使用及压测计划制定等。 gRPC 系列相关代码见 Github ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"1. 安装 可以直接在Release页面下载二进制文件，也可以 clone 仓库手动编译。 下载解压后即可使用 # 下载 $ wget https://github.91chifun.workers.dev/https://github.com//bojand/ghz/releases/download/v0.94.0/ghz-linux-x86_64.tar.gz ghz-linux-x86_64.ta 100%[===================\u003e] 10.41M 1.84MB/s 用时 5.7s # 解压 $ tar -zxvf ghz-linux-x86_64.tar.gz ghz ghz-web LICENSE $ ls ghz ghz-linux-x86_64.tar.gz ghz-web LICENSE # 添加到环境变量 $ sudo vim /etc/profile $ source /etc/profile # 具体位置就是刚解压的位置 $ cat /etc/profile export PATH=$PATH:/home/lixd/17x/env 具体语法 ghz [\u003cflags\u003e] [\u003chost\u003e] ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"2. 参数说明 只列出了常用参数，其他参数可以查看官方文档或者查阅帮助命令ghz -h 大致可以分为三类参数： 基本参数 负载参数 并发参数 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"2.1 基本参数 --config：指定配置文件位置 --proto：指定 proto 文件位置 会从 proto 文件中获取相关信息 --call：指定调用的方法。 具体格式为包名.服务名.方法名 如：--call helloworld.Greeter.SayHello -c：并发请求数 -n：最大请求数，达到后则结束测试 -d：请求参数 JSON格式，如-d '{\"name\":\"Bob\"}' -D：以文件方式指定请求参数，JSON文件位置 如-D ./file.json -o：输出路径 默认输出到 stdout -O/--format：输出格式，有多种格式可选 便于查看的：csv、json、pretty、html： 便于入库的：influx-summary、influx-details：满足InfluxDB line-protocol 格式的输出 以上就是相关的基本参数，有了这些参数基本可以进行测试了。 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"2.2 负载参数 负载参数主要控制ghz每秒发起的请求数（RPS）。 -r/--rps：指定RPS ghz以恒定的RPS进行测试 --load-schedule：负载调度算法，取值如下： const：恒定RPS，也是默认调用算法 step：步进增长RPS，需要配合load-start，load-step，load-end，load-step-duration，和load-max-duration等参数 line：线性增长RPS，需要配合load-start，load-step，load-end，和load-max-duration等参数，其实line就是 step 算法将load-step-duration时间固定为一秒了。 --load-start：step、line 的起始RPS --load-step：step、line 的步进值或斜率值 --load-end：step、line 的负载结束值 --load-max-duration：最大持续时间，到达则结束 例如 -n 10000 -c 10 --load-schedule=step --load-start=50 --load-step=10 --load-step-duration=5s 从50RPS开始，每5秒钟增加10RPS，一直到完成10000请求为止。 -n 10000 -c 10 --load-schedule=step --load-start=50 --load-end=150 --load-step=10 --load-step-duration=5s 从50RPS开始，每5秒钟增加10RPS，最多增加到150RPS，一直到完成10000请求为止。 -n 10000 -c 10 --load-schedule=line --load-start=200 --load-step=-2 --load-end=50 从200RPS开始，每1秒钟降低2RPS，一直降低到50RPS，一直到完成10000请求为止。 line 其实就是 step，只不过是把–load-step-duration固定为1秒了 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"2.3 并发参数 -c：并发woker数， 注意：不是并发请求数 --concurrency-schedule：并发调度算法，和--load-schedule类似 const：恒定并发数，默认值 step：步进增加并发数 line：线性增加并发数 --concurrency-start：起始并发数 --concurrency-end：结束并发数 --concurrency-step：并发数步进值 --concurrency-step-duration：在每个梯段需要持续的时间 --concurrency-max-duration：最大持续时间 例子： -n 100000 --rps 200 --concurrency-schedule=step --concurrency-start=5 --concurrency-step=5 --concurrency-end=50 --concurrency-step-duration=5s 固定RPS200，worker数从5开始，每5秒增加5，最大增加到50。 注意：5个worker时也要完成200RPS，即每个worker需要完成40RPS，到50个worker时只需要每个worker完成4RPS即可达到200RPS。 通过指定负载参数和并发参数可以更加专业的进行压测。 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"2.4 配置文件 所有参数都可以通过配置文件来指定，这也是比较推荐的用法。 比如这样： { \"proto\": \"/path/to/greeter.proto\", \"call\": \"helloworld.Greeter.SayHello\", \"total\": 2000, \"concurrency\": 50, \"data\": { \"name\": \"Joe\" }, \"metadata\": { \"foo\": \"bar\", \"trace_id\": \"{{.RequestNumber}}\", \"timestamp\": \"{{.TimestampUnix}}\" }, \"import-paths\": [ \"/path/to/protos\" ], \"max-duration\": \"10s\", \"host\": \"0.0.0.0:50051\" } ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:2:4","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"3. 使用 该工具有两种使用方式。 1）ghz 二进制文件方式，通过命令行参数或者配置文件指定配置信息 2）ghz/runner编程方式使用，通过代码指定配置信息 二者只是打开方式不同，具体原理是一样的。 首页启动服务端，这里就是要之前HelloWorld教程中的Greeter服务。 lixd@17x:~/17x/projects/grpc-go-example/helloworld/server$ go run main.go 2021/04/17 10:53:46 Serving gRPC on 0.0.0.0:50051 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"3.1 命令行方式 1）基本参数 首先使用基本参数进行测试 ghz -c 10 -n 1000 \\ --insecure \\ --proto ./hello_world.proto \\ --call helloworld.Greeter.SayHello \\ -d '{\"name\":\"Joe\"}' \\ 0.0.0.0:50051 --call helloworld.Greeter.SayHello：说明，具体 proto 文件如下 // 省略其他代码... package helloworld; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {} } 可以看到，包名为helloworld、 service名为Greeter，方法名为 SayHello。 结果如下 Summary: Count: 1000 Total: 87.65 ms Slowest: 6.97 ms Fastest: 0.12 ms Average: 0.75 ms Requests/sec: 11409.21 Response time histogram: 0.118 [1] | 0.803 [801] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 1.487 [131] |∎∎∎∎∎∎∎ 2.172 [27] |∎ 2.857 [18] |∎ 3.542 [12] |∎ 4.226 [0] | 4.911 [0] | 5.596 [0] | 6.281 [0] | 6.966 [10] | Latency distribution: 10 % in 0.35 ms 25 % in 0.43 ms 50 % in 0.57 ms 75 % in 0.75 ms 90 % in 1.23 ms 95 % in 1.62 ms 99 % in 3.31 ms Status code distribution: [OK] 1000 responses 大部分请求都能在3ms左右响应。 2）负载参数 接着增加负载参数 ghz -c 10 -n 1000 \\ --insecure \\ --proto ./hello_world.proto \\ --call helloworld.Greeter.SayHello \\ -d '{\"name\":\"Joe\"}' \\ --load-schedule=step --load-start=50 --load-step=10 --load-step-duration=5s \\ -o report.html -O html \\ 0.0.0.0:50051 这次指定使用HTML方式输出结果，执行完成后可以在当前目录看到输出的HTML文件 $ ls report.html 具体内容如下： 相比之下HTML方式更加直观。 3）并发参数 最后使用并发参数 ghz -c 10 -n 10000 \\ --insecure \\ --proto ./hello_world.proto \\ --call helloworld.Greeter.SayHello \\ -d '{\"name\":\"Joe\"}' \\ --rps 200 --concurrency-schedule=step --concurrency-start=5 --concurrency-step=5 --concurrency-end=50 --concurrency-step-duration=5s \\ -o report.json -O pretty \\ 0.0.0.0:50051 本次以CSV格式打印输出 duration (ms),status,error 1.05,OK, 0.32,OK, 0.30,OK, 0.36,OK, 0.34,OK, 0.29,OK, 0.40,OK, 0.40,OK, 0.62,OK, 0.31,OK, 0.30,OK, 0.48,OK, CSV和JSON格式会将每次请求及其消耗时间、状态等信息一一列出，信息比较全，不过相比HTML不够直观。 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"3.2 ghz/runner编程方式 编程方式更加灵活，同时可以直接使用二进制请求数据也比较方便。 完整代码见 Github 相关代码如下： package main import ( \"log\" \"os\" \"github.com/bojand/ghz/printer\" \"github.com/bojand/ghz/runner\" \"github.com/golang/protobuf/proto\" pb \"github.com/lixd/grpc-go-example/helloworld/helloworld\" ) // 官方文档 https://ghz.sh/docs/intro.html func main() { // 组装BinaryData item := pb.HelloRequest{Name: \"lixd\"} buf := proto.Buffer{} err := buf.EncodeMessage(\u0026item) if err != nil { log.Fatal(err) return } report, err := runner.Run( // 基本配置 call host proto文件 data \"helloworld.Greeter.SayHello\", // 'package.Service/method' or 'package.Service.Method' \"localhost:50051\", runner.WithProtoFile(\"../helloworld/helloworld/hello_world.proto\", []string{}), runner.WithBinaryData(buf.Bytes()), runner.WithInsecure(true), runner.WithTotalRequests(10000), // 并发参数 runner.WithConcurrencySchedule(runner.ScheduleLine), runner.WithConcurrencyStep(10), runner.WithConcurrencyStart(5), runner.WithConcurrencyEnd(100), ) if err != nil { log.Fatal(err) return } // 指定输出路径 file, err := os.Create(\"report.html\") if err != nil { log.Fatal(err) return } rp := printer.ReportPrinter{ Out: file, Report: report, } // 指定输出格式 _ = rp.Print(\"html\") } 运行测试会在当前目录输出report.html文件 $ go run ghz.go $ ls ghz.go report.html ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["gRPC"],"content":"4. 小结 推荐使用ghz/runner编程方式+HTML格式输出结果。 ghz/runner编程方式相比二进制方式更加灵活 HTML格式输出结果更加直观 ","date":"2021-04-16","objectID":"/posts/grpc/10-benchmark/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(十)---gRPC压测工具ghz","uri":"/posts/grpc/10-benchmark/"},{"categories":["Kubernetes"],"content":"Kubernetes Service 的几种访问方式","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"本文主要介绍了 Service 的几种访问方式，包括ClusterIP、NodePort、LoadBalancer、ExternalName等。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"1. 概述 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。 Service 的四种访问方式如下： 1）ClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 2）NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 \u003c节点 IP\u003e:\u003c节点端口\u003e，你可以从集群的外部访问一个 NodePort 服务。 3）LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 4）ExternalName：通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 其中 ClusterIP 为默认方式，只能集群内部访问。NodePort、LoadBalancer 则是向外暴露服务的同时将流量路由到 ClusterIP服务。ExternalName 则是CNAME方式进行服务映射。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2. 详解 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2.1 ClusterIP ClusterIP也是 Service 的默认访问方式。 根据是否生成 ClusterIP 又可分为普通 Service 和 Headless Service 两类： 普通 Service：通过为 Kubernetes 的 Service 分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问，为最常见的方式。 Headless Service：该服务不会分配 Cluster IP，也不通过 kube-proxy 做反向代理和负载均衡。而是通过 DNS 提供稳定的网络络 ID 来访问，DNS 会将Headless Service 的后端（endpoints）直接解析为 PodIP 列表，主要供 StatefulSet 使用。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2.2 NodePort NodePort 也是比较常见的一种访问方式。 YAML 定义如下： apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: type: NodePort ports: - port: 8080 targetPort: 80 protocol: TCP name: http #nodePort: 31703 - port: 443 protocol: TCP name: https #nodePort: 31704 selector: app: nginx 在这个 Service 的定义里，我们声明它的类型是，type=NodePort。然后，我在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口。 如果你不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，你可以通过 kube-apiserver 的–service-node-port-range 参数来修改它。 查看一下自动分配的 NodePort [root@ks ~]# kubectl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx NodePort 10.96.107.207 \u003cnone\u003e 8080:31589/TCP,443:30134/TCP 19m 可以看到，虽然是 NodePort 类型，但还是默认创建了 ClusterIP。 可以通过 \u003cClusterIP\u003e:\u003cservice.Port\u003e 来访问 这里就是 10.96.107.207:8080 或者通过\u003cNodeIP\u003e:\u003cNodePort\u003e方式来访问 这里就是内网IP 192.168.2.141:31589，或者公网IP 123.57.236.125:31589 都可以 NodePort 模式也就非常容易理解,kube-proxy 要做的，就是在每台宿主机上生成这样一条 iptables 规则： -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM KUBE-SVC-67RL4FN6JRUPOJYM其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了 要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示： -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE 这里的原理其实很简单，如下所示： client \\ ^ \\ \\ v \\ node 1 \u003c--- node 2 | ^ SNAT | | ---\u003e v | endpoint 当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。 而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。 可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。所以此时，Pod 就会直接将回复发给client。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。 所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。 当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。 所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。 而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示： client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | v endpoint 当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2.3 LoadBalancer 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service。 --- kind: Service apiVersion: v1 metadata: name: example-service spec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer 在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。 所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:2:3","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2.4 ExternalName 而第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName。举个例子： kind: Service apiVersion: v1 metadata: name: my-service spec: type: ExternalName externalName: my.database.example.com 在上述 Service 的 YAML 文件中，我指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。 这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。 所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。 这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。 此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子： kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:2:4","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"3. 故障诊断 在理解了 Kubernetes Service 机制的工作原理之后，很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"1. Service 无法通过 DNS 访问 你需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题 一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常： # 在一个Pod里执行 $ nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:3:1","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"2. Service 无法通过 ClusterIP 访问 此时首先应该检查的是这个 Service 是否有 Endpoints： $ kubectl get endpoints hostnames NAME ENDPOINTS hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示： $ kubectl logs \u003ckube-proxy-pod-name\u003e I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \"/kube-proxy\" I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier. I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable. I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53] I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53] I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443] I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master I1027 22:14:54.040048 5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCP I1027 22:14:54.040154 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDP I1027 22:14:54.040223 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了 一个 iptables 模式的 Service 对应的规则，它们包括： 1）KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应； 2）KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； 3）KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 4）如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:3:2","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"4. 小结 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。 ClusterIP：集群内部IP，也是默认方法方式。 NodePort：通过节点IP+静态端口访问，NodePort 服务会将流量路由到 ClusterIP 服务。 LoadBalancer：使用云厂商提供的负载均衡向外暴露服务，可以将流量路由到 NodePort 服务或者ClusterIP 服务。 ExternalName：通过返回 CNAME 值的方式将服务映射到指定的域名。 ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"5. 参考 https://kubernetes.io/docs/concepts/services-networking/service/ https://draveness.me/kubernetes-service/ 深入剖析Kubernetes ","date":"2021-04-09","objectID":"/posts/kubernetes/05-service-access/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(五)---Service 的几种访问方式","uri":"/posts/kubernetes/05-service-access/"},{"categories":["Kubernetes"],"content":"Kubernetes Service实现原理","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"本文主要讲述了 Kubernetes 中的 Service 的具体实现原理。所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:0:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"1. 概述 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。 一个最典型的 Service 定义，如下所示： apiVersion: v1 kind: Service metadata: name: hostnames spec: selector: app: hostnames ports: - name: default protocol: TCP port: 80 targetPort: 9376 spec.selector 字段表明这个 Service 只代理携带了 app=hostnames 标签的 Pod。并且，这个 Service 的 80 端口，代理的是 Pod 的 9376 端口。 然后，我们的应用的 Deployment，如下所示： apiVersion: apps/v1 kind: Deployment metadata: name: hostnames spec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP 这个应用的作用，就是每次访问 9376 端口时，返回它自己的 hostname。 而被 selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get endpoints 命令看到它们，如下所示： $ kubectl get endpoints hostnames NAME ENDPOINTS hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。 而此时，通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod 了: $ kubectl get svc hostnamesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhostnames ClusterIP 10.0.1.175 \u003cnone\u003e 80/TCP 5s 这个 VIP 地址是 Kubernetes 自动为 Service 分配的。 $ curl 10.0.1.175:80 hostnames-0uton $ curl 10.0.1.175:80 hostnames-yp2kp $ curl 10.0.1.175:80 hostnames-bvc05 通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。 对于这种方式，我们称为：ClusterIP 模式的 Service。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:1:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"2. 原理 实际上，Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:2:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"固定入口 举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示： -A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 可以看到，这条 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。 而我们前面已经看到，10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。 并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。 因为需要指定具体端口时才会匹配上 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:2:1","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"iptables 链 我们即将跳转到的 KUBE-SVC-NWV5X2332I4OT4T3 规则实际上是一组规则的集合，如下所示： -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR 可以看到，这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。 而随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。 而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。 需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。 这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。 通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示： -A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376 -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376 可以看到，这三条链，其实是三条 DNAT 规则。而且在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。 DNAT规则的作用，就是将流入的IP包的目的地址和端口，改成被代理Pod的IP地址和端口。 这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。 然后，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:2:2","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"3. IPVS 模式 kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。 当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新,很明显会影响到整体性能。 所以说，一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。 而 IPVS 模式的 Service，就是解决这个问题的一个行之有效的方法。 IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示： # ip addr ... 73：kube-ipvs0：\u003cBROADCAST,NOARP\u003e mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff inet 10.0.1.175/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever 而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示： # ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.102.128.4:80 rr -\u003e 10.244.3.6:9376 Masq 1 0 0 -\u003e 10.244.1.7:9376 Masq 1 0 0 -\u003e 10.244.2.3:9376 Masq 1 0 0 可以看到，这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。 这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:3:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"4. DNS 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录），由 kube-dns 模块负责。 对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：\u003cserviceName\u003e.\u003cnamespace\u003e.svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。 而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：\u003cpodName\u003e.\u003cserviceName\u003e.\u003cnamesapce\u003e.svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。 当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。 此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：podIp.myNameSpace.pod.cluster.local。这条记录指向 Pod 的 IP 地址。 而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：myPodName.myServiceName.myNameSpace.svc.cluster.local。这条记录也指向 Pod 的 IP 地址。 ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:4:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"5. 小结 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。 Service 原理： 1）首先给 Service 分配一个VIP，然后增加 iptables 规则将访问该 IP 的请求转发到后续的 iptables 链。 KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应； 2）iptables 链实际是一个集合，包含了各个 Pod 的IP（这些称为 Service 的 Endpoints），使用 Round Robin 方式的负载均衡。 KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 3）然后，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。 上述模式需要维护大量 iptables，在大量Pod的情况下，性能不佳，于是出现了 IPVS 模式。以创建虚拟网卡，并分配虚拟IP的形式，直接使用Linux 的 IPVS 模块，由于将转发逻辑放到了 Linux 内核中执行，性能上有所提升。 DNS： 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。 ClusterIP：\u003cserviceName\u003e.\u003cnamespace\u003e.svc.cluster.local Headless Service：\u003cpodName\u003e.\u003cserviceName\u003e.\u003cnamesapce\u003e.svc.cluster.local ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:5:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["Kubernetes"],"content":"6. 参考 https://kubernetes.io/docs/concepts/services-networking/service/ https://draveness.me/kubernetes-service/ 深入剖析Kubernetes ","date":"2021-04-02","objectID":"/posts/kubernetes/04-service-core/:6:0","tags":["Kubernetes"],"title":"Kubernetes教程(四)---Service核心原理","uri":"/posts/kubernetes/04-service-core/"},{"categories":["protobuf"],"content":"protobuf 核心编码原理","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"本章主要记录了 protobuf 的核心编码原理，包括 Varint 编码、ZigZag编码及 protobuf 特有的 Message Structure 编码结构等。 Protocol buffers 核心就是对单个数据的编码（Varint 编码）以及对数据整体的编码（Message Structure 编码）。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:0:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"1. Varint 编码 protobuf 编码主要依赖于 Varint 编码。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:1:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"原理 Varint 是一种紧凑的表示数字的方法。它用一个或多个字节来表示一个数字，值越小的数字使用越少的字节数。这能减少用来表示数字的字节数。 Varint 中的每个字节（最后一个字节除外）都设置了最高有效位（msb），这一位表示是否还会有更多字节出现。每个字节的低 7 位用于以 7 位组的形式存储数字的二进制补码表示，最低有效组首位。 最高位为1代表后面7位仍然表示数字，否则为0，后面7位用原码补齐。 如果用不到 1 个字节，那么最高有效位设为 0 ，如下面这个例子，1 用一个字节就可以表示，所以 msb 为 0. 0000 0001 如果需要多个字节表示，msb 就应该设置为 1 。例如 300，如果用 Varint 表示的话： 1010 1100 0000 0010 编码方式 1）将被编码数转换为二进制表示 2）从低位到高位按照 7位 一组进行划分 3）将大端序转为小端序，即以分组为单位进行首尾顺序交换 因为 protobuf 使用是小端序，所以需要转换一下 4）给每组加上最高有效位(最后一个字节高位补0，其余各字节高位补1)组成编码后的数据。 5）最后转成 10 进制。 图中对数字123456进行 varint 编码： 1）123456 用二进制表示为1 11100010 01000000， 2）每次从低向高取 7位 变成111 1000100 1000000 3）大端序转为小端序，即交换字节顺序变成1000000 1000100 111 4）然后加上最高有效位(即：最后一个字节高位补0，其余各字节高位补1)变成11000000 11000100 00000111 5）最后再转成 10进制，所以经过 varint 编码后 123456 占用三个字节分别为192 196 7。 解码的过程就是将字节依次取出，去掉最高有效位，因为是小端排序所以先解码的字节要放在低位，之后解码出来的二进制位继续放在之前已经解码出来的二进制的高位最后转换为10进制数完成varint编码的解码过程。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:1:1","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"缺点 负数需要10个字节显示（因为计算机定义负数的符号位为数字的最高位）。 具体是先将负数是转成了 long 类型，再进行 varint 编码，这就是占用 10个 字节的原因了。 protobuf 采取的解决方式：使用 sint32/sint64 类型表示负数，通过先采用 Zigzag 编码，将正数、负数和0都映射到无符号数，最后再采用 varint 编码。 具体实现 github.com/golang/protobuf 编码 const maxVarintBytes = 10 // maximum length of a varint // 返回Varint类型编码后的字节流 func EncodeVarint(x uint64) []byte { var buf [maxVarintBytes]byte var n int // 下面的编码规则需要详细理解: // 1.每个字节的最高位是保留位, 如果是1说明后面的字节还是属于当前数据的,如果是0,那么这是当前数据的最后一个字节数据 // 看下面代码,因为一个字节最高位是保留位,那么这个字节中只有下面7bits可以保存数据 // 所以,如果x\u003e127,那么说明这个数据还需大于一个字节保存,所以当前字节最高位是1,看下面的buf[n] = 0x80 | ... // 0x80说明将这个字节最高位置为1, 后面的x\u00260x7F是取得x的低7位数据, 那么0x80 | uint8(x\u00260x7F)整体的意思就是 // 这个字节最高位是1表示这不是最后一个字节,后面7为是正式数据! 注意操作下一个字节之前需要将x\u003e\u003e=7 // 2.看如果x\u003c=127那么说明x现在使用7bits可以表示了,那么最高位没有必要是1,直接是0就ok!所以最后直接是buf[n] = uint8(x) // // 如果数据大于一个字节(127是一个字节最大数据), 那么继续, 即: 需要在最高位加上1 for n = 0; x \u003e 127; n++ { // x\u00260x7F表示取出下7bit数据, 0x80表示在最高位加上1 buf[n] = 0x80 | uint8(x\u00260x7F) // 右移7位, 继续后面的数据处理 x \u003e\u003e= 7 } // 最后一个字节数据 buf[n] = uint8(x) n++ return buf[0:n] } 解码 func DecodeVarint(buf []byte) (x uint64, n int) { for shift := uint(0); shift \u003c 64; shift += 7 { if n \u003e= len(buf) { return 0, 0 } b := uint64(buf[n]) n++ // 下面这个分成三步走: // 1: b \u0026 0x7F 获取下7bits有效数据 // 2: (b \u0026 0x7F) \u003c\u003c shift 由于是小端序, 所以每次处理一个Byte数据, 都需要向高位移动7bits // 3: 将数据x和当前的这个字节数据 | 在一起 x |= (b \u0026 0x7F) \u003c\u003c shift if (b \u0026 0x80) == 0 { return x, n } } // The number is too large to represent in a 64-bit value. return 0, 0 } ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:1:2","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"2. ZigZag编码 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:2:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"原理 ZigZag 是将符号数统一映射到无符号号数的一种编码方案，具体映射函数为： Zigzag(n) = (n \u003c\u003c 1) ^ (n \u003e\u003e 31), n 为 sint32 时 Zigzag(n) = (n \u003c\u003c 1) ^ (n \u003e\u003e 63), n 为 sint64 时 比如：对于0 -1 1 -2 2映射到无符号数 0 1 2 3 4。 原始值 映射值 0 0 -1 1 1 2 2 3 -2 4 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:2:1","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"具体实现 编码 /** * Encode a ZigZag-encoded 32-bit value. ZigZag encodes signed integers * into values that can be efficiently encoded with varint. (Otherwise, * negative values must be sign-extended to 64 bits to be varint encoded, * thus always taking 10 bytes on the wire.) * * @param n A signed 32-bit integer. * @return An unsigned 32-bit integer, stored in a signed int because * Java has no explicit unsigned support. */ public static int encodeZigZag32(final int n) { // Note: the right-shift must be arithmetic return (n \u003c\u003c 1) ^ (n \u003e\u003e 31); } /** * Encode a ZigZag-encoded 64-bit value. ZigZag encodes signed integers * into values that can be efficiently encoded with varint. (Otherwise, * negative values must be sign-extended to 64 bits to be varint encoded, * thus always taking 10 bytes on the wire.) * * @param n A signed 64-bit integer. * @return An unsigned 64-bit integer, stored in a signed int because * Java has no explicit unsigned support. */ public static long encodeZigZag64(final long n) { // Note: the right-shift must be arithmetic return (n \u003c\u003c 1) ^ (n \u003e\u003e 63); } 解码 /** * Decode a ZigZag-encoded 32-bit value. ZigZag encodes signed integers into values that can be * efficiently encoded with varint. (Otherwise, negative values must be sign-extended to 64 bits * to be varint encoded, thus always taking 10 bytes on the wire.) * * @param n An unsigned 32-bit integer, stored in a signed int because Java has no explicit * unsigned support. * @return A signed 32-bit integer. */ public static int decodeZigZag32(final int n) { return (n \u003e\u003e\u003e 1) ^ -(n \u0026 1); } /** * Decode a ZigZag-encoded 64-bit value. ZigZag encodes signed integers into values that can be * efficiently encoded with varint. (Otherwise, negative values must be sign-extended to 64 bits * to be varint encoded, thus always taking 10 bytes on the wire.) * * @param n An unsigned 64-bit integer, stored in a signed int because Java has no explicit * unsigned support. * @return A signed 64-bit integer. */ public static long decodeZigZag64(final long n) { return (n \u003e\u003e\u003e 1) ^ -(n \u0026 1); } ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:2:2","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"3. Message Structure 编码 protocol buffer 中 message 是一系列键值对。message 的二进制版本只是使用字段号(field’s number 和 wire_type)作为 key。每个字段的名称和声明类型只能在解码端通过引用消息类型的定义（即 .proto 文件）来确定。这一点也是人们常常说的 protocol buffer 比 JSON，XML 安全一点的原因，如果没有数据结构描述 .proto 文件，拿到数据以后是无法解析成正常的数据的。 编码后结果如下 当消息编码时，键和值被连接成一个字节流。当消息被解码时，解析器需要能够跳过它无法识别的字段。这样，可以将新字段添加到消息中，而不会破坏不知道它们的旧程序。这就是所谓的 “向后”兼容性。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"1. wire_type 在 protobuf 中的 wire_type 取值如下表： Type Meaning Userd For 0 Varint int32,int64,uint32,uint64,sint32,sint64,bool,enum 1 64-bit fixed64,sfix64,double 2 Length-delimited string,bytes,embedded messages,oacked repeated field 3 Strart Group groups(deprecated) 4 End Group groups(deprecated) 5 32-bit fixed 32,sfixed32,float 其中 3、4已经废弃了，可选值为0、1、2、5。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:1","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"2. Tag key 是使用该字段的 field_number 与wire_type 取|(或运算)后的值，field_number 是定义 proto 文件时使用的 tag 序号 (field_number \u003c\u003c 3)|wire_type 左移3位是因为wire_type最大取值为5，需要占3个bit，这样左移+或运算之后得到的结果就是，高位为field_number，低位为wire_type。 比如下面这个 message message Test { required int32 a = 1; } field_number=1，wire_type=0，按照公式计算（1«3|0） 结果就是 1000。 低三位 000 表示wire_type = 0； 高位 1 表示 field_number = 1。 再使用 Varints 编码后结果就是 08 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:2","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"3. Signed Integers 编码 Google Protocol Buffer 定义了 sint32 这种类型，采用 zigzag 编码。将所有整数映射成无符号整数，然后再采用 varint 编码方式编码，这样，绝对值小的整数，编码后也会有一个较小的 varint 编码值。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:3","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"4. Non-varint Numbers Non-varint 数字比较简单，double 、fixed64 的 wire_type 为 1，在解析时告诉解析器，该类型的数据需要一个 64 位大小的数据块即可。同理，float 和 fixed32 的 wire_type 为5，给其 32 位数据块即可。两种情况下，都是高位在后，低位在前。 说 Protocol Buffer 压缩数据没有到极限，原因就在这里，因为并没有压缩 float、double 这些浮点类型。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:4","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"5. 字符串 wire_type 类型为 2 的数据，是一种指定长度的编码方式：key + length + content，key 的编码方式是统一的，length 采用 varints 编码方式，content 就是由 length 指定长度的 Bytes。 举例，假设定义如下的 message 格式： message Test2 { optional string b = 2; } 设置该值为\"testing\"，二进制格式查看： 12 07 74 65 73 74 69 6e 67 74 65 73 74 69 6e 67 是“testing”的 UTF8 代码。 此处，key 是16进制表示的，所以展开是： 12 -\u003e 0001 0010，后三位 010 为 wire type = 2，0001 0010 右移三位为 0000 0010，即 tag = 2。 length 此处为 7，后边跟着 7 个bytes，即我们的字符串\"testing\"。 所以 wire_type 类型为 2 的数据，编码的时候会默认转换为 T-L-V (Tag - Length - Value)的形式。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:5","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"6. 嵌入式 message 假设，定义如下嵌套消息： message Test3 { optional Test1 c = 3; } 设置字段为整数150，编码后的字节为： 1a 03 08 96 01 08 96 01 这三个代表的是 150，上面讲解过，这里就不再赘述了。 1a -\u003e 0001 1010，后三位 010 为 wire type = 2，0001 1010 右移三位为 0000 0011，即 tag = 3。 length 为 3，代表后面有 3 个字节，即 08 96 01 。 需要转变为 T - L - V 形式的还有 string, bytes, embedded messages, packed repeated fields （即 wire_type 为 2 的形式都会转变成 T - L - V 形式） ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:6","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"7. Packed Repeated Fields 在 proto3 中 Repeated 字段默认就是以这种方式处理。对于 packed repeated 字段，如果 message 中没有赋值，则不会出现在编码后的数据中。否则的话，该字段所有的元素会被打包到单一一个 key-value 对中，且它的 wire_type=2，长度确定。每个元素正常编码，只不过其前没有标签 tag。例如有如下 message 类型： message Test4 { repeated int32 d = 4 [packed=true]; } 构造一个 Test4 字段，并且设置 repeated 字段 d 3个值：3，270和86942，编码后： 22 // tag 0010 0010(field number 010 0 = 4, wire type 010 = 2) 06 // payload size (设置的length = 6 bytes) 03 // first element (varint 3) 8E 02 // second element (varint 270) 9E A7 05 // third element (varint 86942) 形成了 Tag - Length - Value - Value - Value …… 对。 只有原始数字类型（使用varint，32位或64位）的重复字段才可以声明为“packed”。 有一点需要注意，对于 packed 的 repeated 字段，尽管通常没有理由将其编码为多个 key-value 对，编码器必须有接收多个 key-pair 对的准备。这种情况下，payload 必须是串联的，每个 pair 必须包含完整的元素。 Protocol Buffer 解析器必须能够解析被重新编译为 packed 的字段，就像它们未被 packed 一样，反之亦然。这允许以正向和反向兼容的方式将[packed = true]添加到现有字段。 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:3:7","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"4. 小结 Protocol Buffer 利用 Varint 原理压缩数据，同时使用 Tag - Value (Tag - Length - Value)的编码结构的实现，减少了分隔符的使用，数据存储更加紧凑。 protocol buffers 在序列化方面，与 XML 相比，有诸多优点： 更加简单 数据体积小 3- 10 倍 更快的反序列化速度，提高 20 - 100 倍 可以自动化生成更易于编码方式使用的数据访问类 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:4:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"5. 参考 https://halfrost.com/protobuf_encode https://developers.google.com/protocol-buffers/docs/encoding https://juejin.cn/post/6844903953327456263 ","date":"2021-03-12","objectID":"/posts/protobuf/02-encode-core/:5:0","tags":["protobuf"],"title":"protobuf教程(二)---核心编码原理","uri":"/posts/protobuf/02-encode-core/"},{"categories":["protobuf"],"content":"proto文件中引入其他proto文件","date":"2021-03-06","objectID":"/posts/protobuf/01-import/","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"本章主要介绍了如何在 proto 文件中引入其他 proto 文件。 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:0:0","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"1. 概述 Protocol buffers 是一种语言无关、平台无关的可扩展机制或者说是数据交换格式，用于序列化结构化数据。与 XML、JSON 相比，Protocol buffers 序列化后的码流更小、速度更快、操作更简单。 Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:1:0","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"2. 详解 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:2:0","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"基本定义 一个简单的 protobuf 文件定义如下: syntax = \"proto3\"; option go_package = \"protobuf/import;proto\"; package import; message Computer { string name = 1; } syntax = “proto3”;—指定使用 proto3 语法 option go_package = “protobuf/import;proto”;—前一个参数用于指定生成文件的位置，后一个参数指定生成的 .go 文件的 package 。具体语法如下： option go_package = \"{out_path};out_go_package\"; 注意：这里指定的 out_path 并不是绝对路径，只是相对路径或者说只是路径的一部分，和 protoc 的 --go_out 拼接后才是完整的路径。 也使用--go_opt=paths=source_relative直接指定 protoc 中 指定的是绝对路径，这样就不会去管 protobuf 文件中指定的路径。 package import;—表示当前 protobuf 文件属于 import包，这个package不是 Go 语言中的那个package。 这个 package 主要在导入外部 proto 文件时用到。 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:2:1","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"导入其他proto文件 要导入其他 proto 文件只需要使用import键字，具体如下： import \"protobuf/import/component.proto\"; 导入后则通过被导入文件包名.结构体名使用。 component.proto 文件中 package 指定为 import，所以这里通过 import.CPU 和 import.Memory 语法进行引用。 完整代码如下: syntax = \"proto3\"; option go_package = \"protobuf/import;proto\"; package import; import \"protobuf/import/component.proto\"; message Computer { string name = 1; import.CPU cpu = 2; import.Memory memory = 3; } 导入 compoent.proto 文件，这个也是相对路径，具体和 protoc –proto_path 组合起来才是完整路径。 一般指定为项目根目录的次一级目录，编译的时候直接在根目录编译。 protoc 编译的时候通过 --proto_path 指定在哪个目录去寻找 import 指定的文件。 比如指定 --proto_path=.即表示在当前目录下去寻找protobuf/import/compoent.proto这个文件。 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:2:2","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"3. 完整例子 目录结构如下 lixd@17x:~/17x/projects/grpc-go-example$ tree ├── protobuf │ │ │ └── import │ ├── compoent.proto │ └── computer.proto └── README.md ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:3:0","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"component.proto syntax = \"proto3\"; option go_package = \"protobuf/import;proto\"; package import; message CPU { string Name = 1; int64 Frequency = 2; } message Memory { string Name = 1; int64 Cap = 2; } ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:3:1","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"computer.proto syntax = \"proto3\"; option go_package = \"protobuf/import;proto\"; package import; import \"protobuf/import/component.proto\"; message Computer { string name = 1; import.CPU cpu = 2; import.Memory memory = 3; } ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:3:2","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"protoc 编译 在项目根路径(grpc-go-example)下进行编译 lixd@17x:~/17x/projects/grpc-go-example$ protoc --proto_path=. --go_out=. ./protobuf/import/*.proto 参数详解： 1）–proto_path=. 指定在当前目录( grpc-go-example)寻找 import 的文件（默认值也是当前目录） 然后 protobuf 文件中的 import 路径如下 import \"protobuf/import/component.proto\"; 所以最终会去找 grpc-go-example/protobuf/import/component.proto。 --proto_path和import是可以互相调整的，只需要能找到就行。 建议protoc参数 –proto_path 指定为根目录，proto文件中的import 则从根目录次一级目录开始。 2）–go_out=. 指定将生成文件放在当前目录( grpc-go-example)，同时因为 proto 文件中也指定了目录为protobuf/import,具体如下： option go_package = \"protobuf/import;proto\"; 所以最终生成目录为--go_out+go_package= grpc-go-example/protobuf/import。 可以通过参数 --go_opt=paths=source_relative 来指定使用绝对路径，从而忽略掉 proto 文件中的 go_package 路径，直接生成在 –go_out 指定的路径。 3）./protobuf/import/*.proto 指定编译 import 目录下的所有 proto 文件，由于有文件的引入所以需要一起编译才能生效。 当然也可以一个一个编译，只要把相关文件都编译好即可。 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:3:3","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"Test func Print() { c := Computer{ Name: \"alienware\", Cpu: \u0026CPU{ Name: \"intel\", Frequency: 4096, }, Memory: \u0026Memory{ Name: \"芝奇\", Cap: 8192, }, } fmt.Println(c.String()) } ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:3:4","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["protobuf"],"content":"4. 小结 1）通过import \"{path}\"; 命令引入； 2）导入后通过被导入文件包名.结构体名方式使用； 3）编译时通过--proto_path=. 指定寻找proto文件的目录，一起编译。 ","date":"2021-03-06","objectID":"/posts/protobuf/01-import/:4:0","tags":["protobuf"],"title":"protobuf教程(一)---引入其他proto文件","uri":"/posts/protobuf/01-import/"},{"categories":["Docker"],"content":"Docker容器网络实现分析","date":"2021-02-27","objectID":"/posts/docker/04-container-network/","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"本文主要介绍了 Docker 容器网络实现原理。 ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:0:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"1. 概述 一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。 而所谓“网络栈”，就包括了：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。 需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net=host），即：不开启 Network Namespace，比如： $ docker run –d –net=host --name nginx-host nginx 在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。 像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。 所以，在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口。 这时候，一个显而易见的问题就是：这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？ ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:1:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"2. 网桥 我们可以把每一个容器看做一台主机，它们都有一套独立的“网络栈”。 如果你想要实现两台主机之间的通信，最直接的办法，就是把它们用一根网线连接起来； 而如果你想要实现多台主机之间的通信，那就需要用网线，把它们连接在一台交换机上。 在 Linux 中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上。 此处的端口指的是交换机的物理接口。 而为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。 可是，我们又该如何把这些容器“连接”到 docker0 网桥上呢？ 这时候，我们就需要使用一种名叫Veth Pair的虚拟设备了。 Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。 这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线” ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:2:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"3. 例子 比如，现在我们启动了一个叫作 nginx-1 的容器： $ docker run -d --name nginx-1 nginx 然后进入到这个容器中查看一下它的网络设备： # 在宿主机上 $ docker exec -it nginx-1 /bin/bash # 在容器里 # 此处需要手动安装工具包 apt-get update apt install net-tools root@2b3c181aecf1:/# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.18.0.2 netmask 255.255.0.0 broadcast 172.18.255.255 ether 02:42:ac:12:00:02 txqueuelen 0 (Ethernet) RX packets 5153 bytes 9012676 (8.5 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5002 bytes 349449 (341.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 路由规则 root@a1fe241efc33:/bin# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 172.18.0.1 0.0.0.0 UG 0 0 0 eth0 172.18.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 可以看到，这个容器里有一张叫作 eth0 的网卡，它正是一个 Veth Pair 设备在容器里的这一端。 同时通过 route 命令查看 nginx-1 容器的路由表，我们可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.18.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.18.0.0 路由规则）。 而这个 Veth Pair 设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它，如下所示： [root@iz2ze0ephck4d0aztho5r5z ~]# ifconfig docker0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 ether 02:42:e0:74:96:18 txqueuelen 0 (Ethernet) RX packets 5458 bytes 313097 (305.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5571 bytes 9066021 (8.6 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth14c464c: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether 36:ee:eb:5c:6a:16 txqueuelen 0 (Ethernet) RX packets 5005 bytes 349616 (341.4 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5156 bytes 9012897 (8.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 需要安装bridge-utils工具包 yum install bridge-utils -y [root@iz2ze0ephck4d0aztho5r5z ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242e0749618 no veth14c464c 通过 ifconfig 命令的输出，你可以看到，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth14c464c。并且，通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上。 这时候，如果我们再在这台宿主机上启动另一个 Docker 容器，比如 nginx-2： $ docker run –d --name nginx-2 nginx $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242e0749618 no veth14c464c vethe233e2f 你就会发现一个新的、名叫 vethb4963f3 的虚拟网卡，也被“插”在了 docker0 网桥上。 这时候，如果你在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.18.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的。 具体IP信息可以通过这个 docker network inspect bridge 命令查看 # 进入容器 nginx-1 $ docker exec -it nginx-1 /bin/bash # 安装 ping 工具 # apt install iputils-ping root@a1fe241efc33:/bin# ping 172.18.0.3 PING 172.18.0.3 (172.18.0.3) 56(84) bytes of data. 64 bytes from 172.18.0.3: icmp_seq=1 ttl=64 time=0.102 ms 64 bytes from 172.18.0.3: icmp_seq=2 ttl=64 time=0.093 ms ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:3:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"4. 原理 这其中的原理其实非常简单。 当你在 nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.18.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。 而这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机。 而要通过二层网络到达 nginx-2 容器，就需要有 172.18.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址。 我们前面提到过，这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。 一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥。 所以，在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.18.0.3 所对应的 MAC 地址回复给 nginx-1 容器。 有了这个目的 MAC 地址，nginx-1 容器的 eth0 网卡就可以将数据包发出去。 而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth14c464c 虚拟网卡上。不过，此时这个 veth14c464c 网卡的网络协议栈的资格已经被“剥夺”，所以这个数据包就直接流入到了 docker0 网桥里。 docker0 处理转发的过程，则继续扮演二层交换机的角色。此时，docker0 网桥根据数据包的目的 MAC 地址（也就是 nginx-2 容器的 MAC 地址），在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：vethe233e2f，然后把数据包发往这个端口。 而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。所以，nginx-2 容器看到的情况是，它自己的 eth0 网卡上出现了流入的数据包。 这样，nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。以上，就是同一个宿主机上的不同容器通过 docker0 网桥进行通信的流程了。 熟悉了 docker0 网桥的工作方式，你就可以理解，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。 与之类似地，当你在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里。这个过程的示意图，如下所示： 同样地，当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3，它发出的请求数据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。当然，这个过程的实现要求这两台宿主机本身是连通的。这个过程的示意图，如下所示： 所以：当你遇到容器连不通“外网”的时候，你都应该先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常，往往就能够找到问题的答案了。 ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:4:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"5. 跨主通信 如果在另外一台宿主机（比如：10.168.0.3）上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？这个问题，其实就是容器的跨主通信问题。 在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。 如果我们通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上，不就可以相互通信了吗？ 可以看到，构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。 我们只需要让宿主机收到网络包后能转发到正确的节点，节点收到发给自己的网络包后能转发给正确的 Container 就行了。 ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:5:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"6. 小结 本文介绍了在本地环境下，单机容器网络的实现原理和 docker0 网桥的作用。 这里的关键在于，容器要想跟外界进行通信，它发出的 IP 包就必须从它的 Network Namespace 里出来，来到宿主机上。 而解决这个问题的方法就是：为容器创建一个一端在容器里充当默认网卡、另一端在宿主机上的 Veth Pair 设备。 从容器A中的Veth Pair 设备传递到宿主机的docker0网桥，然后再次通过Veth Pair设备传入容器B。 ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:6:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"7. 参考 https://www.lifewire.com/layers-of-the-osi-model-illustrated-818017 https://en.wikipedia.org/wiki/Iptables https://draveness.me/docker/ https://www.infoq.cn/article/docker-network-and-pipework-open-source-explanation-practice/# 深入解析Kubernetes ","date":"2021-02-27","objectID":"/posts/docker/04-container-network/:7:0","tags":["Docker"],"title":"Docker教程(四)---容器网络实现分析","uri":"/posts/docker/04-container-network/"},{"categories":["Docker"],"content":"Docker容器核心实现原理分析","date":"2021-02-14","objectID":"/posts/docker/03-container-core/","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"本文主要介绍了 Docker容器的核心实现原理，包括 Namespace、Cgroups、rootfs 等。 接触容器也很长时间了，期间也查阅了不少资料，神秘的 Docker 容器也逐渐变得不那么神秘，于是想着简单整理一下 docker 容器的核心实现原理。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:0:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"1. 容器与进程 进程就是程序运行起来后的计算机执行环境的总和。 即：计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:1:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"2. 隔离与限制 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:2:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"1. Namespace Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。 在 Linux 下可以根据隔离的属性不同分为不同的 Namespace ： 1）PID Namespace 2）Mount Namespace 3）UTS Namespace 4）IPC Namespace 5）Network Namespace 6）User Namespace Namespace 存在的问题 最大的问题就是隔离得不彻底。 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 容器中修改了时间，实际修改的是宿主机时间，那么宿主机上所有容器的时间都跟着变化了。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:2:1","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"2. Cgroups Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups 的全称是 Linux Control Group。 它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。 #查看 cgroups 相关文件 $ mount -t cgroup # 结果大概是这样的 cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) 可以看到，在/sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。也就是这台机器当前可以被 Cgroups 进行限制的资源种类。 比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： ls /sys/fs/cgroup/cpu # 目录下大概有这么一些内容 assist cgroup.event_control cgroup.sane_behavior cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat notify_on_release system.slice cgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares release_agent tasks 例子：限制CPU使用 而这样的配置文件又如何使用呢？ 你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下： [root@iz2ze0ephck4d0aztho5r5z cpu]# mkdir container [root@iz2ze0ephck4d0aztho5r5z cpu]# ls container/ cgroup.clone_children cgroup.event_control cgroup.procs cpuacct.stat cpuacct.usage cpuacct.usage_percpu cpu.cfs_period_us cpu.cfs_quota_us cpu.rt_period_us cpu.rt_runtime_us cpu.shares cpu.stat notify_on_release tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。 现在，我们在后台执行这样一条脚本: $ while : ; do : ; done \u0026 [1] 27218 显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 27218。 查看一下CPU占用 $ top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27218 root 20 0 115680 672 152 R 99.9 0.0 2:07.07 bash 果然这个PID=27218的进程占用了差不多100%的CPU。 结下来我们就通过Cgroups对其进行限制，这里就用前面创建的 container这个“控制组”。 我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： $ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1 $ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： $ echo 20000 \u003e /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 这样意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。 接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了： $ echo 27218 \u003e /sys/fs/cgroup/cpu/container/tasks 使用 top 指令查看一下 $ top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27218 root 20 0 115680 672 152 R 20 0.0 2:07.07 bash 果然CPU被限制到了20%. 除 CPU 子系统外，Cgroups 的每一个子系统都有其独有的资源限制能力，比如： blkio，为块设备设定I/O 限制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。 而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令： $ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认： $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 Cgroups 存在的问题 Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。 问题 如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。 造成这个问题的原因就是，","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:2:2","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"3. 容器镜像 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:3:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"1. 文件系统 容器中的文件系统是什么样子的? 因为容器中的文件系统经过 Mount Namespace 隔离，所以应该是独立的。 其中 Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。 不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。 Linux 中chroot命令（change root file system）就能很方便的完成上述工作。 而 Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:3:1","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"2. rootfs 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。 所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。实际上，同一台机器上的所有容器，都共享宿主机操作系统的内核。 这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。 不过，正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:3:2","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"3. 镜像层（Layer） Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 通过引入层（layer）的概念，实现了 rootfs 的复用。不必每次都重新创建一个 rootfs，而是基于某一层进行修改即可。 Docker 镜像层用到了一种叫作**联合文件系统（Union File System）**的能力。Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。 例如将目录A和目录B挂载到目录C下面，这样目录C下就包含目录A和目录B的所有文件。 Docker 镜像分为多个层，然后使用 UFS 将这多个层挂载到一个目录下面，这样这个目录就包含了完整的文件了。 UnionFS 在不同系统有各自的实现，所以Docker的不同发行版使用的也不一样，可以通过 docker info 查看。常见有 aufs（ubuntu常用）、overlay2（centos常用） 镜像只包含了静态文件，但是容器会产生实时数据，所以容器的 rootfs 在镜像的基础上增加了可读写层和 Init 层。 即容器 rootfs包括：只读层（镜像rootfs）+ init 层（容器启动时初始化修改的部分数据） + 可读写层（容器中产生的实时数据）。 只读层（镜像rootfs） 它是这个容器的 rootfs 最下面的几层，即镜像中的所有层的总和，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout） 可读写层（容器中产生的实时数据） 它是这个容器的 rootfs 最上面的一层，它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。 而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中，删除操作实现比较特殊（类似于标记删除）。 AUFS的whiteout的实现是通过在上层的可写的目录下建立对应的whiteout隐藏文件来实现的。 为了实现删除操作，aufs（UnionFS的一种实现） 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。 比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。 init 层（容器启动时初始化修改的部分数据） 它是一个以“-init”结尾的层，夹在只读层和读写层之间，Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 为什么需要init层？ 比如 hostname 这样的数据，原本是属于镜像层的一部分，要修改的话只能在可读写层进行修改，但是又不想在 docker commit 的时候把这些信息提交上去，所以使用init 层来保存这些修改。 可以理解为提交代码的时候一般也不会把各种配置信息一起提交上去。 docker commit 只会提交 只读层和可读写层。 ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:3:3","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"4. 小结 Docker 容器的实现主要使用了如下3个功能： 1）Linux Namespace 的隔离能力 2）Linux Cgroups 的限制能力 3）基于 rootfs 的文件系统 Docker 容器全景图如下： 图源：深入剖析Kubernetes ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:4:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["Docker"],"content":"5. 参考 https://draveness.me/docker/ https://en.wikipedia.org/wiki/Linux_namespaces https://0xax.gitbooks.io/linux-insides/content/Cgroups/linux-cgroups-1.html https://coolshell.cn/articles/17061.html 深入剖析Kubernetes ","date":"2021-02-14","objectID":"/posts/docker/03-container-core/:5:0","tags":["Docker"],"title":"Docker教程(三)---核心实现原理分析","uri":"/posts/docker/03-container-core/"},{"categories":["gRPC"],"content":"gRPC 中的retry自动重试配置","date":"2021-02-06","objectID":"/posts/grpc/09-retry/","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"本文主要记录了如何使用 gRPC 中的 自动重试功能。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC 中已经内置了 retry 功能，可以直接使用，不需要我们手动来实现，非常方便。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"2. Demo ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"Server 为了测试 retry 功能，服务端做了一点调整。 记录客户端的请求次数，只有满足条件的那一次（这里就是请求次数模4等于0的那一次）才返回成功，其他时候都返回失败。 package main import ( \"context\" \"flag\" \"fmt\" \"log\" \"net\" \"sync\" \"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" ) var port = flag.Int(\"port\", 50052, \"port number\") type failingServer struct { pb.UnimplementedEchoServer mu sync.Mutex reqCounter uint reqModulo uint } // maybeFailRequest 手动模拟请求失败 一共请求n次，前n-1次都返回失败，最后一次返回成功。 func (s *failingServer) maybeFailRequest() error { s.mu.Lock() defer s.mu.Unlock() s.reqCounter++ if (s.reqModulo \u003e 0) \u0026\u0026 (s.reqCounter%s.reqModulo == 0) { return nil } return status.Errorf(codes.Unavailable, \"maybeFailRequest: failing it\") } func (s *failingServer) UnaryEcho(ctx context.Context, req *pb.EchoRequest) (*pb.EchoResponse, error) { if err := s.maybeFailRequest(); err != nil { log.Println(\"request failed count:\", s.reqCounter) return nil, err } log.Println(\"request succeeded count:\", s.reqCounter) return \u0026pb.EchoResponse{Message: req.Message}, nil } func main() { flag.Parse() address := fmt.Sprintf(\":%v\", *port) lis, err := net.Listen(\"tcp\", address) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } fmt.Println(\"listen on address\", address) s := grpc.NewServer() // 指定第4次请求才返回成功，用于测试 gRPC 的 retry 功能。 failingservice := \u0026failingServer{ reqModulo: 4, } pb.RegisterEchoServer(s, failingservice) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"Client 客户端则是建立连接的时候通过grpc.WithDefaultServiceConfig()配置好 retry 功能。 package main import ( \"context\" \"flag\" \"log\" \"time\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" ) var ( addr = flag.String(\"addr\", \"localhost:50052\", \"the address to connect to\") // 更多配置信息查看官方文档： https://github.com/grpc/grpc/blob/master/doc/service_config.md // service这里语法为\u003cpackage\u003e.\u003cservice\u003e package就是proto文件中指定的package，service也是proto文件中指定的 Service Name。 // method 可以不指定 即当前service下的所以方法都使用该配置。 retryPolicy = `{ \"methodConfig\": [{ \"name\": [{\"service\": \"echo.Echo\",\"method\":\"UnaryEcho\"}], \"retryPolicy\": { \"MaxAttempts\": 4, \"InitialBackoff\": \".01s\", \"MaxBackoff\": \".01s\", \"BackoffMultiplier\": 1.0, \"RetryableStatusCodes\": [ \"UNAVAILABLE\" ] } }]}` ) func main() { flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithInsecure(), grpc.WithDefaultServiceConfig(retryPolicy)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer func() { if e := conn.Close(); e != nil { log.Printf(\"failed to close connection: %s\", e) } }() c := pb.NewEchoClient(conn) ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() reply, err := c.UnaryEcho(ctx, \u0026pb.EchoRequest{Message: \"Try and Success\"}) if err != nil { log.Fatalf(\"UnaryEcho error: %v\", err) } log.Printf(\"UnaryEcho reply: %v\", reply) } 配置信息具体含义这里先不解释，在后续章节有详细说明。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"Run 先启动服务端 lixd@17x:~/17x/projects/grpc-go-example/features/retry/server$ go run main.go listen on address :50052 2021/02/17 17:35:29 request failed count: 1 在启动客户端 lixd@17x:~/17x/projects/grpc-go-example/features/retry/client$ go run main.go 2021/02/17 17:35:29 UnaryEcho error: rpc error: code = Unavailable desc = maybeFailRequest: failing it exit status 1 emmm 并没有重试。。。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"开启重试 查看文档后发现是需要设置环境变量（GRPC_GO_RETRY=on）。 因为是客户端的重试功能，所以是在客户端设置环境变量，服务端则不需要。 lixd@17x$ export GRPC_GO_RETRY=on lixd@17x$ echo $GRPC_GO_RETRY on 然后重新启动服务端和客户端 lixd@17x:~/17x/projects/grpc-go-example/features/retry/server$ go run main.go listen on address :50052 2021/02/17 17:37:55 request failed count: 1 2021/02/17 17:37:55 request failed count: 2 2021/02/17 17:37:55 request failed count: 3 2021/02/17 17:37:55 request succeeded count: 4 lixd@17x:~/17x/projects/grpc-go-example/features/retry/client$ go run main.go 2021/02/17 17:37:55 UnaryEcho reply: message:\"Try and Success\" 现在重试功能就生效了。 前3次都失败了，且返回错误码是客户端重试策略中指定的UNAVAILABLE,所以都进行了重试，直到第4次成功了就不在重试了。 当然，第4次已经是最大尝试次数了，就算失败也不会重试了。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:2:4","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"3. 配置 Service Config 是以 JSON 格式配置的，具体文档见 service_config.md。 ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"service_config.proto 当前支持的配置信息由 service_config.proto 文件定义，详细信息可以参考该文件，这里贴一部分仅供参考： // Configuration for a method. message MethodConfig { // The names of the methods to which this configuration applies. // - MethodConfig without names (empty list) will be skipped. // - Each name entry must be unique across the entire ServiceConfig. // - If the 'method' field is empty, this MethodConfig specifies the defaults // for all methods for the specified service. // - If the 'service' field is empty, the 'method' field must be empty, and // this MethodConfig specifies the default for all methods (it's the default // config). // // When determining which MethodConfig to use for a given RPC, the most // specific match wins. For example, let's say that the service config // contains the following MethodConfig entries: // // method_config { name { } ... } // method_config { name { service: \"MyService\" } ... } // method_config { name { service: \"MyService\" method: \"Foo\" } ... } // // MyService/Foo will use the third entry, because it exactly matches the // service and method name. MyService/Bar will use the second entry, because // it provides the default for all methods of MyService. AnotherService/Baz // will use the first entry, because it doesn't match the other two. // // In JSON representation, value \"\", value `null`, and not present are the // same. The following are the same Name: // - { \"service\": \"s\" } // - { \"service\": \"s\", \"method\": null } // - { \"service\": \"s\", \"method\": \"\" } message Name { string service = 1; // Required. Includes proto package name. string method = 2; } repeated Name name = 1; // Whether RPCs sent to this method should wait until the connection is // ready by default. If false, the RPC will abort immediately if there is // a transient failure connecting to the server. Otherwise, gRPC will // attempt to connect until the deadline is exceeded. // // The value specified via the gRPC client API will override the value // set here. However, note that setting the value in the client API will // also affect transient errors encountered during name resolution, which // cannot be caught by the value here, since the service config is // obtained by the gRPC client via name resolution. google.protobuf.BoolValue wait_for_ready = 2; // The default timeout in seconds for RPCs sent to this method. This can be // overridden in code. If no reply is received in the specified amount of // time, the request is aborted and a DEADLINE_EXCEEDED error status // is returned to the caller. // // The actual deadline used will be the minimum of the value specified here // and the value set by the application via the gRPC client API. If either // one is not set, then the other will be used. If neither is set, then the // request has no deadline. google.protobuf.Duration timeout = 3; // The maximum allowed payload size for an individual request or object in a // stream (client-\u003eserver) in bytes. The size which is measured is the // serialized payload after per-message compression (but before stream // compression) in bytes. This applies both to streaming and non-streaming // requests. // // The actual value used is the minimum of the value specified here and the // value set by the application via the gRPC client API. If either one is // not set, then the other will be used. If neither is set, then the // built-in default is used. // // If a client attempts to send an object larger than this value, it will not // be sent and the client will see a ClientError. // Note that 0 is a valid value, meaning that the request message // must be empty. google.protobuf.UInt32Value max_request_message_bytes = 4; // The maximum allowed payload size for an individual response or object in a // stream (server-\u003eclient) in bytes. The size which is measured is the // serialized payload after per-message compression (but before stream // compression) in bytes. This applies both to streaming and non-streaming // requests. // // The actual value used is the minim","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"retry config demo 中的配置信息如下： { \"methodConfig\": [{ \"name\": [{\"service\": \"echo.Echo\",\"method\":\"UnaryEcho\"}], \"retryPolicy\": { \"MaxAttempts\": 4, \"InitialBackoff\": \".01s\", \"MaxBackoff\": \".01s\", \"BackoffMultiplier\": 1.0, \"RetryableStatusCodes\": [ \"UNAVAILABLE\" ] }}] } name 指定下面的配置信息作用的 RPC 服务或方法 service：通过服务名匹配，语法为\u003cpackage\u003e.\u003cservice\u003e package就是proto文件中指定的package，service也是proto文件中指定的 Service Name。 method：匹配具体某个方法，proto文件中定义的方法名。 主要关注 retryPolicy，重试策略 MaxAttempts：最大尝试次数 InitialBackoff：默认退避时间 MaxBackoff：最大退避时间 BackoffMultiplier：退避时间增加倍率 RetryableStatusCodes：服务端返回什么错误码才重试 重试机制一般会搭配退避算法一起使用。 即假设第一次请求失败后，等1秒（随便取的一个数）再次请求，又失败后就等2秒在请求，一直重试直达超过指定重试次数或者等待时间就不在重试。 如果不使用退避算法，失败后就一直重试只会增加服务器的压力。如果是因为服务器压力大，导致的请求失败，那么根据退避算法等待一定时间后再次请求可能就能成功。反之直接请求可能会因为压力过大导致服务崩溃。 第一次重试间隔是 random(0, initialBackoff) 第 n 次的重试间隔为 random(0, min( initialBackoff*backoffMultiplier**(n-1) , maxBackoff)) ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"4. 小结 gRPC 中内置了 retry 功能，使用比较简单。 1）客户端建立连接时通过grpc.WithDefaultServiceConfig(retryPolicy)指定重试策略 2）环境变量中开启重试:export GRPC_GO_RETRY=on ","date":"2021-02-06","objectID":"/posts/grpc/09-retry/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(九)---配置retry自动重试","uri":"/posts/grpc/09-retry/"},{"categories":["gRPC"],"content":"使用context进行超时控制","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"本文主要记录了如何使用 context 为 RPC 请求设置超时时间，或者通过 cancel 手动取消本次请求。 ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github 通过 ctx 完成 cancel 和 deadline 功能。 Go 语言中可以通过 ctx 来控制各个 Goroutine，调用 cancel 函数，则该 ctx 上的各个子 Goroutine 都会被一并取消。 gRPC 中同样实现了该功能，在调用方法的时候可以传入 ctx 参数。 gRPC 会通过 HTTP2 HEADERS Frame 来传递相关信息。 ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"2. deadline gRPC 提倡TL;DR: Always set a deadline Deadlines 允许gRPC 客户端设置自己等待多长时间来完成 RPC 操作，直到出现这个错误 DEADLINE_EXCEEDED。但是在正常情况下默认设置是一个很大的数值。 如果不设置截止日期时，如果出现阻塞，那么所有的请求可能在最大请求时间过后才超时，最终可能导致资源被耗尽。 由于类似的问题，在高并发的时候导致了一次事故，具体看数据库连接池该设置多大?记一次由连接池引发的事故 ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"Server 如果客户端传来的消息时 delay 则 sleep 两秒，如果是带[propagate me]前缀的消息则由服务端在延迟 800ms 后发起一次 RPC 调用。 func (s *server) UnaryEcho(ctx context.Context, req *pb.EchoRequest) (*pb.EchoResponse, error) { message := req.Message if strings.HasPrefix(message, \"[propagate me]\") { time.Sleep(800 * time.Millisecond) message = strings.TrimPrefix(message, \"[propagate me]\") return s.client.UnaryEcho(ctx, \u0026pb.EchoRequest{Message: message}) } if message == \"delay\" { time.Sleep(2 * time.Second) } return \u0026pb.EchoResponse{Message: req.Message}, nil } func (s *server) BidirectionalStreamingEcho(stream pb.Echo_BidirectionalStreamingEchoServer) error { for { req, err := stream.Recv() if err == io.EOF { return status.Error(codes.InvalidArgument, \"request message not received\") } if err != nil { return err } message := req.Message if strings.HasPrefix(message, \"[propagate me]\") { time.Sleep(800 * time.Millisecond) message = strings.TrimPrefix(message, \"[propagate me]\") res, err := s.client.UnaryEcho(stream.Context(), \u0026pb.EchoRequest{Message: message}) if err != nil { return err } stream.Send(res) } if message == \"delay\" { time.Sleep(2 * time.Second) } stream.Send(\u0026pb.EchoResponse{Message: message}) } } ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"Client 客户端则是为每次 RPC 调用都指定超时时间为 1秒。 package main import ( \"context\" \"flag\" \"fmt\" \"log\" \"time\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\" ) var addr = flag.String(\"addr\", \"localhost:50051\", \"the address to connect to\") func unaryCall(c pb.EchoClient, requestID int, message string, want codes.Code) { // 每次都指定1秒超时 ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() req := \u0026pb.EchoRequest{Message: message} _, err := c.UnaryEcho(ctx, req) got := status.Code(err) fmt.Printf(\"[%v] wanted = %v, got = %v\\n\", requestID, want, got) } func streamingCall(c pb.EchoClient, requestID int, message string, want codes.Code) { // 每次都指定1秒超时 ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() stream, err := c.BidirectionalStreamingEcho(ctx) if err != nil { log.Printf(\"Stream err: %v\", err) return } err = stream.Send(\u0026pb.EchoRequest{Message: message}) if err != nil { log.Printf(\"Send error: %v\", err) return } _, err = stream.Recv() got := status.Code(err) fmt.Printf(\"[%v] wanted = %v, got = %v\\n\", requestID, want, got) } func main() { flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := pb.NewEchoClient(conn) unaryCall(c, 1, \"world\", codes.OK) unaryCall(c, 2, \"delay\", codes.DeadlineExceeded) unaryCall(c, 3, \"[propagate me]world\", codes.OK) unaryCall(c, 4, \"[propagate me][propagate me]world\", codes.DeadlineExceeded) streamingCall(c, 5, \"[propagate me]world\", codes.OK) streamingCall(c, 6, \"[propagate me][propagate me]world\", codes.DeadlineExceeded) } ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"Run lixd@17x:~/17x/projects/grpc-go-example/features/deadline/server$ go run main.go server listening at port [::]:50051 lixd@17x:~/17x/projects/grpc-go-example/features/deadline/client$ go run main.go [1] wanted = OK, got = OK [2] wanted = DeadlineExceeded, got = DeadlineExceeded [3] wanted = OK, got = OK [4] wanted = DeadlineExceeded, got = DeadlineExceeded [5] wanted = OK, got = OK [6] wanted = DeadlineExceeded, got = DeadlineExceeded 其中请求 2 是传递的 delay 消息服务端会 sleep 两秒，所以触发 deadline，请求4和6 由于有两个[propagate me]前缀，所以会传递两轮，每次 sleep 800ms，再第二轮的时候也会触发 deadline。 请求1为正常请求，请求3和5只传递一轮，只 sleep 800ms 所以没有触发 deadline。 ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"3. cancel 除了等待 deadline 超时之外，客户端还可以主动调用 cancel 取消本次请求。 比如在某次调用中，客户端某个环节报错导致本次请求已经可以直接返回了，这时候在等待服务端返回已经没有意义了。此时就可以直接调用 cancel 取消本次请求，而不是让服务端一直等待到超时才返回。 ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"Server package main import ( \"flag\" \"fmt\" \"io\" \"log\" \"net\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" ) var port = flag.Int(\"port\", 50051, \"the port to serve on\") type server struct { pb.UnimplementedEchoServer } func (s *server) BidirectionalStreamingEcho(stream pb.Echo_BidirectionalStreamingEchoServer) error { for { in, err := stream.Recv() if err != nil { fmt.Printf(\"server: error receiving from stream: %v\\n\", err) if err == io.EOF { return nil } return err } fmt.Printf(\"echoing message %q\\n\", in.Message) stream.Send(\u0026pb.EchoResponse{Message: in.Message}) } } func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } fmt.Printf(\"server listening at port %v\\n\", lis.Addr()) s := grpc.NewServer() pb.RegisterEchoServer(s, \u0026server{}) s.Serve(lis) } ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"Client package main import ( \"context\" \"flag\" \"fmt\" \"log\" \"time\" pb \"github.com/lixd/grpc-go-example/features/proto/echo\" \"google.golang.org/grpc\" ) var addr = flag.String(\"addr\", \"localhost:50051\", \"the address to connect to\") func sendMessage(stream pb.Echo_BidirectionalStreamingEchoClient, msg string) error { fmt.Printf(\"sending message %q\\n\", msg) return stream.Send(\u0026pb.EchoRequest{Message: msg}) } func recvMessage(stream pb.Echo_BidirectionalStreamingEchoClient) { res, err := stream.Recv() if err != nil { fmt.Printf(\"stream.Recv() returned error %v\\n\", err) return } fmt.Printf(\"received message %q\\n\", res.GetMessage()) } func main() { flag.Parse() // 建立连接 conn, err := grpc.Dial(*addr, grpc.WithInsecure()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := pb.NewEchoClient(conn) // 初始化一个带取消功能的ctx ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) stream, err := c.BidirectionalStreamingEcho(ctx) if err != nil { log.Fatalf(\"error creating stream: %v\", err) } // 正常发送消息 if err := sendMessage(stream, \"hello\"); err != nil { log.Fatalf(\"error sending on stream: %v\", err) } if err := sendMessage(stream, \"world\"); err != nil { log.Fatalf(\"error sending on stream: %v\", err) } // 正常接收消息 recvMessage(stream) recvMessage(stream) // 这里调用cancel方法取消 ctx fmt.Println(\"cancelling context\") cancel() // 再次发送消息 这里是否会报错取决于ctx是否检测到前面发送的取消命令(cancel()) if err := sendMessage(stream, \"world\"); err != nil { log.Printf(\"error sending on stream: %v\", err) } // 这里一定会报错 recvMessage(stream) } ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"4. 小结 不管是 cancel 和 deadline 都只需调用方传递对应的 ctx 即可。gRPC 中已经做了对应的实现，所以使用起来和在 Goroutine 中传递 ctx 没有太大的区别。 ctx 可以使用context.WithDeadline()或者context.WithTimeout(),二者效果类似，只是传递的参数不一样。 timeout 只能设置在某一段时间后超时，比如3秒后超时，deadline 则可以设置到具体某个时间点，比如在8点10分20秒的时候返回。类似于 Redis 中的 Expire 和 ExpireAt。 gRPC 系列相关代码见 Github ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["gRPC"],"content":"5. 参考 https://github.com/grpc/grpc-go https://blog.csdn.net/u014229282/article/details/109294837 https://grpc.io/blog/deadlines/ ","date":"2021-01-29","objectID":"/posts/grpc/08-ctx-cancel-deadline/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(八)---使用context进行超时控制","uri":"/posts/grpc/08-ctx-cancel-deadline/"},{"categories":["Redis"],"content":"数据库连接池对性能的影响及其正确设置姿势","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"本文主要记录了一次由 Redis 连接池设置不当引发的事故及其排查和解决过程，同时也记录了数据库连接池的正确设置姿势。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:0:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"1. 结论 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:1:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"内存数据库 像 Redis、Memcache 等内存数据库，因为所有请求直接在内存中完成所以处理速度很快。 具体计算公式如下： 最大连接数 = 最大并发量 / (1000ms / 每次请求耗时ms) 所以我们需要知道集群最大并发量和应用中每次请求Redis来回的耗时。 Redis官网 提供了压测工具。 root@redis://usr/sbin# redis-benchmark -q -n 100000 PING_INLINE: 66269.05 requests per second PING_BULK: 68634.18 requests per second SET: 68399.45 requests per second GET: 67567.57 requests per second INCR: 68259.38 requests per second LPUSH: 67704.80 requests per second RPUSH: 68259.38 requests per second LPOP: 68399.45 requests per second RPOP: 68399.45 requests per second SADD: 68399.45 requests per second HSET: 67567.57 requests per second SPOP: 66844.91 requests per second LPUSH (needed to benchmark LRANGE): 50505.05 requests per second LRANGE_100 (first 100 elements): 65146.58 requests per second LRANGE_300 (first 300 elements): 67204.30 requests per second LRANGE_500 (first 450 elements): 67340.07 requests per second LRANGE_600 (first 600 elements): 67294.75 requests per second MSET (10 keys): 61576.36 requests per second 在我自己的小霸王服务器上都能用 六七万并发，所以单节点10W妥妥的有了。 假设每次请求：来回网络延迟 + redis 处理耗时 为 1ms。 那么 最大连接数 = 10W / (1000ms / 1ms) = 100，即同时用 100个连接就能达到 Redis QPS峰值，发挥出全部性能。 建议：最好能自己测试一下，实在测不了建议设置在 200 作用。 一般 Redis可用连接数都是按W计算的，稍微分配大一点也没什么问题，如果太小了就会无法发挥出 Redis 全部性能。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:1:1","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"磁盘数据库 像 MySQL、PostgreSQL、Oracle 这样将数据存在磁盘上的数据库，由于磁盘I/O的存在，请求处理比较慢，连接池不能设置得像内存数据库那么大。 PostgreSQL 提供了一个公式： connections = ((core_count * 2) + effective_spindle_count) 如果说你的服务器 CPU 是 4核 的，连接池大小应该为 ((4*2)+1)=9。取个整, 我们就设置为 10 吧。 具体测试数据：HikariCP 测试数据 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:1:2","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"2. 问题背景 最近处理了一个线上事故，就是由数据库连接池设置不当引发的。 具体为，某一天的某一个时间点，客户开始反应接口响应很慢： 大部分请求会直接阻塞到超时。 小部分请求是响应比较慢； 然后就开始了排查之旅。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:2:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"3. 排查过程 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:3:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"寻找问题 发现问题后，立马查看了监控、日志、链路追踪信息，具体如下： 服务器监控 CPU、内存 飙升到 80% 左右，CPU 平常只有 20~30%在，而这些服务基本上是不会使用到太多内存的。 负载直接到了 60，还只是4核CPU的机器，可以简单理解为：一般负载超过CPU核数就比较高了。 日志 服务A：调用服务B超时 服务B：调用服务C超时 服务C：调用服务N超时 + 小部分 redis: connection pool timeout。 …省略部分服务 服务N：大量 redis: connection pool timeout 链路追踪 大部分接口都是到了超时时间(3S)才返回。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:3:1","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"定位问题 将各个服务日志连起来查看后，问题都指向了 Redis，于是立马去查了 Redis 监控，结果发现一切正常； 然后就去查了 redis 驱动的源码，看下这个错误提示是什么情况下出现的： redis: connection pool timeout 相关代码如下： // Get 函数用于从Pool取一个 conn func (p *ConnPool) Get() (*Conn, error) { if p.closed() { return nil, ErrClosed } err := p.waitTurn() if err != nil { return nil, err } // 省略 } func (p *ConnPool) waitTurn() error { select { case p.queue \u003c- struct{}{}: return nil default: timer := timers.Get().(*time.Timer) timer.Reset(p.opt.PoolTimeout) select { case p.queue \u003c- struct{}{}: if !timer.Stop() { \u003c-timer.C } timers.Put(timer) return nil case \u003c-timer.C: timers.Put(timer) atomic.AddUint32(\u0026p.stats.Timeouts, 1) return ErrPoolTimeout } } } 其中 waitTurn 函数返回的 ErrPoolTimeout 定义如下： var ErrPoolTimeout = errors.New(\"redis: connection pool timeout\") 那么问题就很明显了，从 Redis 连接池里拿 conn 的时候超时，原因可能有下面几个： 1）连接池设置太小，连接被其他Goroutine拿去使用了，没有来得及归还； 2）Redis 性能瓶颈，导致每次请求要很长时间； 3）网络延迟过大，同样有可能导致每次请求要很长时间。 由于 Redis 监控一切正常，同时应用是在内网环境，网络延迟也特别低，所以是第一点可能性更高。 然后立马查看了配置文件，果然，Redis 连接池只分配了 10个，然后超时时间居然是 10秒。将 Redis 连接池数量调大之后，一切问题都解决了。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:3:2","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"具体分析 既然知道问题是 Redis 连接池设置太小了，那么一切都解释得通了。 1）连接池过小导致大部分请求阻塞在获取连接这一步，一直阻塞到超时，打印错误日志并返回； 2）该超时时间 10s 远大于设置的接口超时时间 3s，所以在阻塞 3s 后时候上游接口已经超时返回了； 3）最终导致每个接口需要阻塞3s才返回。 4）提现在服务器上就是：CPU、内存、负载飙升。 之前由于并发量不是很高，10 个连接池也刚好能处理过来，最近并发量上升后该问题就暴露出来了。 为什么有小部分请求能响应？ 由于是部署在 Kubernetes 集群中的，同时为每个 Pod 配置了 资源上限request 和 limit。cpu 都还好，到达上限后只会让应用跑得慢一点。但是内存就不一样了，超过之后直接 OOM，Pod 被强制 KIll 掉。 Pod 重启后最初到达的部分请求可以获取到 Redis 连接，能正常返回。后续请求只能阻塞到超时了。 补上两个 Redis 的 QPS 统计信息： 调整连接池大小前： 调整连接池大小后： 可以看到，性能确实是被连接池给限制了，所以出现了这次事故。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:3:3","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"4. 小结 1）连接池对性能影响很大，不能随意设置。 2）K8s 中 Pod 资源限制需要合理分配，特别是内存，可以先设置大一点，然后根据统计数据逐渐调整到合适的值。 3）APM 系统很重要 logging：日志收集系统让你在出问题时不用去每台服务器上慢慢翻日志。 tracing：链路追踪系统可以让你清楚的看到系统间各个服务的调用情况。 metrics：监控系统各个指标，便于分析问题。 4）配置中心：微服务一定要有一个配置中心，统一存放配置，否则很难维护。 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:4:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["Redis"],"content":"5. 参考 https://redis.io/topics/benchmarks https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing https://zhuanlan.zhihu.com/p/105845455 ","date":"2021-01-23","objectID":"/posts/redis/db-connection-pool-settings/:5:0","tags":["Redis"],"title":"数据库连接池该设置多大?记一次由连接池引发的事故。","uri":"/posts/redis/db-connection-pool-settings/"},{"categories":["gRPC"],"content":"使用 gRPC-Gateway 同时对外提供  RESTful API 和 gRPC 接口","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"本文主要记录了如何使用 gRPC-Gateway 同时对外提供 RESTful API 和 gRPC 接口。 ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC-Gateway 是Google protocol buffers compiler(protoc)的一个插件。读取 protobuf 定义然后生成反向代理服务器，将 RESTful HTTP API 转换为 gRPC。 换句话说就是将 gRPC 转为 RESTful HTTP API。 源自 coreos 的一篇博客，转载到了 gRPC 官方博客 gRPC with REST and Open APIs。 etcd v3 改用 gRPC 后为了兼容原来的 API，同时要提供 HTTP/JSON 方式的API，为了满足这个需求，要么开发两套 API，要么实现一种转换机制，他们选择了后者，而我们选择跟随他们的脚步。 架构如下 当 HTTP 请求到达 gRPC-Gateway 时，它将 JSON 数据解析为 Protobuf 消息。然后，它使用解析的 Protobuf 消息发出正常的 Go gRPC 客户端请求。Go gRPC 客户端将 Protobuf 结构编码为 Protobuf 二进制格式，然后将其发送到 gRPC 服务器。gRPC 服务器处理请求并以 Protobuf 二进制格式返回响应。Go gRPC 客户端将其解析为 Protobuf 消息，并将其返回到 gRPC-Gateway，后者将 Protobuf 消息编码为 JSON 并将其返回给原始客户端。 简单来说就是生成了一个 HTTP 服务，在具体处理逻辑中去请求 gRPC 服务。 ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"2. 环境准备 环境主要分为 3 部分： 1）Protobuf 相关 Go Protocol buffer compile（protoc） Go Plugins 2）gRPC相关 gRPC Lib gRPC Plugins 3）gRPC-Gateway 相关 gRPC-Gateway ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"1. Protobuf 具体见 Protobuf 章节 ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"2. gRPC 具体见 gRPC章节 ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"3. gRPC-Gateway gRPC-Gateway 只是一个插件，只需要安装一下就可以了。 go get github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"4. 整体流程 大致就是以 .proto 文件为基础，编写插件对 protoc 进行扩展，编译出不同语言不同模块的源文件。 1）首先定义 .proto 文件； 2）然后由 protoc 将 .proto 文件编译成 protobuf 格式的数据； 3）将 2 中编译后的数据传递到各个插件，生成对应语言、对应模块的源代码。 Go Plugins 用于生成 .pb.go 文件 gRPC Plugins 用于生成 _grpc.pb.go gRPC-Gateway 则是 pb.gw.go 其中步骤2和3是一起的，只需要在 protoc 编译时传递不同参数即可。 比如以下命令会同时生成 Go、gRPC 、gRPC-Gateway 需要的 3 个文件。 protoc --go_out . --go-grpc_out . --grpc-gateway_out . hello_world.proto ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:2:4","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"3. 例子 本文所有代码都在这里 Github。 首先确保自己的环境是ok的，具体如下： 1）执行 protoc –version 能打印出版本信息； 2）$GOPATH/bin 目录下有 protoc-gen-go、protoc-gen-go-grpc、protoc-gen-grpc-gateway 这三个可执行文件。 ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"1. gRPC 部分 1）创建.proto文件 创建一个 hello_world.proto 文件，内容如下 具体目录为：proto/helloworld/hello_world.proto syntax = \"proto3\"; option go_package=\".;proto\"; package helloworld; // The greeting service definition service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 2） 生成 Go subs 使用 protoc 编译生成不同模块的源文件，具体命令如下: lixd@17x:~/17x/projects/grpc-go-example/features$ protoc --proto_path=./proto \\ --go_out=./proto --go_opt=paths=source_relative \\ --go-grpc_out=./proto --go-grpc_opt=paths=source_relative \\ ./proto/helloworld/hello_world.proto 具体 protoc 信息可查看 protobuf 章节。 会生成 *.pb.go 和 *_grpc.pb.go 两个文件。 3） Server main.go内容如下： package main import ( \"context\" \"flag\" \"fmt\" \"log\" \"net\" pb \"github.com/lixd/grpc-go-example/features/proto/helloworld\" \"google.golang.org/grpc\" ) var port = flag.Int(\"port\", 50051, \"the port to serve on\") type server struct { pb.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { return \u0026pb.HelloReply{Message: \"hello \" + in.Name }, nil } func main() { // Create a gRPC server object s := grpc.NewServer() // Attach the Greeter service to the server pb.RegisterGreeterServer(s, \u0026server{}) // Serve gRPC Server lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } log.Println(\"Serving gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 4）Client main.go内容如下: package main import ( \"log\" \"golang.org/x/net/context\" \"google.golang.org/grpc\" pb \"i-go/grpc/gateway/proto/helloworld\" ) const ( address = \"localhost:8080\" defaultName = \"world\" ) func main() { conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { panic(err) } defer conn.Close() c := pb.NewGreeterClient(conn) r, err := c.SayHello(context.Background(), \u0026pb.HelloRequest{Name: defaultName}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } log.Printf(\"Greeting: %s\", r.Message) } 5）run 到此分别运行 server.go、client.go ，一个简单的 gRPC demo 就跑起来了。 lixd@17x:~/17x/projects/grpc-go-example/features/gateway/server$ go run main.go 2021/01/30 10:15:53 Serving gRPC on 0.0.0.0:50051 lixd@17x:~/17x/projects/grpc-go-example/features/gateway/client$ go run main.go 2021/01/30 10:17:23 Greeting: hello world ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"2. gRPC-Gateway 部分 接下来需要对之前的 gRPC 代码进行调整，使其能对外提供 RESTful HTTP API。 1）修改 .proto 文件 在原有基础上，添加 gRPC-Gateway 注解。修改后内容如下: syntax = \"proto3\"; option go_package = \"github.com/lixd/grpc-go-example/features/proto/echo\"; package helloworld; import \"google/api/annotations.proto\"; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) { option (google.api.http) = { get: \"/v1/greeter/sayhello\" body: \"*\" }; } } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 主要修改了两个地方 第一步引入annotations.proto import \"google/api/annotations.proto\"; 引入annotations.proto文件，因为添加的注解依赖该文件。 该文件需要手动从 grpc-gateway/third_party/googleapis 目录复制到自己的项目中。 该文件需要手动从 grpc-gateway/third_party/googleapis 目录复制到自己的项目中。 该文件需要手动从 grpc-gateway/third_party/googleapis 目录复制到自己的项目中。 下载链接如下: https://github.com/grpc-ecosystem/grpc-gateway/tree/master/third_party/googleapis/google/api 复制后的目录结构如下： proto ├── google │ └── api │ ├── annotations.proto │ └── http.proto └── helloworld └── hello_world.proto 第二步增加 http 相关注解 rpc SayHello (HelloRequest) returns (HelloReply) { option (google.api.http) = { get: \"/v1/greeter/sayhello\" body: \"*\" }; } 每个方法都必须添加 google.api.http 注解后 gRPC-Gateway 才能生成对应 http 方法。 其中post为 HTTP Method，即 POST 方法，/v1/greeter/sayhello 则是请求路径。 更多语法看这里： https://github.com/googleapis/googleapis/blob/master/google/api/http.proto 2）再次编译 增加 --grpc-gateway_out lixd@17x:~/17x/projects/grpc-go-example/features$ protoc --proto_path=./proto \\ --go_out=./proto --go_opt=paths=source_relative \\ --go-grpc_out=./proto --go-grpc_opt=paths=source_relative \\ --grpc-gateway_out=./proto --grpc-gateway_opt=paths=source_relative \\ ./proto/helloworld/hello_world.proto 本次会多生成一个gw.pb.go 文件，用于启动 HTTP 服务。 其中--proto_path=./proto用于指定 import 文件路径（默认为{$pwd}），即前面引入的google/api/annotations.proto文件的位置。 3）调整 Server 在原有 gRPC 基础上在启动一个 HTTP 服务。 package main import ( \"context\" \"flag\" \"fmt\" \"log\" \"net\" \"net/http\" \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\" pb \"github.com/lixd/grpc-go-example/features/proto/helloworld\" \"google.golang.org/grpc\" ) var port = flag.Int(\"port\", 50051, \"the port to serve on\") var restful = flag.Int(\"restful\", 8080, \"the port to restful serve on\") type server struct { pb.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { return \u0026pb.HelloReply{Message: \"hello \" + in.Name}, nil } func main() { // Create a gRPC server object s := grpc.NewServer() // Attach the Greeter service to the server pb.RegisterGreeterServer(s, \u0026server{}) // Serve gRPC Server lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } log.Println(\"Serving gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) go func() { if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } }() // 2. 启动 HTTP 服务 // Create a client connection to the gRPC server we just started // This is where the gRPC-Gateway proxies the requests conn, err := grpc.Dial( \"localhost:50051\", grpc.WithInsecure(), ) if err != nil { log.Fatalln(\"Failed to dial server:\", err) } gwmux := runtime.NewServeMux() // Register Greeter err = pb.RegisterGreeterHandler(context.Background(), gwmux, conn) if err != nil { log.Fatalln(\"Failed to register gateway:\", err) } gwServer := \u0026http.Server{ Addr: fmt.Sprintf(\":%d\", *restful), Handler: gwmux, } log.Println(\"Serving gRPC-Gateway on http://0.0.0.0\"+fmt.Sprintf(\":%d\", *restful)) log.Fatalln(gwServer.ListenAndServe()) } 主要增加了 启动 HTTP 服务的部分。 4）run 运行并测试效果。 lixd@17x:~/17x/projects/grpc-go-example/features/gateway/server$ go run main.go 2021/01/30 10:32:15 Serving gRPC on 0.0.0.0:50051 2021/01/30 10:32:15 Serving gRPC-Gateway on http://0.0.0.0:8080 gRPC 请求 lixd@17x:~/17x/projects/grpc-go-example/features/gateway/client$ go run main.go 2021/01/30 10:32:18 Greeting: hello world HTTP 请求 $ curl -k http://localhost:8080/v1/greeter/sayhello?name=world {\"message\":\"hello world\"} 将 GET 请求修改为 POST,只需要修改 proto 文件中的注释并重新编译即可 option (google.api.http) =","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"3. 源码分析 首先建立 gRPC 连接，然后New 一个 ServeMux 接着调用了pb.RegisterGreeterHandler() 方法，最后就启动了一个 HTTP 服务。 很明显重点就是pb.RegisterGreeterHandler()这个方法，该方法就是 gRPC-Gateway 生成的。 具体如下: func RegisterGreeterHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error { return RegisterGreeterHandlerClient(ctx, mux, NewGreeterClient(conn)) } func RegisterGreeterHandlerClient(ctx context.Context, mux *runtime.ServeMux, client GreeterClient) error { mux.Handle(\"POST\", pattern_Greeter_SayHello_0, func(w http.ResponseWriter, req *http.Request, pathParams map[string]string) { ctx, cancel := context.WithCancel(req.Context()) defer cancel() inboundMarshaler, outboundMarshaler := runtime.MarshalerForRequest(mux, req) rctx, err := runtime.AnnotateContext(ctx, mux, req, \"/helloworld.Greeter/SayHello\") if err != nil { runtime.HTTPError(ctx, mux, outboundMarshaler, w, req, err) return } resp, md, err := request_Greeter_SayHello_0(rctx, inboundMarshaler, client, req, pathParams) ctx = runtime.NewServerMetadataContext(ctx, md) if err != nil { runtime.HTTPError(ctx, mux, outboundMarshaler, w, req, err) return } forward_Greeter_SayHello_0(ctx, mux, outboundMarshaler, w, req, resp, mux.GetForwardResponseOptions()...) }) return nil } 可以看到直接通过 mux.Handle 注册了一个 HTTP 路由。 mux.Handle(\"POST\", pattern_Greeter_SayHello_0, func(w http.ResponseWriter, req *http.Request, pathParams map[string]string) { }) HTTP Method 为 POST 就是前面在 .proto 文件中指定的。 Pattern（路由信息）pattern_Greeter_SayHello_0 也是前面在 .proto 文件中指定的。具体如下: var ( pattern_Greeter_SayHello_0 = runtime.MustPattern(runtime.NewPattern(1, []int{2, 0, 2, 1, 2, 2}, []string{\"v1\", \"example\", \"echo\"}, \"\")) ) 具体 Handler 的逻辑部分主要是这个方法 resp, md, err := request_Greeter_SayHello_0(rctx, inboundMarshaler, client, req, pathParams) 具体如下: func request_Greeter_SayHello_0(ctx context.Context, marshaler runtime.Marshaler, client GreeterClient, req *http.Request, pathParams map[string]string) (proto.Message, runtime.ServerMetadata, error) { var protoReq HelloRequest var metadata runtime.ServerMetadata newReader, berr := utilities.IOReaderFactory(req.Body) if berr != nil { return nil, metadata, status.Errorf(codes.InvalidArgument, \"%v\", berr) } if err := marshaler.NewDecoder(newReader()).Decode(\u0026protoReq); err != nil \u0026\u0026 err != io.EOF { return nil, metadata, status.Errorf(codes.InvalidArgument, \"%v\", err) } msg, err := client.SayHello(ctx, \u0026protoReq, grpc.Header(\u0026metadata.HeaderMD), grpc.Trailer(\u0026metadata.TrailerMD)) return msg, metadata, err } 可以看到最后是通过 gRPC client 发起的调用 msg, err := client.SayHello(ctx, \u0026protoReq, grpc.Header(\u0026metadata.HeaderMD), grpc.Trailer(\u0026metadata.TrailerMD)) 到这里可以发现 gRPC-Gateway 的具体流程和之前的描述是一致的。 gRPC 系列相关代码见 Github ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"4. 参考 https://grpc-ecosystem.github.io/grpc-gateway/ https://developers.google.com/protocol-buffers ","date":"2021-01-15","objectID":"/posts/grpc/07-grpc-gateway/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(七)---利用Gateway同时提供HTTP和RPC服务","uri":"/posts/grpc/07-grpc-gateway/"},{"categories":["gRPC"],"content":"gRPC 自定义身份校验以提升安全性","date":"2021-01-08","objectID":"/posts/grpc/06-auth/","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"本文主要记录了如何在 gRPC 中使用自定义身份校验以提升服务安全性。 ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github 在 gRPC 中，身份验证被抽象为了credentials.PerRPCCredentials接口： type PerRPCCredentials interface { GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) RequireTransportSecurity() bool } 各方法作用如下： GetRequestMetadata：以 map 的形式返回本次调用的授权信息，ctx 是用来控制超时的，并不是从这个 ctx 中获取。 **RequireTransportSecurity **：指该 Credentials 的传输是否需要需要 TLS 加密，如果返回 true 则说明该 Credentials 需要在一个有 TLS 认证的安全连接上传输，如果当前连接并没有使用 TLS 则会报错： transport: cannot send secure credentials on an insecure connection 具体逻辑为： 在发出请求之前，gRPC 会将 Credentials 存放在 metadata 中进行传递,在真正发起调用之前，gRPC 会通过 GetRequestMetadata 函数，将用户定义的 Credentials 提取出来，并添加到 metadata 中，随着请求一起传递到服务端。 然后服务端从 metadata 中取出 Credentials 进行有效性校验。 gRPC 中已经内置了部分常用的授权方式，如 oAuth2 和 JWT，在 oauth 包中，具体如下： func NewJWTAccessFromFile(keyFile string) (credentials.PerRPCCredentials, error) { jsonKey, err := ioutil.ReadFile(keyFile) if err != nil { return nil, fmt.Errorf(\"credentials: failed to read the service account key file: %v\", err) } return NewJWTAccessFromKey(jsonKey) } func NewOauthAccess(token *oauth2.Token) credentials.PerRPCCredentials { return oauthAccess{token: *token} } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"2. 使用流程 具体分为以下两步： 1）客户端请求时带上 Credentials； 2）服务端取出 Credentials，并验证有效性，一般配合拦截器使用。 ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"Client 客户端添加 Credentials 有两种方式： 1）建立连接时指定 这样授权信息保存在 conn 对象上，通过该连接发起的每个调用都会附带上该授权信息。 func main() { flag.Parse() // 构建一个 PerRPCCredentials。 perRPC := oauth.NewOauthAccess(fetchToken()) creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/ca_cert.pem\"), \"x.test.example.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } conn, err := grpc.Dial(*addr, grpc.WithPerRPCCredentials(perRPC),grpc.WithTransportCredentials(creds)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() rgc := ecpb.NewEchoClient(conn) callUnaryEcho(rgc, \"hello world\") } 2）发起调用时指定 这样可以为每个调用指定不同的 授权信息。 func main() { flag.Parse() creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/ca_cert.pem\"), \"x.test.example.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } conn, err := grpc.Dial(*addr,grpc.WithTransportCredentials(creds)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() client := ecpb.NewEchoClient(conn) // 构建一个 PerRPCCredentials。 cred := oauth.NewOauthAccess(fetchToken()) resp, err := client.UnaryEcho(context.Background(), \u0026ecpb.EchoRequest{Message: \"hello world\"},grpc.PerRPCCredentials(cred)) if err != nil { log.Fatalf(\"client.UnaryEcho(_) = _, %v: \", err) } fmt.Println(\"UnaryEcho: \", resp.Message) } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"Server 服务端则是获取授权信息并校验有效性。 gPRC 传输的时候把授权信息存放在 metada 的，所以需要先获取 metadata。通过metadata.FromIncomingContext即可从 ctx 中取出本次调用的 metadata，然后再从 MD 中取出授权信息并校验即可。 metadata 结构如下： type MD map[string][]string 可以看到 MD 是一个 map ，授权信息在这个map中具体怎么存的由 PerRPCCredentials接口的GetRequestMetadata函数实现。 // valid 校验认证信息有效性。 func valid(authorization []string) bool { if len(authorization) \u003c 1 { return false } token := strings.TrimPrefix(authorization[0], \"Bearer \") return token == \"some-secret-token\" } // ensureValidToken 用于校验 token 有效性的一元拦截器。 func ensureValidToken(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { // 如果 token不存在或者无效，直接返回错误，否则就调用真正的RPC方法。 md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, errMissingMetadata } if !valid(md[\"authorization\"]) { return nil, errInvalidToken } return handler(ctx, req) } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"3. Demo 这里主要演示如何实现 自定义 Auth。 ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"1. MyAuth 这里主要实现credentials.PerRPCCredentials接口和自定义验证逻辑两部分。 1）实现credentials.PerRPCCredentials接口 这里就简单使用 Username+Password 进行身份验证。 type MyAuth struct { Username string Password string } // GetRequestMetadata 定义授权信息的具体存放形式，最终会按这个格式存放到 metadata map 中。 func (a *MyAuth) GetRequestMetadata(context.Context, ...string) (map[string]string, error) { return map[string]string{\"username\": a.Username, \"password\": a.Password}, nil } // RequireTransportSecurity 是否需要基于 TLS 加密连接进行安全传输 func (a *MyAuth) RequireTransportSecurity() bool { return false } 授权信息通过 MyAuth 结构体传递，然后通过gRPC框架内部通过 GetRequestMetadata 方法获取。 2）具体token的验证逻辑 这里就简单判断一下客户端传过来的信息是否等于服务端启动时指定的信息。 const ( Admin = \"admin\" Root = \"root\" ) func NewMyAuth() *MyAuth { return \u0026MyAuth{ Username: Admin, Password: Root, } } // IsValidAuth 具体的验证逻辑 func IsValidAuth(ctx context.Context) error { var ( user string password string ) // 从 ctx 中获取 metadata md, ok := metadata.FromIncomingContext(ctx) if !ok { return status.Errorf(codes.InvalidArgument, \"missing metadata\") } // 从metadata中获取授权信息 // 这里之所以通过md[\"username\"]和md[\"password\"] 可以取到对应的授权信息 // 是因为我们自定义的 GetRequestMetadata 方法是按照这个格式返回的. if val, ok := md[\"username\"]; ok { user = val[0] } if val, ok := md[\"password\"]; ok { password = val[0] } // 简单校验一下 用户名密码是否正确. if user != Admin || password != Root { return status.Errorf(codes.Unauthenticated, \"Unauthorized\") } return nil } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"2. 服务端 服务端主要修改点： 1）服务启动时指定授权信息； 2）在业务逻辑执行前增加身份校验。 func main() { flag.Parse() cert, err := tls.LoadX509KeyPair(data.Path(\"x509/server.crt\"), data.Path(\"x509/server.key\")) if err != nil { log.Fatalf(\"failed to load key pair: %s\", err) } s := grpc.NewServer(grpc.UnaryInterceptor(myEnsureValidToken), grpc.Creds(credentials.NewServerTLSFromCert(\u0026cert))) pb.RegisterEchoServer(s, \u0026ecServer{}) lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } log.Println(\"Serving gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 具体 token 验证逻辑直接调用 auth 中的实现即可,服务端只需要将其包装到拦截器中： // myEnsureValidToken 自定义 token 校验 func myEnsureValidToken(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { // 如果返回err不为nil则说明token验证未通过 err := authentication.IsValidAuth(ctx) if err != nil { return nil, err } return handler(ctx, req) } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"3. 客户端 客户端只需要在请求时带上验证信息即可。 func main() { flag.Parse() // 构建一个自定义的PerRPCCredentials。 myAuth := authentication.NewMyAuth() creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/ca_cert.pem\"), \"x.test.example.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(creds), grpc.WithPerRPCCredentials(myAuth)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() client := ecpb.NewEchoClient(conn) callUnaryEcho(client, \"hello world\") } ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"4. Test Server lixd@17x:~/17x/projects/grpc-go-example/features/authentication/server$ go run main.go 2021/01/24 22:17:35 Serving gRPC on 0.0.0.0:50051 Client lixd@17x:~/17x/projects/grpc-go-example/features/authentication/client$ go run main.go UnaryEcho: hello world 授权信息正确则可以正常请求，故意传一个错误的数据测试一下 lixd@17x:~/17x/projects/grpc-go-example/features/authentication/client$ go run main.go 2021/01/24 22:18:19 client.UnaryEcho(_) = _, rpc error: code = Unauthenticated desc = Unauthorized: exit status 1 ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:3:4","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"4. 小结 1）实现credentials.PerRPCCredentials接口就可以把数据当做 gRPC 中的 Credential 在添加到 metadata 中，跟着请求一起传递到服务端； 2）服务端从 ctx 中解析 metadata，然后从 metadata 中获取 授权信息并进行验证； 3）可以借助 Interceptor 实现全局身份验证。 4）客户端可以通过 DialOption 为所有请求统一指定授权信息，或者通过 CallOption 为每一个请求分别指定授权信息。 gRPC 系列相关代码见 Github ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"5. 参考 https://books.studygolang.com/advanced-go-programming-book/ch4-rpc/ch4-05-grpc-hack.html https://github.com/grpc/grpc-go https://grpc.io/docs/guides/auth ","date":"2021-01-08","objectID":"/posts/grpc/06-auth/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(六)---自定义身份校验","uri":"/posts/grpc/06-auth/"},{"categories":["gRPC"],"content":"gRPC拦截器使用","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"本文主要介绍了 gPRC中 的拦截器(Interceptor)和具体使用实例。 ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC 提供了 Interceptor 功能，包括客户端拦截器和服务端拦截器。可以在接收到请求或者发起请求之前优先对请求中的数据做一些处理后再转交给指定的服务处理并响应，很适合在这里处理验证、日志等流程。 gRPC-go 在 v1.28.0版本增加了多 interceptor 支持，可以在不借助第三方库（go-grpc-middleware）的情况下添加多个 interceptor 了。 go-grpc-middleware 中也提供了多种常用 interceptor ，可以直接使用。 在 gRPC 中，根据拦截的方法类型不同可以分为拦截 Unary 方法的一元拦截器，和作用于 Stream 方法的流拦截器。 同时还分为服务端拦截器和客户端拦截器，所以一共有以下4种类型: grpc.UnaryServerInterceptor grpc.StreamServerInterceptor grpc.UnaryClientInterceptor grpc.StreamClientInterceptor ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"2. 定义 ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"客户端拦截器 使用客户端拦截器 只需要在 Dial的时候指定相应的 DialOption 即可。 Unary Interceptor 客户端一元拦截器类型为 grpc.UnaryClientInterceptor，具体如下： type UnaryClientInterceptor func(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error 可以看到，所谓的拦截器其实就是一个函数，可以分为预处理(pre-processing)、调用RPC方法(invoking RPC method)、后处理(post-processing)三个阶段。 参数含义如下: ctx：Go语言中的上下文，一般和 Goroutine 配合使用，起到超时控制的效果 method：当前调用的 RPC 方法名 req：本次请求的参数，只有在处理前阶段修改才有效 reply：本次请求响应，需要在处理后阶段才能获取到 cc：gRPC 连接信息 invoker：可以看做是当前 RPC 方法，一般在拦截器中调用 invoker 能达到调用 RPC 方法的效果，当然底层也是 gRPC 在处理。 opts：本次调用指定的 options 信息 作为一个客户端拦截器，可以在处理前检查 req 看看本次请求带没带 token 之类的鉴权数据，没有的话就可以在拦截器中加上。 Stream Interceptor type StreamClientInterceptor func(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, streamer Streamer, opts ...CallOption) (ClientStream, error) 由于 StreamAPI 和 UnaryAPI有所不同，因此拦截器方面也有所区别，比如 req 参数变成了 streamer 。同时其拦截过程也有所不同，不在是处理 req resp，而是对 streamer 这个流对象进行包装，比如说实现自己的 SendMsg 和 RecvMsg 方法。 然后在这些方法中的预处理(pre-processing)、调用RPC方法(invoking RPC method)、后处理(post-processing)各个阶段加入自己的逻辑。 ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"服务端拦截器 服务端拦截器和客户端拦截器类似，就不做过多描述。使用客户端拦截器 只需要在 NewServer 的时候指定相应的 ServerOption 即可。 Unary Interceptor 定义如下： type UnaryServerInterceptor func(ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler) (resp interface{}, err error) 参数具体含义如下： ctx context.Context：请求上下文 req interface{}：RPC 方法的请求参数 info *UnaryServerInfo：RPC 方法的所有信息 handler UnaryHandler：RPC 方法真正执行的逻辑 Stream Interceptor type StreamServerInterceptor func(srv interface{}, ss ServerStream, info *StreamServerInfo, handler StreamHandler) error ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"3. UnaryInterceptor 一元拦截器可以分为3个阶段： 1）预处理(pre-processing) 2）调用RPC方法(invoking RPC method) 3）后处理(post-processing) ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Client // unaryInterceptor 一个简单的 unary interceptor 示例。 func unaryInterceptor(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { // pre-processing start := time.Now() err := invoker(ctx, method, req, reply, cc, opts...) // invoking RPC method // post-processing end := time.Now() logger(\"RPC: %s, req:%v start time: %s, end time: %s, err: %v\", method, req, start.Format(time.RFC3339), end.Format(time.RFC3339), err) return err } invoker(ctx, method, req, reply, cc, opts...) 是真正调用 RPC 方法。因此我们可以在调用前后增加自己的逻辑：比如调用前检查一下参数之类的，调用后记录一下本次请求处理耗时等。 建立连接时通过 grpc.WithUnaryInterceptor 指定要加载的拦截器即可。 func main() { flag.Parse() creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/server.crt\"), \"www.lixueduan.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } // 建立连接时指定要加载的拦截器 conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(creds), grpc.WithUnaryInterceptor(unaryInterceptor)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() client := ecpb.NewEchoClient(conn) callUnaryEcho(client, \"hello world\") } ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Server 服务端的一元拦截器和客户端类似： func unaryInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { start := time.Now() m, err := handler(ctx, req) end := time.Now() // 记录请求参数 耗时 错误信息等数据 logger(\"RPC: %s,req:%v start time: %s, end time: %s, err: %v\", info.FullMethod, req, start.Format(time.RFC3339), end.Format(time.RFC3339), err) return m, err } 服务端则是在 NewServer 时指定拦截器： func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } creds, err := credentials.NewServerTLSFromFile(data.Path(\"x509/server.crt\"), data.Path(\"x509/server.key\")) if err != nil { log.Fatalf(\"failed to create credentials: %v\", err) } s := grpc.NewServer(grpc.Creds(creds), grpc.UnaryInterceptor(unaryInterceptor)) pb.RegisterEchoServer(s, \u0026server{}) log.Println(\"Server gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Test Server 2021/01/24 19:18:09 Server gRPC on 0.0.0.0:50051 unary echoing message \"hello world\" LOG: RPC: /echo.Echo/UnaryEcho,req:message:\"hello world\" start time: 2021-01-24T19:18:10+08:00, end time: 2021-01-24T19:18:10+08:00, err: \u003cnil\u003e Client LOG: RPC: /echo.Echo/UnaryEcho, req:message:\"hello world\" start time: 2021-01-24T19:18:10+08:00, end time: 2021-01-24T19:18:10+08:00, err: \u003cnil\u003e UnaryEcho: hello world ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"4. StreamInterceptor 流拦截器过程和一元拦截器有所不同，不过同样可以分为3个阶段： 1）预处理(pre-processing) 2）调用RPC方法(invoking RPC method) 3）后处理(post-processing) 预处理阶段和一元拦截器类似，但是调用RPC方法和后处理这两个阶段则完全不同。 StreamAPI 的请求和响应都是通过 Stream 进行传递的，更进一步是通过 Streamer 调用 SendMsg 和 RecvMsg 这两个方法获取的。 然后 Streamer 又是调用RPC方法来获取的，所以在流拦截器中我们可以对 Streamer 进行包装，然后实现 SendMsg 和 RecvMsg 这两个方法。 ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Client 本例中通过结构体嵌入的方式，对 Streamer 进行包装，在 SendMsg 和 RecvMsg 之前打印出具体的值。 // wrappedStream 用于包装 grpc.ClientStream 结构体并拦截其对应的方法。 type wrappedStream struct { grpc.ClientStream } func newWrappedStream(s grpc.ClientStream) grpc.ClientStream { return \u0026wrappedStream{s} } func (w *wrappedStream) RecvMsg(m interface{}) error { logger(\"Receive a message (Type: %T) at %v\", m, time.Now().Format(time.RFC3339)) return w.ClientStream.RecvMsg(m) } func (w *wrappedStream) SendMsg(m interface{}) error { logger(\"Send a message (Type: %T) at %v\", m, time.Now().Format(time.RFC3339)) return w.ClientStream.SendMsg(m) } // streamInterceptor 一个简单的 stream interceptor 示例。 func streamInterceptor(ctx context.Context, desc *grpc.StreamDesc, cc *grpc.ClientConn, method string, streamer grpc.Streamer, opts ...grpc.CallOption) (grpc.ClientStream, error) { s, err := streamer(ctx, desc, cc, method, opts...) if err != nil { return nil, err } // 返回的是自定义的封装过的 stream return newWrappedStream(s), nil } 连接时则通过 grpc.WithStreamInterceptor 指定要加载的拦截器。 func main() { flag.Parse() creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/server.crt\"), \"www.lixueduan.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } // 建立连接时指定要加载的拦截器 conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(creds), grpc.WithStreamInterceptor(streamInterceptor)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() client := ecpb.NewEchoClient(conn) // callUnaryEcho(client, \"hello world\") callBidiStreamingEcho(client) } ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:4:1","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Server 和客户端类似。 type wrappedStream struct { grpc.ServerStream } func newWrappedStream(s grpc.ServerStream) grpc.ServerStream { return \u0026wrappedStream{s} } func (w *wrappedStream) RecvMsg(m interface{}) error { logger(\"Receive a message (Type: %T) at %s\", m, time.Now().Format(time.RFC3339)) return w.ServerStream.RecvMsg(m) } func (w *wrappedStream) SendMsg(m interface{}) error { logger(\"Send a message (Type: %T) at %v\", m, time.Now().Format(time.RFC3339)) return w.ServerStream.SendMsg(m) } func streamInterceptor(srv interface{}, ss grpc.ServerStream, info *grpc.StreamServerInfo, handler grpc.StreamHandler) error { // 包装 grpc.ServerStream 以替换 RecvMsg SendMsg这两个方法。 err := handler(srv, newWrappedStream(ss)) if err != nil { logger(\"RPC failed with error %v\", err) } return err } 相似的，通过grpc.StreamInterceptor指定要加载的拦截器。 func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } creds, err := credentials.NewServerTLSFromFile(data.Path(\"x509/server.crt\"), data.Path(\"x509/server.key\")) if err != nil { log.Fatalf(\"failed to create credentials: %v\", err) } s := grpc.NewServer(grpc.Creds(creds), grpc.StreamInterceptor(streamInterceptor)) pb.RegisterEchoServer(s, \u0026server{}) log.Println(\"Server gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:4:2","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"Test Server lixd@17x:~/17x/projects/grpc-go-example/features/interceptor/server$ go run main.go 2021/01/24 19:58:12 Server gRPC on 0.0.0.0:50051 LOG: Receive a message (Type: *echo.EchoRequest) at 2021-01-24T19:58:14+08:00 bidi echoing message \"Request 1\" LOG: Send a message (Type: *echo.EchoResponse) at 2021-01-24T19:58:14+08:00 LOG: Receive a message (Type: *echo.EchoRequest) at 2021-01-24T19:58:14+08:00 bidi echoing message \"Request 2\" LOG: Send a message (Type: *echo.EchoResponse) at 2021-01-24T19:58:14+08:00 LOG: Receive a message (Type: *echo.EchoRequest) at 2021-01-24T19:58:14+08:00 Client lixd@17x:~/17x/projects/grpc-go-example/features/interceptor/client$ go run main.go LOG: Send a message (Type: *echo.EchoRequest) at 2021-01-24T19:58:14+08:00 LOG: Send a message (Type: *echo.EchoRequest) at 2021-01-24T19:58:14+08:00 LOG: Receive a message (Type: *echo.EchoResponse) at 2021-01-24T19:58:14+08:00 BidiStreaming Echo: Request 1 LOG: Receive a message (Type: *echo.EchoResponse) at 2021-01-24T19:58:14+08:00 BidiStreaming Echo: Request 2 LOG: Receive a message (Type: *echo.EchoResponse) at 2021-01-24T19:58:14+08:00 ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:4:3","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"5. 小结 1）拦截器分类与定义 gRPC 拦截器可以分为：一元拦截器和流拦截器，服务端拦截器和客户端拦截器。 一共有以下4种类型: grpc.UnaryServerInterceptor grpc.StreamServerInterceptor grpc.UnaryClientInterceptor grpc.StreamClientInterceptor 拦截器本质上就是一个特定类型的函数，所以实现拦截器只需要实现对应类型方法（方法签名相同）即可。 2）拦截器执行过程 一元拦截器 1）预处理 2）调用RPC方法 3）后处理 流拦截器 1）预处理 2）调用RPC方法 获取 Streamer 3）后处理 调用 SendMsg 、RecvMsg 之前 调用 SendMsg 、RecvMsg 调用 SendMsg 、RecvMsg 之后 3）拦截器使用及执行顺序 配置多个拦截器时，会按照参数传入顺序依次执行 所以，如果想配置一个 Recovery 拦截器则必须放在第一个，放在最后则无法捕获前面执行的拦截器中触发的 panic。 同时也可以将 一元和流拦截器一起配置，gRPC 会根据不同方法选择对应类型的拦截器执行。 最后推荐一下这个 go-grpc-middleware，该仓库提供了多种常用拦截器。 gRPC 系列相关代码见 Github ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"6. 参考 https://eddycjy.com/posts/go/grpc/2018-10-10-interceptor/ https://github.com/grpc/grpc-go https://github.com/grpc-ecosystem/go-grpc-middleware ","date":"2021-01-02","objectID":"/posts/grpc/05-interceptor/:6:0","tags":["gRPC"],"title":"gRPC(Go)教程(五)---拦截器Interceptor","uri":"/posts/grpc/05-interceptor/"},{"categories":["gRPC"],"content":"gRPC中如何通过TLS证书建立安全连接，让数据能够加密处理","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"本文记录了gRPC 中如何通过 TLS 证书建立安全连接，让数据能够加密处理，包括证书制作和CA签名校验等。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC 内置了以下 encryption 机制： 1）SSL / TLS：通过证书进行数据加密； 2）ALTS：Google开发的一种双向身份验证和传输加密系统。 只有运行在 Google Cloud Platform 才可用，一般不用考虑。 gRPC 中的连接类型一共有以下3种： 1）insecure connection：不使用TLS加密 2）server-side TLS：仅服务端TLS加密 3）mutual TLS：客户端、服务端都使用TLS加密 我们之前案例中使用的都是 insecure connection， conn, err := grpc.Dial(addr,grpc.WithInsecure()) 通过指定 WithInsecure option 来建立 insecure connection，不建议在生产环境使用。 本章将记录如何使用 server-side TLS 和mutual TLS来建立安全连接。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"2. server-side TLS ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"1. 流程 服务端 TLS 具体包含以下几个步骤： 1）制作证书，包含服务端证书和 CA 证书； 2）服务端启动时加载证书； 3）客户端连接时使用CA 证书校验服务端证书有效性。 也可以不使用 CA证书，即服务端证书自签名。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"2. 制作证书 具体证书相关，点击查看证书制作章节，实在不行可以直接使用本教程 Github 仓库中提供的证书文件。 CA 证书 # 生成.key 私钥文件 $ openssl genrsa -out ca.key 2048 # 生成.csr 证书签名请求文件 $ openssl req -new -key ca.key -out ca.csr -subj \"/C=GB/L=China/O=lixd/CN=www.lixueduan.com\" # 自签名生成.crt 证书文件 $ openssl req -new -x509 -days 3650 -key ca.key -out ca.crt -subj \"/C=GB/L=China/O=lixd/CN=www.lixueduan.com\" 服务端证书 和生成 CA证书类似，不过最后一步由 CA 证书进行签名，而不是自签名。 然后openssl 配置文件可能位置不同，需要自己修改一下。 $ find / -name \"openssl.cnf\" # 生成.key 私钥文件 $ openssl genrsa -out server.key 2048 # 生成.csr 证书签名请求文件 $ openssl req -new -key server.key -out server.csr \\ -subj \"/C=GB/L=China/O=lixd/CN=www.lixueduan.com\" \\ -reqexts SAN \\ -config \u003c(cat /etc/ssl/openssl.cnf \u003c(printf \"\\n[SAN]\\nsubjectAltName=DNS:*.lixueduan.com,DNS:*.refersmoon.com\")) # 签名生成.crt 证书文件 $ openssl x509 -req -days 3650 \\ -in server.csr -out server.crt \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -extensions SAN \\ -extfile \u003c(cat /etc/ssl/openssl.cnf \u003c(printf \"\\n[SAN]\\nsubjectAltName=DNS:*.lixueduan.com,DNS:*.refersmoon.com\")) 到此会生成以下 6 个文件： ca.crt ca.csr ca.key server.crt server.csr server.key 会用到的有下面这3个： 1）ca.crt 2）server.key 3）server.crt ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"3. 服务端 服务端代码修改点如下： 1）NewServerTLSFromFile 加载证书 2）NewServer 时指定 Creds。 func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } // 指定使用服务端证书创建一个 TLS credentials。 creds, err := credentials.NewServerTLSFromFile(data.Path(\"x509/server.crt\"), data.Path(\"x509/server.key\")) if err != nil { log.Fatalf(\"failed to create credentials: %v\", err) } // 指定使用 TLS credentials。 s := grpc.NewServer(grpc.Creds(creds)) ecpb.RegisterEchoServer(s, \u0026ecServer{}) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"4. 客户端 客户端代码主要修改点： 1）NewClientTLSFromFile 指定使用 CA 证书来校验服务端的证书有效性。 注意：第二个参数域名就是前面生成服务端证书时指定的CN参数。 2）建立连接时 指定建立安全连接 WithTransportCredentials。 func main() { flag.Parse() // 客户端通过ca证书来验证服务的提供的证书 creds, err := credentials.NewClientTLSFromFile(data.Path(\"x509/ca.crt\"), \"www.lixueduan.com\") if err != nil { log.Fatalf(\"failed to load credentials: %v\", err) } // 建立连接时指定使用 TLS conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(creds)) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() rgc := ecpb.NewEchoClient(conn) callUnaryEcho(rgc, \"hello world\") } ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:4","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"5. Test Server lixd@17x:~/17x/projects/grpc-go-example/features/encryption/server-side-TLS/server$ go run main.go 2021/01/24 18:00:25 Server gRPC on 0.0.0.0:50051 Client lixd@17x:~/17x/projects/grpc-go-example/features/encryption/server-side-TLS/client$ go run main.go UnaryEcho: hello world 抓包结果如下 可以看到成功开启了 TLS。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:2:5","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"3. mutual TLS server-side TLS 中虽然服务端使用了证书，但是客户端却没有使用证书，本章节会给客户端也生成一个证书，并完成 mutual TLS。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"1. 制作证书 # 生成.key 私钥文件 $ openssl genrsa -out server.key 2048 # 生成.csr 证书签名请求文件 $ openssl req -new -key server.key -out server.csr \\ -subj \"/C=GB/L=China/O=lixd/CN=www.lixueduan.com\" \\ -reqexts SAN \\ -config \u003c(cat /etc/ssl/openssl.cnf \u003c(printf \"\\n[SAN]\\nsubjectAltName=DNS:*.lixueduan.com,DNS:*.refersmoon.com\")) # 签名生成.crt 证书文件 $ openssl x509 -req -days 3650 \\ -in server.csr -out server.crt \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -extensions SAN \\ -extfile \u003c(cat /etc/ssl/openssl.cnf \u003c(printf \"\\n[SAN]\\nsubjectAltName=DNS:*.lixueduan.com,DNS:*.refersmoon.com\")) 这里又会生成3个文件，需要的是下面这两个： client.crt client.key 到此为止，我们已经有了如下5个文件： ca.crt client.crt client.key server.crt server.key ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"2. 服务端 mutual TLS 中服务端、客户端改动都比较多。 具体步骤如下： 1）加载服务端证书 2）构建用于校验客户端证书的 CertPool 3）使用上面的参数构建一个 TransportCredentials 4）newServer 是指定使用前面创建的 creds。 具体改动如下： 看似改动很大，其实如果你仔细查看了前面 NewServerTLSFromFile 方法做的事的话，就会发现是差不多的，只有极个别参数不同。 修改点如下： 1）tls.Config的参数ClientAuth，这里改成了tls.RequireAndVerifyClientCert，即服务端也必须校验客户端的证书，之前使用的默认值(即不校验) 2）tls.Config的参数ClientCAs，由于之前都不校验客户端证书，所以也没有指定用什么证书来校验。 func main() { // 从证书相关文件中读取和解析信息，得到证书公钥、密钥对 certificate, err := tls.LoadX509KeyPair(data.Path(\"x509/server.crt\"), data.Path(\"x509/server.key\")) if err != nil { log.Fatal(err) } // 创建CertPool，后续就用池里的证书来校验客户端证书有效性 // 所以如果有多个客户端 可以给每个客户端使用不同的 CA 证书，来实现分别校验的目的 certPool := x509.NewCertPool() ca, err := ioutil.ReadFile(data.Path(\"x509/ca.crt\")) if err != nil { log.Fatal(err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatal(\"failed to append certs\") } // 构建基于 TLS 的 TransportCredentials creds := credentials.NewTLS(\u0026tls.Config{ // 设置证书链，允许包含一个或多个 Certificates: []tls.Certificate{certificate}, // 要求必须校验客户端的证书 可以根据实际情况选用其他参数 ClientAuth: tls.RequireAndVerifyClientCert, // NOTE: this is optional! // 设置根证书的集合，校验方式使用 ClientAuth 中设定的模式 ClientCAs: certPool, }) s := grpc.NewServer(grpc.Creds(creds)) ecpb.RegisterEchoServer(s, \u0026ecServer{}) lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } log.Println(\"Serving gRPC on 0.0.0.0\" + fmt.Sprintf(\":%d\", *port)) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"3. 客户端 客户端改动和前面服务端差不多，具体步骤都一样，除了不能指定校验策略之外基本一样。 大概是因为客户端必校验服务端证书，所以没有提供可选项。 func main() { // 加载客户端证书 certificate, err := tls.LoadX509KeyPair(data.Path(\"x509/client.crt\"), data.Path(\"x509/client.key\")) if err != nil { log.Fatal(err) } // 构建CertPool以校验服务端证书有效性 certPool := x509.NewCertPool() ca, err := ioutil.ReadFile(data.Path(\"x509/ca.crt\")) if err != nil { log.Fatal(err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatal(\"failed to append ca certs\") } creds := credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{certificate}, ServerName: \"www.lixueduan.com\", // NOTE: this is required! RootCAs: certPool, }) conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(creds)) if err != nil { log.Fatalf(\"DialContext error:%v\", err) } defer conn.Close() client := ecpb.NewEchoClient(conn) callUnaryEcho(client, \"hello world\") } ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"4. Test Server lixd@17x:~/17x/projects/grpc-go-example/features/encryption/mutual-TLS/server$ go run main.go 2021/01/24 18:02:01 Serving gRPC on 0.0.0.0:50051 Client lixd@17x:~/17x/projects/grpc-go-example/features/encryption/mutual-TLS/client$ go run main.go UnaryEcho: hello world 一切正常，大功告成。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:3:4","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"4. FAQ 问题 error:rpc error: code = Unavailable desc = connection error: desc = \"transport: authentication handshake failed: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\" 由于之前使用的不是 SAN 证书，在Go版本升级到1.15后出现了该问题。 原因 Go 1.15 版本开始废弃 CommonName 并且推荐使用 SAN 证书，导致依赖 CommonName 的证书都无法使用了。 解决方案 1）开启兼容：设置环境变量 GODEBUG 为 x509ignoreCN=0 2）使用 SAN 证书 本教程已经修改成了 SAN 证书，所以不会遇到该问题了。 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"5. 小结 本章主要讲解了 gRPC 中三种类型的连接，及其具体配置方式。 1）insecure connection 2）server-side TLS 3）mutual TLS gRPC 系列相关代码见 Github ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"6. 参考 https://grpc.io/docs/guides/auth https://dev.to/techschoolguru/how-to-secure-grpc-connection-with-ssl-tls-in-go-4ph https://www.openssl.org/docs/manmaster/ https://www.jianshu.com/p/37ded4da1095 ","date":"2020-12-25","objectID":"/posts/grpc/04-encryption-tls/:6:0","tags":["gRPC"],"title":"gRPC(Go)教程(四)---通过SSL/TLS建立安全连接","uri":"/posts/grpc/04-encryption-tls/"},{"categories":["gRPC"],"content":"gRPC stream 推送流使用教程","date":"2020-12-20","objectID":"/posts/grpc/03-stream/","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"本文主要讲述了 gRPC 中的四种类型的方法使用，包括普通的 Unary API 和三种 Stream API：ServerStreaming、ClientStreaming、BidirectionalStreaming。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC 中的 Service API 有如下4种类型： 1）UnaryAPI：普通一元方法 2）ServerStreaming：服务端推送流 3）ClientStreaming：客户端推送流 4）BidirectionalStreaming：双向推送流 Unary API 就是普通的 RPC 调用，例如之前的 HelloWorld 就是一个 Unary API，本文主要讲解 Stream API。 Stream 顾名思义就是一种流，可以源源不断的推送数据，很适合大数据传输，或者服务端和客户端长时间数据交互的场景。Stream API 和 Unary API 相比，因为省掉了中间每次建立连接的花费，所以效率上会提升一些。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"2. proto 文件定义 echo.proto文件定义如下： syntax = \"proto3\"; option go_package = \"github.com/lixd/grpc-go-example/features/proto/echo\"; package echo; // Echo 服务，包含了4种类型API service Echo { // UnaryAPI rpc UnaryEcho(EchoRequest) returns (EchoResponse) {} // SServerStreaming rpc ServerStreamingEcho(EchoRequest) returns (stream EchoResponse) {} // ClientStreamingE rpc ClientStreamingEcho(stream EchoRequest) returns (EchoResponse) {} // BidirectionalStreaming rpc BidirectionalStreamingEcho(stream EchoRequest) returns (stream EchoResponse) {} } message EchoRequest { string message = 1; } message EchoResponse { string message = 1; } 编译 lixd@17x:~/17x/projects/grpc-go-example/features/proto/echo$ protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ ./echo.proto ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"3. UnaryAPI 比较简单，没有需要特别注意的地方。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Server type Echo struct { pb.UnimplementedEchoServer } // UnaryEcho 一个普通的UnaryAPI func (e *Echo) UnaryEcho(ctx context.Context, req *pb.EchoRequest) (*pb.EchoResponse, error) { log.Printf(\"Recved: %v\", req.GetMessage()) resp := \u0026pb.EchoResponse{Message: req.GetMessage()} return resp, nil } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Client func main() { // 1.建立连接 获取client conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() client := pb.NewEchoClient(conn) // 2.执行各个Stream的对应方法 unary(client) } func unary(client pb.EchoClient) { resp, err := client.UnaryEcho(context.Background(), \u0026pb.EchoRequest{Message: \"hello world\"}) if err != nil { log.Printf(\"send error:%v\\n\", err) } fmt.Printf(\"Recved:%v \\n\", resp.GetMessage()) } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Test Server lixd@17x:~/17x/projects/grpc-go-example/features/stream/server$ go run main.go 2021/01/23 19:00:30 Serving gRPC on 0.0.0.0:50051 2021/01/23 19:00:36 Recved: hello world Client lixd@17x:~/17x/projects/grpc-go-example/features/stream/client$ go run main.go Recved:hello world ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"4. ServerStream 服务端流：服务端可以发送多个数据给客户端。 使用场景: 例如图片处理的时候，客户端提供一张原图，服务端依次返回多个处理后的图片。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Server 注意点：可以多次调用 stream.Send() 来返回多个数据。 // ServerStreamingEcho 客户端发送一个请求 服务端以流的形式循环发送多个响应 /* 1. 获取客户端请求参数 2. 处理完成后返回过个响应 3. 最后返回nil表示已经完成响应 */ func (e *Echo) ServerStreamingEcho(req *pb.EchoRequest, stream pb.Echo_ServerStreamingEchoServer) error { log.Printf(\"Recved %v\", req.GetMessage()) // 具体返回多少个response根据业务逻辑调整 for i := 0; i \u003c 2; i++ { // 通过 send 方法不断推送数据 err := stream.Send(\u0026pb.EchoResponse{Message: req.GetMessage()}) if err != nil { log.Fatalf(\"Send error:%v\", err) return err } } // 返回nil表示已经完成响应 return nil } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:4:1","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Client 注意点：调用方法获取到的不是单纯的响应，而是一个 stream。通过 stream.Recv() 接收服务端的多个返回值。 /* 1. 建立连接 获取client 2. 通过 client 获取stream 3. for循环中通过stream.Recv()依次获取服务端推送的消息 4. err==io.EOF则表示服务端关闭stream了 */ func serverStream(client pb.EchoClient) { // 2.调用获取stream stream, err := client.ServerStreamingEcho(context.Background(), \u0026pb.EchoRequest{Message: \"Hello World\"}) if err != nil { log.Fatalf(\"could not echo: %v\", err) } // 3. for循环获取服务端推送的消息 for { // 通过 Recv() 不断获取服务端send()推送的消息 resp, err := stream.Recv() // 4. err==io.EOF则表示服务端关闭stream了 退出 if err == io.EOF { log.Println(\"server closed\") break } if err != nil { log.Printf(\"Recv error:%v\", err) continue } log.Printf(\"Recv data:%v\", resp.GetMessage()) } } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:4:2","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Test Server lixd@17x:~/17x/projects/grpc-go-example/features/stream/server$ go run main.go 2021/01/23 19:06:18 Serving gRPC on 0.0.0.0:50051 2021/01/23 19:06:27 Recved Hello World Client lixd@17x:~/17x/projects/grpc-go-example/features/stream/client$ go run main.go 2021/01/23 19:06:27 Recv data:Hello World 2021/01/23 19:06:27 Recv data:Hello World 2021/01/23 19:06:27 server closed ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:4:3","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"5. ClientStream 和 ServerStream 相反，客户端可以发送多个数据。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Server 注意点：循环调用 stream.Recv() 获取数据，err == io.EOF 则表示数据已经全部获取了，最后通过 stream.SendAndClose() 返回响应。 // ClientStreamingEcho 客户端流 /* 1. for循环中通过stream.Recv()不断接收client传来的数据 2. err == io.EOF表示客户端已经发送完毕关闭连接了,此时在等待服务端处理完并返回消息 3. stream.SendAndClose() 发送消息并关闭连接(虽然在客户端流里服务器这边并不需要关闭 但是方法还是叫的这个名字，内部也只会调用Send()) */ func (e *Echo) ClientStreamingEcho(stream pb.Echo_ClientStreamingEchoServer) error { // 1.for循环接收客户端发送的消息 for { // 2. 通过 Recv() 不断获取客户端 send()推送的消息 req, err := stream.Recv() // Recv内部也是调用RecvMsg // 3. err == io.EOF表示已经获取全部数据 if err == io.EOF { log.Println(\"client closed\") // 4.SendAndClose 返回并关闭连接 // 在客户端发送完毕后服务端即可返回响应 return stream.SendAndClose(\u0026pb.EchoResponse{Message: \"ok\"}) } if err != nil { return err } log.Printf(\"Recved %v\", req.GetMessage()) } } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:5:1","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Client 注意点：通过多次调用 stream.Send() 像服务端推送多个数据，最后调用 stream.CloseAndRecv() 关闭stream并接收服务端响应。 // clientStream 客户端流 /* 1. 建立连接并获取client 2. 获取 stream 并通过 Send 方法不断推送数据到服务端 3. 发送完成后通过stream.CloseAndRecv() 关闭steam并接收服务端返回结果 */ func clientStream(client pb.EchoClient) { // 2.获取 stream 并通过 Send 方法不断推送数据到服务端 stream, err := client.ClientStreamingEcho(context.Background()) if err != nil { log.Fatalf(\"Sum() error: %v\", err) } for i := int64(0); i \u003c 2; i++ { err := stream.Send(\u0026pb.EchoRequest{Message: \"hello world\"}) if err != nil { log.Printf(\"send error: %v\", err) continue } } // 3. 发送完成后通过stream.CloseAndRecv() 关闭steam并接收服务端返回结果 // (服务端则根据err==io.EOF来判断client是否关闭stream) resp, err := stream.CloseAndRecv() if err != nil { log.Fatalf(\"CloseAndRecv() error: %v\", err) } log.Printf(\"sum: %v\", resp.GetMessage()) } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:5:2","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Test Server lixd@17x:~/17x/projects/grpc-go-example/features/stream/server$ go run main.go 2021/01/23 19:10:34 Serving gRPC on 0.0.0.0:50051 2021/01/23 19:10:42 Recved hello world 2021/01/23 19:10:42 Recved hello world 2021/01/23 19:10:42 client closed Cliet lixd@17x:~/17x/projects/grpc-go-example/features/stream/client$ go run main.go 2021/01/23 19:10:42 sum: ok ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:5:3","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"6. BidirectionalStream 双向推送流则可以看做是结合了 ServerStream 和 ClientStream 二者。客户端和服务端都可以向对方推送多个数据。 注意：服务端和客户端的这两个Stream 是独立的。 ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:6:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Server 注意点：一般是使用两个 Goroutine，一个接收数据，一个推送数据。最后通过 return nil 表示已经完成响应。 // BidirectionalStreamingEcho 双向流服务端 /* // 1. 建立连接 获取client // 2. 通过client调用方法获取stream // 3. 开两个goroutine（使用 chan 传递数据） 分别用于Recv()和Send() // 3.1 一直Recv()到err==io.EOF(即客户端关闭stream) // 3.2 Send()则自己控制什么时候Close 服务端stream没有close 只要跳出循环就算close了。 具体见https://github.com/grpc/grpc-go/issues/444 */ func (e *Echo) BidirectionalStreamingEcho(stream pb.Echo_BidirectionalStreamingEchoServer) error { var ( waitGroup sync.WaitGroup msgCh = make(chan string) ) waitGroup.Add(1) go func() { defer waitGroup.Done() for v := range msgCh { err := stream.Send(\u0026pb.EchoResponse{Message: v}) if err != nil { fmt.Println(\"Send error:\", err) continue } } }() waitGroup.Add(1) go func() { defer waitGroup.Done() for { req, err := stream.Recv() if err == io.EOF { break } if err != nil { log.Fatalf(\"recv error:%v\", err) } fmt.Printf(\"Recved :%v \\n\", req.GetMessage()) msgCh \u003c- req.GetMessage() } close(msgCh) }() waitGroup.Wait() // 返回nil表示已经完成响应 return nil } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:6:1","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Client 注意点：和服务端类似，不过客户端推送结束后需要主动调用 stream.CloseSend() 函数来关闭Stream。 // bidirectionalStream 双向流 /* 1. 建立连接 获取client 2. 通过client获取stream 3. 开两个goroutine 分别用于Recv()和Send() 3.1 一直Recv()到err==io.EOF(即服务端关闭stream) 3.2 Send()则由自己控制 4. 发送完毕调用 stream.CloseSend()关闭stream 必须调用关闭 否则Server会一直尝试接收数据 一直报错... */ func bidirectionalStream(client pb.EchoClient) { var wg sync.WaitGroup // 2. 调用方法获取stream stream, err := client.BidirectionalStreamingEcho(context.Background()) if err != nil { panic(err) } // 3.开两个goroutine 分别用于Recv()和Send() wg.Add(1) go func() { defer wg.Done() for { req, err := stream.Recv() if err == io.EOF { fmt.Println(\"Server Closed\") break } if err != nil { continue } fmt.Printf(\"Recv Data:%v \\n\", req.GetMessage()) } }() wg.Add(1) go func() { defer wg.Done() for i := 0; i \u003c 2; i++ { err := stream.Send(\u0026pb.EchoRequest{Message: \"hello world\"}) if err != nil { log.Printf(\"send error:%v\\n\", err) } time.Sleep(time.Second) } // 4. 发送完毕关闭stream err := stream.CloseSend() if err != nil { log.Printf(\"Send error:%v\\n\", err) return } }() wg.Wait() } ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:6:2","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"Test Server lixd@17x:~/17x/projects/grpc-go-example/features/stream/server$ go run main.go 2021/01/23 19:14:56 Serving gRPC on 0.0.0.0:50051 Recved :hello world Recved :hello world Client lixd@17x:~/17x/projects/grpc-go-example/features/stream/client$ go run main.go Recved:hello world Recved:hello world Server Closed ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:6:3","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"7. 小结 客户端或者服务端都有对应的 推送 或者 接收对象，我们只要 不断循环 Recv(),或者 Send() 就能接收或者推送了！ gRPC Stream 和 goroutine 配合简直完美。通过 Stream 我们可以更加灵活的实现自己的业务。如 订阅，大数据传输等。 Client发送完成后需要手动调用Close()或者CloseSend()方法关闭stream，Server端则return nil就会自动 Close。 1）ServerStream 服务端处理完成后return nil代表响应完成 客户端通过 err == io.EOF判断服务端是否响应完成 2）ClientStream 客户端发送完毕通过`CloseAndRecv关闭stream 并接收服务端响应 服务端通过 err == io.EOF判断客户端是否发送完毕，完毕后使用SendAndClose关闭 stream并返回响应。 3）BidirectionalStream 客户端服务端都通过stream向对方推送数据 客户端推送完成后通过CloseSend关闭流，通过err == io.EOF`判断服务端是否响应完成 服务端通过err == io.EOF判断客户端是否响应完成,通过return nil表示已经完成响应 通过err == io.EOF来判定是否把对方推送的数据全部获取到了。 客户端通过CloseAndRecv或者CloseSend关闭 Stream，服务端则通过SendAndClose或者直接 return nil来返回响应。 gRPC 系列相关代码见 Github ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:7:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"8. 参考 https://grpc.io/docs/languages/go/basics/#server-side-streaming-rpc https://github.com/grpc/grpc-go ","date":"2020-12-20","objectID":"/posts/grpc/03-stream/:8:0","tags":["gRPC"],"title":"gRPC(Go)教程(三)---Stream 推送流","uri":"/posts/grpc/03-stream/"},{"categories":["gRPC"],"content":"gRPC之hello world","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"本文主要对 gRPC 框架做了简单的介绍，同时记录了一个简单的 hello wolrd 教程。 ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"1. 概述 gRPC 系列相关代码见 Github gRPC 是一个高性能、通用的开源 RPC 框架，其由 Google 主要面向移动应用开发并基于 HTTP/2 协议标准而设计，基于 ProtoBuf(Protocol Buffers) 序列化协议开发，且支持众多开发语言。 与许多 RPC 系统类似，gRPC 也是基于以下理念：定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 gRPC 服务器来处理客户端调用。在客户端拥有一个存根能够像服务端一样的方法。 gRPC 默认使用 protocol buffers，这是 Google 开源的一套成熟的结构数据序列化机制（当然也可以使用其他数据格式如 JSON）。 ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"2. 环境准备 1）protoc 首先需要安装 protocol buffers compile 即 protoc 和 Go Plugins。 具体见 Protobuf 章节 2）gRPC 然后是安装 gRPC 。 $ go get -u google.golang.org/grpc 国内由于某些原因，安装超时的话可以在这里查看解决办法：https://github.com/grpc/grpc-go#FAQ 或者设置 GOPROXY ,具体看这里 https://goproxy.cn 3）gRPC plugins 接着是下载 gRPC Plugins，用于生成 gRPC 相关源代码。 $ go get google.golang.org/grpc/cmd/protoc-gen-go-grpc ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"3. Helloworld 环境 首先确保自己的环境是 OK 的： 1）终端输入 protoc –version 能打印出版本信息； 2）$GOPATH/bin 目录下有 protoc-gen-go、protoc-gen-go-grpc 这两个可执行文件。 本教程版本信息如下： protoc 3.14.0 protoc-gen-go v1.25.0 gPRC v1.35.0 protoc-gen-go-grpc 1.0.1 项目结构如下： helloworld/ ├── client │ └── main.go ├── helloworld │ ├── hello_world_grpc.pb.go │ ├── hello_world.pb.go │ └── hello_world.proto └── server └── main.go ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"1. 编写.proto文件 hello_world.proto文件内容如下： //声明proto的版本 只有 proto3 才支持 gRPC syntax = \"proto3\"; // 将编译后文件输出在 github.com/lixd/grpc-go-example/helloworld/helloworld 目录 option go_package = \"github.com/lixd/grpc-go-example/helloworld/helloworld\"; // 指定当前proto文件属于helloworld包 package helloworld; // 定义一个名叫 greeting 的服务 service Greeter { // 该服务包含一个 SayHello 方法 HelloRequest、HelloReply分别为该方法的输入与输出 rpc SayHello (HelloRequest) returns (HelloReply) {} } // 具体的参数定义 message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 这里定义了一个服务 Greeter，其中有一个方法名为 SayHello。其接收参数为HelloRequest类型，返回HelloReply类型。 服务定义为： // 定义一个名叫 greeting 的服务 service Greeter { // 该服务包含一个 SayHello 方法 HelloRequest、HelloReply分别为该方法的输入与输出 rpc SayHello (HelloRequest) returns (HelloReply) {} } ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:1","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"2. 编译生成源代码 使用 protoc 编译生成对应源文件，具体命令如下: protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ ./hello_world.proto 会在当前目录生成hello_world.pb.go和hello_world_grpc.pb.go两个文件。 具体 protobuf 如何定义，各个参数的作用见 [protobuf][protobuf] ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:2","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"3. Server package main import ( \"context\" \"log\" \"net\" pb \"github.com/lixd/grpc-go-example/helloworld/helloworld\" \"google.golang.org/grpc\" ) const ( port = \":50051\" ) // greeterServer 定义一个结构体用于实现 .proto文件中定义的方法 // 新版本 gRPC 要求必须嵌入 pb.UnimplementedGreeterServer 结构体 type greeterServer struct { pb.UnimplementedGreeterServer } // SayHello 简单实现一下.proto文件中定义的 SayHello 方法 func (g *greeterServer) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { log.Printf(\"Received: %v\", in.GetName()) return \u0026pb.HelloReply{Message: \"Hello \" + in.GetName()}, nil } func main() { listen, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() // 将服务描述(server)及其具体实现(greeterServer)注册到 gRPC 中去. // 内部使用的是一个 map 结构存储，类似 HTTP server。 pb.RegisterGreeterServer(s, \u0026greeterServer{}) log.Println(\"Serving gRPC on 0.0.0.0\" + port) if err := s.Serve(listen); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 具体步骤如下: 1）定义一个结构体，必须包含pb.UnimplementedGreeterServer 对象； 2）实现 .proto文件中定义的API； 3）将服务描述及其具体实现注册到 gRPC 中； 4）启动服务。 ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:3","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"4. Client package main import ( \"context\" \"log\" \"os\" \"time\" pb \"github.com/lixd/grpc-go-example/helloworld/helloworld\" \"google.golang.org/grpc\" ) const ( address = \"localhost:50051\" defaultName = \"world\" ) func main() { conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := pb.NewGreeterClient(conn) // 通过命令行参数指定 name name := defaultName if len(os.Args) \u003e 1 { name = os.Args[1] } ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHello(ctx, \u0026pb.HelloRequest{Name: name}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } log.Printf(\"Greeting: %s\", r.GetMessage()) } 具体步骤如下： 1）首先使用 grpc.Dial() 与 gRPC 服务器建立连接； 2）使用 pb.NewGreeterClient(conn)获取客户端； 3）通过客户端调用ServiceAPI方法client.SayHello。 ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:4","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"5. Test 先运行服务端 lixd@17x:~/17x/projects/grpc-go-example/helloworld/server$ go run main.go 2021/01/23 14:47:20 Serving gRPC on 0.0.0.0:50051 2021/01/23 14:47:32 Received: world 2021/01/23 14:47:52 Received: 指月 然后运行客户端 lixd@17x:~/17x/projects/grpc-go-example/helloworld/client$ go run main.go 2021/01/23 14:47:32 Greeting: Hello world lixd@17x:~/17x/projects/grpc-go-example/helloworld/client$ go run main.go 指月 2021/01/23 14:47:52 Greeting: Hello 指月 到此为止 gRPC 版的 hello world 已经完成了。 ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:3:5","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"4. 小结 使用 gRPC 的 3个 步骤: 1）需要使用 protobuf 定义接口，即编写 .proto 文件; 2）然后使用 protoc 工具配合编译插件编译生成特定语言或模块的执行代码，比如 Go、Java、C/C++、Python 等。 3）分别编写 server 端和 client 端代码，写入自己的业务逻辑。 gRPC 系列相关代码见 Github ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"5. 参考 https://grpc.io/docs/languages/go/quickstart/ https://github.com/grpc/grpc-go ","date":"2020-12-19","objectID":"/posts/grpc/02-hello-world/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(二)---Hello gRPC","uri":"/posts/grpc/02-hello-world/"},{"categories":["gRPC"],"content":"Protobuf基本使用","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"本文主要记录了 Protobuf 的基本使用。包括 编译器 protoc 、Go Plugins 安装及 .proto文件定义、编译等。 ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"1. 概述 Protocol buffers 是一种语言无关、平台无关的可扩展机制或者说是数据交换格式，用于序列化结构化数据。与 XML、JSON 相比，Protocol buffers 序列化后的码流更小、速度更快、操作更简单。 Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"2. Protocol Compiler protoc 用于编译 protocolbuf (.proto文件) 和 protobuf 运行时。 protoc 是 C++ 写的，比较简单的安装方式是直接下载编译好的二进制文件。 release 版本下载地址如下 https://github.com/protocolbuffers/protobuf/releases 下载操作系统对应版本然后解压然后配置一下环境变量即可。 比如windows就下载protoc-3.14.0-win64.zip 然后把解压后的xxx\\protoc-3.14.0-win64\\bin配置到环境变量。 linux则下载protoc-3.14.0-linux-x86_64.zip 解压 $ unzip protoc-3.14.0-linux-x86_64.zip -d protoc-3.14.0-linux-x86_64 新增环境变量 $ sudo vim /etc/profile 增加如下内容 #记得改成自己的路径 export PATH=$PATH:/home/lixd/17x/protoc-3.14.0-linux-x86_64/bin 使其生效 $ source /etc/profile 查看是否安装成功 $ protoc --version libprotoc 3.14.0 ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"3. Go Plugins 出了安装 protoc 之外还需要安装各个语言对应的编译插件，我用的 Go 语言，所以还需要安装一个 Go 语言的编译插件。 go get google.golang.org/protobuf/cmd/protoc-gen-go ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:3:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"4. Demo ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:4:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"创建.proto 文件 hello_world.proto //声明proto的版本 只有 proto3 才支持 gRPC syntax = \"proto3\"; // 将编译后文件输出在 github.com/lixd/grpc-go-example/helloworld/helloworld 目录 option go_package = \"github.com/lixd/grpc-go-example/helloworld/helloworld\"; // 指定当前proto文件属于helloworld包 package helloworld; // 定义一个名叫 greeting 的服务 service Greeter { // 该服务包含一个 SayHello 方法 HelloRequest、HelloReply分别为该方法的输入与输出 rpc SayHello (HelloRequest) returns (HelloReply) {} } // 具体的参数定义 message HelloRequest { string name = 1; } message HelloReply { string message = 1; } ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:4:1","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"protoc 编译 编译命令 # Syntax: protoc [OPTION] PROTO_FILES $ protoc --proto_path=IMPORT_PATH --go_out=OUT_DIR --go_opt=paths=source_relative path/to/file.proto 这里简单介绍一下 golang 的编译姿势: –proto_path或者-I ：指定 import 路径，可以指定多个参数，编译时按顺序查找，不指定时默认查找当前目录。 .proto 文件中也可以引入其他 .proto 文件，这里主要用于指定被引入文件的位置。 –go_out：golang编译支持，指定输出文件路径 其他语言则替换即可，比如 --java_out 等等 –go_opt：指定参数，比如--go_opt=paths=source_relative就是表明生成文件输出使用相对路径。 path/to/file.proto ：被编译的 .proto 文件放在最后面 $ protoc --go_out=. hello_word.proto 编译后会生成一个hello_word.pb.go文件。 到此为止就ok了。 ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:4:2","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"5. 编译过程 可以把 protoc 的编译过程分成简单的两个步骤： 1）解析 .proto 文件，编译成 protobuf 的原生数据结构保存在内存中； 2）把 protobuf 相关的数据结构传递给相应语言的编译插件，由插件负责将接收到的 protobuf 原生结构渲染输出为特定语言的模板。 具体过程如图所示： protoc 中原生包含了部分语言（java、php、python、ruby等等）的编译插件，但是没有 Go 语言的，所以需要额外安装一个插件。 具体原生支持见源码https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/compiler/main.cc 同样的，后续讲到的 gRPC Plugins、gRPC-Gateway 也是一个个的 protoc 编译插件，将 .proto 文件编译成对应模块需要的源文件。 ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:5:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["gRPC"],"content":"6. 参考 https://developers.google.com/protocol-buffers https://github.com/protocolbuffers/protobuf https://studygolang.com/articles/12673 ","date":"2020-12-18","objectID":"/posts/grpc/01-protobuf/:6:0","tags":["gRPC"],"title":"gRPC(Go)教程(一)---Protobuf","uri":"/posts/grpc/01-protobuf/"},{"categories":["elasticsearch"],"content":"基于 Docker 快速部署 ELK 日志系统","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"ELK 是由 Elasticsearch、Logstash、Kibana、Filebeat 等开源软件组成的一个组合体。 ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:0:0","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"1. 概述 整个流程大概是这样子的： 完整架构如下所示： Elasticsearch Elasticsearch是一个高度可扩展全文搜索和分析引擎，基于 Apache Lucene 构建，能对大容量的数据进行接近实时的存储、搜索和分析操作，可以处理大规模日志数据，比如 Nginx、Tomcat、系统日志等功能。 Logstash 数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置；支持普通 log、自定义json 格式的日志解析。 Kibana 数据分析和可视化平台。通常与 Elasticsearch 配合使用，对其中数据进行搜索、分析和以统计图表的方式展示。 Filebeat 轻量级的采集器，主要用于从日志文件中采集日志。 App 将日志输出到各自的日志文件中，由 Filebeat 进行采集，并发送到 Logstash，Logstash则对日志进行过滤分类等操作，最后存储到 Elasticsearch，Kibana 则作为可视化界面从 Elasticsearch 中 读取数据并展示。 ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:1:0","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"2. 部署 本文基于 docker compose 部署一个最基本的 ELK 平台，生产环境则建议使用集群模式。 docker 环境安装点这里 其中 ELK 可以部署在单独的机器上，Filebeat 则需要部署在每一个需要采集日志的机器上。 手动创建 docker network # syntax docker network create {network-name} $ docker network create elk ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:2:0","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"1. Elasticsearch version: '3.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.8.0 container_name: elk-es environment: # JVM 参数 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" # 以单节点方式启动 ES - discovery.type=single-node volumes: - ./data:/usr/share/elasticsearch/data - ./logs:/usr/share/elasticsearch/logs ports: - 9200:9200 networks: default: external: name: elk ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:2:1","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"2. Logstash version: '3.2' services: logstash: image: logstash:7.8.0 container_name: elk-logstash restart: always volumes: # 指定管道配置文件 - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf:rw environment: - elasticsearch.hosts=http://elk-es:9200 - \"LS_JAVA_OPTS=-Xmx256m -Xms256m\" ports: - 5044:5044 networks: default: external: name: elk logstash.conf 配置文件如下 logstash 由管道组成，而一个管道由 input、output 和 filter 三个元素组成 input { # 来源beats beats { # 端口 port =\u003e \"5044\" } } # 分析、过滤插件，可以多个 filter { grok { # 将日志内容存储到 Message 字段中 match =\u003e { \"message\" =\u003e \"%{COMBINEDAPACHELOG}\"} } geoip { # 将客户端IP存储到 clientip 字段 source =\u003e \"clientip\" } } output { # 选择elasticsearch elasticsearch { hosts =\u003e [\"http://elk-es:9200\"] # {fields} {service} 来自于 filebeat.conf 可以自定义字段名 # {@metadata} {version} 则是自带属性 index =\u003e \"%{[fields][service]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" } } ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:2:2","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"3. Kibana version: '3.2' services: kibana: image: kibana:7.8.0 container_name: elk-kibana environment: SERVER_NAME: kibana.local ELASTICSEARCH_HOSTS: http://elk-es:9200 I18N_LOCALE: zh-en ports: - 5601:5601 networks: default: external: name: elk ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:2:3","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"4. Filebeat version: '3.2' services: # filebeat 从文件中读取日志并发送给 Logstash filebeat: image: elastic/filebeat:7.8.0 container_name: elk-filebeat restart: always volumes: # 将宿主机目录挂载到容器中 - /var/logs/elk:/var/logs # 指定配置文件 - ./filebeat.conf.yml:/usr/share/filebeat/filebeat.yml - ./logs:/usr/share/filebeat/logs - ./data:/usr/share/filebeat/data networks: default: external: name: elk filebeat.conf.yml 配置文件如下： filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false # 数据源 filebeat.inputs: # user - type: log enabled: true scan_frequency: 5s fields: service: elk # 增加IP字段用于区分不同的机器 #clientip: ${SERVER_IP} 在环境变量中增加该值 或者写死在配置文件中 clientip: 192.168.1.244 paths: # 这里指定的是容器内的目录 需要在启动时将外部目录挂载到容器里才行 - /var/logs/*.log # 发送到 logstash output.logstash: hosts: [ \"elk-logstash:5044\" ] 容器启动时将宿主机/var/logs目录挂载到了容器中的/var/logs，所以采集/var/logs/*.log日志就是采集宿主机上的文件。 ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:2:4","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["elasticsearch"],"content":"3. 测试 手动写入一些日志看是否会被采集到 ES 中。 cd /var/log mkdir elk cd elk/ echo 333 INFO \u003e\u003e elk.log 不出意外的话 10s 左右就可以在 Kibana 中看到自己写的日志了。 因为 filebeat 默认采集频率是 10秒。 日志内容如下所示： ","date":"2020-12-11","objectID":"/posts/elasticsearch/02-elk-deploy/:3:0","tags":["elasticsearch"],"title":"Elasticsearch 教程(二)---基于 Docker 快速部署 ELK 日志系统","uri":"/posts/elasticsearch/02-elk-deploy/"},{"categories":["Redis"],"content":"Redis数据类型及其底层数据结构","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"Redis 为什么快？一方面是因为它是内存数据库，所有操作都在内存上完成自然很快。另一方面就要归功于它的数据结构。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:0:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"1. 概述 Redis 包含 5 大基本数据类型 String（字符串） List（列表） Hash （哈希） Set（集合） Sorted Set（有序集合） 当然这些只是 Redis 键值对中值的数据类型，也就是数据的保存形式。 具体底层实现如下 简单动态字符串 双向链表 压缩列表 哈希表 跳表 整数数组 二者对应关系如下图所示： 可以看到除了 String 之外，其他数据类型都有两个底层数据结构实现，这是因为 Redis 为了节省内存，会根据数据量大小来选择不同的数据结构。 String 也会根据数据长度使用不同的编码方式。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:1:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"2. 底层数据类型 整数数组 数组优点是访问快，通过下标访问元素快 ，只需要 O(1) 时间复杂度,但是增删慢，增删元素时却需要移动前或后的所有元素，需要 O(n) 时间复杂度。 双向链表 链表和数组恰好相反。 增删快，只需要修改节点指针即可，O(1) 时间复杂度。访问慢，只能通过遍历来访问指定元素，需要 O(n) 时间复杂度。 跳表 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。 跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。 空间换时间 优化了操作的时间复杂度 经过优化后访问从链表的 O(n) 时间复杂度提升到了 O(logn)，增删也因为需要维护多级索引，从 O(1) 降低到了 O(logn)。 哈希表 哈希表读写都可以通过哈希值直接找到对应元素，只需要O(1) 时间复杂度。但是哈希表最大问题是无法范围查询。 压缩列表 相比之下压缩列表是比较少见的一一种数据结构，具体实现如下： 表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数,表尾还有一个 zlend，表示列表结束。 可以看到各个元素都紧凑的挨在一起，内存利用率极高。 定位第一个或最后一个元素，只需要通过表头的 3 个字段即可找到，O(1) 时间复杂度，其他元素则只能遍历整个列表，O(n) 时间复杂度。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:2:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"3. 详解 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"1. String 在Redis内部String对象的编码可以是 int 、 raw 或者 embstr int 如果一个字符串对象保存的是整数值， 并且这个整数值可以用 long 类型来表示， 那么字符串对象会将整数值保存在字符串对象结构的 ptr属性里面（将 void* 转换成 long ）， 并将字符串对象的编码设置为 int 。 raw 如果字符串的长度大于 44 字节， 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值， 并将对象的编码设置为 raw 。 embstr 如果这个字符串值的长度小于等于 44 字节， 那么将使用 embstr 编码的方式来保存这个字符串值。 embstr 编码是专门用于保存短字符串的一种优化编码方式， 这种编码和 raw 编码一样， 都使用 redisObject 结构和 sdshdr 结构来表示字符串对象， 但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间， 空间中依次包含 redisObject 和 sdshdr 两个结构， long double 类型的浮点数在 Redis 中也是作为字符串值来保存的： 如果我们要保存一个浮点数到字符串对象里面， 那么程序会先将这个浮点数转换成字符串值， 然后再保存起转换所得的字符串值。 为什么是44字节？ embstr 是一块连续的内存区域，由 redisObject 和 sdshdr 组成。其中 redisObject 占16个字节，当 buf 内的字符串长度是 44 时，sdshdr 的大小为 3+44+1=48，(3 为 sdshdr 的元数据，1则是 ‘\\0’ 44 为真正的数据)，加起来刚好 64。 Redis 默认使用 Jemalloc 分配内存。jemalloc 会分配 8，16，32，64 等大小的内存。embstr 最小为 16+3+1=20，分配32字节的话，只能存12字节的数据，有点小了，所以按照 64 字节来算，当字符数小于 44 时，64 字节依然够用。这个默认 44 就是这样来的。 3.2 版本之前是 39，sdshdr 中的 3 字节以前是8字节。https://blog.csdn.net/XiyouLinux_Kangyijie/article/details/78045385 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:1","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"2. Hash 在 Redis 内部 Hash 对象的编码可以是 ziplist和hashtable 数据量小的时候使用 ziplist 节省空间，数据量大的时候用 hashtable 以降低操作复杂度。 ziplist ziplist 编码的哈希对象使用压缩列表作为底层实现， 每当2有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾， 因此： 保存了同一键值对的两个节点总是紧挨在一起， 保存键的节点在前， 保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向， 而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 hashtable hashtable 编码的哈希对象使用字典作为底层实现， 哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象， 对象中保存了键值对的键； 字典的每个值都是一个字符串对象， 对象中保存了键值对的值。 编码转换 当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码。 注意：同样的上限值是可以修改的，具体请看配置文件中关于 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:2","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"3. List 在Redis内部List对象的编码可以是ziplist和linkedlist。 数据量小的时候使用 ziplist 节省空间，数据量大的时候用 linkedlist 降低操作复杂度。 ziplist ziplist 编码的列表对象使用压缩列表作为底层实现， 每个压缩列表节点（entry）保存了一个列表元素。 linkedlist linkedlist 编码的列表对象使用双端链表作为底层实现， 每个双端链表节点（node）都保存了一个字符串对象， 而每个字符串对象都保存了一个列表元素。 编码转换 当列表对象可以同时满足以下两个条件时， 列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个； 不能满足这两个条件的列表对象需要使用 linkedlist 编码。 注意：同样的上限值是可以修改的， 具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:3","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"4. Set 在Redis内部Set对象的编码可以是intset(整数集合)和hashtable(字典)。 intset intset 编码的集合对象使用整数集合作为底层实现， 集合对象包含的所有元素都被保存在整数集合里面。 hashtable hashtable 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 NULL 编码转换 当集合对象可以同时满足以下两个条件时， 对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 注意:第二个条件的上限值是可以修改的， 具体请看配置文件中关于 set-max-intset-entries 选项的说明。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:4","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"5. ZSet 在 Redis 内部 Set 对象的编码可以是ziplist或者skiplist。 其中skiplist编码由skiplist和hashtable一起实现。 ziplist ziplist 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score）。 压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向。 skiplist skiplist 编码的有序集合对象使用 zset 结构作为底层实现， 一个 zset 结构同时包含一个字典和一个跳跃表 typedef struct zset { zskiplist *zsl; dict *dict; } zset; zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素， 每个跳跃表节点都保存了一个集合元素： 跳跃表节点的 object 属性保存了元素的成员， 而跳跃表节点的 score 属性则保存了元素的分值。 通过这个跳跃表， 程序可以对有序集合进行范围型操作， 比如 ZRANK 、ZRANGE 等命令就是基于跳跃表 API 来实现的。 除此之外， zset 结构中的 dict 字典为有序集合创建了一个从成员到分值的映射， 字典中的每个键值对都保存了一个集合元素： 字典的键保存了元素的成员， 而字典的值则保存了元素的分值。 通过这个字典， 程序可以用 O(1) 复杂度查找给定成员的分值， ZSCORE 命令就是根据这一特性实现的 两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存。 skiplist 编码的有序集合对象会是下图(8-16)所示的样子 而对象所使用的 zset 结构将会是图 8-17 所示的样子。 为什么 skiplist 编码要用两种数据结构来实现? 虽然只用字典或者跳跃表也可以实现，但是会丢失掉另一个数据结构的特性。 假设只有字典,那么只需范围操作时需要对数据进行排序处理，至少需要 O(NlogN) 复杂度和 O(N) 的空间。 如果只有跳跃表则根据 key 获取 score 这一操作复杂度将从 O(1) 提升到 O(logN)。 所以最终选择了字典和跳跃表一起实现。 编码转换 当有序集合对象可以同时满足以下两个条件时， 对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 注意：同样的上限值是可以修改的，具体请看配置文件中关于 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 选项的说明。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:3:5","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"4. 小结 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:4:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"1. 操作复杂度 1.单元素操作是基础 指每一种集合类型对单个数据实现的增删改查操作 这些操作的复杂度由集合采用的数据结构决定 2.范围操作非常耗时 指集合类型中的遍历操作，可以返回集合中的所有数据 这类操作的复杂度一般是 O(N)，比较耗时，我们应该尽量避免 建议使用SCAN系列命令，实现了渐进式遍历，每次只返回有限数量的数据，避免阻塞Redis 3.统计操作通常高效 指集合类型对集合中所有元素个数的记录 例如 LLEN 和 SCARD。这类操作复杂度只有 O(1) 这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计 4.例外情况只有几个 指某些数据结构的特殊记录 例如压缩列表和双向链表都会记录表头和表尾的偏移量 这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，复杂度只有O(1) ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:4:1","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"2. Redis为什么快 String、Hash 和 Set使用哈希表实现，复杂度 O(1) SortedSet 也使用跳表进行优化，复杂度 O(logn) List采用压缩列表加双向链表则比较慢 O(n) 复杂度,但是 Push、Pop 等对表头尾元素的操作效率很高，所以需要因地制宜的使用 List,将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合。 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:4:2","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"5. 参考 《Redis 设计与实现》 Redis 核心技术实战 ","date":"2020-12-05","objectID":"/posts/redis/05-basic-datastructure-implement/:5:0","tags":["Redis"],"title":"Redis教程(五)---Redis 数据类型","uri":"/posts/redis/05-basic-datastructure-implement/"},{"categories":["Redis"],"content":"Redis 全局数据结构","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"Redis 为了操作效率使用哈希表来存储键值对，访问时只需要 O(1) 时间复杂度。 ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:0:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"1. 全局哈希表 为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。 哈希表由多个哈希桶组成，哈希桶中的 entry 元素中保存了*key和*value指针，分别指向了实际的键和值。 哈希表让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。 key-value 访问过程： 通过 key 计算出对应的哈希值 根据哈希值计算出对应的哈希桶索引 根据索引找到对应 entry，如果是哈希链则挨个查找到对应的 entry String 类型则 entry 中的 value 就是要查找的值，集合类型则需要在 value 中进一步查找 ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:1:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"2. 哈希冲突 当你往哈希表中写入大量数据时，哈希冲突是不可避免的问题。 哈希冲突也就是指，两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。 Redis 解决哈希冲突的方式，就是链式哈希（链地址法）。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。 但是，这里依然存在一个问题，哈希冲突链上的元素只能通过指针逐一查找再操作。 如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低。 所以，Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。 ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:2:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"3. Rehash 为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2，下文将当前正使用的称作主哈希表，用于扩容的称作备用哈希表。 一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步： 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间。 到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来的哈希表 1 留作下一次 rehash 扩容备用。 为了避免第二步拷贝数据阻塞 Redis 主线程，Redis 采用了 渐进式 Rehash。 简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求。 每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中； 等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。 即 每处理一个请求就从主哈希表拷贝一个哈希桶到备用哈希表。将一次拷贝拆分为多次拷贝从而减少拷贝过程中对 Redis 主线程的影响。 注意： 因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:3:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"4. 小结 Redis 使用哈希表存储键值对 采用链地址法解决哈希冲突 为避免阻塞主线程，采用渐进式 Rehash ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:4:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Redis"],"content":"5. 参考资料 《Redis 设计与实现》 Redis 核心技术实战 https://en.wikipedia.org/wiki/Hash_table ","date":"2020-11-27","objectID":"/posts/redis/04-global-datastructure/:5:0","tags":["Redis"],"title":"Redis教程(四)---全局数据结构","uri":"/posts/redis/04-global-datastructure/"},{"categories":["Tracing"],"content":"Jaeger线上环境部署","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"本文主要记录了如何在生产环境中部署 Jaeger各个组件，包括 Agent、Collector、Query、Storage 等等。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:0:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"1. 概述 测试部署时有官方提供的 all-in-one 的镜像，同时直接将数据存储在内存中，所以部署起来比较方便。 但是线上则建议单独部署各个组件和存储后端（这里采用ES存储）。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:1:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"1. 架构 完整架构包含如下组件： 1）jaeger-agent 2）jaeger-collector 3）jaeger-query 4）jaeger-ingester 5）elasticsearch 6）kafka ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:1:1","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"2. 数据流 具体流程如下： 1）客户端通过 6831 端口上报数据给 agent 2）agent通过 14250 端口将数据发送给 collector 3）collector 将数据写入 kafka 4）Ingester 从 kafka中读取数据并写入存储后端 5）query 从存储后端查询数据并展示 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:1:2","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"2. 各组件介绍 暂时只部署collector、agent、query和es这四个组件。 其中collector、query和es可以只部署一个。 但是agent则建议部署在每一台需要追踪的主机上，这样可以离 client 近一点。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"1. agent jaeger-agent 是客户端代理，需要部署在每台主机上。 Port Protocol Function 6831 UDP 客户端上报jaeger.thrift compact协议数据，大部分客户端都使用这个 6832 UDP jaeger.thrift binary协议数据。为node客户端单独开的一个端口，因为node 不支持jaeger.thrift compact协议 5778 HTTP 服务器配置 5775 UDP zipkin.thrift compact 兼容zipkin的 14271 HTTP 健康检查和 metrics ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:1","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"2. collector 收集器，可以部署多个。收集 agent 发来的数据并写入 db 或 kafka。 Port Protocol Function 14250 gRPC jaeger-agent通过该端口将收集的 span以 model.proto 格式发送到 collector 14268 HTTP 客户端可以通过该端口直接将 span发送到 collector。 9411 HTTP 用于兼容 zipkin 14269 HTTP 健康检查和 metrics ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:2","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"3. query UI 界面，主要做数据展示。 Port Protocol Function 16686 HTTP 默认url localhost:16686 16686 gRPC gRPC查询服务？ 16687 HTTP 健康检查和 metrics ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:3","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"4. ingester 主要从 kafka 中读取数据并写入存储后端。 Port Protocol Function 14270 HTTP 健康检查和 metrics ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:4","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"5. Storage Backends 用于存储收集的数据。 支持 Cassandra 和 Elasticsearch。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:5","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"6.Kafka 可以在收集器和后端存储之间做缓冲。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:2:6","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"3. docker-compose 使用 docker-compose进行部署。 首先创建一个 docker network。 docker network create jaeger 然后将各个组件都放到一个网络里，这样会比较方便。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:3:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"collector-query 这里暂时collector query 放在一起。 测试完成后可以把 LOG_LEVEL=debug 这个删掉。 version: '3.2' services: # jaeger-collector 收集器 jaeger-collector: image: jaegertracing/jaeger-collector container_name: jaeger-collector environment: - SPAN_STORAGE_TYPE=elasticsearch - ES_SERVER_URLS=http://jaeger-es:9200 - ES_USERNAME=elastic - LOG_LEVEL=debug ports: - 9411:9411 - 14250:14250 - 14268:14268 - 14269:14269 # jaeger-query UI jaeger-query: image: jaegertracing/jaeger-query container_name: jaeger-query environment: - SPAN_STORAGE_TYPE=elasticsearch - ES_SERVER_URLS=http://jaeger-es:9200 - ES_USERNAME=elastic - LOG_LEVEL=debug ports: - 16686:16686 - 16687:16687 networks: default: external: name: jaeger ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:3:1","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"es version: '3.2' services: # elasticsearch jaeger存储后端 单独部署 elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.8.0 container_name: jaeger-es environment: - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 volumes: - ./data:/usr/share/elasticsearch/data ports: - 9201:9200 networks: default: external: name: jaeger ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:3:2","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"kibana 为了方便管理ES中的数据还是部署一个kibana吧 注意：Kibana的版本必须和ES一致 version: '3.2' services: # kibana 方便观察es中的数据 kibana: image: 'kibana:7.8.0' container_name: jaeger-kibana environment: SERVER_NAME: kibana.local ELASTICSEARCH_HOSTS: http://jaeger-es:9200 I18N_LOCALE: zh-en ports: - 5602:5601 networks: default: external: name: jaeger ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:3:3","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"agent version: '3.2' services: # jaeger-agent 单独部署到各个需要采集的机器上 jaeger-agent: image: jaegertracing/jaeger-agent container_name: jaeger-agent environment: - REPORTER_GRPC_HOST_PORT=jaeger-collector:14250 - LOG_LEVEL=debug ports: - 5775:5775/udp - 5778:5778 - 6831:6831/udp - 6832:6832/udp - 14271:14271 networks: default: external: name: jaeger REPORTER_GRPC_HOST_PORT 用于指定collector的地址，这里是同一个网络所以能直接通过container_name 访问。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:3:4","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"4. Metrics 默认情况下，Jaeger microservices以Prometheus格式公开指标。 只需要对外暴露相应端口号即可。 默认端口号如下 Component Port jaeger-agent 14271 jaeger-collector 14269 jaeger-query 16687 jaeger-ingester 14270 all-in-one 14269 也可以自定义 1）--admin-http-port 指定端口号 2）--metrics-backend指定指标数据格式，默认为Prometheus格式，可选格式为expvar. 3）--metrics-http-route指定路由，默认为/metrics 一般用默认格式的就可以了。 在 Prometheus 中增加相应采集任务即可。 ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:4:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Tracing"],"content":"5. 参考 https://www.jaegertracing.io/docs/1.20/deployment/ ","date":"2020-11-13","objectID":"/posts/tracing/05-jaeger-deploy/:5:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(五)---Jaeger线上环境部署","uri":"/posts/tracing/05-jaeger-deploy/"},{"categories":["Golang"],"content":"sync.Cond源码分析及其基本使用介绍","date":"2020-11-05","objectID":"/posts/go/cond/","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"1. 概述 Golang 的 sync 包中的 Cond 实现了一种条件变量，可以使用在多个Reader等待共享资源 ready 的场景（如果只有一读一写，一个锁或者channel就搞定了）。 Cond的汇合点：多个goroutines等待、1个goroutine通知事件发生。 比较适合任务调用场景，一个 Master goroutine 通知事件发生，多个 Worker goroutine 在资源没准备好的时候就挂起，等待通知。 使用方法 // 创建Cond cond := sync.NewCond(new(sync.Mutex)) // 挂起goroutine cond.L.Lock() cond.Wait() // 唤醒一个 cond.Signal() // 唤醒所有 cond.Broadcast() 基本使用大概是需要等待的时候通过 Wait() 将 Goroutine 挂起，资源准备好的时候再通过 Signal() 或者 Broadcast() 将挂起中的 Goroutine 唤醒。 一个简单的 Demo func main() { var ( locker sync.Mutex cond = sync.NewCond(\u0026locker) wg sync.WaitGroup ) for i := 0; i \u003c 10; i++ { wg.Add(1) go func(number int) { // wait()方法内部是先释放锁 然后在加锁 所以这里需要先 Lock() cond.L.Lock() defer cond.L.Unlock() cond.Wait() // 等待通知,阻塞当前 goroutine fmt.Printf(\"g %v ok~ \\n\", number) wg.Done() }(i) } for i := 0; i \u003c 5; i++ { // 每过 50毫秒 唤醒一个 goroutine cond.Signal() time.Sleep(time.Millisecond * 50) } time.Sleep(time.Millisecond * 50) // 剩下5个 goroutine 一起唤醒 cond.Broadcast() fmt.Println(\"Broadcast...\") wg.Wait() } ","date":"2020-11-05","objectID":"/posts/go/cond/:1:0","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"2. 源码分析 go version 1.14.7 ","date":"2020-11-05","objectID":"/posts/go/cond/:2:0","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"1. Cond /* package: sync file： cond.go line: 21 */ type Cond struct { noCopy noCopy L Locker notify notifyList checker copyChecker } // NewCond() 返回指针，保证多 goroutine 获取到的是同一个实例。 func NewCond(l Locker) *Cond { return \u0026Cond{L: l} } noCopy：noCopy对象，实现了sync.Locker接口，使得内嵌 noCopy 的对象在进行 go vet 静态检查的时候，可以检查出是否被复制。 noCopy 具体见 Go issues 8005 /* package: sync file： cond.go line: 94 */ type noCopy struct{} func (*noCopy) Lock() {} func (*noCopy) Unlock() {} L：实现了 sync.Locker 接口的锁对象，通常使用 Mutex 或 RWMutex 。 /* package: sync file： mutex.go line: 31 */ type Locker interface { Lock() Unlock() } notify：notifyList 对象，维护等待唤醒的 goroutine 队列,使用链表实现。 /* package: sync file： runtime.go line: 33 */ type notifyList struct { wait uint32 notify uint32 lock uintptr head unsafe.Pointer tail unsafe.Pointer } checker：copyChecker 对象，实际上是 uintptr 对象，保存自身对象地址。 /* package: sync file： cond.go line: 79 */ type copyChecker uintptr func (c *copyChecker) check() { if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026\u0026 !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026\u0026 uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\"sync.Cond is copied\") } } 1）检查当前 checker 对象的地址是否等于保存在 checker 中的地址 由于第一次比较的时候 checker 中没有存地址所以第一次比较肯定是不相等的，于是有了后续 2 3步。 2）对 checker 进行 CAS 操作，如果 checker 中存储的地址值为空（就是0）就把当前 checker 对象的地址值存进去 3）第三步和第一步一样，再比较一下。 主要是防止在第一步比较发现不相等之后，第二步 CAS 之前，其他 goroutine 也在执行这个方法，并发的将 checker 赋值了，导致这里判定的时候第二步 CAS 失败，返回 false，然后错误的抛出一个 panic，所以执行第三步在比较一下是否相等。如果其他 goroutine 抢先执行 CAS 修改了 checker 中的值导致这里第二步也返回 false 的话，第三步的判定也会是相等的，不会抛出 panic 4）如果 3 个条件都成立，那 checker 肯定是被复制了，就是由于 cond 被复制引起的。 check 方法在第一次调用的时候，会将 checker 对象地址赋值给 checker，也就是将自身内存地址赋值给自身。 再次调用 checker 方法的时候，会将当前 checker 对象地址值与 checker 中保存的地址值（原始地址）进行比较，若不相同则表示当前 checker 的地址不是第一次调用 check 方法时候的地址，即 cond 对象被复制了，导致checker 被重新分配了内存地址。 ","date":"2020-11-05","objectID":"/posts/go/cond/:2:1","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"2. Wait /* package: sync file： cond.go line: 52 */ func (c *Cond) Wait() { // 1.每次操作之前都要检测一下 cond 是否被复制了。 c.checker.check() // 2.将 notifyList 中的 wait 值加1并返回之前的值 t := runtime_notifyListAdd(\u0026c.notify) // 3.释放锁，因此在调用Wait方法前，必须保证获取到了cond的锁，否则会报错 c.L.Unlock() // 4.将当前goroutine挂起，等待唤醒信号 runtime_notifyListWait(\u0026c.notify, t) // 5.gorountine被唤醒，重新获取锁 c.L.Lock() } 第二步代码如下： /* package: runtime file： sema.go line: 479 */ func notifyListAdd(l *notifyList) uint32 { return atomic.Xadd(\u0026l.wait, 1) - 1 } 第四步代码如下： /* package: runtime file： sema.go line: 488 */ // 获取当前 goroutine 添加到链表末端，然后 goparkunlock 函数休眠阻塞当前 goroutine // goparkunlock 函数会让出当前处理器的使用权并等待调度器的唤醒 func notifyListWait(l *notifyList, t uint32) { // 1.锁住 notify 队列 lock(\u0026l.lock) // 2.判断传入的等待序号t是否小于当前已经唤醒的序号notify // 如果是则说明当前 goroutine 不需要阻塞了 直接解锁并返回 // 有可能执行这步之前 goroutine 就已经被唤醒了 if less(t, l.notify) { unlock(\u0026l.lock) return } // 3.获取当前 goroutine，设置相关参数，将当前等待数赋值给 ticket s := acquireSudog() s.g = getg() s.ticket = t s.releasetime = 0 t0 := int64(0) if blockprofilerate \u003e 0 { t0 = cputicks() s.releasetime = -1 } // 4.将当前 goroutine 写入到链表尾部 if l.tail == nil { l.head = s } else { l.tail.next = s } l.tail = s // 5. 调用 goparkunlock 函数将当前 goroutine 挂起，等待唤醒信号 goparkunlock(\u0026l.lock, \"semacquire\", traceEvGoBlockCond, 3) if t0 != 0 { blockevent(s.releasetime-t0, 2) } releaseSudog(s) } ","date":"2020-11-05","objectID":"/posts/go/cond/:2:2","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"3. Signal /* package: sync file： cond.go line: 64 */ func (c *Cond) Signal() { // 1.复制检查 c.checker.check() // 2.顺序唤醒一个等待的gorountine runtime_notifyListNotifyOne(\u0026c.notify) } /* package: runtime file： sema.go line: 554 */ func notifyListNotifyOne(l *notifyList) { // 1.等待序号和唤醒序号相同则说明没有需要唤醒的 goroutine 直接返回 if atomic.Load(\u0026l.wait) == atomic.Load(\u0026l.notify) { return } // 2.锁住队列后再检查一遍等待序号和唤醒序号是否相同即判断有没有需要唤醒的 goroutine，没有则解锁后直接返回 lock(\u0026l.lock) t := l.notify if t == atomic.Load(\u0026l.wait) { unlock(\u0026l.lock) return } // 3.到这里就说明有需要唤醒的 goroutine，于是先将 notify序号+1 atomic.Store(\u0026l.notify, t+1) // 4.然后就开始唤醒 goroutine 了 for p, s := (*sudog)(nil), l.head; s != nil; p, s = s, s.next { // 4.1 找到 ticket等于当前唤醒序号的 goroutine if s.ticket == t { // 4.2 然后将其从等待唤醒链表中移除（因为这个 goroutine 马上就要被唤醒了） n := s.next if p != nil { p.next = n } else { l.head = n } if n == nil { l.tail = p } unlock(\u0026l.lock) s.next = nil // 4.3 然后唤醒这个 goroutine readyWithTime(s, 4) return } } // 4.4 最后解锁队列 unlock(\u0026l.lock) } ","date":"2020-11-05","objectID":"/posts/go/cond/:2:3","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"4. Broadcast 唤醒链表中所有的阻塞中的goroutine，还是使用readyWithTime来实现这个功能 /* package: sync file： cond.go line: 73 */ func (c *Cond) Broadcast() { // 1.复制检查 c.checker.check() // 2.唤醒所有在等待的 goroutine runtime_notifyListNotifyAll(\u0026c.notify) } 这里和 notifyListNotifyOne()差不多，只是一次性唤醒所有 goroutine。 /* package: runtime file： sema.go line: 522 */ func notifyListNotifyAll(l *notifyList) { // 1.等待序号和唤醒序号相同则说明没有需要唤醒的 goroutine 直接返回 if atomic.Load(\u0026l.wait) == atomic.Load(\u0026l.notify) { return } // 2. 将链表头尾指针置为空（可以看做是清空整个等待队列） // 但是需要将当前的链表头保存下来，不然等会找不到链表中的数据了 lock(\u0026l.lock) s := l.head l.head = nil l.tail = nil // 3.直接将notify需要赋值成等待序号（这样表示当前没有需要唤醒的 goroutine 了） // 前面唤醒一个的时候这里是+1 atomic.Store(\u0026l.notify, atomic.Load(\u0026l.wait)) unlock(\u0026l.lock) // 4.最后 for 循环唤醒链表中所有等待状态的 goroutine for s != nil { next := s.next s.next = nil readyWithTime(s, 4) s = next } } ","date":"2020-11-05","objectID":"/posts/go/cond/:2:4","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"3. 小结 基本使用 1）资源未准备好时，使用 Wait() 方法将 goroutine 挂起，由底层实现，会让出 CPU 时间片，从而避免使用无意义的循环浪费系统资源。 2）资源准备好时通过 Signal() 或者 Broadcast() 方法唤醒一个或多个被挂起的 goroutine。 等待唤醒流程 1）所有相关数据都是存在 notifyList 中的，包括 goroutine 和一些计数信息。 2）其中的 wait 和 notify 可以理解为等待序号和唤醒序号，都是自增值，wait 在有新 goroutine 等待时+1，notify 则在唤醒一个 goroutine 时+1。 3）等待状态的 goroutine 信息则存放在链表中，等待时加入链表尾部，唤醒时移除。 4）每个等待链表的 goroutine 都会将当前的 wait（等待序号）赋值给 ticket 字段，唤醒的时候会将 ticket=唤醒序号的 goroutine 唤醒。 5）当 wait==notify 时表示没有 goroutine 需要被唤醒，wait\u003enotify 时表示有 goroutine 需要被唤醒，wait 恒大于等于 notify。 noCopy Cond在内部持有一个等待队列 notifyList ，这个队列维护所有等待在这个 Cond 的 goroutine。如果 Cond 被复制则会导致其中的等待队列也被复制，最终可能会导致在唤醒 goroutine 的时候出现错误。 Kubernetes 的调度中也用到了 sync.Cond 有兴趣的可以研究一下。 https://mp.weixin.qq.com/s/rKLiazgWzneJfpfgHay8cA https://github.com/kubernetes/kubernetes/blob/0599ca2bcfcae7d702f95284f3c2e2c2978c7772/pkg/scheduler/internal/queue/scheduling_queue.go ","date":"2020-11-05","objectID":"/posts/go/cond/:3:0","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Golang"],"content":"4. 参考 https://ieevee.com/tech/2019/06/15/cond.html https://segmentfault.com/a/1190000019957459 https://www.jianshu.com/p/7b59d1d92a95 http://www.pydevops.com/2016/12/04/go-cond源码剖析-3/ ","date":"2020-11-05","objectID":"/posts/go/cond/:4:0","tags":["Golang"],"title":"Go语言sync.Cond源码分析","uri":"/posts/go/cond/"},{"categories":["Linux"],"content":"Linux下几种常见IO模型","date":"2020-10-24","objectID":"/posts/linux/06-io-model/","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"本文主要简单分析了 Linux 下的几种常见的 I/O 模型。包括 阻塞式I/O（BIO）、非阻塞式I/O（NIO）、I/O多路复用（select、epoll）等。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:0:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"1. 概述 IO 是主存和外部设备 ( 硬盘、终端和网络等 ) 拷贝数据的过程。 IO 是操作系统的底层功能实现，底层通过 I/O 指令进行完成。在本教程中，我们所说的 IO 指的都是网络 IO。 技术的出现一定是为了解决当前技术的某些痛点，IO 模型演化也是如此。从最初的 BIO 到 NIO 、select、epoll 都是为了解决某些不得不解决的问题。 五种 I/O 模型 1）阻塞式I/O：blocking IO 2）非阻塞式I/O： nonblocking IO 3）I/O复用（select，poll，epoll…）：IO multiplexing 4）信号驱动式I/O（SIGIO）：signal driven IO 5）异步I/O（POSIX的aio_系列函数）：asynchronous IO 在这里，我们以一个网络IO来举例，对于一个 network IO (以read举例)，它会涉及到两个系统对象：一个是调用这个 IO 的进程，另一个就是系统内核(kernel)。当一个 read 操作发生时，它会经历两个阶段： 阶段1：等待数据准备 (Waiting for the data to be ready)，即将数据从硬件接口到读取到内核态 阶段2： 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)，即将数据从内核态复制到用户态 当进程请求 I/O 操作的时候，它执行一个系统调用 syscall 将控制权移交给内核。当内核以这种方式被调用，它随即采取任何必要步骤，找到进程所需数据，并把数据传送到用户空间内的指定缓冲区。 内核试图对数据进行高速缓存或预读取，因此进程所需数据可能已经在内核空间里了。如果是这样，该数据只需简单地拷贝出来即可。如果数据不在内核空间，则进程被挂起，内核着手把数据读进内存。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:1:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"2. 阻塞式I/O 在 linux 中，默认情况下所有的 socket 都是 blocking，一个典型的读操作流程大概是这样： 第一步：通常涉及等待数据从网络中到达。当所有等待数据到达时，它被复制到内核中的某个缓冲区。 第二步：把数据从内核缓冲区复制到应用程序缓冲区。 当用户进程调用了 recvfrom 这个系统调用，kernel 就开始了 I/O 的第一阶段：准备数据。对于 network io 来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的 UDP 包），这个时候 kernel 就要等待足够的数据到来。而在用户进程这边，整 个进程会被阻塞。第二阶段当 kernel 等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。 所以，blocking IO 的特点就是在 IO 执行的两个阶段都被 block了。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:2:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"栗子 举个栗子，如下所示是一个简单的 socket server： func main() { // 建立socket，监听端口 第一步:绑定端口 netListen, err := net.Listen(\"tcp\", \"localhost:9800\") if err != nil { panic(err) } // defer 关闭资源，以免引起内存泄漏 defer netListen.Close() Log(\"Waiting for clients\") for { // 第二步:等待连接 conn, err := netListen.Accept() if err != nil { continue } Log(conn.RemoteAddr().String(), \" tcp connect success\") // 使用goroutine来处理用户的请求 go handleConnection(conn) } } 说明 首先 listen 监听端口，然后在一个 for循环中 等待 accept（accept 过程是阻塞的），也就是说每次只能在处理某个请求或者阻塞在 accept。于是每次请求都开一个新的 goroutine 来处理。go handleConnection(conn)。 具体的调用 # 获取socket fd 文件描述符 假设是 fd5 socket = fd5 # 绑定端口 bind 9800 # 监听 这个 文件描述符 listen fd5 # 然后 accept 等待连接过来 比如这里 fd6 就是进来的连接 accept fd5 = fd6 # 然后从 fd6 读取数据 recvfrom fd6 可以看到过程中会发生很多 syscall 系统调用。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:2:1","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"问题 这样做存在很多问题，需要为每个连接开一个 goroutine ，如果有 1W 链接那就要开 1W goroutine 。 1）消耗资源，每个 goroutine 是需要一定内存的。 2）cpu 对这么多 goroutine 调度也会浪费资源 3）最大问题是 这个 accept 是阻塞的，所以才需要开这么多 goroutine 当然了 goroutine 协程 相对于 线程 是非常轻量级的，调度也是有自己的 GPM 模型，goroutine 的切换也全是在用户态，相较之下已经比线程好很多了。 但是一旦涉及到系统调用 就会很慢，那么能不能让 accept 不阻塞呢？ 就是因为 accept 是阻塞的，所以只能开多个 线程 or 协程 来勉强使用。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:2:2","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"解决方案 kernel 中 提供了 sock_nonblock 方法，可以实现非阻塞。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:2:3","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"3. 非阻塞式I/O linux 下，可以通过设置 socket 使其变为 non-blocking。当对一个 non-blocking socket 执行读操作时，流程是这个样子： 从图中可以看出，当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error。 从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 erro r时，它就知道数据还没有准备好，于是它可以再次 发送 read 操作。 一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，用户进程第一个阶段不是阻塞的,需要不断的主动询问kernel数据好了没有；第二个阶段依然总是阻塞的。 使用 sock_nonblock 使得该过程非阻塞 # 获取socket fd 文件描述符 假设是 fd5 socket = fd5 # 绑定端口 bind 9800 # 监听 这个 文件描述符 listen fd5 # 然后 accept 等待连接过来 比如这里 fd6 就是进来的连接 accept fd5 = fd6 # 然后从 fd6 读取数据 recvfrom fd6 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:3:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"问题 虽然是非阻塞了，但是还是存在另外的问题，比如 C10K 问题。 假设有 1W 客户端，那么每次循环都需要对每个客户端进行一个 系统调用，假设 1W 客户端里其实就只有 1 个是有消息的，相当于另外 9999 次都是浪费。 每次循环时间复杂度为 O(n),但是实际上只需要处理有消息的连接，即O(m)。 n 为连接数，m 为有消息的连接数 同样的 kernel 通过 提供 select 方法来解决这个问题。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:3:1","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"解决方案 Linux kernel 通过 提供 select 方法来解决这个问题。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:3:2","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"4. 多路复用I/O IO 复用同非阻塞 IO 本质一样，不过利用了新的 select 系统调用，由内核来负责本来是请求进程该做的轮询操作。 看似比非阻塞 IO 还多了一个系统调用开销，不过因为可以支持多路 IO，才算提高了效率。 多路复用IO复用的是什么？ 我认为复用的是系统调用。NIO中是一次系统调用判定一个连接有没有数据，多路复用则通过 select 函数在一次系统调用中传递多个fd，实现一次调用校验多个连接是否有数据。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:4:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"1. select 它的基本原理就是 select 这个 function 会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。它的流程如图： select 方法接收多个 文件描述符，最后会返回需要处理的那几个文件描述符。 这样就不需要遍历所有连接了，只需要处理有消息的那几个。 假设有 1W 连接，即 1W 个文件描述符。 以前： 循环中 为了处理那几个有消息的连接，调用 1W 次 syscall； 现在：就先调用 select(fds) ,把 1W 个 文件描述符发给 kernel，内核处理后，假设只有 2个 连接是有消息的，需要处理，就只会返回对应的 fd,比如这里是 fd6、fd7。然后程序拿到需要处理的 fd 列表后再挨个处理，这样就减少了 9998 次 syscall。 注意 多路复用返回的是状态，具体读写操作还是由程序来控制。 问题 那么这个模型有没有问题呢？ 问题就是 每次 都要发 1W 个 fd 到 kernel ,然后内核中也要完成 1W 次遍历O(n)，但是以前是 1W 次系统调用，现在是 1次系统调用，里面遍历 1W 次，还是优化了不少的。不过还可以继续优化： 1） 每次要传 1W 个 fd 给内核 2） 内核每次要遍历 1W 个 fd,才能返回需要处理的 fds 解决方案 如何优化呢？ 在内核中开辟一块空间，每次有新的 fd 就直接存到这个空间里，不需要了就移除掉。这样问题1 就解决了。 问题2 的话就不好处理了，除非内核中提供某种机制，不需要自己去遍历 fds，等有消息来的时候 主动通知内核哪个 fd 来消息了，这样就很完美了。 于是 kernel 中就提供了 epoll 来解决这些问题。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:4:1","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"2. epoll epoll 与 select 相比就是 不需要自己去遍历 fds 了,由事件驱动的方式，有消息了就主动通知。 epoll 会为每个 fd 设置一个回调方法，事件触发后会执行回调函数，通过该回调函数会自动把当前fd添加到一个链表中，只需要遍历这个链表就能获取到所有准备好数据的fd。 一共有 3 个方法 1）epoll_create() 2）epoll_ctl() 3）epoll_wait() 具体信息都可以通过man name进行查看，比如man epoll_create() 如果提示 未找到 man 手册的话 可以手动安装一下 以下是 ubuntu 的安装命令 centos 用 yum install 应该是一样的 apt-get install manpages-de manpages-de-dev manpages-dev glibc-doc manpages-posix-dev manpages-posix epoll_create() epoll_create() returns a file descriptor referring to the new epoll in‐ stance. # 返回一个文件描述符。 # 这个就是前面说的，开内核中开辟一块空间来存放 fds 返回的文件描述符就是描述这块空间的 epoll_ctl() # 用于管理 前面创建的空间中的 fds # 比如新增一个 fd，或者删除某个 fd epoll_wait() # 这个就是等 等着某个 fd需要处理了就返回 不用自己去遍历了 伪代码 # 获取socket fd 文件描述符 假设是 fd5 socket = fd5 # 绑定端口 bind 9800 # 监听 这个 文件描述符 listen fd5 # 开辟一块用于存储 fds 的空间，假设为 fd8 epoll_create fd8 # 然后第一件事就是把 前面的 server 端 socket fd5 存进去 apoll_ctl(fd8,add,fd5,accept) # 然后就开始等待了 epoll_wait(fd8) # 假设此时有连接进来了 假设为 fd6 accept fd5---\u003e fd6 # 也是 第一件事就存进去 apoll_ctl(fd8,fd6) ... # 最后 epoll 空间里肯定就存了很多 fd 那么问题来了，他是如何指定那个 fd 需要处理的? 就是靠的事件驱动，收到消息后，网卡向 CPU 发送一个 中断事件,CPU 根据这个中断号就能判断出具体是什么事件了，然后通过回调方法就去拿到数据。 epoll 因为采用 mmap的机制, 使得 内核 socket buffer 和用户空间的 buffer 共享, 从而省去了 socket data copy, 这也意味着, 当 epoll 回调上层的 callback 函数来处理 socket 数据时, 数据已经从内核层 “自动” 到了用户空间。 epoll和select区别 select 需要遍历所有注册的I/O事件，找出准备好的的I/O事件。 而 epoll 则是由内核主动通知哪些I/O事件需要处理，不需要用户线程主动去反复查询，因此大大提高了事件处理的效率。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:4:2","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"5. 信号驱动式I/O ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:5:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"6. 异步I/O 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都 完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 在这整个过程中，进程完全没有被block。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:6:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"7. 小结 BIO：内核准备数据时会阻塞，直到数据准备好才返回，然后将数据从内核拷贝到用户程序内存。 因为会阻塞，所以只能用多线程，每个线程处理一个IO，但是多线程消耗大，所以此时不能支持太高并发。 NIO：内核数据没准备好直接返回，由用户程序进行轮训系统调用查看数据是否准备好。 相对BIO优势是非阻塞了，但是有多次系统调用耗费资源。 select：直接将所有fd发送给内核，由内核分别检查数据是否准备好了，减少大量的系统调用。 将多次系统调用降低到了一次，不过还是存在fd复制的消耗 epoll：直接将fd存在内核空间中，省去了select时将fd拷贝到内核空间的消耗。 fd存在内核空间，省去了拷贝的消耗。基本没有太大问题了，如果优化可以考虑降低内核空间的占用。 1）阻塞式I/O：内核准备数据时会阻塞，直到数据准备好才返回，然后将数据从内核拷贝到用户程序内存。 因为会阻塞，所以只能用多线程，每个线程处理一个IO，但是多线程消耗大，所以此时不能支持太高并发。 2）非阻塞I/O：内核数据没准备好直接返回，由用户程序进行轮训系统调用查看数据是否准备好。 相对BIO优势是非阻塞了，但是有多次系统调用耗费资源。 3）多路复用I/O select：将 fd 发送给内核，由内核遍历检测数据是否准备好，（由应用进行遍历转为内核进行遍历）将多次系统调用转为一次系统调用，不过还是存在fd复制的消耗将。 epoll：将 fd 存在内存中，省去 fd 传输过程，同时 socket 数据准备好时，由硬件（网卡）发起中断，再次省去系统调用。基本没有太大问题了，如果优化可以考虑降低内核空间的占用。 4）信号驱动式I/O：用得不多 5）异步I/O：发起调用后直接返回，调用处理完发送信号通知，异步操作，没有任何阻塞。 其实前四种 I/O 模型都是同步 I/O 操作，他们的区别在于第一阶段，而他们的第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom 调用。 可以看到比较优秀的 I/O 模型就是 epoll 了，redis、nginx等等中间件中也是广泛的在使用 epoll。 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:7:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Linux"],"content":"8. 参考 《UNIX网络编程：卷一》 https://www.zhihu.com/question/19732473/answer/241673170 http://blog.chinaunix.net/uid-14874549-id-3487338.html http://www.tianshouzhi.com/api/tutorials/netty/221 ","date":"2020-10-24","objectID":"/posts/linux/06-io-model/:8:0","tags":["Linux"],"title":"Linux下几种常见IO模型","uri":"/posts/linux/06-io-model/"},{"categories":["Nginx"],"content":"Nginx访问日志简单分析","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"本文主要记录了如何对 Nginx 的访问日志进行一些简单的分析。例如分析每日的 PV、UV等指标。 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:0:0","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"1. 概述 Nginx访问日志记录了Nginx的所有请求，默认会存储在nginx/logs/access.log文件中，也可以在配置文件中通过access_log参数自定义存放位置。 如果实在找不到可以通过如下命令查询 # find / -name \"access.log\" /usr/local/nginx/logs/access.log 简单查看一些文件中的内容 less access.log 内容大概是这样子的 183.69.208.21 - - [16/May/2020:10:04:59 +0000] \"GET / HTTP/1.1\" 200 4230 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\" 主要包含了IP、日期、HTTP Method和协议版本号 状态码 设备信息等。 本次分析主要用到了awk命令 AWK 是一种处理文本文件的语言，是一个强大的文本分析工具。可以简单的理解为一种脚本语言吧。 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:1:0","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"2. 具体分析 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:0","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"1. PV数 pv是page view的缩写，即页面浏览,这里我们可以简单的把用户的每一次请求都看做一次访问。 于是通过access.log统计PV数就是统计内容的行数了。 具体命令如下： wc -l access.log #结果如下 108544 access.log 总PV数大约是108544。 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:1","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"2. 每天的PV数 只需要在PV数的基础上增加按日期分组即可。 这里就要用到awk命令了，具体命令如下： awk '{print substr($4 , 2, 11)}' access.log|\\ sort |uniq -c|\\ sort -rn|\\ head -n 10 #结果如下 1901 21/May/2020 1755 23/Jun/2020 1667 15/Jul/2020 1620 24/May/2020 1593 24/Aug/2020 1579 02/Oct/2020 1485 16/Oct/2020 1387 23/May/2020 1361 29/May/2020 1274 01/Sep/2020 具体解析： # AWK 基本语法 # $1 表示第一行内容 awk '{print $1}' access.log awk '{print substr($4 , 2, 11)}' access.log 这句表示对第四列进行截取然后打印出来 这里截取出来的刚好就是日期 uniq -c 则是计数（记录重复的有多少个） sort -rn 排序 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:2","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"3. UV数 UV即Unique visitor，唯一访客，每个用户只算一次。 可以简单的把每个IP当做一个独立访客，这样只需要对于UV我们可以按IP进行分组统计就行了。 awk '{print $1}' access.log |\\ sort | uniq |\\ wc -l # 注意 需要先sort在uniq去重，因为uniq只会对相邻的行进行去重 #结果如下 12354 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:3","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"4. 每天的UV数 同样的UV基础上增加按日期分组 awk '{print substr($4,2,11) \" \" $1}' access.log |\\ sort | uniq |\\ awk '{uv[$1]++;next}END{for(date in uv) print uv[date] \" \" date}'|\\ sort -rn|\\ head -n 10 # head 用于控制显示多少个head -n 10 即显示前10 # 结果如下 333 16/Oct/2020 321 12/Oct/2020 321 04/Oct/2020 319 29/May/2020 304 17/Jul/2020 299 05/Aug/2020 294 09/Oct/2020 293 23/Aug/2020 293 07/Oct/2020 290 28/Sep/2020 这次命令稍微复杂了一点点，具体分析如下 # 将日期（$4）和IP($1)提取出来重组成新的内容 awk '{print substr($4,2,11) \" \" $1}' access.log |\\ # 排序并去重 这样统计出来的就是UV了 sort | uniq |\\ # 取出每一行的第一列即日期 并将计数+1 最后for循环打印出来 awk '{uv[$1]++;next}END{for(date in uv) print uv[date] \" \" date}' # 最后在按倒序排序 sort -rn # 只输出前10条记录 head -n 10 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:4","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"5. 统计哪些设备访问过 cat access.log |\\ awk '{devices[$12]++;next} END {for (d in devices) print devices[d] \" \" d}'|\\ sort -rn|\\ head -n 10 # 结果如下 88396 \"Mozilla/5.0 9835 \"-\" 4021 \"Mozilla/4.0 1705 1422 \"Go-http-client/1.1\" 847 \"fasthttp\" 759 \"Sogou 531 \"serpstatbot/1.0 330 \"MauiBot 323 \"Mozilla\" ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:5","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"6. 统计被访问最多的页面 cat access.log|\\ awk '{print $7}'|sort|uniq -c|\\ grep post|\\ sort -rn|\\ head -n 10 # grep post 过滤掉其他记录 只统计post下的页面 # 结果如下 783 /post/etcd/05-watch/ 565 /post/etcd/06-why-mvcc/ 403 /post/etcd/03-v3-analyze/ 354 /post/grpc/00-faq/ 335 /post/elasticsearch/01-install-by-docker/ 285 /post/etcd/04-etcd-architecture/ 253 /post/grpc/04-interceptor/ 231 /post/etcd/01-install/ 230 /post/git/04-git-reduce/ 225 /post/mysql/04-cap-lock/ ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:2:6","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["Nginx"],"content":"3. 小结 主要用到了以下几个命令 wc 统计文件内容行数、字数、字节数 -l 只统计行数 cat 查看文件内容 会一次性加载出所有内容，不建议用在大文件上 ls -h 先查看文件大小 awk文本分析工具 sort 排序 默认升序 -r参数指定降序 默认按字符排序（该情况下2\u003e10 因为 2\u003e1） -n 参数指定按照数值大小排序 grep过滤 支持正则表达式 uniq 去重 大部分情况下需要和sort配合使用 head 控制输出行数 和sort配合可以只输出topn 管道操作符 | 通过该命令可以将一个命令的输出作为另一个命令的输出 非常常用 ","date":"2020-10-16","objectID":"/posts/nginx/05-accesslog-analysis/:3:0","tags":["Nginx"],"title":"Nginx教程(五)---访问日志简单分析","uri":"/posts/nginx/05-accesslog-analysis/"},{"categories":["MySQL"],"content":"MySQL中几种JOIN算法的简单分析","date":"2020-08-10","objectID":"/posts/mysql/09-join/","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"本文主要记录了 MySQL的JOIN语句的NLJ、BLJ和MySQL8.0新增的Hash Join算法，及相关优化如MRR、BKA等，最后回答了到底能不能使用JOIN，驱动表又该如何选择等问题。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:0:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"1.概述 join语句常见算法为NLJ、BNL和MySQL8.0新增的Hash Join，同时也会使用到MRR和BKA优化。 然后是两个问题 1）到底该不该使用join？ 2）如果用那么驱动表又该如何选择？ ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:1:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"2. join算法 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:2:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"1. Index Nested-Loop Join（NLJ） NLJ算法跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。 假设为如下查询语句 # 被驱动表表t2 a列上有索引 select * from t1 straight_join t2 on (t1.a=t2.a); NLJ具体执行过程如下： 1）从表 t1 中读入一行数据 R 2）从数据行 R 中，取出 a 字段到表 t2 里去查找 3）取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分 4）重复执行步骤 1 到 3，直到表 t1 的末尾循环结束 如果不使用 join 又该如何手动实现呢： 1）执行select * from t1，查出表 t1 的所有数据 2）循环遍历这 100 行数据： 从每一行 R 取出字段 a 的值 $R.a 执行select * from t2 where a=$R.a 把返回的结果和 R 构成结果集的一行。 整个过程中扫描行数一致，但是总共执行了 101 条语句，比直接 join 多了 100 次交互。除此之外，客户端还要自己拼接 SQL 语句和结果。 到底该不该使用join？ 可以看到，在被驱动表可以使用索引的情况下，使用join效率比手动实现更高。 如何选择驱动表？ 在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索，所以应该让小表来做驱动表。 假设被驱动表行数为M，需要先走二级索引，在走主键索引，时间复杂度为2*log2M 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次 因此整个执行过程，近似复杂度是 N + N2log2M 显然，N 对扫描行数的影响更大，因此应该让小表来做驱动表 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:2:1","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"2. Simple Nested-Loop Join 如果查询字段上没有索引，MySQL 还会用前面同样的算法吗? 如果表t1,t2分别都有10万行，无法走索引那么一共需要扫描10万*10万行。 这就是Simple Nested-Loop Join 算法，虽然看起来结果是正确的，但是效率太低了。 当然，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:2:2","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"3. Block Nested-Loop Join(BNL) 由于Simple Nested-Loop Join 算法实在太笨了，所以MySQL对其进行了优化，就是这里的BLJ算法。 具体流程如下： 1）把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 2）扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回 同样需要扫描两个表，但是比较操作是在内存中比较，速度上会快很多，性能也更好 如果表t1实在太大了，内存都放不下则会进行分段处理,一块一块的加载到内存中，所以叫 Block Nested-Loop Join。 1）扫描表 t1，顺序读取数据行放入 join_buffer 中，只放了一部分 join_buffer 就满了，继续第 2 步； 2）扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回 3）清空 join_buffer； 4）继续扫描表 t1，顺序读取后续的数据放入 join_buffer 中，继续执行第 2 步。 驱动表选择？ 可以看到，驱动表被分成多少块，被驱动表就会被扫描多少次，所以还是推荐小表做驱动表，以减少被驱动表的扫描次数。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:2:3","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"4. Hash join MySQL8.0中新增了Hash join方式，不过只能用于等值连接。 Hash join 不需要索引的支持。大多数情况下，hash join 比之前的 BNL 算法在没有索引时的等值连接更加高效。 具体步骤： 1）把把驱动表相关字段存入Join Buffer，这一步和BNL套路相同。 2）（build）把Join Buffer中对应的字段值生成一个散列表，保存在内存中。 3）（probe）扫描被驱动表，对被驱动表中的相关字段进行散列并比较。 在最好的场景下，如果Join Buffer能覆盖驱动表所有相关字段，那么在查询的过程中驱动表和被驱动表都只需要扫描一次，如果散列算法够好，比较次数也只是被驱动表的记录数。 在MySQL8.0以后优化器会优先选择使用hash join。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:2:4","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"3. 问题 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:3:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"1. 可以用join吗？ 1）如果可以使用 NLJ 算法，也就是说可以用上被驱动表上的索引，其实是没问题的,当然了Hash join也是可以的。 2）如果使用 BNL 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 所以你在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:3:1","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"2. 如何选择驱动表？ 根据前面的分析可以发现，不管是NLJ算法还是BLJ算法，都是推荐使用小表来做驱动表。 那么如何确定小表呢？ 并不是表的数据量小就是小表，而是应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:3:2","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"4. join相关优化 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:4:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"1. Multi-Range Read 夺命三连 What MRR是一种将查询时的随机读转为顺序读的优化手段。 Why 那当然是磁盘的随机读比顺序读慢嘛。 我们都知道对除Id以外字段做范围查询时先走二级索引（如果有索引），然后根据主键id去主键索引树一条条查找，这个过程也叫做回表。 主键索引是按主键顺序排的，但是按二级索引范围查询出来的结果，主键不一定是顺序排列的，大多数情况下都是乱序的。 所以对主键索引来说每次查询都是随机读。 How 举个栗子 优化前： 1）根据查询条件在二级索引树中找到主键id； 2）去主键索引根据id找到对应数据； 3）重复步骤12直到查询出所有满足条件的行； 优化后： 1）根据二级索引，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ; 2）将 read_rnd_buffer 中的 id 进行递增排序； 3）根据排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回； 可以看到主要就是将id暂时存在到read_rnd_buffer 中，将其排序后再去主键索引中查询。 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:4:1","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"2. Batched Key Access What 这个 BKA 算法，其实就是对 NLJ 算法的优化，因为NLJ是先在驱动表中取一条数据然后去被驱动表中匹配的。 这样的话就用不了前面的MRR优化了。 Why 虽然NLJ因为可以用到被驱动表的索引，效率不错，但还是有进一步优化的空间，BKA算法则是对其的一个优化， 使得优化后的NLJ也可以用到MRR。 How 优化前： 从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了 优化后： 先把表 t1 的数据取出来一部分，先放到一个临时内存 join_buffer中，这样就可以使用MRR进行优化了，将这部分数据排好序后再去t2中查询。 该算法需要手动开启，命令如下 set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on'; ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:4:2","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["MySQL"],"content":"5. 参考 MySQL45讲 https://dev.mysql.com/doc/refman/8.0/en/nested-loop-joins.html https://dev.mysql.com/doc/refman/8.0/en/hash-joins.html https://dev.mysql.com/doc/refman/8.0/en/mrr-optimization.html https://dev.mysql.com/doc/refman/8.0/en/bnl-bka-optimization.html https://blog.csdn.net/horses/article/details/102690076 ","date":"2020-08-10","objectID":"/posts/mysql/09-join/:5:0","tags":["MySQL"],"title":"MySQL教程(九)---MySQL几种JOIN算法","uri":"/posts/mysql/09-join/"},{"categories":["Golang"],"content":"go test 使用指南","date":"2020-07-25","objectID":"/posts/go/gotest/","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"本来主要为 Go 语言中的测试工具 go test 使用指南，最后顺便测试了一下几种字符串拼接方式的性能差距。 ","date":"2020-07-25","objectID":"/posts/go/gotest/:0:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"1. 概述 Go 语言中的测试依赖 go test 命令。编写测试代码和编写普通的 Go 代码过程是类似的，并不需要学习新的语法、规则或工具。 go tes t命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以**_test.go**为后缀名的源代码文件都是 go test 测试的一部分，不会被 go build 编译到最终的可执行文件中。 在*_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。 类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 测试文件以_test.go结尾，且放在同一位置，例如 test | —— calc.go | —— calc_test.go ","date":"2020-07-25","objectID":"/posts/go/gotest/:1:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"2. go test go test 是 Go 语言自带的测试工具，其中包含的是两类，单元测试和性能测试。 ","date":"2020-07-25","objectID":"/posts/go/gotest/:2:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"1. 运行模式 根据输入参数不同， go test 有两种运行模式。 1. 本地目录模式 在没有包参数（例如 go test 或 go test -v ）调用时发生。 在此模式下， go test 编译当前目录中找到的包和测试，然后运行测试二进制文件。在这种模式下，caching 是禁用的。 在包测试完成后，go test 打印一个概要行，显示测试状态、包名和运行时间。 2. 包列表模式 在使用显式包参数调用 go test 时发生（例如 go test math ， go test ./... 甚至是 go test . ）。 在此模式下，go 测试编译并测试在命令上列出的每个包。如果一个包测试通过， go test 只打印最终的 ok 总结行。如果一个包测试失败， go test 将输出完整的测试输出。如果使用 -bench 或 -v 标志，则 go test 会输出完整的输出，甚至是通过包测试，以显示所请求的基准测试结果或详细日志记录。 ","date":"2020-07-25","objectID":"/posts/go/gotest/:2:1","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"2. 参数解读 通过 go help test 可以看到 go test 的使用说明： 1. 语法 go test [-c] [-i] [build flags] [packages] [flags for test binary] 2. 变量 go test 的变量列表如下： test.short : 一个快速测试的标记，在测试用例中可以使用 testing.Short() 来绕开一些测试 test.outputdir : 输出目录 test.coverprofile : 测试覆盖率参数，指定输出文件 test.run : 指定正则来运行某个/某些测试用例 test.memprofile : 内存分析参数，指定输出文件 test.memprofilerate : 内存分析参数，内存分析的抽样率 test.cpuprofile : cpu分析输出参数，为空则不做cpu分析 test.blockprofile : 阻塞事件的分析参数，指定输出文件 test.blockprofilerate : 阻塞事件的分析参数，指定抽样频率 test.timeout : 超时时间 test.cpu : 指定cpu数量 test.parallel : 指定运行测试用例的并行数 3. 参数 参数解读： 关于build flags，调用go help build，这些是编译运行过程中需要使用到的参数，一般设置为空 关于packages，调用go help packages，这些是关于包的管理，一般设置为空 关于flags for test binary，调用go help testflag，这些是go test过程中经常使用到的参数 -c : 编译 go tes t成为可执行的二进制文件，但是不运行测试。 -i : 安装测试包依赖的package，但是不运行测试。 -v: 是否输出全部的单元测试用例（不管成功或者失败），默认没有加上，所以只输出失败的单元测试用例。 -run=pattern: 只跑哪些单元测试用例 -bench=patten: 只跑那些性能测试用例 -benchmem : 是否在性能测试的时候输出内存情况 **-benchtime t **: 性能测试运行的时间，默认是1s -cpuprofile cpu.out : 是否输出cpu性能分析文件 -cover: 测试覆盖率 -coverprofile=file ：输出测试覆盖率到文件 -memprofile mem.out : 是否输出内存性能分析文件 -blockprofile block.out : 是否输出内部goroutine阻塞的性能分析文件 -memprofilerate n : 内存性能分析的时候有一个分配了多少的时候才打点记录的问题。 这个参数就是设置打点的内存分配间隔，也就是profile中一个sample代表的内存大小。默认是设置为512 * 1024的。如果你将它设置为1，则每分配一个内存块就会在profile中有个打点，那么生成的profile的sample就会非常多。如果你设置为0，那就是不做打点了。 你可以通过设置memprofilerate=1和GOGC=off来关闭内存回收，并且对每个内存块的分配进行观察。 -blockprofilerate n: 基本同上，控制的是goroutine阻塞时候打点的纳秒数。默认不设置就相当于-test.blockprofilerate=1，每一纳秒都打点记录一下 -parallel n : 性能测试的程序并行cpu数，默认等于GOMAXPROCS。 -timeout t : 如果测试用例运行时间超过t，则抛出panic -cpu 1,2,4 : 程序运行在哪些CPU上面，使用二进制的1所在位代表，和nginx的nginx_worker_cpu_affinity是一个道理 -short : 将那些运行时间较长的测试用例运行时间缩短 ","date":"2020-07-25","objectID":"/posts/go/gotest/:2:2","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"3. 类型 ","date":"2020-07-25","objectID":"/posts/go/gotest/:3:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"1. 单元测试 1）文件名必须以xx_test.go命名 2）方法必须是Test[^a-z]开头 3）方法参数必须 t *testing.T 4）使用go test 执行单元测试 ","date":"2020-07-25","objectID":"/posts/go/gotest/:3:1","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"2. 性能测试 基准测试的基本格式如下： func BenchmarkName(b *testing.B){ // ... } 基准测试以 Benchmark 为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。 基准测试并不会默认执行，需要增加-bench参数。 ","date":"2020-07-25","objectID":"/posts/go/gotest/:3:2","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"3. 示例函数 被 go test 特殊对待的第三种函数就是示例函数，它们的函数名以Example为前缀。它们既没有参数也没有返回值。标准格式如下： func ExampleName() { // ... } func ExampleFib() { fmt.Println(Fib(1)) // Output:1 } go test 会将打印的内容与 下面的注释Output对比，相同则通过。 ","date":"2020-07-25","objectID":"/posts/go/gotest/:3:3","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"4. 其他函数 1. TestMain 如果测试文件包含函数:func TestMain(m *testing.M)那么生成的测试会先调用 TestMain(m)，然后再运行具体测试。 2. 子测试 t.Run()开启子测试。 t.Run(tt.name, func(t *testing.T) { if got := Fib(tt.args.n); got != tt.want { t.Errorf(\"Fib() = %v, want %v\", got, tt.want) } }) ","date":"2020-07-25","objectID":"/posts/go/gotest/:3:4","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"4. 例子 一个简单的递归求斐波那契数列方法 func Fib(n int) int { if n \u003c 2 { return n } return Fib(n-1) + Fib(n-2) } ","date":"2020-07-25","objectID":"/posts/go/gotest/:4:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"1. 单元测试 func TestFib(t *testing.T) { type args struct { n int } tests := []struct { name string args args want int }{ {\"0\", args{0}, 0}, {\"1\", args{1}, 1}, {\"2\", args{2}, 1}, {\"3\", args{3}, 2}, {\"4\", args{4}, 3}, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Fib(tt.args.n); got != tt.want { t.Errorf(\"Fib() = %v, want %v\", got, tt.want) } }) } } 以表格的形式组织参数，后续增加新的参数也非常方便。 运行测试 linux 下为 -run=. windows 下需要写成-run=\".\" 才行 # 正确 测试通过 $ go test -run=. PASS ok hello/test/bench 0.002s # 测试失败 出现错误 $ go test -run=. --- FAIL: TestFib (0.00s) --- FAIL: TestFib/4 (0.00s) fib_test.go:23: Fib() = 3, want 33 FAIL exit status 1 FAIL hello/test/bench 0.002s ","date":"2020-07-25","objectID":"/posts/go/gotest/:4:1","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"2. 性能测试 func BenchmarkFib(b *testing.B) { for i := 0; i \u003c b.N; i++ { Fib(10) } } 运行 $ go test -bench=. goos: linux goarch: amd64 pkg: hello/test/bench BenchmarkFib-6 3946929 301 ns/op PASS ok hello/test/bench 1.502s 对比测试，看一下不同的值跑的时间相差多少 func benchmarkFib(b *testing.B, n int) { for i := 0; i \u003c b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 结果如下 $ go test -bench=. goos: linux goarch: amd64 pkg: hello/test/bench BenchmarkFib1-6 707485189 1.71 ns/op BenchmarkFib2-6 218838684 4.84 ns/op BenchmarkFib3-6 149152461 7.95 ns/op BenchmarkFib10-6 4022001 305 ns/op BenchmarkFib20-6 31034 39250 ns/op BenchmarkFib40-6 2 601045684 ns/op PASS ok hello/test/bench 10.483s lixd@17x:~/17x/projects/hello/test/bench$ ","date":"2020-07-25","objectID":"/posts/go/gotest/:4:2","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"3. 示例函数 示例，一方面是文档的效果，是关于某个功能的使用例子；另一方面，可以被当做测试运行。 通常，示例代码会放在单独的示例文件中，命名为 example_test.go。 func ExampleFib() { fmt.Println(Fib(1)) // Output:1 } 运行结果如下 === RUN ExampleFib --- PASS: ExampleFib (0.00s) PASS ","date":"2020-07-25","objectID":"/posts/go/gotest/:4:3","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"4. 字符串拼接 最后来测一下字符串拼接的几种方法 const numbers = 100 func BenchmarkSprintf(b *testing.B) { b.ResetTimer() for idx := 0; idx \u003c b.N; idx++ { var s string for i := 0; i \u003c numbers; i++ { s = fmt.Sprintf(\"%v%v\", s, i) } } b.StopTimer() } func BenchmarkStringBuilder(b *testing.B) { b.ResetTimer() for idx := 0; idx \u003c b.N; idx++ { var builder strings.Builder for i := 0; i \u003c numbers; i++ { builder.WriteString(strconv.Itoa(i)) } _ = builder.String() } b.StopTimer() } func BenchmarkBytesBuf(b *testing.B) { b.ResetTimer() for idx := 0; idx \u003c b.N; idx++ { var buf bytes.Buffer for i := 0; i \u003c numbers; i++ { buf.WriteString(strconv.Itoa(i)) } _ = buf.String() } b.StopTimer() } func BenchmarkStringAdd(b *testing.B) { b.ResetTimer() for idx := 0; idx \u003c b.N; idx++ { var s string for i := 0; i \u003c numbers; i++ { s += strconv.Itoa(i) } } b.StopTimer() } 结果 # n = 2 字符串较少的时候 $ go test -bench=. -benchmem goos: linux goarch: amd64 pkg: hello/test/str BenchmarkSprintf-6 5333647 226 ns/op 32 B/op 3 allocs/op BenchmarkStringBuilder-6 40148308 32.4 ns/op 8 B/op 1 allocs/op BenchmarkBytesBuf-6 20827609 55.7 ns/op 64 B/op 1 allocs/op BenchmarkStringAdd-6 30649676 39.2 ns/op 2 B/op 1 allocs/op PASS ok hello/test/str 5.241s # n = 100 字符串比较多的时候 $ go test -bench=. -benchmem goos: linux goarch: amd64 pkg: hello/test/str BenchmarkSprintf-6 63482 19152 ns/op 12179 B/op 297 allocs/op BenchmarkStringBuilder-6 1299618 887 ns/op 504 B/op 6 allocs/op BenchmarkBytesBuf-6 1000000 1111 ns/op 688 B/op 4 allocs/op BenchmarkStringAdd-6 166837 6042 ns/op 9776 B/op 99 allocs/op PASS ok hello/test/str 5.718s 结论 字符串少的时候，直接相加和 bytes.Buffer、strings.Builder 相差不大，但是 strings.Sprintf 也特别慢 字符串多的时候，bytes.Buffer、strings.Builder 相差不大，另外两个就已经很慢了 bytes.Buffer、strings.Builder 使用缓存，不会频繁分配内存，所以快 strings.Sprintf 和 add 两个会分配大量内存所以就很慢， string 是只读的，所以每次会创建一个新的 string 一般推荐使用 strings.Builder ，如果字符串很少则可以直接相加也差距不大， 不管什么情况都最好不要使用 strings.Sprintf ","date":"2020-07-25","objectID":"/posts/go/gotest/:4:4","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"5. 参考 https://golang.org/pkg/testing/ https://www.calhoun.io/how-to-test-with-go/ https://books.studygolang.com/The-Golang-Standard-Library-by-Example/chapter09/09.1.html https://medium.com/rungo/unit-testing-made-easy-in-go-25077669318 ","date":"2020-07-25","objectID":"/posts/go/gotest/:5:0","tags":["Golang"],"title":"Go语言测试 gotest","uri":"/posts/go/gotest/"},{"categories":["Golang"],"content":"Go语言性能分析利器 pprof 基本使用详解","date":"2020-07-17","objectID":"/posts/go/pprof/","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"本文主要记录了 Go 语言中的性能分析利器 pprof 的基本使用，包括 Profiling 采集、可视化分析和火焰图的生成与查看等。 ","date":"2020-07-17","objectID":"/posts/go/pprof/:0:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"1. 概述 ","date":"2020-07-17","objectID":"/posts/go/pprof/:1:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"1. 是什么 Profiling 是指在程序执行过程中，收集能够反映程序执行状态的数据。 在软件工程中，性能分析（performance analysis，也称为 profiling），是以收集程序运行时信息为手段研究程序行为的分析方法，是一种动态程序分析的方法。 Go 语言自带的 pprof 库就可以分析程序的运行情况，并且提供可视化的功能。它包含两个相关的库： runtime/pprof：对于只跑一次的程序，例如每天只跑一次的离线预处理程序，调用 pprof 包提供的函数，手动开启性能数据采集。 net/http/pprof：对于在线服务，对于一个 HTTP Server，访问 pprof 提供的 HTTP 接口，获得性能数据。当然，实际上这里底层也是调用的 runtime/pprof 提供的函数，封装成接口对外提供网络访问。 ","date":"2020-07-17","objectID":"/posts/go/pprof/:1:1","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"2. 可以做什么 pprof 是 Go 语言中分析程序运行性能的工具，它能提供各种性能数据： 类型 描述 备注 allocs 内存分配情况的采样信息 可以用浏览器打开，但可读性不高 blocks 阻塞操作情况的采样信息 可以用浏览器打开，但可读性不高 cmdline 显示程序启动命令及参数 可以用浏览器打开，这里会显示 ./go-pprof-practice goroutine 当前所有协程的堆栈信息 可以用浏览器打开，但可读性不高 heap 堆上内存使用情况的采样信息 可以用浏览器打开，但可读性不高 mutex 锁争用情况的采样信息 可以用浏览器打开，但可读性不高 profile CPU 占用情况的采样信息 浏览器打开会下载文件 threadcreate 系统线程创建情况的采样信息 可以用浏览器打开，但可读性不高 trace 程序运行跟踪信息 浏览器打开会下载文件，本文不涉及，可另行参阅《深入浅出 Go trace》 由于直接阅读采样信息缺乏直观性，我们需要借助 go tool pprof 命令来排查问题，这个命令是 go 原生自带的，所以不用额外安装。 ","date":"2020-07-17","objectID":"/posts/go/pprof/:1:2","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"3. 怎么用 流程其实很简单，可以分为两个部分 1）首先是采集数据 - 两种方式 工具型应用 HTTP Server 2）然后就是进行分析了 Report generation：报告生成 Interactive terminal use：交互式终端使用 Web interface：Web 界面 ","date":"2020-07-17","objectID":"/posts/go/pprof/:1:3","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"4. 环境准备 1）graphviz 生成 svg 图的时候需要用到该工具 安装方式可以看这里https://graphviz.gitlab.io/download/ ","date":"2020-07-17","objectID":"/posts/go/pprof/:1:4","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"2. 数据采集 工具型应用和服务型应用数据采集稍微有一点点不一样，分开讲。 ","date":"2020-07-17","objectID":"/posts/go/pprof/:2:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"1. 工具型应用 如果你的应用程序是运行一段时间就结束退出类型。 那么最好的办法是在应用退出的时候把 profiling 的报告保存到文件中，进行分析。对于这种情况，可以使用 runtime/pprof 库。 首先在代码中导入runtime/pprof工具： import \"runtime/pprof\" 1. CPU // 程序运行时开启统计 pprof.StartCPUProfile(w io.Writer) // 程序结束时关闭 pprof.StopCPUProfile() 例如 file, _ := os.Create(\"./cpu.pprof\") // 在当前路径下创建一个cpu.pprof文件 pprof.StartCPUProfile(file) // 往文件中记录CPU profile信息 defer func() { // 退出之前 停止采集 pprof.StopCPUProfile() file.Close() }() 2. Heap // 程序退出前记录即可 pprof.WriteHeapProfile(w io.Writer) 例如 file, _ := os.Create(\"./mem.pprof\") pprof.WriteHeapProfile(file) f2.Close() 3. 指定采集指标 也可以指定采集指标 // 可选值 goroutine、threadcreate、heap、allocs、block、mutex pprof.Lookup(\"Name\") 例如 fileG, _ := os.Create(\"./goroutine.pprof\") pprof.Lookup(\"goroutine\").WriteTo(fileG, 1) 4. 优点 这种形式的优点就是灵活。 pprof.StartCPUProfile() pprof.StopCPUProfile() 可以在任意地方进行采集，可以针对单个方法甚至某一行代码，而不是整个应用。 采集内存信息之前甚至可以手动调用 GC ,模拟出各种情况。 runtime.GC() pprof.WriteHeapProfile() ","date":"2020-07-17","objectID":"/posts/go/pprof/:2:1","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"2. 服务型应用 如果你的应用程序是一直运行的，比如 web 应用，那么可以使用 net/http/pprof 库，它能够在提供 HTTP 服务进行分析。 如果使用了默认的 http.DefaultServeMux，只需要在你的web server端代码中按如下方式导入net/http/pprof import _ \"net/http/pprof\" 如果你使用自定义的 Mux，则需要手动注册一些路由规则： r.HandleFunc(\"/debug/pprof/\", pprof.Index) r.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline) r.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) r.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) r.HandleFunc(\"/debug/pprof/trace\", pprof.Trace) 如果你使用的是gin框架，那么推荐使用github.com/DeanThompson/ginpprof 例如 import ( // 省略... _ \"net/http/pprof\" ) func main() { flag.Parse() //远程获取pprof数据 go func() { log.Println(http.ListenAndServe(\"localhost:8080\", nil)) }() // 省略... } 编译运行之后在浏览器访问 http://localhost:8080/debug/pprof/ 这个路径下还有几个子页面： /debug/pprof/profile：访问这个链接会自动进行 CPU profiling，持续 30s，并生成一个文件供下载 /debug/pprof/heap： Memory Profiling 的路径，访问这个链接会得到一个内存 Profiling 结果的文件 /debug/pprof/block：block Profiling 的路径 /debug/pprof/goroutines：运行的 goroutines 列表，以及调用关系 其实就是 pprof 包对外提供了接口，当我们调用这些接口的时候，就会去实时的采集对应 profile 信息。 例如 # syntax go tool pprof source # -seconds 指定采集时间 默认是 30s go tool pprof http://127.0.0.1:8080/debug/pprof/profile?-seconds=10 采集成功后或自动保存到 home 目录下。 Saved profile in /home/lixd/pprof/pprof.server.samples.cpu.002.pb.gz ","date":"2020-07-17","objectID":"/posts/go/pprof/:2:2","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"3. 模拟负载 由于获取的 Profiling 数据是动态的，要想获得有效的数据，需要保证应用处于较大的负载。否则如果应用处于空闲状态，得到的结果可能没有任何意义。 1. 压测 使用压测工具的同时，进行 pprof 以达到最好的效果。 压测工具推荐使用 https://github.com/wg/wrk https://github.com/adjust/go-wrk 压测同时采集数据。 例如 # 压测 go-wrk -n 50000 http://127.0.0.1:8080/hello # 采集 go tool pprof http://127.0.0.1:8080/debug/pprof/profile?-seconds=10 2. 性能测试 go test 命令有两个参数和 pprof 相关，它们分别指定生成的 CPU 和 Memory profiling 保存的文件： -cpuprofile：cpu profiling 数据要保存的文件地址 -memprofile：memory profiling 数据要报文的文件地址 我们还可以选择将pprof与性能测试相结合，比如： 比如下面执行测试的同时，也会执行 CPU profiling，并把结果保存在 cpu.prof 文件中： go test -bench . -cpuprofile=cpu.prof 比如下面执行测试的同时，也会执行 Mem profiling，并把结果保存在 cpu.prof 文件中： go test -bench . -memprofile=./mem.prof ","date":"2020-07-17","objectID":"/posts/go/pprof/:2:3","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"3. 数据分析 通过前面两种方式，获取到数据后即可进行分析。 我们可以使用 go tool pprof 命令行工具。 go tool pprof 最简单的使用方式为: go tool pprof [binary] [source] 其中： binary 是应用的二进制文件，用来解析各种符号； source 表示 profile 数据的来源，可以是本地的文件，也可以是 http 地址。 测试代码如下 func main() { runtime.SetMutexProfileFraction(1) // 开启对锁调用的跟踪 runtime.SetBlockProfileRate(1) // 开启对阻塞操作的跟踪 // record cpu info to file file, err := os.Create(\"./cpu.pprof\") if err != nil { fmt.Printf(\"create cpu pprof failed, err:%v\\n\", err) return } if err := pprof.StartCPUProfile(file); err != nil { fmt.Printf(\"could not start CPU profile :%v\\n\", err) return } defer func() { pprof.StopCPUProfile() file.Close() }() for i := 0; i \u003c 10; i++ { logic() } } // logic logic code with some bug for test func logic() { // normal logic fmt.Println(\"logic\") // bad logic loop for i := 0; i \u003c 1000000000; i++ { } } 1. 通过交互式终端使用 例如：分析前面保存的 cpu.pprof go tool pprof cpu.pprof 会进入一个交互式界面： Type: cpu Time: Jul 18, 2020 at 12:21pm (CST) Duration: 2.74s, Total samples = 2.62s (95.56%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) 输入 help查看命令列表，输入o查看参数列表。 不过比较常用的只有 3 个命令： top 查看资源较高的调用。 list ``list 代码片段`查看问题代码具体位置。 web 在Web Browser上图形化显示当前的资源监控内容 这里就要用到前面安装的 graphviz 我们可以在交互界面输入top(也可以指定个数 比如 top3)来查看程序中占用 CPU 比较多的函数： (pprof) top Showing nodes accounting for 2.59s, 98.85% of 2.62s total Dropped 13 nodes (cum \u003c= 0.01s) flat flat% sum% cum cum% 2.55s 97.33% 97.33% 2.60s 99.24% main.logic 0.04s 1.53% 98.85% 0.04s 1.53% runtime.asyncPreempt 0 0% 98.85% 2.60s 99.24% main.main 0 0% 98.85% 2.60s 99.24% runtime.main 0 0% 98.85% 0.02s 0.76% runtime.mstart 0 0% 98.85% 0.02s 0.76% runtime.mstart1 0 0% 98.85% 0.02s 0.76% runtime.sysmon 参数说明 flat：给定函数上运行耗时 flat%：同上的 CPU 运行耗时总比例 sum%：给定函数累积使用 CPU 总比例 cum：当前函数加上它之上的调用运行总耗时 cum%：同上的 CPU 运行耗时总比例 最后一列为函数名称，在大多数的情况下，我们可以通过这五列得出一个应用程序的运行情况，加以优化 可以看到 大部分 CPU 都消耗在了 main.logic 这个函数。 接下来就可以使用list命令具体分析一个这个函数 (pprof) list main.logic Total: 2.62s ROUTINE ======================== main.logic in /home/lixd/17x/projects/hello/test/pprof/tool/tool.go 2.55s 2.60s (flat, cum) 99.24% of Total . . 50:} . . 51: . . 52:// logic logic code with some bug for test . . 53:func logic() { . . 54: // normal logic . 50ms 55: fmt.Println(\"logic\") . . 56: // bad logic loop 2.55s 2.55s 57: for i := 0; i \u003c 1000000000; i++ { . . 58: . . 59: } . . 60:} . . 61: . . 62:// memory record memory info to file 可以清楚的看到，大部分时间都消耗在 for loop 了。 2. svg 如果觉得在命令行查看不够直观的话，也可以直接输入web命令，生成 svg 图像进行查看。 web命令的实际行为是产生一个 .svg文件，并调用系统里设置的默认打开 .svg 的程序打开它。如果系统里打开 .svg 的默认程序并不是浏览器（比如代码编辑器），需要设置一下默认使用浏览器打开 .svg 文件。 大概是这个样子的。 关于图形的说明： 每个框代表一个函数，理论上框的越大表示占用的CPU资源越多。 方框之间的线条代表函数之间的调用关系，线条上的数字表示函数调用的次数。 方框中的第一行数字表示当前函数占用CPU的百分比，第二行数字表示当前函数累计占用CPU的百分比。 这样的话遇到比较复杂的调用关系，还是比较麻烦，很难看出其中的关系，所以可以使用下面的 火焰图。 3. 火焰图 也可以使用 go 自带的工具生成火焰图。 获取cpuprofile go tool pprof http://127.0.0.1:8080/debug/pprof/profile?-seconds=10 时间到后会生成一个类似pprof.samples.cpu.003.pb.gz的文件 生成火焰图 go tool pprof -http=:8081 ~/pprof/pprof.samples.cpu.001.pb.gz 在浏览器中即可查看到相关信息了 view 中可以选择查询各种内容：具体如下 说明： 宽度越大表示占用的 CPU 时间越多。 然后图中的各种条是可以点的，这样看起来更加方便 感觉下来还是这种方式比较方便 ","date":"2020-07-17","objectID":"/posts/go/pprof/:3:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"4. 小结 pprof 使用一共两个步骤： 1）采集数据 工具型应用 服务型应用 2）分析数据 交互式终端 svg 火焰图 最后附上一个有意思的故事: 事情的起因是这样的，有人发表了一篇文章，用各种语言实现了一个算法，结果用 go 写的程序非常慢，而 C++ 则最快。然后 Russ Cox 就鸣不平了，哪受得了这个气？马上启用 pprof 大杀器进行优化。最后，程序不仅更快，而且使用的内存更少了！ 链接 https://blog.golang.org/pprof ","date":"2020-07-17","objectID":"/posts/go/pprof/:4:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Golang"],"content":"5. 参考 https://golang.org/pkg/net/http/pprof/ https://segmentfault.com/a/1190000016412013 https://blog.wolfogre.com/posts/go-ppof-practice/ https://cizixs.com/2017/09/11/profiling-golang-program/ https://xargin.com/pprof-and-flamegraph/ ","date":"2020-07-17","objectID":"/posts/go/pprof/:5:0","tags":["Golang"],"title":"Go语言之pprof性能分析利器","uri":"/posts/go/pprof/"},{"categories":["Git"],"content":"Git 仓库瘦身方案及 Git 仓库膨胀原理","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"本文主要记录了 如何给 Git 仓库瘦身和 Git 仓库膨胀的原理。 Git 仓库瘦身 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:0:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"1. 问题 曾经年少轻狂 啥都往 Git 仓库里放。 现在看着那高达1个G 的 仓库 后悔莫及呀。 最近发现一个仓库居然有 300M 这么大。 分析了下发现.git文件夹就有 290M。 每次 pull 的时候那叫一个慢呀，所以网上查了下如何给 git 仓库瘦身。 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:1:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"2. 解决方案 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:2:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"1. bfg 工具 通过bfg工具，永久删除 git 里的大文件 官网 https://rtyley.github.io/bfg-repo-cleaner 需要配一下 Java 运行环境 第一步 通过 mirror 方式 clone 仓库 git clone --mirror git://example.com/some-big-repo.git 第二步 清理大文件 # 100M 表示清除大于 100M 的文件 可调 java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.git 如果知道文件名 也可以清除指定文件 比如清除 password.txt bfg --replace-text password.txt my-repo.git 第三步 清除缓存数据 cd some-big-repo.git git reflog expire --expire=now --all \u0026\u0026 git gc --prune=now --aggressive 第四步 推送到远端 git push 如果仓库里有 commit 来自 pull request 的话 push 这里会报如下错误 ! [remote rejected] refs/pull/1/head -\u003e refs/pull/1/head (deny updating a hidden ref) 暂时没找到解决方案，只能使用方案二或方案三了。 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:2:1","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"2. clear history 如果前面方案一 最后一步出错的话可以试一下方案二或方案三。 注意：以下操作会直接清除掉所有提交历史 首先移除 .git文件夹 rm -rf .git 接着重新 init 并提交 git init git add . git commit -m \"Initial commit\" 最后设置 remote 并 push git remote add origin git@github.com:\u003cYOUR ACCOUNT\u003e/\u003cYOUR REPOS\u003e.git git push -u --force origin master 具体看这里 https://gist.github.com/stephenhardy/5470814 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:2:2","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"3. 手动移除 如果前两种方案就不满意的话，这里还有第三种方案。 手动删除大文件 首先先找出git中最大的文件 git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -g | tail -5 结果大概是这样的 $ git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -g | tail -5 494580d30b106e7f5bf27bf7deed5af33fc80e00 blob 42314931 18206133 368694903 91bc73a5400c80bf6bd6de6544b420d2f55d2a99 blob 43997239 18921853 290304930 2c73a462bb20affe462c36eaf65cc2d2188a82f6 blob 55561789 21433258 182854804 aa5923eb2531825e4fa38c50385389bef31f1cc0 blob 60930637 28939719 2565863 b5f530aca5c7e76563ddf6458199303220e67ee3 blob 80517316 25578808 75067267 第一列为文件 id。 第二列为文件类型，第三列则是文件大小(单位字节)。 可以看到 最大的一个文件是 80M。 接着根据 id 查询文件名 git rev-list --objects --all | grep id 例如 $ git rev-list --objects --all | grep b5f530aca5c7e76563ddf6458199303220e67ee3 b5f530aca5c7e76563ddf6458199303220e67ee3 server/admin/admin 可以看到 这个最大的 80M 的文件居然是编译后的二进制文件。 删除文件 查看名字后 如果觉得没问题就可以开始删除文件了。 git filter-branch --force --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch filename' --tag-name-filter cat -- --all 比如要删除上面那个二进制文件 git filter-branch --force --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch server/admin/admin' --tag-name-filter cat -- --all 最后 push git push --force --all ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:2:3","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"3. 分析 为什么.git/objects/pack文件夹会变得非常大？ 首先看一下 Git 目录结构 ├── HEAD ├── branches ├── index ├── config ├── description ├── hooks/ ├── logs/ │ ├── HEAD │ └── refs │ └── heads │ └── master ├── objects/ │ ├── 88 │ │ └── 23efd7fa394844ef4af3c649823fa4aedefec5 │ ├── 91 │ │ └── 0fc16f5cc5a91e6712c33aed4aad2cfffccb73 │ ├── 9f │ │ └── 4d96d5b00d98959ea9960f069585ce42b1349a │ ├── info │ └── pack └── refs/ ├── heads │ └── master └── tags 具体如下 hooks/ 钩子 存放一些 shell 脚本 info/ 仓库信息 logs/ 保存所有更新的引用记录 objects/ 存放所有的 Git 对象 refs/ 目录下有 heads 和 tags 两个目录，heads 存放最新一次提交的哈希值 COMMIT_EDITMSG 最新提交的一次提交注释（git commit -m “……”。即commit提交时引号里的注释），git系统不会用到，给用户一个参考 description 仓库的描述信息，主要给gitweb等git托管系统使用 config Git 仓库配置文件 index 暂存区 HEAD记录了一个路径，映射到ref引用，能够找到下一次commit的前一次哈希值 每次 git add都会生成一个blob 对象，存放在objects 目录下。 这个blob 对象里保存的是什么呢? Git在 add 文件时，会把文件完整的保存成一个新的 blob 对象。通过 git gc 打包或者每次git push的时候 Git 都会自动执行一次打包过程，将 Blob 对象合并成一个包文并生成一个索引文件。 索引文件中包含了每个 Blob 对象在包文件中的偏移信息，Git 在打包的过程中使用了增量编码方案，只保存 Blob 对象的不同版本之间的差异，这使得仓库会瘦身。 既然Git会对Blob对象进行合并优化，那么objects文件夹为什么还会那么大呢? 因为当 Blob 对象在合并时不能对 .a 进行差异化比较，所以每次在添加 .a 文件时，都会保存一份 .a文件，用于后续代码还原时使用。 所以当频繁更换 .a 文件时，objects下的 pack 文件会越来越大。 虽然这个 .a 文件后续可能用不到删除了，但是pack中的这个 .a 文件的缓存还是会一直存在。 这就是为什么 .git 文件夹有时候会变得超级大。 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:3:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"4. 小结 本文记录了 Git 仓库变大的现象及其原因，同时也提供了 3 种解决方案。 最后再次提醒 千万不要在 git 仓库中存放会经常修改的那种 .a文件 千万不要在 git 仓库中存放会经常修改的那种 .a文件 千万不要在 git 仓库中存放会经常修改的那种 .a文件 重要的话说三遍。 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:4:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Git"],"content":"5. 参考 https://rtyley.github.io/bfg-repo-cleaner https://gist.github.com/stephenhardy/5470814 https://www.jianshu.com/p/4f2ccb48da77 https://blog.csdn.net/weixin_43897044/article/details/87260235 ","date":"2020-06-25","objectID":"/posts/git/04-git-reduce/:5:0","tags":["Git"],"title":"Git教程(四)---Git 仓库瘦身","uri":"/posts/git/04-git-reduce/"},{"categories":["Tracing"],"content":"Jaeger 在 gin框架和 gRPC 中的使用","date":"2020-05-04","objectID":"/posts/tracing/04-jaeger-gin-grpc/","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(四)---Jaeger 在 gin框架和 gRPC 中的使用","uri":"/posts/tracing/04-jaeger-gin-grpc/"},{"categories":["Tracing"],"content":"本文通过简单的例子记录了如何在 gin 框架和 gRPC 中使用 Jaeger 进行链路追踪。 ","date":"2020-05-04","objectID":"/posts/tracing/04-jaeger-gin-grpc/:0:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(四)---Jaeger 在 gin框架和 gRPC 中的使用","uri":"/posts/tracing/04-jaeger-gin-grpc/"},{"categories":["Tracing"],"content":"1. Gin 通过 Middleware 可以追踪到最外层的 Handler，更深层方法需要追踪的话可以通过 ctx 将 span 传递到各个方法中去进一步追踪。 http 请求使用 request.Header 做载体。 package middleware import ( \"context\" \"github.com/gin-gonic/gin\" \"github.com/opentracing/opentracing-go\" \"github.com/opentracing/opentracing-go/ext\" \"i-go/apm/trace/config\" ) // Jaeger 通过 middleware 将 tracer 和 ctx 注入到 gin.Context 中 func Jaeger() gin.HandlerFunc { return func(c *gin.Context) { var parentSpan opentracing.Span tracer, closer := config.NewTracer(\"gin-demo\") defer closer.Close() // 直接从 c.Request.Header 中提取 span,如果没有就新建一个 spCtx, err := opentracing.GlobalTracer().Extract(opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(c.Request.Header)) if err != nil { parentSpan = tracer.StartSpan(c.Request.URL.Path) defer parentSpan.Finish() } else { parentSpan = opentracing.StartSpan( c.Request.URL.Path, opentracing.ChildOf(spCtx), opentracing.Tag{Key: string(ext.Component), Value: \"HTTP\"}, ext.SpanKindRPCServer, ) defer parentSpan.Finish() } // 然后存到 g.ctx 中 供后续使用 c.Set(\"tracer\", tracer) c.Set(\"ctx\", opentracing.ContextWithSpan(context.Background(), parentSpan)) c.Next() } } 然后在 gin 中添加这个 middleware 即可。 e := gin.New() e.Use(middleware.Jaeger()) 需要更细粒度的追踪，只需要将 span 传递到各个方法即可 func Register(e *gin.Engine) { e.GET(\"/ping\", Ping) } func Ping(c *gin.Context) { psc, _ := c.Get(\"ctx\") ctx := psc.(context.Context) doPing1(ctx) doPing2(ctx) c.JSON(200, gin.H{\"message\": \"pong\"}) } func doPing1(ctx context.Context) { span, _ := opentracing.StartSpanFromContext(ctx, \"doPing1\") defer span.Finish() time.Sleep(time.Second) fmt.Println(\"pong\") } func doPing2(ctx context.Context) { span, _ := opentracing.StartSpanFromContext(ctx, \"doPing2\") defer span.Finish() time.Sleep(time.Second) fmt.Println(\"pong\") } ","date":"2020-05-04","objectID":"/posts/tracing/04-jaeger-gin-grpc/:1:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(四)---Jaeger 在 gin框架和 gRPC 中的使用","uri":"/posts/tracing/04-jaeger-gin-grpc/"},{"categories":["Tracing"],"content":"2. gRPC 追踪 gRPC 则通过拦截器实现。 这里使用使用 gRPC 的metadata 来做载体。 // ClientInterceptor grpc client func ClientInterceptor(tracer opentracing.Tracer) grpc.UnaryClientInterceptor { return func(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { span, _ := opentracing.StartSpanFromContext(ctx, \"call gRPC\", opentracing.Tag{Key: string(ext.Component), Value: \"gRPC\"}, ext.SpanKindRPCClient) defer span.Finish() md, ok := metadata.FromOutgoingContext(ctx) if !ok { md = metadata.New(nil) } else { md = md.Copy() } err := tracer.Inject(span.Context(), opentracing.TextMap, MDReaderWriter{md}) if err != nil { span.LogFields(log.String(\"inject-error\", err.Error())) } newCtx := metadata.NewOutgoingContext(ctx, md) err = invoker(newCtx, method, req, reply, cc, opts...) if err != nil { span.LogFields(log.String(\"call-error\", err.Error())) } return err } } func ServerInterceptor(tracer opentracing.Tracer) grpc.UnaryServerInterceptor { return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) ( resp interface{}, err error) { md, ok := metadata.FromIncomingContext(ctx) if !ok { md = metadata.New(nil) } // 服务端拦截器则是在MD中把 span提取出来 spanContext, err := tracer.Extract(opentracing.TextMap, MDReaderWriter{md}) if err != nil \u0026\u0026 err != opentracing.ErrSpanContextNotFound { fmt.Print(\"extract from metadata error: \", err) } else { span := tracer.StartSpan( info.FullMethod, ext.RPCServerOption(spanContext), opentracing.Tag{Key: string(ext.Component), Value: \"gRPC\"}, ext.SpanKindRPCServer, ) defer span.Finish() ctx = opentracing.ContextWithSpan(ctx, span) } return handler(ctx, req) } } MDReaderWriter 结构如下 为了做载体，必须要实现 opentracing.TextMapWriter opentracing.TextMapReader 这两个接口。 // TextMapWriter is the Inject() carrier for the TextMap builtin format.With // it, the caller can encode a SpanContext for propagation as entries in a map // of unicode strings. type TextMapWriter interface { Set(key, val string) } // TextMapReader is the Extract() carrier for the TextMap builtin format. With it, // the caller can decode a propagated SpanContext as entries in a map of // unicode strings. type TextMapReader interface { ForeachKey(handler func(key, val string) error) error } // metadata 读写 type MDReaderWriter struct { metadata.MD } // 为了 opentracing.TextMapReader ，参考 opentracing 代码 func (c MDReaderWriter) ForeachKey(handler func(key, val string) error) error { for k, vs := range c.MD { for _, v := range vs { if err := handler(k, v); err != nil { return err } } } return nil } // 为了 opentracing.TextMapWriter，参考 opentracing 代码 func (c MDReaderWriter) Set(key, val string) { key = strings.ToLower(key) c.MD[key] = append(c.MD[key], val) } 然后建立连接或者启动服务的时候把拦截器添加上即可 建立连接 func main() { // tracer tracer, closer := config.NewTracer(\"gRPC-hello\") defer closer.Close() ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() // conn conn, err := grpc.DialContext( ctx, \"localhost:50051\", grpc.WithInsecure(), grpc.WithBlock(), grpc.WithUnaryInterceptor( grpcMiddleware.ChainUnaryClient( interceptor.ClientInterceptor(tracer), ), ), ) if err != nil { fmt.Println(\"grpc conn err:\", err) return } client := proto.NewHelloClient(conn) r, err := client.SayHello(context.Background(), \u0026proto.HelloReq{Name: \"xiaoming\"}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } log.Printf(\"Greeting: %s\", r.Message) } 启动服务 func main() { lis, err := net.Listen(\"tcp\", \"50051\") if err != nil { log.Fatalf(\"failed to listen: %v\", err) } tracer, closer := config.NewTracer(\"gRPC-hello\") defer closer.Close() // UnaryInterceptor s := grpc.NewServer(grpc.UnaryInterceptor( grpc_middleware.ChainUnaryServer( interceptor.ServerInterceptor(tracer), ), )) proto.RegisterHelloServer(s, \u0026helloServer{}) if err := s.Serve(lis); err != nil { panic(err) } } ","date":"2020-05-04","objectID":"/posts/tracing/04-jaeger-gin-grpc/:2:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(四)---Jaeger 在 gin框架和 gRPC 中的使用","uri":"/posts/tracing/04-jaeger-gin-grpc/"},{"categories":["Tracing"],"content":"3. 完整代码 完整代码见Github ","date":"2020-05-04","objectID":"/posts/tracing/04-jaeger-gin-grpc/:3:0","tags":["Tracing","Jaeger"],"title":"分布式链路追踪教程(四)---Jaeger 在 gin框架和 gRPC 中的使用","uri":"/posts/tracing/04-jaeger-gin-grpc/"},{"categories":["Tracing"],"content":"Jaeger 部署与简单使用","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"本文主要记录了分布式链路追踪框架 Jaeger 测试环境部署及其简单使用。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:0:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"1. 测试环境部署 Jaeger 官方提供了 all-in-one 的 docker 镜像，可以基于此进行一键部署。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:1:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"Docker docker 命令如下： $ docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:1.20 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:1:1","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"Docker Compose 也可以使用 docker compose 来启动 version: '3.1' services: db: image: jaegertracing/all-in-one restart: always environment: COLLECTOR_ZIPKIN_HTTP_PORT: 9411 ports: - 5775:5775/udp - 6831:6831/udp - 6832:6832/udp - 5778:5778 - 16686:16686 - 14268:14268 - 9411:9411 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:1:2","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"binary 甚至还可以通过下载二进制文件直接运行。 下载地址 https://www.jaegertracing.io/download/#binaries 启动参数 $ jaeger-all-in-one --collector.zipkin.http-port=9411 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:1:3","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"UI 界面 启动之后就可以在http://localhost:16686看到 Jaeger 的 UI 界面了。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:1:4","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"2. Hello World ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:2:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"1. 说明 大致步骤如下： 1）初始化一个 tracer 2）记录一个简单的 span 3）在span上添加注释信息 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:2:1","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"2. 例子 func main() { // 解析命令行参数 if len(os.Args) != 2 { panic(\"ERROR: Expecting one argument\") } // 1.初始化 tracer tracer, closer := config.NewTracer(\"hello\") defer closer.Close() // 2.开始新的 Span （注意:必须要调用 Finish()方法span才会上传到后端） span := tracer.StartSpan(\"say-hello\") defer span.Finish() helloTo := os.Args[1] helloStr := fmt.Sprintf(\"Hello, %s!\", helloTo) // 3.通过tag、log记录注释信息 // LogFields 和 LogKV底层是调用的同一个方法 span.SetTag(\"hello-to\", helloTo) span.LogFields( log.String(\"event\", \"string-format\"), log.String(\"value\", helloStr), ) span.LogKV(\"event\", \"println\") println(helloStr) } func NewTracer(service string) (opentracing.Tracer, io.Closer) { // 参数详解 https://www.jaegertracing.io/docs/1.20/sampling/ cfg := jaegerConfig.Configuration{ ServiceName: service, // 采样配置 Sampler: \u0026jaegerConfig.SamplerConfig{ Type: jaeger.SamplerTypeConst, Param: 1, }, Reporter: \u0026jaegerConfig.ReporterConfig{ LogSpans: true, CollectorEndpoint: http://localhost:14268/api/traces, // 将span发往jaeger-collector的服务地址 }, } tracer, closer, err := cfg.NewTracer(jaegerConfig.Logger(jaeger.StdLogger)) if err != nil { panic(fmt.Sprintf(\"ERROR: cannot init Jaeger: %v\\n\", err)) } opentracing.SetGlobalTracer(tracer) return tracer, closer } 运行上述例子后就可以在 Jaeger UI 界面看到对应的链路信息了。 go run hello.go xiaoming ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:2:2","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"3. 使用 ctx 包装 tracer ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:3:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"1. 说明 1）通过opentracing.ChildOf(rootSpan.Context())保留span之间的因果关系。 2）通过ctx来实现在各个功能函数知之间传递span。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:3:1","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"2. span 因果关系 span 是链路追踪里的最小组成单元，为了保留各个功能之间的因果关系，必须在各个方法之间传递 span 并且新建span时指定opentracing.ChildOf(rootSpan.Context()),否则新建的span会是独立的，无法构成一个完整的 trace。 比如方法A调用了B、C、D，那么就需要将方法A中的span传递到方法BCD中。 childSpan := rootSpan.Tracer().StartSpan( \"formatString\", opentracing.ChildOf(rootSpan.Context()), ) 通过opentracing.ChildOf(rootSpan.Context())建立两个span之间的引用关系，如果不指定则会创建一个新的span（UI中查看的时候就是一个新的 trace）。 将前面的例子稍微修改一下，将formatString和printHello提成单独的方法，并新增span参数。 func main() { // 解析命令行参数 if len(os.Args) != 2 { panic(\"ERROR: Expecting one argument\") } // 1.初始化 tracer tracer, closer := config.NewTracer(\"hello\") defer closer.Close() // 2.开始新的 Span （注意:必须要调用 Finish()方法span才会上传到后端） span := tracer.StartSpan(\"say-hello\") defer span.Finish() helloTo := os.Args[1] helloStr := formatString(span, helloTo) printHello(span, helloStr) } func formatString(span opentracing.Span, helloTo string) string { childSpan := span.Tracer().StartSpan( \"formatString\", opentracing.ChildOf(span.Context()), ) defer childSpan.Finish() return fmt.Sprintf(\"Hello, %s!\", helloTo) } func printHello(span opentracing.Span, helloStr string) { childSpan := span.Tracer().StartSpan( \"printHello\", opentracing.ChildOf(span.Context()), ) defer childSpan.Finish() println(helloStr) } 运行之后可以清楚的在 UI 界面中看到say-hello由formatString和printHello两个功能组成。 go run hello.go xiaoming ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:3:2","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"3. 通过 ctx 传递 span 前面虽然保留的 span 的因果关系，但是需要在各个方法中传递 span。这可能会污染整个程序，我们可以借助 Go 语言中的 context.Context对象来进行传递。 实例代码如下： ctx := context.Background() ctx = opentracing.ContextWithSpan(ctx, span) helloStr := formatString(ctx, helloTo) printHello(ctx, helloStr) func formatString(ctx context.Context, helloTo string) string { span, _ := opentracing.StartSpanFromContext(ctx, \"formatString\") defer span.Finish() ... func printHello(ctx context.Context, helloStr string) { span, _ := opentracing.StartSpanFromContext(ctx, \"printHello\") defer span.Finish() ... opentracing.StartSpanFromContext()返回的第二个参数是子ctx,如果需要的话可以将该子ctx继续往下传递，而不是传递父ctx。 需要注意的是opentracing.StartSpanFromContext()默认使用GlobalTracer来开始一个新的 span，所以使用之前需要设置 GlobalTracer。 opentracing.SetGlobalTracer(tracer) ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:3:3","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"4. 追踪 rpc ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:4:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"1. 说明 通过Inject(spanContext, format, carrier) and Extract(format, carrier)来实现在RPC调用中传递上下文。 format 则为编码方式，由OpenTracing API定义，具体如下： 1）TextMap–span上下文被编码为字符串键-值对的集合 2）Binary–span上下文被编码为字节数组 3）HTTPHeaders–span上下文被作为 HTTPHeader carrier 则是底层实现的抽象：比如 TextMap 的实现则是一个包含 Set(key, value) 函数的接口。Binary 则是 io.Writer接口。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:4:1","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"2. 例子 一个追踪 http 请求的demo。 Inject 客户端通过 Inject方法将span注入到req.Header中去，随着请求发送到服务端。 // \"github.com/opentracing/opentracing-go/ext\" ext.SpanKindRPCClient.Set(span) ext.HTTPUrl.Set(span, url) ext.HTTPMethod.Set(span, \"GET\") span.Tracer().Inject( span.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header), ) func formatString(ctx context.Context, helloTo string) string { span, _ := opentracing.StartSpanFromContext(ctx, \"formatString\") defer span.Finish() client := http.Client{} v := url.Values{} v.Set(\"helloTo\", helloTo) url := \"http://localhost:8081/format?\" + v.Encode() req, err := http.NewRequest(\"GET\", url, nil) if err != nil { panic(err.Error()) } ext.SpanKindRPCClient.Set(span) ext.HTTPUrl.Set(span, url) ext.HTTPMethod.Set(span, \"GET\") span.Tracer().Inject( span.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header), ) resp, err := client.Do(req) if err != nil { ext.LogError(span, err) panic(err.Error()) } all, err := ioutil.ReadAll(resp.Body) if err != nil { ext.LogError(span, err) panic(err.Error()) } defer resp.Body.Close() helloStr := string(all) span.LogFields( log.String(\"event\", \"string-format\"), log.String(\"value\", helloStr), ) return helloStr } Extract 服务端则通过Extract方法，解析请求头中的 span信息。 spanCtx, _ := tracer.Extract(opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(r.Header)) span := tracer.StartSpan(\"format\", ext.RPCServerOption(spanCtx)) defer span.Finish() func main() { tracer, closer := config.NewTracer(\"formatter\") defer closer.Close() http.HandleFunc(\"/format\", func(w http.ResponseWriter, r *http.Request) { spanCtx, _ := tracer.Extract(opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(r.Header)) span := tracer.StartSpan(\"format\", ext.RPCServerOption(spanCtx)) defer span.Finish() helloTo := r.FormValue(\"helloTo\") helloStr := fmt.Sprintf(\"Hello, %s!\", helloTo) span.LogFields( otlog.String(\"event\", \"string-format\"), otlog.String(\"value\", helloStr), ) w.Write([]byte(helloStr)) }) log.Fatal(http.ListenAndServe(\":8081\", nil)) } ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:4:2","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"5. Baggage ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:5:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"1. 说明 我们可以在 span 中存储参数，然后该参数会跟着 span 传递到整个 trace。 这样的好处在于我们只需要修改一个地方就可以在整个trace中获取到该参数，而不用修改trace中的每一个地方。 但是也不要方太多数据进去，否则后续每次请求都会增加额外的开销。 ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:5:1","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"2. 例子 客户端存入参数 // after starting the span span.SetBaggageItem(\"greeting\", greeting) 服务端获取 greeting := span.BaggageItem(\"greeting\") ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:5:2","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"6. 参考 https://www.jaegertracing.io/docs/1.20/getting-started/ ","date":"2020-05-03","objectID":"/posts/tracing/03-jaeger-quick-start/:6:0","tags":["Jaeger"],"title":"分布式链路追踪教程(三)---Jaeger简单使用","uri":"/posts/tracing/03-jaeger-quick-start/"},{"categories":["Tracing"],"content":"分布式链路追踪框架简单对比","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"本文主要对分布式链路追踪框架做了简单对比，包括 Zipkin、Jaeger等。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:0:0","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"1. 概述 分布式链路追踪系统其中最著名的是 Google Dapper 论文所介绍的 Dapper。源于 Google 为了解决可能由不同团队，不同语言，不同模块，部署在不同服务器，不同数据中心的所带来的软件复杂性（很难去分析，无法做定位），构建了一个的分布式跟踪系统。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:1:0","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"2. 对比 分布式链路追踪有大量相关产品，具体如下： Twitter：Zipkin。 Uber：Jaeger。 Elastic Stack：Elastic APM。 Apache：SkyWalking（国内开源爱好者吴晟开源）。 Naver：Pinpoint（韩国公司开发）。 阿里：鹰眼。 大众点评：Cat。 京东：Hydra。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:2:0","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"1. Jaeger 由Uber开源，Jaeger 目前由 Cloud Native Computing Foundation（CNCF）托管，是 CNCF 的第七个顶级项目（于 2019 年 10 月毕业）。 架构图如下 Jaeger Client：Jaeger 客户端，是 Jaeger 针对 OpenTracing API 的特定语言实现，可用于手动或通过与 OpenTracing 集成的各种现有开源框架（例如Flask，Dropwizard，gRPC等）来检测应用程序以进行分布式跟踪。 Jaeger Agent：Jaeger 客户端代理，在 UDP 端口上监听所接受的跨度并将其分批发送给 Collector。 Jaeger Collector：Jaeger 收集器，顾名思义是面向 Agent，用于收集/管理链路的追踪信息。 Jaeger Query：数据查询与前端界面展示。 Jaeger Ingester：可从 Kafka 读取数据并写入其他的存储介质（Cassandra，Elasticsearch） ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:2:1","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"2. Zipkin Zipkin由Twitter开源于2012年。 架构图如下 Zipkin Collector：Zipkin 收集器，用于收集/管理链路的追踪信息。 Storage：Zipkin 数据存储，支持 Cassandra、ElasticSearch 和 MySQL 等第三方存储。 Zipkin Query Service：数据存储并建立索引后，用于查找和检索跟踪信息。 Web UI：数据查询与前端界面展示。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:2:2","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"3. 对比 可以看到 Jaeger 和 Zipkin 架构上是非常接近的。相关对比的话可以查看下面的几篇文章。 https://epsagon.com/observability/zipkin-or-jaeger-the-best-open-source-tools-for-distributed-tracing/ https://logz.io/blog/zipkin-vs-jaeger/ https://thenewstack.io/jaeger-vs-zipkin-battle-of-the-open-source-tracing-tools/ https://sematext.com/blog/jaeger-vs-zipkin-opentracing-distributed-tracers/ Zipkin 相对成熟,开源于2012年，同时也比较简单，Java 系大部分都会选择 Zipkin。 Jaeger 则是 CNCF 旗下，对 K8s 有较好的兼容性，Go 语言系可能是个不错的选择。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:2:3","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"3. 小结 大多数在初始选型时都会选择亲和性比较强的追踪系统，就像是 Jaeger 属于 Go，Zipkin、Skywalking 是 Java 系居多，三者都完全兼容 OpenTracing，只是架构上多少有些不同，且都是基于 Google Dapper 发散，因此所支持的基本功能和查询页面优雅与否很重要。 另外近两年基于 ServiceMesh 的 ”无” 侵入式链路追踪也广受欢迎，似乎是一个被看好的方向，其代表作之一 Istio 便是使用 CNCF 出身的 Jaeger，且 Jaeger 还兼容 Zipkin，在这点上 Jaeger 完胜。 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:3:0","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"4. 参考 https://jishuin.proginn.com/p/763bfbd2cb0f https://my.oschina.net/u/3770892/blog/3005395 https://juejin.im/post/6844903560732213261 ","date":"2020-05-02","objectID":"/posts/tracing/02-framework-compare/:4:0","tags":["Tracing"],"title":"分布式链路追踪教程(二)---框架选型","uri":"/posts/tracing/02-framework-compare/"},{"categories":["Tracing"],"content":"分布式链路追踪规范 Opentracing 基本概念","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"在微服务架构的系统中，请求在各服务之间流转，调用链错综复杂，一旦出现了问题和异常，很难追查定位，这个时候就需要链路追踪来帮忙了。链路追踪系统能追踪并记录请求在系统中的调用顺序，调用时间等一系列关键信息，从而帮助我们定位异常服务和发现性能瓶颈。 ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:0:0","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"1. 概述 Opentracing 是分布式链路追踪的一种规范标准，是 CNCF（云原生计算基金会）下的项目之一。和一般的规范标准不同，Opentracing 不是传输协议，消息格式层面上的规范标准，而是一种语言层面上的 API 标准。以 Go 语言为例，只要某链路追踪系统实现了 Opentracing 规定的接口（interface），符合Opentracing 定义的表现行为，那么就可以说该应用符合 Opentracing 标准。这意味着开发者只需修改少量的配置代码，就可以在符合 Opentracing 标准的链路追踪系统之间自由切换。 opentracing-go ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:1:0","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"2. Data Model 在使用 Opentracing 来实现全链路追踪前，有必要先了解一下它所定义的数据模型。 ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:2:0","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"Span Span 是一条追踪链路中的基本组成要素，一个 Span 表示一个独立的工作单元，比如可以表示一次函数调用，一次 HTTP 请求等等。Span 会记录如下基本要素: 服务名称(operation name) 服务的开始时间和结束时间 K/V形式的Tags K/V形式的Logs SpanContext References：该span对一个或多个span的引用（通过引用SpanContext） Tags Tags以K/V键值对的形式保存用户自定义标签，主要用于链路追踪结果的查询过滤。例如： http.method=\"GET\",http.status_code=200。其中key值必须为字符串，value必须是字符串，布尔型或者数值型。Span 中的 tag 仅自己可见，不会随着 SpanContext 传递给后续 Span。 例如： span.SetTag(\"http.method\",\"GET\") span.SetTag(\"http.status_code\",200) Logs Logs 与 tags 类似，也是 K/V 键值对形式。与 tags 不同的是，logs 还会记录写入 logs 的时间，因此 logs 主要用于记录某些事件发生的时间。logs 的 key 值同样必须为字符串，但对 value 类型则没有限制。例如： span.LogFields( log.String(\"event\", \"soft error\"), log.String(\"type\", \"cache timeout\"), log.Int(\"waited.millis\", 1500), ) Opentracing列举了一些惯用的Tags和Logs： semantic_conventions SpanContext SpanContext携带着一些用于跨服务通信的（跨进程）数据，主要包含： 足够在系统中标识该span的信息，比如：span_id,trace_id。 Baggage Items，为整条追踪连保存跨服务（跨进程）的K/V格式的用户自定义数据。 Baggage Items Baggage Items 与 tags 类似，也是 K/V键值对。与 tags 不同的是： 其 key 跟 value 都只能是字符串格式 Baggage items 不仅当前 span 可见，其会随着 SpanContext 传递给后续所有的子 span。要小心谨慎的使用baggage items——因为在所有的span中传递这些K,V会带来不小的网络和CPU开销。 ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:2:1","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"References Opentracing 定义了两种引用关系:ChildOf和FollowFrom。 1）ChildOf: 父span的执行依赖子span的执行结果时，此时子span对父span的引用关系是ChildOf。比如对于一次RPC调用，服务端的span（子span）与客户端调用的span（父span）是ChildOf关系。 2）FollowFrom：父span的执不依赖子span执行结果时，此时子span对父span的引用关系是FollowFrom。FollowFrom常用于异步调用的表示，例如消息队列中consumerspan与producerspan之间的关系。 ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:2:2","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"Trace Trace表示一次完整的追踪链路，trace由一个或多个span组成。下图示例表示了一个由8个span组成的trace: [Span A] ←←←(the root span) | +------+------+ | | [Span B] [Span C] ←←←(Span C is a `ChildOf` Span A) | | [Span D] +---+-------+ | | [Span E] [Span F] \u003e\u003e\u003e [Span G] \u003e\u003e\u003e [Span H] ↑ ↑ ↑ (Span G `FollowsFrom` Span F) 时间轴的展现方式会更容易理解： ––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–\u003e time [Span A···················································] [Span B··············································] [Span D··········································] [Span C········································] [Span E·······] [Span F··] [Span G··] [Span H··] 示例来源：the-opentracing-data-model ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:2:3","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"Inject/Extract 为了实现分布式系统中的链路追踪，Opentracing 提供了 Inject/Extract 用于在请求中注入 SpanContext 或者从请求中提取出 SpanContext。 客户端通过 Inject 将 SpanContext 注入到载体中，随着请求一起发送到服务端。 服务端则通过 Extract 将 SpanContext 提取出来,进行后续处理。 ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:2:4","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Tracing"],"content":"3. 参考 https://opentracing.io/docs/overview/ https://juejin.im/post/6844903942309019661 https://github.com/opentracing/specification/blob/master/specification.md https://github.com/opentracing/specification/blob/master/semantic_conventions.md https://github.com/yurishkuro/opentracing-tutorial/tree/master/go ","date":"2020-05-01","objectID":"/posts/tracing/01-opentracing/:3:0","tags":["Tracing"],"title":"分布式链路追踪教程(一)---Opentracing 基本概念","uri":"/posts/tracing/01-opentracing/"},{"categories":["Docker"],"content":"docker和docker-compose 快速安装","date":"2020-04-20","objectID":"/posts/docker/02-install/","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"本文主要记录了 Docker 和 Docker-Compose 的安装过程。 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:0:0","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"1. Docker 安装 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:1:0","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"1. CentOS Docker 要求 CentOS 系统的内核版本高于 3.10。 1）通过 uname -r 命令查看你当前的内核版本 $ uname -r 2）使用 root 权限登录 Centos。确保 yum 包更新到最新。 $ sudo yum update 3）卸载旧版本(如果安装过旧版本的话) $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 4）安装需要的软件包 $ sudo yum install -y yum-utils 5）设置yum源 $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo #这里可能会因为网络问题出错 可以替换成阿里的源 $ sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 6）可以查看所有仓库中所有docker版本，并选择特定版本安装 $ yum list docker-ce --showduplicates | sort -r 7）安装docker # 安装最新版本 $ sudo yum install docker-ce docker-ce-cli containerd.io # 指定安装版本 $ sudo yum install docker-ce-\u003cVERSION_STRING\u003e docker-ce-cli-\u003cVERSION_STRING\u003e containerd.io 8）启动并加入开机启动 [root@localhost ~]# systemctl start docker [root@localhost ~]# systemctl enable docker 9）验证安装是否成功(有 client 和 service 两部分表示 docker 安装启动都成功了 [root@localhost ~]# docker version Version: 18.09.4 API version: 1.39 Go version: go1.10.8 Git commit: d14af54266 Built: Wed Mar 27 18:34:51 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.4 API version: 1.39 (minimum version 1.12) Go version: go1.10.8 Git commit: d14af54 Built: Wed Mar 27 18:04:46 2019 OS/Arch: linux/amd64 Experimental: false 测试 Docker 是否安装正确 $ docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world ca4f61b1923c: Pull complete Digest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/ For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 若能正常输出以上信息，则说明安装成功。 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:1:1","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"2. Ubuntu 同样的，如果安装过旧版本先执行卸载操作 $ sudo apt-get remove docker docker-engine docker.io containerd runc 在新主机上首次安装 Docker Engine-Community 之前，需要设置 Docker 仓库。之后，您可以从仓库安装和更新 Docker 。 1） 更新 apt 包索引。 $ sudo apt-get update 2)）安装依赖包 $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 3）添加 Docker 的官方 GPG 密钥： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 通过搜索指纹的后8个字符，验证您现在是否拥有带有指纹的密钥。 $ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) \u003cdocker@docker.com\u003e sub rsa4096 2017-02-22 [S] 4）设置软件源信息 $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" # 如果安装慢的话可以换国内的源,下面是中国科技大学的 $ sudo add-apt-repository \\ \"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 5）更新并安装 Docker CE $ sudo apt-get update # 安装最新版本 $ sudo apt-get install docker-ce docker-ce-cli containerd.io # 安装指定版本 $ sudo apt-get install docker-ce=\u003cVERSION_STRING\u003e docker-ce-cli=\u003cVERSION_STRING\u003e containerd.io 6)）测试 Docker 是否安装成功，输入以下指令，打印出版本信息则安装成功: $ docker version ","date":"2020-04-20","objectID":"/posts/docker/02-install/:1:2","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"3. 添加用户组 添加当前用户到 docker 用户组，可以不用 sudo 运行 docker（可选） Docker 的守护线程绑定的是 unix socket，而不是 TCP 端口，这个套接字默认属于 root，其他用户可以通过sudo 去访问这个套接字文件。所以 docker 服务进程都是以 root 账户运行。 解决的方式是创建 docker 用户组，把应用用户加入到 docker 用户组里面。只要 docker 组里的用户都可以直接执行 docker 命令。 # 创建 docker 用户组 # 一般安装的时候就已经创建好了 会提示已存在 sudo groupadd docker # 将自己加入到 docker 用户组 # sudo usermod -aG docker $USER sudo usermod -aG docker lixd # 重启 docker sudo systemctl restart docker # 给docker.sock添加权限 好像每次重启docker后都需要重新添加权限。。 sudo chmod a+rw /var/run/docker.sock ","date":"2020-04-20","objectID":"/posts/docker/02-install/:1:3","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"2. 配置Docker镜像加速器 鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:2:0","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"1. 添加镜像源 对于Ubuntu 16.04+、Debian 8+、CentOS 7等使用systemd的系统修改如下： 在/etc/docker/daemon.json中新增以下内容（如果文件不存在请新建该文件）： $ vi /etc/docker/daemon.json 写入如下内容 # 推荐使用阿里云镜像加速器 # 路径：登录阿里云--\u003e控制台--\u003e产品与服务--\u003e容器镜像服务--\u003e镜像中心--\u003e镜像加速器 { \"registry-mirrors\": [ \"https://xxx.mirror.aliyuncs.com\" ] } 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:2:1","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"2.检查加速器是否生效 需要重新启动 Docker 服务。 $ sudo systemctl daemon-reload $ sudo systemctl restart docker 配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效 在命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。 Registry Mirrors: https://registry.docker-cn.com/ ","date":"2020-04-20","objectID":"/posts/docker/02-install/:2:2","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"3. Docker Compose ","date":"2020-04-20","objectID":"/posts/docker/02-install/:3:0","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"1. 简介 Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。 其代码目前在 https://github.com/docker/compose上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 ","date":"2020-04-20","objectID":"/posts/docker/02-install/:3:1","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"2. 安装 在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。 例如，在 Linux 64 位系统上直接下载对应的二进制包。 GitHub 下载慢可以使用该镜像 http://get.daocloud.io/ # 第一步 下载二进制文件到/usr/local/bin/位置 $ curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose # 第二步 赋予可执行权限 $ chmod +x /usr/local/bin/docker-compose # 查看版本号 $ docker-compose version ","date":"2020-04-20","objectID":"/posts/docker/02-install/:3:2","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"3. 卸载 如果是二进制包方式安装的，删除二进制文件即可。 $ sudo rm /usr/local/bin/docker-compose ","date":"2020-04-20","objectID":"/posts/docker/02-install/:3:3","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["Docker"],"content":"4. 参考 https://docs.docker.com/engine https://docs.docker.com/compose/ ","date":"2020-04-20","objectID":"/posts/docker/02-install/:4:0","tags":["Docker"],"title":"Docker教程(二)---docker和docker-compose 快速安装","uri":"/posts/docker/02-install/"},{"categories":["MySQL"],"content":"MySQL MVCC与undolog简要分析","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"本文主要对 MySQL 的 MVCC 和 undolog 做了简要分析。包括 undolog 是如何做到回滚的，MVCC 一致性读又是如何实现的等等。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:0:0","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"1. undolog MySQL 中 undolog 是 MVCC 技术实现的重要组成部分，一致性读功能也需要用到 undolog。当然，更重要的一点就是用于回滚。如事务执行失败后，通过 undolog 进行回滚恢复到之前的状态。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:1:0","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"1. 内容 undolog 是一个链表结构。 每次对数据进行 INSERT、UPDATE（DELETE） 时，都会将旧数据存储到 undolog 中。然后在数据行中修改字段DB_ROLL_PTR 指向对应的 undolog，以便在需要时查询到之前的数据。 可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:1:1","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"2. 存储 InnoDB 存储引擎对 undolog 的管理采用段的方式。 undolog records 存储于 undolog segment 中，undolog segment 又存储在 rollback segment 中，rollback segment 则存放在 undo tablespace 中（MySQL5.6之前是在 system tablespace）。 如果是 临时表的 undolog 则会存放在 global temporary tablespace 中。 另外，undo log也会产生 redo log，因为 undo log 也需要实现持久性保护。 对于 INSERT 操作的 undolog，在事务提交后就可以删除了。因为这是第一个版本所以并不需要存储旧数据。 而对于 UPDATE（DELETE） 操作则提交后也不能删除（可能其他事务的一致性读还在用这个 undolog），需要存储直到没有与之关联的快照事务时才能删除。 其中 DELETE 操作会先打上删除标记，然后由 purge 线程来删除。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:1:2","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"3. 并发事务数限制 同时还有很重要的一点就是，一个 undolog segment 只能同时被一个事务使用。也就是说 InnoDB 存储引擎的并发事务数还会受到 undolog segment 数量限制。 那么 undolog segment 数量到底有多少呢？ 公式如下： (innodb_page_size / 16) * innodb_rollback_segments * number of undo tablespaces 1）undo tablespace undo tablespace 默认是 2（MySQL8.0）,可以通过如下语法进行增删 # 增加 名字比较是 xxx.ibu CREATE UNDO TABLESPACE tablespace_name ADD DATAFILE 'file_name.ibu'; # 删除（需要先设置为不活跃状态） ALTER UNDO TABLESPACE tablespace_name SET INACTIVE; DROP UNDO TABLESPACE tablespace_name; 2）rollback segment rollback segment 由参数innodb_rollback_segments 设置，每个 undo tablespace 最大支持 128 个 rollback segment。 3）undolog segment undolog segment 个数则由 InnoDB PageSize 决定，具体为 pagesize / 16,所以默认 pagesize 16K 能存储 1024 个undolog segment 。 需要注意的是，执行 INSERT 操作会占用一个 undolog segment，如果同时还执行 UPDATE（UPDATE、DELETE）则还会占用一个。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:1:3","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"2. MVCC Multi-Version Concurrency Control 多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问；在编程语言中实现事务内存。 通过存储多版本数据来减少并发控制中锁的使用，以提升性能，适合读多写少的场景。 MySQL 中的 MVCC 是 InnoDB 存储引擎实现的。通过 MVCC 技术，InnoDB 存储引擎可以同时执行对同一记录的读操作和写操作，而不需要等写操作释放锁，极大增加并发性。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:2:0","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"1. 实现 InnoDB 存储引擎在数据的每一行中都增加了 3 个隐藏字段用于存储必要数据。 1）DB_TRX_ID – 6 字节，记录最后插入或更新该行时的 事务 Id（删除操作在内部被看做 更新，内部只是更新了该记录的删除标志位）。 2）DB_ROLL_PTR –7字节，回滚指针，指向对应的 undolog ，用于撤销操作 3）DB_ROW_ID –6字节，主键Id，如果建表时显式指定了主键则不会出现该行。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:2:1","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"2. 回滚 对应 InnoDB 存储引擎来说用户操作只有以下两种情况： 1）INSERT–插入一行数据，该行数据 DB_TRX_ID 的值就是插入这条数据的事务Id，DB_ROLL_PTR此时还是空的。 2）UPDATE–修改（删除）该行数据，DB_TRX_ID 则更新为当前事务Id，同时把修改之前的数据（只记录有更新的字段）写入到 undolog，DB_ROLL_PTR则指向 undolog 对应位置。 回滚时通过字段DB_ROLL_PTR找到 undolog 中记录的旧数据回滚即可。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:2:2","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"3. 一致性读 undolog 中记录了这么多个版本，查询时到底查哪个版本？ 在 InnoDB 中，开启一个新事务的时候，会创建一个 read_view（读视图），由查询时所有活跃（即未提交）事务的Id数组（将其中最小的记作 min_id）和当前已创建的最大事务Id（记作max_id）组成。 需要注意的是：最大事务Id并不是活跃事务中Id最大的那个。 read_view是针对全表的，session级别的。例如在查询A表时会生成一个read_view，然后在同一个session中查询B表会使用前面生成的readview，不会针对B表生成新的。 假设当前事务Id为 trx_id ，版本链比较规则如下： 1）如果（trx_id\u003cmin_id）– 表示这个版本事务已提交，数据可见 2）如果（min_id\u003c=trx_id\u003c=max_id） 情况1：若 trx_id 不在数组中，表示这个版本是有已经提交的事务生成的（后续解释），数据可见。 情况2：如果trx_id不在id数组中，说明是由还未提交的事务生成的，数据不可见 3）如果（max_id\u003ctrx_id）– 表示这个版本是由将来（这里的将来是相对于当前事务来说的）启动的事务生成的，数据肯定不可见。 对于情况二解释如下： MySQL 中的事务 Id 是自增的数字，所以数字大小可以判断事务的开始先后，但是不能判断提交先后。所以情况二中如果不在活跃事务列表说明这个事务虽然是后创建的但是已经先提交了。 所以查询时就根据这个规则，顺着版本链，把不可见的过滤掉，出现的第一个可见数据就是结果。 具体源码看这里 https://github.com/facebook/mysql-5.6/blob/42a5444d52f264682c7805bf8117dd884095c476/storage/innobase/include/read0read.h#L125 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:2:3","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"3. 小结 1）InnoDB 存储引擎中，每次修改记录都会在 undolog 中存储旧数据。 2）MySQL 真正数据表中只存最新版本记录，同时除了第一次插入的时候，其他情况 roll_ptr 都会有值。 3）历史版本数据都在 undolog 中，所以可以根据 roll_ptr 字段在 undolog 中找到多个版本数据。 4）当 undolog 不会被用到时会被移除。 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:3:0","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"4. 参考 https://dev.mysql.com/doc/refman/8.0/en/innodb-multi-versioning.html https://dev.mysql.com/doc/refman/8.0/en/innodb-undo-tablespaces.html http://blog.sina.com.cn/s/blog_4673e603010111ty.html https://blog.csdn.net/Waves___/article/details/105295060 ","date":"2020-04-10","objectID":"/posts/mysql/08-mvcc-undolog/:4:0","tags":["MySQL"],"title":"MySQL教程(八)---MVCC与undolog","uri":"/posts/mysql/08-mvcc-undolog/"},{"categories":["MySQL"],"content":"MySQL redolog与binlog的简单分析","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"本文主要对 MySQL 的 binlog 和 redolog 进行了详细分析，同时最后对两者进行了总结比较。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:0:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"1. 概述 MySQL 中有如下几种日志文件，分别是： 重做日志(redo log) 回滚日志(undo log) 二进制日志(binlog) 错误日志(errorlog) 慢查询日志(slow query log) 一般查询日志(general log) 中继日志(relay log) 其中redolog和undolog 为 InnoDB 存储引擎独有，与事务操作息息相关，binlog也与事务操作有一定的关系，同时对数据复制、恢复有很大帮助。 这三种日志，对理解 MySQL 中的事务操作有着重要的意义，所以有必要好好了解一下这三种日志。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:1:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"2. redolog ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:2:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"1. 作用 确保事务的持久性 防止在发生故障的时间点，尚有脏页未写入磁盘导致数据丢失问题。在重启 MySQL 服务的时候，会根据 redolog 进行重做，恢复崩溃时暂未写入磁盘的数据，从而达到事务的持久性这一特性。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:2:1","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"2. 内容 redolog 是物理格式的日志，记录的是物理数据页的修改信息。 redolog 是顺序写入 redolog file 的物理文件中去的。 当更新一条数据时，InnoDB 会找到要更新的行数据，把做了什么修改写到 redolog 中，并把这行数据更新到内存中，整个过程就算完成了。 redolog file 是固定大小的（如下图），所以为了能够一直记录它只能采用循环写入方式，write pos为当前记录的位置，checkpoint为当前可以擦除的位置，代表更新的行已经完成数据库的磁盘更改,可以覆盖掉了。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:2:2","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"3. 磁盘文件 1. 产生时间 事务开始之后就产生 redo log，redolog 的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，就开始逐步写入到 redolog文件中。 2. 释放时间 当对应事务的脏页写入到磁盘之后，redolog 的使命也就完成了（落盘后就不用担心数据丢失了），redolog 中对应事务占用的空间就可以重用（被覆盖）。 3. 物理文件 默认情况下，对应的物理文件位于数据库的 data 目录下的ib_logfile1、ib_logfile2。 相关参数如下： 1）innodb_log_files_in_group – 指定文件个数（默认2） 2）innodb_log_file_size – 指定文件大小 3）innodb_mirrored_log_groups – 指定日志文件副本个数，主要用于保护数据（默认1） ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:2:3","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"4. 刷盘时间 如果每次都写入磁盘势必会对性能造成较大影响，所以 MySQL 进行了相应优化。 对于写入 redolog 文件的操作不直接刷盘，而是先写入内存中的重做日志缓冲（redolog buffer），然后根据用户设置参数（innodb_flush_log_at_trx_commit）来执行刷盘逻辑。 第一个刷盘逻辑用户无法调整：在主线程中每秒会将 redolog buffer 写入磁盘，不论事务是否已经提交，也不管用户指定的什么参数。 第二个刷盘逻辑则由参数innodb_flush_log_at_trx_commit控制，可选值为0、1、2三个。 0–代表当 commit 时，不进行任何操作。而是等待主线程每秒进行刷盘，从LogBuffer写入到 OS Buffer并调用fsync写入磁盘上的日志文件， 1–代表在 commit 时直接将 redolog buffer 同步写到磁盘； 2–代表直接写入OS Buffer，但是异步写到磁盘，即不能完全保证 commit 时肯定能落盘，只是有这个动作。 所以为了保证事务的持久性必须将innodb_flush_log_at_trx_commit的值设置为1，即每当事务提交时就必须保证事务都已经写入重做日志文件，设置为其他值都可能出现事务的丢失。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:2:4","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"3. binlog ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:3:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"1. 作用 1）数据复制，比如在主从复制中，从库利用主库上的 binlog 进行重播，实现主从同步。 2）数据恢复，比如基于时间点的还原。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:3:1","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"2. 内容 binlog 是逻辑格式的日志，binlog 中存储的内容称之为事件，每一个数据库更新操作(Insert、Update、Delete，不包括Select)等都对应一个事件（event）。 简单地说 binlog 中存储的是更新数据库的 SQL 语句，但又不完全是 SQL 语句这么简单， binlog 中同时包括了用户执行的SQL语句(增删改)的反向 SQL 语句。 比如：delete 操作在 binlog 会有 delete本身和其反向的 insert 这两条记录；update 则同时存储着update执行前后的版本的信息；insert则对应着delete和insert本身的信息。 因此可以基于 binlog 做到类似于 oracle 的闪回功能，其实都是依赖于 binlog 中的日志记录。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:3:2","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"3. 生命周期 1. 产生时间 事务提交的时候，会一次性将事务中的 SQL 语句（一个事务可能对应多个 SQL 语句）按照一定的格式记录到 binlog 中。 这里与 redolog 很明显的差异就是 redolog 并不一定是在事务提交的时候刷新到磁盘，redolog是在事务开始之后就逐步写入磁盘。因此对于事务的提交，即便是较大的事务，redolog 都是很快的。 但是对 binlog 来说，较大事务的提交可能会变得比较慢一些，这是因为 binlog 是在事务提交的时候一次性写入的。 2. 释放时间 binlog 的默认是保持时间由参数 expire_logs_days 控制（默认为0，即不自动移除），也就是说对于非活动的日志文件，在生成时间超过 expire_logs_days 配置天数之后，会被自动删除。 3. 物理文件 binlog 文件存放路径由参数 log_bin_basename 控制，binlog 日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新生成新的日志文件（如：binlog.0001,binlog.0002）。 对于每个 binlog 文件，会通过一个统一的 index（目录）文件来组织。 相关参数如下： 1）log_bin_basename – binlog 文件存放路径 2）max_binlog_size – 单个 binlog 文件最大大小（默认 1G） 4. 刷盘时间 事务提交时直接刷盘。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:3:3","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"4. binlog 与 redolog 的区别 binlog 的作用之一是还原数据库，这与 redolog 很类似，很多人混淆过，但是两者有本质的不同。 比较主要的三个区别如下： 1）实现层面不同 redolog 是 InnoDB 引擎特有的； binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 2）内容不同 redo log 是物理日志，记录的是 “在某个数据页上做了什么修改”； binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如 “在某一行的某一列上进行什么修改”。 3）大小不同 redolog 空间是固定的，只能循环写。 binlog 大小无限制，是可以追加写入的，达到上限后会滚动更新。 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:4:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"5. 参考 https://dev.mysql.com/doc/refman/8.0/en/innodb-redo-log.html https://www.cnblogs.com/wy123/p/8365234.html https://dev.mysql.com/doc/refman/8.0/en/server-logs.html https://dev.mysql.com/doc/refman/8.0/en/mysqlbinlog.html https://laijianfeng.org/2019/03/MySQL-Binlog-介绍/ 专栏 MySQL 45讲 ","date":"2020-04-05","objectID":"/posts/mysql/07-binlog-redolog-undolog/:5:0","tags":["MySQL"],"title":"MySQL教程(七)---redolog与binlog","uri":"/posts/mysql/07-binlog-redolog-undolog/"},{"categories":["MySQL"],"content":"MySQL的 JOIN 语句执行流程详解","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"本文主要记录了 MySQL 中的 JOIN 语句具体执行流程,同时分析了 ON 与 WHERE 条件的区别。 ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:0:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"1. 执行流程 一个完整的 SQL 语句中会被拆分成多个子句，子句的执行过程中会产生虚拟表（VT），经过各种条件后生成的最后一张虚拟表就是返回的结果。 以下是 JOIN 查询的通用结构： SELECT \u003crow_list\u003e FROM \u003cleft_table\u003e \u003cinner|left|right\u003e JOIN \u003cright_table\u003e ON \u003cjoin condition\u003e WHERE \u003cwhere_condition\u003e 它的执行顺序如下： SQL 语句里第一个被执行的总是 FROM 子句 1）FROM：执行 FROM 子句对两张表进行笛卡尔积操作， 对左右两张表执行笛卡尔积，产生第一张表 vt1。行数为 n*m（ n 为左表的行数，m 为右表的行数) 2）ON: 执行 ON 子句过滤掉不满足条件的行 根据 ON 的条件逐行筛选 vt1，将结果插入 vt2 中 3）JOIN:添加外部行 如果是 LEFT JOIN，则先遍历一遍左表的每一行，其中不在 vt2 的行会被添加到 vt2，该行的剩余字段将被填充为NULL，形成 vt3；IGHT JOIN同理。但如果指定的是 INNER JOIN，则不会添加外部行，上述插入过程被忽略，vt3 就是 vt2。 4）WHERE: 条件过滤 根据 WHERE 条件对 vt3 进行条件过滤产生 vt4 5）SELECT: 查询指定字段 取出 vt4 的指定字段形成 vt5 ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:1:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"2. 例子 创建一个用户信息表： CREATE TABLE `user_info` ( `userid` int(11) NOT NULL, `name` varchar(255) NOT NULL, UNIQUE `userid` (`userid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 再创建一个用户余额表： CREATE TABLE `user_account` ( `userid` int(11) NOT NULL, `money` bigint(20) NOT NULL, UNIQUE `userid` (`userid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 随便导入一些数据： insert into user_info values(1001,'x'),(1002,'y'),(1003,'z'),(1004,'a'),(1005,'b'),(1006,'c'),(1007,'d'),(1008,'e'); insert into user_account values(1001,22),(1002,30),(1003,8),(1009,11); 一共 8 个用户有用户名，4 个用户的账户有余额。 取出 userid 为 1003 的用户姓名和余额，SQL如下： SELECT i.name, a.money FROM user_info as i LEFT JOIN user_account as a ON i.userid = a.userid WHERE a.userid = 1003; 第一步：执行 FROM 子句对两张表进行笛卡尔积操作 笛卡尔积操作后会返回两张表中所有行的组合，左表 userinfo 有 8 行，右表 useraccount 有 4 行，生成的虚拟表vt1 就是 8*4=32 行： mysql\u003e SELECT * FROM user_info as i LEFT JOIN user_account as a ON 1; +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1009 | 11 | | 1001 | x | 1003 | 8 | | 1001 | x | 1002 | 30 | | 1001 | x | 1001 | 22 | | 1002 | y | 1009 | 11 | | 1002 | y | 1003 | 8 | | 1002 | y | 1002 | 30 | | 1002 | y | 1001 | 22 | | 1003 | z | 1009 | 11 | | 1003 | z | 1003 | 8 | | 1003 | z | 1002 | 30 | | 1003 | z | 1001 | 22 | | 1004 | a | 1009 | 11 | | 1004 | a | 1003 | 8 | | 1004 | a | 1002 | 30 | | 1004 | a | 1001 | 22 | | 1005 | b | 1009 | 11 | | 1005 | b | 1003 | 8 | | 1005 | b | 1002 | 30 | | 1005 | b | 1001 | 22 | | 1006 | c | 1009 | 11 | | 1006 | c | 1003 | 8 | | 1006 | c | 1002 | 30 | | 1006 | c | 1001 | 22 | | 1007 | d | 1009 | 11 | | 1007 | d | 1003 | 8 | | 1007 | d | 1002 | 30 | | 1007 | d | 1001 | 22 | | 1008 | e | 1009 | 11 | | 1008 | e | 1003 | 8 | | 1008 | e | 1002 | 30 | | 1008 | e | 1001 | 22 | +--------+------+--------+-------+ 32 rows in set (0.00 sec) 第二步：执行 ON子句过滤掉不满足条件的行 ON i.userid = a.userid 过滤之后 vt2 如下： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ 第三步：JOIN 添加外部行 LEFT JOIN会将左表未出现在 vt2 的行插入进 vt2，每一行的剩余字段将被填充为NULL，RIGHT JOIN同理。 本例中用的是LEFT JOIN，所以会将左表user_info剩下的行都添上 生成表 vt3： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | | 1004 | a | NULL | NULL | | 1005 | b | NULL | NULL | | 1006 | c | NULL | NULL | | 1007 | d | NULL | NULL | | 1008 | e | NULL | NULL | +--------+------+--------+-------+ 第四步：WHERE条件过滤 WHERE a.userid = 1003 生成表 vt4： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ 第五步：SELECT SELECT i.name, a.money 生成 vt5： +------+-------+ | name | money | +------+-------+ | z | 8 | +------+-------+ 虚拟表 vt5 作为最终结果返回给客户端。 介绍完联表的过程之后，我们看看常用JOIN的区别。 ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:2:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"3. 几种JOIN 区别 INNER JOIN…ON…: 返回 左右表互相匹配的所有行（因为只执行上文的第二步ON过滤，不执行第三步 添加外部行） LEFT JOIN…ON…: 返回左表的所有行，若某些行在右表里没有相对应的匹配行，则将右表的列在新表中置为NULL RIGHT JOIN…ON…: 返回右表的所有行，若某些行在左表里没有相对应的匹配行，则将左表的列在新表中置为NULL ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:3:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"1. INNER JOIN 拿上文的第三步添加外部行来举例，若LEFT JOIN替换成INNER JOIN，则会跳过这一步，生成的表 vt3 与 vt2 一模一样： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:3:1","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"2. RIGHT JOIN 若LEFT JOIN替换成RIGHT JOIN，则生成的表 vt3 如下： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | | NULL | NULL | 1009 | 11 | +--------+------+--------+-------+ 因为 useraccount（右表）里存在 userid = 1009 这一行，而 userinfo（左表）里却找不到这一行的记录，所以会在第三步插入以下一行： | NULL | NULL | 1009 | 11 | ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:3:2","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"3. FULL JOIN 上文引用的文章中提到了标准SQL定义的FULL JOIN，这在mysql里是不支持的，不过我们可以通过LEFT JOIN + UNION + RIGHT JOIN 来实现FULL JOIN： SELECT * FROM user_info as i RIGHT JOIN user_account as a ON a.userid=i.userid union SELECT * FROM user_info as i LEFT JOIN user_account as a ON a.userid=i.userid; 他会返回如下结果： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | | NULL | NULL | 1009 | 11 | | 1004 | a | NULL | NULL | | 1005 | b | NULL | NULL | | 1006 | c | NULL | NULL | | 1007 | d | NULL | NULL | | 1008 | e | NULL | NULL | +--------+------+--------+-------+ ps：其实我们从语义上就能看出LEFT JOIN和RIGHT JOIN没什么差别，两者的结果差异取决于左右表的放置顺序，以下内容摘自mysql官方文档： RIGHT JOIN works analogously to LEFT JOIN. To keep code portable across databases, it is recommended that you use LEFT JOIN instead of RIGHT JOIN. 所以当你纠结使用LEFT JOIN还是RIGHT JOIN时，尽可能只使用LEFT JOIN吧。 ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:3:3","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"4. ON和WHERE的区别 上文把 JOIN 的执行顺序了解清楚之后，ON 和 WHERE 的区别也就很好理解了。 举例说明： SELECT * FROM user_info as i LEFT JOIN user_account as a ON i.userid = a.userid and i.userid = 1003; SELECT * FROM user_info as i LEFT JOIN user_account as a ON i.userid = a.userid where i.userid = 1003; 第一种情况LEFT JOIN在执行完第二步ON子句后，筛选出满足i.userid = a.userid and i.userid = 1003 的行，生成表 vt2，然后执行第三步 JOIN 子句，将外部行添加进虚拟表生成 vt3 即最终结果： vt2: +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ vt3: +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | NULL | NULL | | 1002 | y | NULL | NULL | | 1003 | z | 1003 | 8 | | 1004 | a | NULL | NULL | | 1005 | b | NULL | NULL | | 1006 | c | NULL | NULL | | 1007 | d | NULL | NULL | | 1008 | e | NULL | NULL | +--------+------+--------+-------+ 而第二种情况LEFT JOIN在执行完第二步 ON 子句后，筛选出满足i.userid = a.userid的行，生成表 vt2；再执行第三步 JOIN 子句添加外部行生成表 vt3；然后执行第四步 WHERE 子句，再对 vt3 表进行过滤生成 vt4，得的最终结果： vt2: +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ vt3: +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1001 | x | 1001 | 22 | | 1002 | y | 1002 | 30 | | 1003 | z | 1003 | 8 | | 1004 | a | NULL | NULL | | 1005 | b | NULL | NULL | | 1006 | c | NULL | NULL | | 1007 | d | NULL | NULL | | 1008 | e | NULL | NULL | +--------+------+--------+-------+ vt4: +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ 如果将上例的LEFT JOIN替换成INNER JOIN，不论将条件过滤放到ON还是WHERE里，结果都是一样的，因为INNER JOIN不会执行第三步添加外部行。 SELECT * FROM user_info as i INNER JOIN user_account as a ON i.userid = a.userid and i.userid = 1003; SELECT * FROM user_info as i INNER JOIN user_account as a ON i.userid = a.userid where i.userid = 1003; 返回结果都是： +--------+------+--------+-------+ | userid | name | userid | money | +--------+------+--------+-------+ | 1003 | z | 1003 | 8 | +--------+------+--------+-------+ 只要记住联表语句执行过程之后，应对各种 JOIN 语句应该都比较轻松了。 ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:4:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"5. 参考 https://dev.mysql.com/doc/refman/8.0/en/join.html https://mp.weixin.qq.com/s/LfbSmwwDy5QkxXsicjMjLA https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins ","date":"2020-03-30","objectID":"/posts/mysql/06-join-process/:5:0","tags":["MySQL"],"title":"MySQL教程(六)---JOIN 语句执行流程","uri":"/posts/mysql/06-join-process/"},{"categories":["MySQL"],"content":"MySQL 幻读与InnoDB 间隙锁（Gap Lock）","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"本文对 MySQL 事务隔离级别及其常见问题进行了分析，同时记录了 InnoDB 是如何通过间隙锁来解决幻读的，最后还分析了为什么大部分业务事务隔离级别会使用读已提交级别。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:0:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"1. 常见问题 在不考虑事务隔离级别的情况下，DB 操作可能出现以下几个问题： 1）脏读：事务 A 读取了事务 B 更新的数据，然后 事务B 进行回滚操作，那么 事务 A 读取到的数据就是脏数据。 指一个事务读取到了另外事务中未提交的数据。 2）不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。 指一个事务读取到了事务中提交的 update 的数据。 3）幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候新增了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 指一个事务读取到了事务中提交的 insert 的数据。 不可重复读和幻读最大的区别，一者是对已存在的行进行操作导致，一者是对不存在的行进行操作导致。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:1:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"2. 如何解决 ISO 和 ANIS SQL 提供了4 种事务隔离级别的标准。 read-uncommitted read-committed repeatable-read serializable 为了解决这些问题，InnoDB 存储引擎同样提供了 4 种事务隔离级别。 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 读已提交（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 对InnoDB否 串行化（serializable） 否 否 否 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:2:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"1. 脏读 将事务隔离级别提升到读已提交（read-committed）即可限制，禁止其他事务访问当前事务未提交的数据，这样就不会出现脏读的情况。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:2:1","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"2. 不可重复读 将事务隔离级别提升到可重复读（repeatable-read）即可。 该事务隔离级别下，每次开启事务都会新建一个快照，在当前事务中的多次查询都是基于此快照进行的，不会查询到其他事务提交的数据，所以也不会出现 不可重复读的问题。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:2:2","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"3. 幻读 这个就比较复杂了，只有 InnoDB 存储引擎下把事务隔离级别调到可重复读（repeatable-read）才能限制该问题。 幻读与不可重复读的区别：不可重复读是对当前已存在的数据进行更新，导致后续的查询与之前的查询结果不一致，而幻读是新增了满足当前查询条件的行，导致前后查询结果不一致。 解决不可重复读只需要每次将满足条件的行，加锁即可。 当是幻读则不能通过该方式解决，因为当前行都不存在怎么加锁。 于是 MySQL 中为了解决这个问题，新增了一种锁：间隙锁（Gap Lock）。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:2:3","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"3. 间隙锁（Gap Lock） 间隙锁是 InnoDB 行锁中的一种。 产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的间隙。因此，为了解决幻读问题，mysql InnoDB 只好引入新的锁，也就是间隙锁 (GapLock)。间隙锁，锁的就是两个值之间的空隙，因此间隙锁只与往间隙里写入记录这个操作冲突 。值得注意的是，间隙锁只在隔离级别是 可重复读隔离级别下才会生效。 1）行锁(Record Lock)：锁直接加在索引记录上面。 2）间隙锁(Gap Lock)：锁加在不存在的空闲空间，可以是两个索引记录之间，也可能是第一个索引记录之前或者最后一个索引之后的空间，两边都是开区间。 3）Next-Key Lock：行锁与间隙锁组合起来用就叫做 Next-Key Lock，是一个前开后闭区间。 默认情况下，InnoDB工作在 重复读（repeatable-read）的隔离情况下，并且以 Next-Key Lock 的方式对数据进行加锁，这样就可以有效地防止幻读的发生。 Next-key Lock 是行锁与间隙锁的组合，这样，当 InnoDB 扫描索引记录的时候，会首先对选中的索引记录加上行锁(Record Lock)，再对索引记录两边的间隙加上间隙锁(Gap Lock)。如果一个间隙被事务 A 加了锁，其他事务是不能在这个间隙插入记录的。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:3:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"4. 加锁规则 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:4:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"1. 概述 1）原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 2）原则 2：查找过程中访问到的对象才会加锁。 3）优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 4）优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 5）一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 MySQL 后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，即 5.x 系列 \u003c=5.7.24，8.0 系列 \u003c=8.0.13。 建表语句和初始化如下： CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:4:1","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"2. 等值查询间隙锁 sessionA sessionB sessionC begin; update t set d = d+1 where id = 7; insert into t values(8,8,8);(blocked) update t set d = d+1 where id =10;(Query OK) 由于表 t 中没有 id=7 的记录，所以用我们上面提到的加锁规则判断一下的话： 1）根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10]； 2）同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。 所以，session B 要往这个间隙里面插入 id=8 的记录会被锁住，但是 session C 修改 id=10 这行是可以的 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:4:2","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"3. 非唯一索引等值锁 sessionA sessionB sessionC begin; select id from t where c =5 lock in share mode; update t set d = d+1 where id =5; insert into t values(7,7,7);(blocked) 这里 session A 要给索引 c 上 c=5 的这一行加上读锁。 1）根据原则 1，加锁单位是 next-key lock，因此会给 (0,5]加上 next-key lock。要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。 2）根据原则 2，访问到的都要加锁，因此要给 (5,10]加 next-key lock。 3）但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。 4）根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:4:3","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"5. 间隙锁带来的问题 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:5:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"1. 死锁 1 CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 间隙锁的引入可能会导致死锁。前面提到了间隙锁只与往间隙里写入记录这个操作冲突。同一个间隙多次加锁是不会冲突的，于是问题来了。 SessionA SessionB begin; select * from t where id = 9 for update; begin; select * from t where id = 9 for update; insert into values(9,9,9);(blocked) insert into values(9,9,9);(ERROR Deadlock found) 分析如下： 1）session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10); 2）session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功； 3）session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待； 4）session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。 至此，两个 session 进入互相等待状态，形成死锁。当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:5:1","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"2. 死锁 2 sessionA sessionB begin； select id from t where c= 10 lock in share mode; update t set d= d+1 where c = 10;(blocked) insert into t values(8,8,8); ERROR Deadlock found when trying to get lock;try restaring transaction. 现在，我们按时间顺序来分析一下为什么是这样的结果。 1）session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock(5,10] 和间隙锁 (10,15)； 2）session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待；然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。 你可能会问，session B 的 next-key lock 不是还没申请成功吗？其实是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步： 1）先是加 (5,10) 的间隙锁，加锁成功； 2）然后加 c=10 的行锁，这时候才被锁住的。 也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:5:2","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"6. 小结 本文对 MySQL 事务隔离级别及其常见问题进行了分析，同时记录了 InnoDB 是如何解决幻读的。 最后一个问题，既然间隙锁能解决其他锁都解决不了的问题（幻读），那么为什么大部分业务还是使用 读提交事务隔离级别呢（只有在可重复读级别下间隙锁才生效）? 1）间隙锁的引入可能会出现死锁。 2）间隙锁的引入，可能会导致同样的语句锁住更大的范围，对并发度有一定影响。 3）在读提交隔离级别下，锁的范围更小，锁的时间更短。 因为读提交隔离级别下有一个优化：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交 所以大部分业务都使用的读提交（Read Commit）级别，同时为了解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。 ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:6:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["MySQL"],"content":"7. 参考 《Mysql技术内幕InnoDB存储引擎》 极客专栏-\u003cMySQL 实战45讲\u003e ","date":"2020-03-25","objectID":"/posts/mysql/04-cap-lock/:7:0","tags":["MySQL"],"title":"MySQL教程(四)---MySQL 幻读与 InnoDB 间隙锁（Gap Lock）","uri":"/posts/mysql/04-cap-lock/"},{"categories":["elasticsearch"],"content":"elasticsearch 开发环境搭建","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"本文主要记录了 Elasticsearch、Kibana 的安装部署流程，及其目录简单介绍。同时也记录了 Elasticsearch 分词器的安装即介绍等。 ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:0:0","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"1. 概述 Elasticsearch 是一个分布式的开源搜索和分析引擎，在 Apache Lucene 的基础上开发而成。以其简单的 REST 风格 API、分布式特性、速度和可扩展性而闻名，是 Elastic Stack 的核心组件 ELK 中的 E Kibana是一个针对 Elasticsearch 的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch 索引中的数据。使用 Kibana，可以通过各种图表进行高级数据分析及展示。 ELK 中的 K Logstash TODO ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:1:0","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"2. Docker 安装 使用 docker-compose 一键安装 docker-compose 安装 看这里 ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:2:0","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"0. 环境准备 调整用户 mmap 计数,否则启动时可能会出现内存不足的情况 # 查看当前限制 $ sysctl vm.max_map_count vm.max_map_count = 65530 临时修改 - 不需要重启 [root@iZ2zeahgpvp1oasog26r2tZ vm]# sysctl -w vm.max_map_count=262144 vm.max_map_count = 262144 永久修改 - 需要重启 vi /etc/sysctl.cof # 增加 如下内容 vm.max_map_count = 262144 同时需要提前创建相关目录,大概结构是这样的 es/ /data /logs /plugins --es.yml --cerebro.yml --kibana.yml 手动创建一个 docker 网络 $ docker network create elk 同时需要创建对应目录并授予访问权限。 ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:2:1","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"1. Elasticsearch # es.yml version: '3.2' services: elasticsearch: image: elasticsearch:7.8.0 container_name: elk-es restart: always environment: # 开启内存锁定 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" # 指定单节点启动 - discovery.type=single-node ulimits: # 取消内存相关限制 用于开启内存锁定 memlock: soft: -1 hard: -1 volumes: - ./data:/usr/share/elasticsearch/data - ./logs:/usr/share/elasticsearch/logs - ./plugins:/usr/share/elasticsearch/plugins ports: - 9200:9200 networks: default: external: name: elk 启动命令 $ docker-compose -f es.yml up ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:2:2","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"2. cerebro cerebro 是一个简单的 ES 集群监控工具 # cerebro.yml version: '3.2' services: cerebro: image: lmenezes/cerebro:0.9.2 container_name: cerebro restart: always ports: - \"9000:9000\" command: - -Dhosts.0.host=http://elk-es:9200 networks: default: external: name: elk $ docker-compose -f cerebro .yml up ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:2:3","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"3. Kibana # kibana.yml version: '3.2' services: kibana: image: kibana:7.8.0 container_name: elk-kibana restart: always environment: ELASTICSEARCH_HOSTS: http://elk-es:9200 I18N_LOCALE: zh-CN ports: - 5601:5601 networks: default: external: name: elk $ docker-compose -f kibana.yml up ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:2:4","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"3. 分词器 ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:3:0","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"1. 安装 分词器安装很简单，一条命令搞定 ./elasticsearch-plugin install {分词器的下载地址} 比如安装 ik 分词器 ./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.8.0/elasticsearch-analysis-ik-7.8.0.zip 常用分词器列表 ​ IK 分词器 https://github.com/medcl/elasticsearch-analysis-ik 拼音分词器 https://github.com/medcl/elasticsearch-analysis-pinyin 分词器版本需要和elasticsearch版本对应，并且安装完插件后需重启Es，才能生效 分词器版本需要和elasticsearch版本对应，并且安装完插件后需重启Es，才能生效 分词器版本需要和elasticsearch版本对应，并且安装完插件后需重启Es，才能生效 插件安装其实就是下载 zip 包然后解压到 plugins 目录下。 Docker 安装的话可以通过 Volume 的方式放在宿主机，或者进入容器用命令行安装也是一样的。 比如前面创建的 plugins 目录就是存放分词器的，Elasticsearch 启动时会自动加载 该目录下的分词器。 ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:3:1","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["elasticsearch"],"content":"2. 测试 在 Kibana 中通过 Dev Tools 可以方便的执行各种操作。 #ik_max_word 会将文本做最细粒度的拆分 #ik_smart 会做最粗粒度的拆分 #pinyin 拼音 POST _analyze { \"analyzer\": \"ik_max_word\", \"text\": [\"剑桥分析公司多位高管对卧底记者说，他们确保了唐纳德·特朗普在总统大选中获胜\"] } ","date":"2020-03-20","objectID":"/posts/elasticsearch/01-install-by-docker/:3:2","tags":["elasticsearch"],"title":"Elasticsearch教程(一)--使用docker-compose快速搭建 elasticsearch","uri":"/posts/elasticsearch/01-install-by-docker/"},{"categories":["MySQL"],"content":"MySQL 常见数据类型的简单分析","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"本文主要对 MySQL 常见数据类型进行了简单分析与总结，包括数值、字符、日期和NULL等。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:0:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"1. 概述 本文基于 MySQL 8.0 版本。 MySQL 中常用数据类型可以分为以下几类： 1）数值 2）字符串 3）日期 4）NULL ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:1:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"2. 数值 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:2:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"1. 整形 整形也就是 INT，根据大小不同又细分为如下几种： Type Storage (Bytes) Range（SIGNED） Range（UNSIGNED） TINYINT 1 -128~127 0~255 SMALLINT 2 -32768~32767 0~65535 MEDIUMINT 3 -8388608~8388607 0~16777215 INT 4 -2147483648~2147483647 0~4294967295 BIGINT 8 -2^63~2^63-1 0~2^64-1 INT(N)形式 在开发中，我们会碰到有些定义整型的写法是int(10)，int(N)我们只需要记住三点： 1）不推荐这样用，MySQL官方也准备取消对该用法的支持。 2）N 表示的是最大显示宽度，也只会影响显示，不足的用 0 补足，超过长度的不会被截字，也是直接显示整个数字。 3）可以使用LPAD()函数来实现 ZEROFILL。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:2:1","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"2. 浮点型 Type Storage (Bytes) 备注 FLOAT 4 精确到约7位小数 DOUBLE 8 精确到约15位小数 FLOAT(M,D)、DOUBLE(M、D) 的用法规则： D 表示浮点型数据小数部分的位数，假如超过 D 位则四舍五入，即1.233 四舍五入为 1.23，1.237 四舍五入为 1.24 M 表示浮点型数据总共的位数，M=5,D=2 则表示总共支持五位，其中小数点前只支持三位数， 当我们不指定M、D的时候，会按照实际的精度来处理，最多处理到硬件支持的极限。 不推荐继续使用FLOAT(M,D)写法，在 MySQL 8.0.17 版本已经废弃。同时 UNSIGNED FLOAT写法也被废弃。 FLOAT(P) 写法，其中P取值为0~24时转为FLAOT类型，25~53时转为DOUBLE类型。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:2:2","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"3. 定点型 数据类型 字节数 备注 DECIMAL 对DECIMAL(M,D) ，如果M\u003eD，为M+2否则为D+2 精确小数值 MySQL stores DECIMAL values in binary format DECIMAL(M,D)用法： M 为总位数，默认为 10，最大为 65。 D 位小数位数，默认为 0，最大为30。 最后讲一下decimal和float/double的区别，主要体现在两点上： float/double在db中存储的是近似值，而decimal则是以字符串形式进行保存的 decimal(M,D)的规则和float/double相同，但区别在float/double在不指定M、D时默认按照实际精度来处理而decimal在不指定M、D时默认为decimal(10, 0) 浮点型和定点型都可以增加UNSIGNED，但是返回却不会发生变化，同时不推荐使用该功能了。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:2:3","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"2. 字符 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:3:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"1.char、varchar 数据类型 大小（字节） 备注 CHAR 0-255 定长字符串 VARCHAR 0-65535 变长字符串 char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30)或者VARCHAR(30) 就可以存储 30 个字符。 关于 char 和 varchar 的对比： 1）存储 char 为固定长度(建表时指定)，不足指定长度时会用空格补齐。 varchar 可变长度，会存储实际字符串。 2）大小 char 实际大小就是字符所占字节数 varchar 实际大小由三部分组成 字符所占字节数+长度记录值(1或2字节)+NULL标志位(1字节) 3）查询 char 类型查询时会处理掉结尾的所有空格（因为存储时做了补齐操作） varchar 则不会处理 varchar型数据占用空间大小及可容纳最大字符串限制探究 这部分和具体编码方式有关，详情如下： MySQL要求一个行的定义长度不能超过 65535 即 64K 对于未指定 varchar 字段 not null的表，会有1个字节专门表示该字段是否为 null varchar(M)会有一个空间用于记录字符串长度 当M范围为0\u003c=M\u003c=255时会专门有一个字节记录varchar型字符串长度 当M\u003e255时会专门有两个字节记录varchar型字符串的长度 把这一点和上一点结合，那么65535个字节实际可用的为65535-3=65532个字节 不同编码方式，占用字节数也不同 所有英文无论其编码方式，都占用1个字节，因此最大M=65532； 对于gbk编码，一个汉字占两个字节，因此最大M=65532/2=32766； 对于utf8编码，一个汉字占3个字节，因此最大M=65532/3=21844； 对于utfmb4编码方式，1个字符最大可能占4个字节，那么varchar(M)，M最大为65532/4=16383 同样的，上面是一行中只有varchar型数据的情况，如果存在其他列，比如同时存在int、double、char这些数据，需要把这些数据所占据的空间减去，才能计算varchar(M)型数据M最大等于多少。 由于mysql自身的特点，如果一个数据表存在varchar字段，则表中的char字段将自动转为varchar字段。在这种情况下设置的char是没有意义的。所以要想利用char的高效率，要保证该表中不存在varchar字段；否则，应该设为varchar字段。 这里暂时没找到具体原因。。只有下面这个 InnoDB encodes fixed-length fields greater than or equal to 768 bytes in length as variable-length fields, which can be stored off-page. For example, a CHAR(255) column can exceed 768 bytes if the maximum byte length of the character set is greater than 3, as it is with utf8mb4. ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:3:1","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"2. text、blob text存储的是字符串而blob存储的是二进制字符串，简单说blob是用于存储例如图片、音视频这种文件的二进制数据的。 数据类型 大小（字节） 备注 TINYTEXT 0~255 TEXT 0-65 535 单精度浮点型 MEDIUMTEXT 0-16 777 215 LONGTEXT 0-4 294 967 295（4GB） TINYBLOB 0~255 BLOB 0-65 535 单精度浮点型 MEDIUMBLOB 0-16 777 215 LONGBLOB 0-4 294 967 295（4GB） text和varchar是一组既有区别又有联系的数据类型，其联系在于当varchar(M)的M大于某些数值时，varchar会自动转为text,具体条件如下： M\u003e255 时转为 tinytext M\u003e500 时转为 text M\u003e20000 时转为 mediumtext 所以过大的内容 varchar 和 text 没有区别，同时 varchar(M) 和 text 的区别在于： 单行 64K 即 65535字节的空间，varchar 只能用 65533 字节，但是 text 可以 65535 字节全部用起来 text 可以指定 text(M)，但是 M 无论等于多少都没有影响 text 不允许有默认值，varchar 允许有默认值 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:3:2","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"3. 日期类型 接着我们看一下 MySQL 中的日期类型，MySQL 支持五种形式的日期类型： 数据类型 大小（字节） 格式 范围 备注 date 3 yyyy-MM-dd - 存储日期值 time 3 HH:mm:ss – 存储时分秒 year 1 yyyy – 存储年 datetime 8 yyyy-MM-dd HH:mm:ss 1000-01-01 00:00:00——9999-12-31 23:59:59 存储日期+时间 timestamp 4 yyyy-MM-dd HH:mm:ss 19700101080001——20380119111407 存储日期+时间，可作时间戳 重点关注一下 datetime 与 timestamp两种类型的区别： 大小 datetime 占 8 字节 timestamp 占4字节 范围 -大小不同，取值范围肯定也不同 datetime 的存储范围为1000-01-01 00:00:00——9999-12-31 23:59:59 timestamp 存储的时间范围为19700101080001——20380119111407 默认值 datetime 默认值为空，当插入的值为 null 时，该列的值就是 null； timestamp 默认值不为空，当插入的值为null 的时候，MySQL会取当前时间 时区 datetime 存储的时间与时区无关 timestamp 存储的时间及显示的时间都依赖于当前时区 MySQL converts TIMESTAMP values from the current time zone to UTC for storage, and back from UTC to the current time zone for retrieval. (This does not occur for other types such as DATETIME.) 如何选择 timestamp 记录经常变化的更新 / 创建 / 发布 / 日志时间 / 购买时间 / 登录时间 / 注册时间等，并且是近来的时间，够用，时区自动处理，比如说做海外购或者业务可能拓展到海外 datetime 记录固定时间如服务器执行计划任务时间 / 健身锻炼计划时间等，在任何时区都是需要一个固定的时间要做某个事情。超出 timestamp 的时间，如果需要时区必须记得时区处理 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:4:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"4. NULL ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:5:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"1. 概述 NULL 值为 MySQL 中的一个特殊值。 “NULL columns require additional space in the row to record whether their values are NULL. For MyISAM tables, each NULL column takes one bit extra, rounded up to the nearest byte.” NULL 代表的是一种不确定性，所以需要一个额外空行来标注当前是NULL值。 通常用 IS NULL 或者 NOT NULL 来判定一个实例数据是否为不确定的，而不是直接 == 来进行值比较。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:5:1","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"2. NULL与空值的区别 MySQL中，NULL 是未知的，且占用空间的。NULL使得索引、索引统计和值都更加复杂，并且影响优化器的判断。 空值('')是不占用空间的，注意空值的''之间是没有空格。 在进行 count() 统计某列的记录数的时候，如果采用的 NULL 值，会被系统自动忽略掉，但是空值是会进行统计到其中的。 判断 NULL 使用 IS NULL 或者 NOT NULL，但判断空字符使用 =’’或者 \u003c\u003e’’`来进行处理。 对于 timestamp 数据类型，如果插入 NULL 值，则出现的值是当前系统时间。插入空值，则会出现'0000-00-00 00:00:00’ 。 对于已经创建好的表，普通的列将 NULL 修改为 NOT NULL 带来的性能提升比较小，所以调优时没有必要特意一一查找并将其修改为 NOT NULL。 对于已经创建好的表，如果计划在列上创建索引，那么尽量修改为 NOT NULL，并且使用 0 或者一个特殊值或者空值''。 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:5:2","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"3. NULL值的坑 NULL作为布尔值的时候，不为1也不为0，任何值和NULL使用运算符（\u003e、\u003c、\u003e=、\u003c=、!=、\u003c\u003e）或者（in、not in、any/some、all），返回值都为NULL，判断是否为空只能用IS NULL、IS NOT NULL 当IN和NULL比较时，无法查询出为NULL的记录 当NOT IN 后面有NULL值时，不论什么情况下，整个sql的查询结果都为空 count(字段)无法统计字段为NULL的值，count(*)可以统计值为null的行 NULL导致的坑让人防不胜防，强烈建议创建字段的时候字段不允许为NULL，给个默认值 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:5:3","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"4. FAQ 问题1：索引列允许为NULL，对性能影响有多少？ 存储大量的NULL值，除了计算更复杂之外，数据扫描的代价也会更高一些 问题2：索引列允许为NULL，会额外存储更多字节吗？ 定义列值允许为NULL并不会增加物理存储代价，但对索引效率的影响要另外考虑 ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:5:4","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"5. 参考 https://dev.mysql.com/doc/refman/8.0/en/data-types.html https://www.cnblogs.com/xrq730/p/8446246.html https://learnku.com/laravel/t/2495/select-the-appropriate-mysql-date-time-type-to-store-your-time ","date":"2020-03-20","objectID":"/posts/mysql/05-data-type/:6:0","tags":["MySQL"],"title":"MySQL教程(五)---常见数据类型","uri":"/posts/mysql/05-data-type/"},{"categories":["MySQL"],"content":"SQL92与SQL99 标准中 JOIN 语句的差异点和 MySQL 中 JOIN 语句的具体实现","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"本文主要记录了 SQL92 标准与 SQL99 标准中 JOIN 语句的一些差异情况。 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:0:0","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"1. 概述 SQL有两个主要的标准，分别是SQL92和SQL99。92和99代表了标准提出的时间，SQL92就是92年提出的标准规范。当然除了SQL92和SQL99以外，还存在SQL-86、SQL-89、SQL:2003、SQL:2008、SQL:2011和SQL:2016等其他的标准。 实际上最重要的SQL标准就是SQL92和SQL99。一般来说SQL92的形式更简单，但是写的SQL语句会比较长，可读性较差。而SQL99相比于SQL92来说，语法更加复杂，但可读性更强。 推荐使用 SQL99 标准 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:1:0","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"2. SQL92 和 SQL99 差异点 SQL92 和 SQL99 在 JOIN 连接 写法上的一些区别。 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:0","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"1. CROSS JOIN # SQL92 SELECT * FROM player, team # SQL99 SELECT * FROM player CROSS JOIN team 多表连接 # SQL92 SELECT * FROM t1,t2,t3 # SQL99 SELECT * FROM t1 CROSS JOIN t2 CROSS JOIN t3 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:1","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"2. NATURAL JOIN NATURAL JOIN译作自然连接 SQL99 中新增的，和 SQL92 中的等值连接差不多。 # SQL92 player_id, a.team_id, player_name, height, team_name FROM player as a, team as b WHERE a.team_id = b.team_id # SQL99 SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team 实际上，在 SQL99 中用 NATURAL JOIN 替代了 WHERE player.team_id = team.team_id。 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:2","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"3. ON 条件 同样是 SQL99 中新增的。 SQL92 中多表连接直接使用,逗号进行连接，条件也直接使用WHERE。 SELECT * FROM A,B WHERE xxx SELECT p.player_name, p.height, h.height_level FROM player AS p, height_grades AS h WHERE p.height BETWEEN h.height_lowest AND h.height_highest SQL99 中则使用 JOIN 表示连接, ON 表示条件。 SELECT * FROM A JOIN B ON xxx SELECT p.player_name, p.height, h.height_level FROM player as p JOIN height_grades as h ON height BETWEEN h.height_lowest AND h.height_highest ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:3","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"4. USING连接 USING 就是等值连接的一种简化形式,SQL99 中新增的。 当我们进行连接的时候，可以用 USING 指定数据表里的同名字段进行等值连接。 SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id) 同时使用JOIN USING可以简化JOIN ON的等值连接，它与下面的 SQL 查询结果是相同的: SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:4","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"5. SELF JOIN 自连接的原理在 SQL92 和 SQL99 中都是一样的，只是表述方式不同。 # SQL92 SELECT b.player_name, b.height FROM player as a , player as b WHERE a.player_name = '布雷克-格里芬' and a.height \u003c b.height # SQL99 SELECT b.player_name, b.height FROM player as a JOIN player as b ON a.player_name = '布雷克-格里芬' and a.height \u003c b.height ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:5","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"6. 小结 在 SQL92 中进行查询时，会把所有需要连接的表都放到 FROM 之后，然后在 WHERE 中写明连接的条件。比如： SELECT ... FROM table1 t1, table2 t2, ... WHERE ... 而 SQL99 在这方面更灵活，它不需要一次性把所有需要连接的表都放到 FROM 之后，而是采用 JOIN 的方式，每次连接一张表，可以多次使用 JOIN 进行连接。 另外，我建议多表连接使用 SQL99 标准，因为层次性更强，可读性更强，比如： SELECT ... FROM table1 JOIN table2 ON ... JOIN table3 ON ... SQL99 采用的这种嵌套结构非常清爽，即使再多的表进行连接也都清晰可见。如果你采用 SQL92，可读性就会大打折扣。 最后一点就是，SQL99 在 SQL92 的基础上提供了一些特殊语法，比如NATURAL JOIN和JOIN USING。它们在实际中是比较常用的，省略了 ON 后面的等值条件判断，让 SQL 语句更加简洁。 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:2:6","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"3. MySQL 中的 JOIN 差异 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:3:0","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"1. LEFT JOIN At the parser stage, queries with right outer join operations are converted to equivalent queries containing only left join operations. In the general case, the conversion is performed such that this right join: (T1, ...) RIGHT JOIN (T2, ...) ON P(T1, ..., T2, ...) Becomes this equivalent left join: (T2, ...) LEFT JOIN (T1, ...) ON P(T1, ..., T2, ...) 在 MySQL 的执行引擎 SQL解析阶段，都会将 right join 转换为 left join 解释 因为Mysql只实现了nested-loop算法，该算法的核心就是外表驱动内表 for each row in t1 matching range { for each row in t2 matching reference key { for each row in t3 { if row satisfies join conditions, send to client } } } 由此可知，它必须要保证外表先被访问。 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:3:1","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"2. INNER JOIN All inner join expressions of the form T1 INNER JOIN T2 ON P(T1,T2) are replaced by the list T1,T2, and P(T1,T2) being joined as a conjunct to the WHERE condition (or to the join condition of the embedding join, if there is any) INNER JOIN FROM (T1, ...) INNER JOIN (T2, ...) ON P(T1, ..., T2, ...) 转换为如下： FROM (T1, ..., T2, ...) WHERE P(T1, ..., T2, ...) 相当于将 INNER JOIN 转换成了 CROSS JOIN 由于在 MySQL 中JOIN、CROSS JOIN、INNER JOIN是等效的,所以这样转换是没有问题的。 In MySQL, JOIN, CROSS JOIN, and INNER JOIN are syntactic equivalents (they can replace each other). In standard SQL, they are not equivalent. INNER JOIN is used with an ON clause, CROSS JOIN is used otherwise. 官网文档: https://dev.mysql.com/doc/refman/8.0/en/join.html ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:3:2","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"3. LEFT JOIN to INNER JOIN Mysql引擎在一些特殊情况下，会将left join转换为inner join。 SELECT * FROM T1 LEFT JOIN T2 ON P1(T1,T2) WHERE P(T1,T2) AND R(T2) 这里涉及到两个问题： 1.为什么要做这样的转换？ 2.什么条件下才可以做转换？ 首先，做转换的目的是为了提高查询效率。在上面的示例中，where条件中的R(T2)原本可以极大地过滤不满足条件的记录，但由于nested loop算法的限制，只能先查T1，再用T1驱动T2。当然，不是所有的left join都能转换为inner join，这就涉及到第2个问题。如果你深知left join和inner join的区别就很好理解第二个问题的答案（不知道两者区别的请自行百度）： left join是以T1表为基础，让T2表来匹配，对于没有被匹配的T1的记录，其T2表中相应字段的值全为null。也就是说，left join连表的结果集包含了T1中的所有行记录。与之不同的是，inner join只返回T1表和T2表能匹配上的记录。也就是说，相比left join，inner join少返回了没有被T2匹配上的T1中的记录。那么，如果where中的查询条件能保证返回的结果中一定不包含不能被T2匹配的T1中的记录，那就可以保证left join的查询结果和inner join的查询结果是一样的，在这种情况下，就可以将left join转换为inner join。 我们再回过头来看官网中的例子： T2.B IS NOT NULL T2.B \u003e 3 T2.C \u003c= T1.C T2.B \u003c 2 OR T2.C \u003e 1 如果上面的R(T2)是上面的任意一条，就能保证inner join的结果集中一定没有不能被T2匹配的T1中的记录。以T2.B \u003e 3为例，对于不能被T2匹配的T1中的结果集，其T2中的所有字段都是null，显然不满足T2.B \u003e 3。 相反，以下R(T2)显然不能满足条件，原因请自行分析： T2.B IS NULL T1.B \u003c 3 OR T2.B IS NOT NULL T1.B \u003c 3 OR T2.B \u003e 3 ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:3:3","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"4. 参考 https://blog.csdn.net/Saintyyu/article/details/100170320 https://dev.mysql.com/doc/refman/8.0/en/join.html 极客专栏–\u003cSQL知识必会\u003e ","date":"2020-03-15","objectID":"/posts/mysql/03-join-diff/:4:0","tags":["MySQL"],"title":"MySQL教程(三)---SQL标准中JOIN语句差异及MySQL中的JOIN实现","uri":"/posts/mysql/03-join-diff/"},{"categories":["MySQL"],"content":"SQL JOIN 语句详解","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"本文主要记录了 SQL 中的各种 JOIN 语句。包括 INNER JOIN、LEFT JOIN、RIGHT JOIN、FULL JOIN 等等。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:0:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"1. SQL 标准 根据 SQL 标准不同，JOIN 也有一定的差异。 SQL92 标准支持 笛卡尔积、等值连接、非等值连接、外连接、自连接。 SQL99 标准支持 交叉连接、自然连接、ON 连接、 USING 连接、外连接、自连接。 本文大部分内容来自网络,主要进行了整理和加入了一些自己的理解。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:1:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"2. INNER JOIN INNER JOIN 一般译作内连接。内连接查询能将左表（表 A）和右表（表 B）中能关联起来的数据连接后返回。 基本语法如下: SELECT * FROM A INNER JOIN B ON xxx ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"1. EQUI JOIN EQUI JOIN 通常译作等值连接。 在连接条件中使用**等于号=**运算符比较被连接列的列值，其查询结果中列出被连接表中的所有列，包括其中的重复列。 # 隐式 INNER JOIN (使用逗号分隔多个表) SELECT p.player_id, p.player_name,p.team_id, t.team_name FROM player AS p, team AS t WHERE p.team_id = t.team_id # 显式 INNER JOIN SELECT p.player_id, p.player_name,p.team_id,t.team_name FROM player AS p INNER JOIN team AS t ON p.team_id = t.team_id ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:1","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"2. USING连接 USING 就是等值连接的一种简化形式,SQL99 中新增的。 当我们进行连接的时候，可以用USING指定数据表里的同名字段进行等值连接。 SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id) 同时使用 JOIN USING 可以简化 JOIN ON 的等值连接，它与下面的SQL查询结果是相同的: SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:2","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"3. NATURAL JOIN NATURAL JOIN 通常译作自然连接，也是EQUI JOIN的一种，其结构使得具有相同名称的关联表的列将只出现一次。 在连接条件中使用**等于号=**运算符比较被连接列的列值，但它使用选择列表指出查询结果集合中所包括的列，并删除连接表中的重复列。 所谓自然连接就是在等值连接的情况下，当连接属性X与Y具有相同属性组时，把在连接结果中重复的属性列去掉。 自然连接是在广义笛卡尔积R×S中选出同名属性上符合相等条件元组，再进行投影，去掉重复的同名属性，组成新的关系。 # SQL92 EQUI JOIN player_id, a.team_id, player_name, height, team_name FROM player as a, team as b WHERE a.team_id = b.team_id # SQL99 NATURAL JOIN SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team 实际上，在SQL99中用NATURAL JOIN替代了 WHERE player.team_id = team.team_id。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:3","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"4. NON EQUI JOIN NON EQUI JOIN 通常译作非等值连接 在连接条件使用除等于运算符以外的比较运算符比较被连接的列的列值。这些运算符包括\u003e、\u003e=、\u003c=、\u003c、!\u003e、!\u003c和\u003c\u003e。 SELECT p.player_name, p.height, h.height_level FROM player AS p INNER JOIN height_grades AS h on p.height BETWEEN h.height_lowest AND h.height_highest ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:4","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"5. CROSS JOIN 返回左表与右表之间符合条件的记录的迪卡尔集。 基本语法如下: SELECT * FROM player CROSS JOIN team 需要注意的是 CROSS JOIN 后没有 ON 条件。 实际上 CROSS JOIN 只是 无条件 INNER JOIN 的 专用关键字而已 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:5","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"6. SELF JOIN 就是和自己进行连接查询，给一张表取两个不同的别名，然后附上连接条件。 比如: 我们想要查看比布雷克·格里芬高的球员都有谁，以及他们的对应身高。 正常情况下需要分两步完成： 1）找出布雷克·格里芬的身高信息 SELECT height FROM player WHERE player_name = '布雷克-格里芬' 2）根据 1 中的身高 查询出对应球员。 SELECT player_name, height FROM player WHERE height \u003e xxx 或者使用子查询 SELECT player_name, height FROM player WHERE height \u003e (SELECT height FROM player WHERE player_name = '布雷克-格里芬') 同样的使用自连接一样可以完成该需求。 SELECT b.player_name, b.height FROM player AS a INNER JOIN player AS b ON a.player_name = '布雷克-格里芬' AND a.height \u003c b.height FROM player AS a INNER JOIN player AS b 这样就生成了两个虚拟表进行 INNER JOIN。 然后通过ON和WHERE两个条件ON a.player_name = '布雷克-格里芬' AND a.height \u003c b.height 找到想要的记录。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:2:6","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"3. OUTER JOIN 外连接包括 3 种： LEFT (OUTER ) JOIN RIGHT (OUTER ) JOIN FULL (OUTER ) JOIN ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:3:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"1. LEFT JOIN LEFT JOIN 一般被译作左连接，也写作 LEFT OUTER JOIN。左连接查询会返回左表（表 A）中所有记录，右表中关联数据列也会被一起返回(不管右表中有没有关联的数据)。 基本语法如下: SELECT * FROM A LEFT JOIN B ON xxx 例如: 查询球员和队名。 SELECT p.player_name,t.team_name FROM player AS p LEFT JOIN team AS t ON p.team_id = t.team_id ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:3:1","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"2. RIGHT JOIN RIGHT JOIN 一般被译作右连接，也写作 RIGHT OUTER JOIN。右连接查询会返回右表（表 B）中所有记录，左表中找到的关联数据列也会被一起返回(不管左表中有没有关联的数据)。 基本语法如下: SELECT * FROM A RIGHT JOIN B ON xxx 例如: 查询球员和队名。 SELECT p.player_name,t.team_name FROM player AS p RIGHT JOIN team AS t ON p.team_id = t.team_id LEFT JOIN 和 RIGHT JOIN 是可以转换的，主要还是根据需求来。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:3:2","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"3. FULL JOIN FULL JOIN 一般被译作全连接，在某些数据库中也叫作 FULL OUTER JOIN。 外连接查询能返回左右表里的所有记录，其中左右表里能关联起来的记录被连接后返回。 基本语法如下: SELECT * FROM A FULL OUTER JOIN B ON xxx 例如 SELECT p.player_name,t.team_name FROM player AS p FULL JOIN team AS t ON p.team_id = t.team_id 当前 MySQL 还不支持 FULL OUTER JOIN,不过可以通过 LEFT JOIN+ UNION+RIGHT JOIN 实现。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:3:3","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"4. 延伸用法 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:4:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"1. LEFT JOIN EXCLUDING INNER JOIN 返回左表有但右表没有关联数据的记录集。 例如: SELECT * FROM A INNER JOIN B ON A.id = B.id WHERE B.id IS NULL 主要是通过ON后面的WHERE条件进行过滤。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:4:1","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"2. RIGHTJOIN EXCLUDING INNER JOIN 返回右表有但左表没有关联数据的记录集。 例如: SELECT * FROM A INNER JOIN B ON A.id = B.id WHERE A.id IS NULL 同 LEFT JOIN EXCLUDING INNER JOIN 只是改变了 WHERE 条件。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:4:2","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"3. FULL JOIN EXCLUDING INNER JOIN 返回左表和右表里没有相互关联的记录集。 例如: SELECT * FROM A FULL OUTER JOIN B ON A.id = B.id WHERE A.id IS NULL OR B.id IS NULL ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:4:3","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"5. 小结 关于 JOIN ，SQL92 与 SQL99 在写法上有一些差异，不过内在都是相同的。 由 C.L. Moffatt提供的另一个版本 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:5:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"6. 理解 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:6:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"1. 概述 以下是自己的理解，可能存在错误或不准确的地方。如果发现问题希望一定指正，3q。 笛卡尔积 首先 笛卡尔积 就是把 两张表每一行 一一对应起来形成新表。 假设 A 表有 M 行 B 表有 N 行，那么 JOIN 后形成的 笛卡尔积 就有 M*N 条记录。 其余的 内连接、外连接都是在 这个笛卡尔积的基础上进行过滤。 SELECT * FROM A XXX JOIN B ON yyy WHERE zzz XXX JOIN可以替换为INNER JOIN、LEFT JOIN、RIGHT JOIN、FULL JOIN等等。 隐式条件 在选择使用什么 JOIN 的时候就包含了一个 隐式条件。 比如 INNER JOIN 只会返回两表中能关联的数据。 LEFT JOIN 就会返回左表全部数据，如果右表有关联数据就返回关联数据，没有就返回 NULL。 其他同理。 显式条件 同时ON 后面还可以跟条件用于过滤，这个算是显式条件。 ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:6:1","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"2. 例子 这里是用的 MySQL 8.0 SELECT VERSION() 8.0.20 准备数据，两张简单的表。 CREATE TABLE a( id int PRIMARY key AUTO_INCREMENT, value varchar(24) NOT NULL ) ENGINE = INNODB; CREATE TABLE b( id int PRIMARY key AUTO_INCREMENT, value varchar(24) NOT NULL ) ENGINE = INNODB; INSERT INTO a VALUES(1,'a'),(2,'b'),(3,'c') INSERT INTO b VALUES(1,'c'),(2,'d'),(3,'e') mysql\u003e select * from a; +----+-------+ | id | value | +----+-------+ | 1 | a | | 2 | b | | 3 | c | +----+-------+ 3 rows in set (0.00 sec) mysql\u003e select * from b; +----+-------+ | id | value | +----+-------+ | 1 | c | | 2 | d | | 3 | e | +----+-------+ 3 rows in set (0.00 sec) 执行下列查询语句，你会结果是一样的。 SELECT * FROM a INNER JOIN b; SELECT * FROM a CROSS JOIN b; SELECT * FROM a JOIN b; SELECT * FROM a LEFT JOIN b ON 1=1; SELECT * FROM a RIGHT JOIN b ON 1=1; 结果如下 mysql\u003e SELECT * FROM a RIGHT JOIN b ON 1=1; +------+-------+----+-------+ | id | value | id | value | +------+-------+----+-------+ | 3 | c | 1 | c | | 2 | b | 1 | c | | 1 | a | 1 | c | | 3 | c | 2 | d | | 2 | b | 2 | d | | 1 | a | 2 | d | | 3 | c | 3 | e | | 2 | b | 3 | e | | 1 | a | 3 | e | +------+-------+----+-------+ 9 rows in set (0.00 sec) 说明各种 JOIN 都是在 笛卡尔积 基础上进行过滤的。 由于没有显式条件 所以 隐式条件的作用也体现不出现 增加一个条件 SELECT * FROM a INNER JOIN b ON a.value=b.value; SELECT * FROM a LEFT JOIN b ON a.value=b.value; SELECT * FROM a RIGHT JOIN b ON a.value=b.value; 结果如下 mysql\u003e SELECT * FROM a INNER JOIN b ON a.value=b.value; +----+-------+----+-------+ | id | value | id | value | +----+-------+----+-------+ | 3 | c | 1 | c | +----+-------+----+-------+ 1 row in set (0.00 sec) mysql\u003e SELECT * FROM a LEFT JOIN b ON a.value=b.value; +----+-------+------+-------+ | id | value | id | value | +----+-------+------+-------+ | 1 | a | NULL | NULL | | 2 | b | NULL | NULL | | 3 | c | 1 | c | +----+-------+------+-------+ 3 rows in set (0.00 sec) mysql\u003e SELECT * FROM a RIGHT JOIN b ON a.value=b.value; +------+-------+----+-------+ | id | value | id | value | +------+-------+----+-------+ | 3 | c | 1 | c | | NULL | NULL | 2 | d | | NULL | NULL | 3 | e | +------+-------+----+-------+ 3 rows in set (0.00 sec) ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:6:2","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["MySQL"],"content":"7. 参考 https://dev.mysql.com/doc/refman/8.0/en/join.html https://www.zhihu.com/question/34559578 https://coolshell.cn/articles/3463.html https://www.w3resource.com/slides/sql-joins-slide-presentation.php https://www.cnblogs.com/zxlovenet/p/4005256.html https://www.w3school.com.cn/sql/sql_union.asp https://mazhuang.org/2017/09/11/joins-in-sql https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins https://mp.weixin.qq.com/s/t8JCJSP__qh11U2zl8hCeQ ","date":"2020-03-10","objectID":"/posts/mysql/02-join-detail/:7:0","tags":["MySQL"],"title":"MySQL教程(二)---SQL JOIN 语句详解","uri":"/posts/mysql/02-join-detail/"},{"categories":["etcd"],"content":"etcd v3为什么要选择MVCC及MVCC大致原理","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"本文主要分析了 etcd v3 为什么选择了 MVCC，以及 etcd v3 中的 MVCC 大致实现原理。 第二次更新于 2022-01-14 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:0:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"1. 为什么选择MVCC etcd v2 是一个内存数据库，整个数据库拥有一个Stop-the-World的大锁，通过锁机制来解决并发带来的数据竞争。 但是存在并发性能问题： 1）锁的粒度不好控制，每次都会锁整个数据库 2）写锁和读锁相互阻塞。 3）前面的事务会阻塞后面的事务，对并发性能影响很大。 同时在高并发环境下还存在另一个严重的问题： watch 机制可靠性问题：etcd 中的 watch 机制会依赖旧数据，v2 版本基于滑动窗口实现的 watch 机制，只能保留最近的 1000 条历史事件版本，当 etcd server 写请求较多、网络波动时等场景，很容易出现事件丢失问题，进而又触发 client 数据全量拉取，产生大量 expensive request，甚至导致 etcd 雪崩。 熟悉 Kubernetes 的朋友肯定知道，Kubernetes 使用 etcd 做存储，因此 etcd 的问题对 Kubernetes 有很直观的影响，具体如下： etcd 并发性能问题导致 Kubernetes 集群规模受限。 watch 机制可靠性问题直接影响到 Kubernetes controller 的正常运行。 在 Kubernetes 中，各种各样的控制器实现了 Deployment、StatefulSet、Job 等功能强大的 Workload。控制器的核心思想是监听、比较资源实际状态与期望状态是否一致，若不一致则进行协调工作，使其最终一致。而这些特性的实现都严重依赖 etcd 的 watch 机制。 而 etcd 背后的公司 CoreOS 也是 Kubernetes 容器生态圈的核心成员之一，此时的 Kubernetes 和 Docker 公司还处于一个激烈的对抗之中，因此，此时的 etcd 迫切的需要解决以上的两个问题。 那么 etcd v3 为什么要选择 MVCC 呢？ 解决并发问题的方法有很多，而MVCC 在解决并发问题的同时，还能通多存储多版本数据来解决watch 机制可靠性问题。 因此 etcd v3 版本果断选择了基于 MVCC 来实现多版本并发控制。 于是v3则采用了MVCC，以一种优雅的方式解决了锁带来的问题。 执行写操作或删除操作时不会再原数据上修改而是创建一个新版本。 这样并发的读取操作仍然可以读取老版本的数据，写操作也可以同时进行。 这个模式的好处在于读操作不再阻塞，事实上根本就不需要锁。 客户端读key的时候指定一个版本号，服务端保证返回比这个版本号更新的数据，但不保证返回最新的数据。 MVCC能最大化地实现高效地读写并发，尤其是高效地读，非常适合读多写少的场景。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:1:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"2. MVCC 初体验 如下面的命令所示，第一次 key hello 更新完后，我们通过 get 命令获取下它的 key-value 详细信息。正如你所看到的，除了 key、value 信息，还有各类版本号。 这里我们重点关注 mod_revision，它表示 key 最后一次修改时的 etcd 版本号。 当我们再次更新 key hello 为 world2 后，然后通过查询时指定 key 第一次更新后的版本号，你会发现我们查询到了第一次更新的值，甚至我们执行删除 key hello 后，依然可以获得到这个值。那么 etcd 是如何实现的呢? # 指定使用 v3 版本API $ export ETCDCTL_API=3 # 更新key hello为world1 $ etcdctl put hello world1 OK # 通过指定输出模式为json,查看key hello更新后的详细信息 $ etcdctl get hello -w=json { \"kvs\":[ { \"key\":\"aGVsbG8=\", \"create_revision\":2, \"mod_revision\":2, \"version\":1, \"value\":\"d29ybGQx\" } ], \"count\":1 } # 再次修改key hello为world2 $ etcdctl put hello world2 OK # 确认修改成功,最新值为wolrd2 $ etcdctl get hello hello world2 # 指定查询版本号,获得了hello上一次修改的值 $ etcdctl get hello --rev=2 hello world1 # 删除key hello $ etcdctl del hello 1 # 删除后指定查询版本号3,获得了hello删除前的值 $ etcdctl get hello --rev=3 hello world2 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:2:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"3. 整体架构 下图是 MVCC 模块的一个整体架构图，整个 MVCC 特性由 treeIndex、Backend/boltdb 组成。 当你执行 put 命令后，请求经过 gRPC KV Server、Raft 模块流转，对应的日志条目被提交后，Apply 模块开始执行此日志内容。 Apply 模块通过 MVCC 模块来执行 put 请求，持久化 key-value 数据。 MVCC 模块将请求请划分成两个类别，分别是读事务（ReadTxn）和写事务（WriteTxn）。读事务负责处理 range 请求，写事务负责 put/delete 操作。读写事务基于 treeIndex、Backend/boltdb 提供的能力，实现对 key-value 的增删改查功能。 treeIndex 模块基于内存版 B-tree 实现了 key 索引管理，它保存了用户 key 与版本号（revision）的映射关系等信息。 Backend 模块负责 etcd 的 key-value 持久化存储，主要由 ReadTx、BatchTx、Buffer 组成，ReadTx 定义了抽象的读事务接口，BatchTx 在 ReadTx 之上定义了抽象的写事务接口，Buffer 是数据缓存区。 etcd 设计上支持多种 Backend 实现，目前实现的 Backend 是 boltdb。boltdb 是一个基于 B+ tree 实现的、支持事务的 key-value 嵌入式数据库。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:3:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"4. treeIndex 模块 对于 etcd v2 来说，当你通过 etcdctl 发起一个 put hello 操作时，etcd v2 直接更新内存树，这就导致历史版本直接被覆盖，无法支持保存 key 的历史版本。 在 etcd v3 中引入 treeIndex 模块正是为了解决这个问题，支持保存 key 的历史版本，提供稳定的 Watch 机制和事务隔离等能力。 etcd 在每次修改 key 时会生成一个全局递增的版本号（revision）。 然后通过数据结构 B-tree 保存用户 key 与版本号之间的关系； 再以版本号作为 boltdb key，以用户的 key-value 等信息作为 boltdb value，保存到 boltdb。 根据上述存储逻辑可知，boltdb 中只能通过reversion来查询数据,但是客户端都是通过 key 来查询的。所以 etcd 在内存中使用 treeIndex 模块 维护了一个kvindex,保存的就是 key-reversion 之间的映射关系，用来加速查询。 为什么 etcd 使用 B-tree 而不使用哈希表、平衡二叉树？ 从功能特性上分析， 因 etcd 支持范围查询，因此保存索引的数据结构也必须支持范围查询才行。所以哈希表不适合，而 B-tree 支持范围查询。 从性能上分析，平横二叉树每个节点只能容纳一个数据、导致树的高度较高，而 B-tree 每个节点可以容纳多个数据，树的高度更低，更扁平，涉及的查找次数更少，具有优越的增、删、改、查性能。 需要范围查询排除了哈希表，从性能上排除了平衡二叉树。 B-tree 结构如下： 在 treeIndex 中，每个节点的 key 是一个 keyIndex 结构，etcd 就是通过它保存了用户的 key 与版本号的映射关系。 每个 B-tree 节点保存的具体内容如下： type keyIndex struct { key []byte // 用户的key名称，比如我们案例中的\"hello\" modified revision // 最后一次修改key时的etcd版本号,比如我们案例中的刚写入hello为world1时的，版本号为2 generations []generation // generation保存了一个key若干代版本号信息，每代中包含对key的多次修改的版本号列表 } keyIndex 中包含用户的 key、最后一次修改 key 时的 etcd 版本号、key 的若干代（generation）版本号信息，每代中包含对 key 的多次修改的版本号列表。 generations 表示一个 key 从创建到删除的过程，每代对应 key 的一个生命周期的开始与结束。 当你第一次创建一个 key 时，会生成第 0 代，后续的修改操作都是在往第 0 代中追加修改版本号。 当你把 key 删除后，它就会生成新的第 1 代，一个 key 不断经历创建、删除的过程，它就会生成多个代。 generation 结构详细信息如下： type generation struct { ver int64 //表示此key的修改次数 created revision //表示generation结构创建时的版本号 revs []revision //每次修改key时的revision追加到此数组 } generation 结构中包含此 key 的修改次数、generation 创建时的版本号、对此 key 的修改版本号记录列表。 你需要注意的是版本号（revision）并不是一个简单的整数，而是一个结构体。revision 结构及含义如下： type revision struct { main int64 // 一个全局递增的主版本号，随put/txn/delete事务递增，一个事务内的key main版本号是一致的 sub int64 // 一个事务内的子版本号，从0开始随事务内put/delete操作递增 } revision 包含 main 和 sub 两个字段： main 是全局递增的版本号，它是个 etcd 逻辑时钟，随着 put/txn/delete 等事务递增。 sub 是一个事务内的子版本号，从 0 开始随事务内的 put/delete 操作递增。 比如启动一个空集群，全局版本号默认为 1，执行下面的 txn 事务，它包含两次 put、一次 get 操作，那么按照我们上面介绍的原理，全局版本号随读写事务自增，因此是 main 为 2，sub 随事务内的 put/delete 操作递增，因此 key hello 的 revison 为{2,0}，key world 的 revision 为{2,1}。 # 以交互模式进入 etcd 事务 $ etcdctl txn -i /usr/local/bin # etcdctl txn -i compares: # 判定条件 这里不填，直接按回车键跳过 success requests (get, put, del): # 判定条件成功时需要执行的命令 手动输入 put hello 1 get hello put world 2 failure requests (get, put, del): # 判定条件失败时时需要执行的命令 这里也为空 直接跳过 SUCCESS # SUCCESS 为前面 compares 的结果 由于没有填任何条件 所以当然是成功的 OK # 第一条 put 命令的结果 hello # 第二条 get 命令的结果 1 OK # 第三条 put 命令的结果 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:4:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"5. MVCC 大致流程 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:5:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"1. put 一个 put 命令流程如下图所示： **1）第一步：查询 keyIndex ** 首先它需要从 treeIndex 模块中查询 key 的 keyIndex 索引信息。 keyIndex 中存储了 key 的创建版本号、修改的次数等信息，这些信息在事务中发挥着重要作用，因此会存储在 boltdb 的 value 中。 2）第二步：写入 boltdb 其次 etcd 会根据当前的全局版本号（空集群启动时默认为 1）自增，生成 put hello 操作对应的版本号 revision{2,0}，这就是 boltdb 的 key。 boltdb 的 value 是 mvccpb.KeyValue 结构体，它是由用户 key、value、create_revision、mod_revision、version、lease 组成。它们的含义分别如下： create_revision 表示此 key 创建时的版本号。在本例中，key hello 是第一次创建，那么值就是 2。当你再次修改 key hello 的时候，写事务会从 treeIndex 模块查询 hello 第一次创建的版本号，也就是 keyIndex.generations[i].created 字段，赋值给 create_revision 字段； mod_revision 表示 key 最后一次修改时的版本号，即 put 操作发生时的全局版本号加 1； version 表示此 key 的修改次数。每次修改的时候，写事务会从 treeIndex 模块查询 hello 已经历过的修改次数，也就是 keyIndex.generations[i].ver 字段，将 ver 字段值加 1 后，赋值给 version 字段。 填充好 boltdb 的 KeyValue 结构体后，这时就可以通过 Backend 的写事务 batchTx 接口将 key{2,0},value 为 mvccpb.KeyValue 保存到 boltdb 的缓存中，并同步更新 buffer，如上图中的流程二所示。 3）第三步：更新 treeIndex 然后 put 事务需将本次修改的版本号与用户 key 的映射关系保存到 treeIndex 模块中，也就是上图中的流程三。 因为 key hello 是首次创建，treeIndex 模块它会生成 key hello 对应的 keyIndex 对象，并填充相关数据结构。 keyIndex 填充后的结果如下所示： key hello的keyIndex: key: \"hello\" modified: \u003c2,0\u003e generations: [{ver:1,created:\u003c2,0\u003e,revisions: [\u003c2,0\u003e]} ] key 为 hello，modified 为最后一次修改版本号 \u003c2,0\u003e，key hello 是首次创建的，因此新增一个 generation 代跟踪它的生命周期、修改记录； generation 的 ver 表示修改次数，首次创建为 1，后续随着修改操作递增； generation.created 表示创建 generation 时的版本号为 \u003c2,0\u003e； revision 数组保存对此 key 修改的版本号列表，每次修改都会将将相应的版本号追加到 revisions 数组中。 4）第四步：持久化 通过以上流程，一个 put 操作终于完成，但是此时数据还并未持久化。 为了提升 etcd 的写吞吐量、性能，一般情况下（默认堆积的写事务数大于 1 万才在写事务结束时同步持久化），数据持久化由 Backend 的异步 goroutine 完成，它通过事务批量提交，定时将 boltdb 页缓存中的脏数据提交到持久化存储磁盘中。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:5:1","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"2. get 执行 get 命令时，，MVCC 模块首先会创建一个读事务对象（TxnRead）。在 etcd 3.4 中 Backend 实现了 ConcurrentReadTx， 也就是并发读特性。 并发读特性的核心原理是创建读事务对象时，它会全量拷贝当前写事务未提交的 buffer 数据，并发的读写事务不再阻塞在一个 buffer 资源锁上，实现了全并发读。 1）第一步：查询版本号 首先需要根据 key 从 treeIndex 模块获取版本号（因我们未带版本号读，默认是读取最新的数据）。treeIndex 模块从 B-tree 中，根据 key 查找到 keyIndex 对象后，匹配有效的 generation，返回 generation 的 revisions 数组中最后一个版本号{2,0}给读事务对象。 2）第二步：查询 blotdb 读事务对象根据此版本号为 key，通过 Backend 的并发读事务（ConcurrentReadTx）接口，优先从 buffer 中查询，命中则直接返回，否则从 boltdb 中查询此 key 的 value 信息。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:5:2","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"3. del 当执行 del 命令时 etcd 实现的是延期删除模式，原理与 key 更新类似。 与更新 key 不一样之处在于： 一方面，生成的 boltdb key 版本号{4,0,t}追加了删除标识（tombstone, 简写 t），boltdb value 变成只含用户 key 的 KeyValue 结构体。 另一方面 treeIndex 模块也会给此 key hello 对应的 keyIndex 对象，追加一个空的 generation 对象，表示此索引对应的 key 被删除了。 当你再次查询 hello 的时候，treeIndex 模块根据 key hello 查找到 keyindex 对象后，若发现其存在空的 generation 对象，并且查询的版本号大于等于被删除时的版本号，则会返回空。 那么 key 打上删除标记后有哪些用途呢？什么时候会真正删除它呢？ 一方面删除 key 时会生成 events，Watch 模块根据 key 的删除标识，会生成对应的 Delete 事件。 另一方面，当你重启 etcd，遍历 boltdb 中的 key 构建 treeIndex 内存树时，你需要知道哪些 key 是已经被删除的，并为对应的 key 索引生成 tombstone 标识。 而真正删除 treeIndex 中的索引对象、boltdb 中的 key 是通过压缩 (compactor) 组件异步完成。 正因为 etcd 的删除 key 操作是基于以上延期删除原理实现的，因此只要压缩组件未回收历史版本，我们就能从 etcd 中找回误删的数据。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:5:3","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"6. 小结 1）MVCC 模块由 treeIndex、boltdb 组成 treeIndex 模块基于 Google 开源的 btree 库实现 2）treeIndex 保存了用户 key 与版本号关系。blotdb 中的 key 为 treeIndex 中记录的版本号，value 包含用户 key-value、各种版本号、lease 的 mvccpb.KeyValue 结构体。 3）当你未带版本号查询 key 时，etcd 返回的是 key 最新版本数据。 4）删除一个数据时，etcd 并未真正删除它，而是基于 lazy delete 实现的异步删除，真正删除 key 是通过 etcd 的压缩组件去异步实现的。 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:6:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"7. 参考 《etcd实战课》 《云原生分布式存储基石:etcd深入解析》 https://segmentfault.com/a/1190000021787011 ","date":"2020-02-27","objectID":"/posts/etcd/06-why-mvcc/:7:0","tags":["etcd"],"title":"etcd教程(六)---etcd多版本并发控制","uri":"/posts/etcd/06-why-mvcc/"},{"categories":["etcd"],"content":"etcd v3 watch机制原理分析","date":"2020-02-17","objectID":"/posts/etcd/05-watch/","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"本文主要分析了 etcd 的 watch 机制实现原理。 第二次更新于 2022-1-22 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:0:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"1. 概述 为了避免客户端的反复轮询， etcd 提供了 watch 机制。客户端 watch 一系列 key，当这些被 watch的 key 更新时， etcd 就会通知客户端。 熟悉 Kubernetes 的朋友一定知道，etcd 的 watch 特性是 Kubernetes 控制器的工作基础。 在 Kubernetes 中，各种各样的控制器实现了 Deployment、StatefulSet、Job 等功能强大的 Workload。控制器的核心思想是监听、比较资源实际状态与期望状态是否一致，若不一致则进行协调工作，使其最终一致，这主要依赖于 etcd 的 Watch 机制。 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:1:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"2. Watch 特性初体验 启动一个空集群，更新两次 key hello 后，使用 Watch 特性获取 key hello 的历史修改记录。 $ etcdctl put hello world1 $ etcdctl put hello world2 $ etcdctl watch hello -w=json --rev=1 { \"Events\":[ { \"kv\":{ \"key\":\"aGVsbG8=\", \"create_revision\":2, \"mod_revision\":2, \"version\":1, \"value\":\"d29ybGQx\" } }, { \"kv\":{ \"key\":\"aGVsbG8=\", \"create_revision\":2, \"mod_revision\":3, \"version\":2, \"value\":\"d29ybGQy\" } } ], \"CompactRevision\":0, \"Canceled\":false, \"Created\":false } 可以看到，基于 Watch 特性，你可以快速获取到你感兴趣的数据变化事件，这也是 Kubernetes 控制器工作的核心基础。 在这过程中，其实有以下四大核心问题： 第一，client 获取事件的机制，etcd 是使用轮询模式还是推送模式呢？两者各有什么优缺点？ 第二，事件是如何存储的？ 会保留多久？watch 命令中的版本号具有什么作用？ 第三，当 client 和 server 端出现短暂网络波动等异常因素后，导致事件堆积时，server 端会丢弃事件吗？若你监听的历史版本号 server 端不存在了，你的代码该如何处理？ 第四，如果你创建了上万个 watcher 监听 key 变化，当 server 端收到一个写请求后，etcd 是如何根据变化的 key 快速找到监听它的 watcher 呢？ ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:2:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"3. 4大核心问题 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:3:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"1. 轮询 vs 流式推送 首先第一个问题是：client 获取事件机制，etcd 是使用轮询模式还是推送模式呢？两者各有什么优缺点？ etcd v2 轮询模式 在 etcd v2 Watch 机制实现中，使用的是 HTTP/1.x 协议，实现简单、兼容性好，每个 watcher 对应一个 TCP 连接。client 通过 HTTP/1.1 协议长连接定时轮询 server，获取最新的数据变化事件。 然而当你的 watcher 成千上万的时，即使集群空负载，大量轮询也会产生一定的 QPS，server 端会消耗大量的 socket、内存等资源，导致 etcd 的扩展性、稳定性无法满足 Kubernetes 等业务场景诉求。 etcd v3 推送模式 在 etcd v3 中，为了解决 etcd v2 的以上缺陷，使用的是基于 HTTP/2 的 gRPC 协议，双向流的 Watch API 设计，实现了连接多路复用。 HTTP/2 多路复用 在 HTTP/2 协议中，HTTP 消息被分解独立的帧（Frame），交错发送，帧是最小的数据单位。每个帧会标识属于哪个流（Stream），流由多个数据帧组成，每个流拥有一个唯一的 ID，一个数据流对应一个请求或响应包。 通过以上机制，HTTP/2 就解决了 HTTP/1 的请求阻塞、连接无法复用的问题，实现了多路复用、乱序发送。 etcd 基于以上介绍的 HTTP/2 协议的多路复用等机制，实现了一个 client/TCP 连接支持多 gRPC Stream， 一个 gRPC Stream 又支持多个 watcher，如下图所示。同时事件通知模式也从 client 轮询优化成 server 流式推送，极大降低了 server 端 socket、内存等资源。 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:3:1","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"2. 滑动窗口 vs MVCC 第二个问题是：etcd 中的事件是如何存储的，其本质是历史版本存储。 etcd 经历了从滑动窗口到 MVCC 机制的演变，滑动窗口是仅保存有限的最近历史版本到内存中，而 MVCC 机制 则将历史版本保存在磁盘中，避免了历史版本的丢失，极大的提升了 Watch 机制的可靠性。 相关文章： etcd教程(六)—etcd多版本并发控制 etcd教程(十二)—etcd mvcc 源码分析 etcd v2 滑动窗口 etcd v2 它使用的是如下一个简单的环形数组来存储历史事件版本，当 key 被修改后，相关事件就会被添加到数组中来。若超过 eventQueue 的容量，则淘汰最旧的事件。在 etcd v2 中，eventQueue 的容量是固定的 1000，因此它最多只会保存 1000 条事件记录，不会占用大量 etcd 内存导致 etcd OOM。 type EventHistory struct { Queue eventQueue StartIndex uint64 LastIndex uint64 rwl sync.RWMutex } 但是它的缺陷显而易见的，固定的事件窗口只能保存有限的历史事件版本，是不可靠的。当写请求较多的时候、client 与 server 网络出现波动等异常时，很容易导致事件丢失，client 不得不触发大量的 expensive 查询操作，以获取最新的数据及版本号，才能持续监听数据。 etcd v3 mvcc etcd v3 的 MVCC 机制，正如上一节课所介绍的，就是为解决 etcd v2 Watch 机制不可靠而诞生。相比 etcd v2 直接保存事件到内存的环形数组中，etcd v3 则是将一个 key 的历史修改版本保存在 boltdb 里面。boltdb 是一个基于磁盘文件的持久化存储，因此它重启后历史事件不像 etcd v2 一样会丢失，同时你可通过配置压缩策略，来控制保存的历史版本数。 那么watch 命令中的版本号具有什么作用呢? 版本号是 etcd 逻辑时钟，当 client 因网络等异常出现连接闪断后，通过版本号，它就可从 server 端的 boltdb 中获取错过的历史事件，而无需全量同步，它是 etcd Watch 机制数据增量同步的核心。 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:3:2","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"3. 可靠的事件推送机制 第三个问题：当 client 和 server 端出现短暂网络波动等异常因素后，导致事件堆积时，server 端会丢弃事件吗？若你监听的历史版本号 server 端不存在了，你的代码该如何处理？ 第三个问题的本质是可靠事件推送机制，要搞懂它，我们就得弄懂 etcd Watch 特性的整体架构、核心流程，下图是 Watch 特性整体架构图。 当你通过 etcdctl 或 API 发起一个 watch key 请求的时候，etcd 的 gRPCWatchServer 收到 watch 请求后，会创建一个 serverWatchStream, 它负责接收 client 的 gRPC Stream 的 create/cancel watcher 请求 (recvLoop goroutine)，并将从 MVCC 模块接收的 Watch 事件转发给 client(sendLoop goroutine)。 当 serverWatchStream 收到 create watcher 请求后，serverWatchStream 会调用 MVCC 模块的 WatchStream 子模块分配一个 watcher id，并将 watcher 注册到 MVCC 的 WatchableKV 模块。 在 etcd 启动的时候，WatchableKV 模块会运行 syncWatchersLoop 和 syncVictimsLoop goroutine，分别负责不同场景下的事件推送，它们也是 Watch 特性可靠性的核心之一。 etcd 核心解决方案是复杂度管理，问题拆分。 etcd 根据不同场景，对问题进行了分解，将 watcher 按场景分类，实现了轻重分离、低耦合。我首先给你介绍下 synced watcher、unsynced watcher 它们各自的含义。 synced watcher，顾名思义，表示此类 watcher 监听的数据都已经同步完毕，在等待新的变更。 如果你创建的 watcher 未指定版本号 (默认 0)、或指定的版本号大于 etcd sever 当前最新的版本号 (currentRev)，那么它就会保存到 synced watcherGroup 中。 unsynced watcher，表示此类 watcher 监听的数据还未同步完成，落后于当前最新数据变更，正在努力追赶。 如果你创建的 watcher 指定版本号小于 etcd server 当前最新版本号，那么它就会保存到 unsynced watcherGroup 中。 victim watcher：表示此类 watcher 存在推送失败的 event ，需要由异步任务执行重试操作。 从以上介绍中，我们可以将可靠的事件推送机制拆分成最新事件推送、异常场景重试、历史事件推送机制三个子问题来进行分析。 watcher 状态转换关系如下图所示： 最新事件推送机制 当 etcd 收到一个写请求，key-value 发生变化的时候，处于 syncedGroup 中的 watcher，是如何获取到最新变化事件并推送给 client 的呢？ 当你创建完成 watcher 后，此时你执行 put hello 修改操作时，如上图所示，请求经过 KVServer、Raft 模块后 Apply 到状态机时，在 MVCC 的 put 事务中，它会将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 在 put 事务结束时，如下面的精简代码所示，它会将 KeyValue 转换成 Event 事件，然后回调 watchableStore.notify 函数（流程 5）。notify 会匹配出监听过此 key 并处于 synced watcherGroup 中的 watcher，同时事件中的版本号要大于等于 watcher 监听的最小版本号，才能将事件发送到此 watcher 的事件 channel 中。 serverWatchStream 的 sendLoop goroutine 监听到 channel 消息后，读出消息立即推送给 client（流程 6 和 7），至此，完成一个最新修改事件推送。 evs := make([]mvccpb.Event, len(changes)) for i, change := range changes { evs[i].Kv = \u0026changes[i] if change.CreateRevision == 0 { evs[i].Type = mvccpb.DELETE evs[i].Kv.ModRevision = rev } else { evs[i].Type = mvccpb.PUT } } tw.s.notify(rev, evs) 注意接收 Watch 事件 channel 的 buffer 容量默认 1024(etcd v3.4.9)。若 client 与 server 端因网络波动、高负载等原因导致推送缓慢，buffer 满了，事件会丢失吗？ 异常场景重试机制 若出现 channel buffer 满了，etcd 为了保证 Watch 事件的高可靠性，并不会丢弃它，而是将此 watcher 从 synced watcherGroup 中删除，然后将此 watcher 和事件列表保存到一个名为受害者 victim 的 watcherBatch 结构中，通过异步机制重试保证事件的可靠性。 那么若因网络波动、CPU 高负载等异常导致 watcher 处于 victim 集合中后，etcd 是如何处理这种 slow watcher 呢？ 在介绍 Watch 机制整体架构时，我们知道 WatchableKV 模块会启动两个异步 goroutine，其中一个是 syncVictimsLoop，正是它负责 slower watcher 的堆积的事件推送。 它的基本工作原理是，遍历 victim watcherBatch 数据结构，尝试将堆积的事件再次推送到 watcher 的接收 channel 中。若推送失败，则再次加入到 victim watcherBatch 数据结构中等待下次重试。 若推送成功，watcher 监听的最小版本号 (minRev) 小于等于 server 当前版本号 (currentRev)，说明可能还有历史事件未推送，需加入到 unsynced watcherGroup 中，由下面介绍的历史事件推送机制，推送 minRev 到 currentRev 之间的事件。 若 watcher 的最小版本号大于 server 当前版本号，则加入到 synced watcher 集合中，进入上面介绍的最新事件通知机制。 历史事件推送机制 WatchableKV 模块的另一个 goroutine，syncWatchersLoop，正是负责 unsynced watcherGroup 中的 watcher 历史事件推送。 在历史事件推送机制中，如果你监听老的版本号已经被 etcd 压缩了，client 该如何处理？ 要了解这个问题，我们就得搞清楚 syncWatchersLoop 如何工作，它的核心支撑是 boltdb 中存储了 key-value 的历史版本。 syncWatchersLoop，它会遍历处于 unsynced watcherGroup 中的每个 watcher，为了优化性能，它会选择一批 unsynced watcher 批量同步，找出这一批 unsynced watcher 中监听的最小版本号。 因 boltdb 的 key 是按版本号存储的，因此可通过指定查询的 key 范围的最小版本号作为开始区间，当前 server 最大版本号作为结束区间，遍历 boltdb 获得所有历史数据。 然后将 KeyValue 结构转换成事件，匹配出监听过事件中 key 的 watcher 后，将事件发送给对应的 watcher 事件接收 channel 即可。发送完成后，watcher 从 unsynced watcherGroup 中移除、添加到 synced watcherGroup 中，如下面的 watcher 状态转换图黑色虚线框所示。 若 watcher 监听的版本号已经小于当前 etcd server 压缩的版本号，历史变更数据就可能已丢失，因此 etcd server 会返回 **ErrCompacted **错误给 client。client 收到此错误后，需重新获取数据最新版本号后，再次 Watch。 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:3:3","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"4. 高效的事件匹配 第四个问题：如果你创建了上万个 watcher 监听 key 变化，当 server 端收到一个写请求后，etcd 是如何根据变化的 key 快速找到监听它的 watcher 呢？一个个遍历 watcher 吗？ 当收到创建 watcher 请求的时候，它会把 watcher 监听的 key 范围插入到上面的区间树中，区间的值保存了监听同样 key 范围的 watcher 集合 /watcherSet。 当产生一个事件时，etcd 首先需要从 map 查找是否有 watcher 监听了单 key，其次它还需要从区间树找出与此 key 相交的所有区间，然后从区间的值获取监听的 watcher 集合。 区间树支持快速查找一个 key 是否在某个区间内，时间复杂度 O(LogN)，因此 etcd 基于 map 和区间树实现了 watcher 与事件快速匹配，具备良好的扩展性。 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:3:4","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"5. 小结 整体流程如下图所示，可以分为两个部分: 1）创建 watcher 2）watch 事件推送 1）创建 watcher 发起一个 watch key 请求的时候，etcd 的 gRPCWatchServer 收到 watch 请求后，会创建一个 serverWatchStream, 它负责接收 client 的 gRPC Stream 的 create/cancel watcher 请求 (recvLoop goroutine)，并将从 MVCC 模块接收的 Watch 事件转发给 client(sendLoop goroutine)。 当 serverWatchStream 收到 create watcher 请求后，serverWatchStream 会调用 MVCC 模块的 WatchStream 子模块分配一个 watcher id，并将 watcher 注册到 MVCC 的 WatchableKV 模块。 在 etcd 启动的时候，WatchableKV 模块会运行 syncWatchersLoop 和 syncVictimsLoop goroutine，分别负责不同场景下的事件推送，它们也是 Watch 特性可靠性的核心之一。 2）watch 事件推送 当你创建完成 watcher 后，此时你执行 put hello 修改操作时，请求经过 KVServer、Raft 模块后 Apply 到状态机时，在 MVCC 的 put 事务中，它会将本次修改的后的 mvccpb.KeyValue 保存到一个 changes 数组中。 在 put 事务结束时，它会将 KeyValue 转换成 Event 事件，然后回调 watchableStore.notify 函数（如下精简代码所示）。notify 会匹配出监听过此 key 并处于 synced watcherGroup 中的 watcher，同时事件中的版本号要大于等于 watcher 监听的最小版本号，才能将事件发送到此 watcher 的事件 channel 中。 serverWatchStream 的 sendLoop goroutine 监听到 channel 消息后，读出消息立即推送给 client，至此，完成一个最新修改事件推送。 evs := make([]mvccpb.Event, len(changes)) for i, change := range changes { evs[i].Kv = \u0026changes[i] if change.CreateRevision == 0 { evs[i].Type = mvccpb.DELETE evs[i].Kv.ModRevision = rev } else { evs[i].Type = mvccpb.PUT } } tw.s.notify(rev, evs) etcd v3 相对 v2 的提升： 1）基于 HTTP/2 的 gRPC Stream，使用多路复用机制 + 流式推送极大降低了 server 端 socket、内存等资源。 2）使用 MVCC 机制避免了历史事件的丢失 其他： 3）通过最新事件推送、异常事件重试、历史事件推送保证了推送的可靠性 4）基于 map 和区间树实现了 watcher 与事件快速匹配 ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:4:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"6. 参考 《云原生分布式存储基石:etcd深入解析》 《etcd 实战课》 https://etcd.io/docs/v3.4.0/ ","date":"2020-02-17","objectID":"/posts/etcd/05-watch/:5:0","tags":["etcd"],"title":"etcd教程(五)---watch机制原理分析","uri":"/posts/etcd/05-watch/"},{"categories":["etcd"],"content":"etcd简单分析及其与zookeeper的区别","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"本文主要对etcd进行了简单的分析，同时和zookeeper进行了简单的对比。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:0:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"1. etcd架构 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:1:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"1. 概述 etcd 基于 Raft 协议，通过复制日志文件的方式来保证数据的强一致性。 客户端应用写一个 key 时，首先会存储到 etcd Leader 上，然后再通过 Raft 协议复制到 etcd 集群的所有成员中，以此维护各成员（节点）状态的一致性与实现可靠性。 虽然 etcd 是一个强一致性的系统，但也支持从非 Leader 节点读取数据以提高性能，而且写操作仍然需要 Leader 支持，所以当发生网络分时，写操作仍可能失败。 etcd 具有一定的容错能力，假设集群中共有N个节点，即便集群中( n-1) /2个节点发生了故障，只要剩下的( n+1) /2 个节点达成一致， 也能操作成功,因此，它能够有效地应对网络分区和机器故障带来的数据丢失风险。 etcd 默认数据一更新就落盘持久化，数据持久化存储使用 WAL (write ahead log） ，预写式日志。 格式 WAL 记录了数据变化的全过程，在 etcd 中所有数据在提交之前都要先写入 WAL 中； etcd Snapshot （快照）文件则存储了某一时刻 etcd 的所有数据，默认设置为每 10 000 条记录做一次快照，经过快照后WAL 文件即可删除。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:1:1","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"2. 四要素 etc 在设计的时候重点考虑了如下的四个要素： 1. 简单 支持RESTful风格的HTTP+JSON的API v3版本增加了对gRPC的支持 同时也提供rest gateway进行转化 Go语言编写，跨平台，部署和维护简单 使用Raft算法保证强一致性，Raft算法可理解性好 2. 安全 支持TLS客户端安全认证 3. 性能 单实例(V3)支持每秒10KQps 4. 可靠 使用 Raft 算法充分保证了分布式系统数据的强一致性 etcd 集群是一个分布式系统，由多个节点相互通信构成整体的对外服务，每个节点都存储了完整的数据，并且通过 Raft 协议保证了每个节点维护的数据都是一致的。 etcd可以扮演两大角色： 持久化的键值存储系统 分布式系统数据一致性服务提供者 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:1:2","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"3. 架构模块 etcd(Server)大体上可以分为网络层(http(s) server)、Raft模块、复制状态机(RSM)和存储模块,具体如下： 网络层:提供网络数据读写功能，监听服务端口，完成集群节点之间数据通信，收发客户端数据。 Raft模块：Raft强一致性算法的具体实现。 存储模块：涉及KV存储、WAL文件、Snapshot管理等，用户处理etcd支持的各类功能的事务，包括数据索引 节点状态变更、监控与反馈、事件处理与执行 ，是 etcd 对用户提供的大多数 API 功能的具体实现。 复制状态机：这是一个抽象的模块，状态机的数据维护在内存中，定期持久化到磁盘，每次写请求都会持久化到 WAL 文件，并根据写请求的内容修改状态机数据。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:1:3","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"4. 执行流程 通常，一个用户的请求发送过来，会经由 HTTP ( S) Server 转发给存储模块进行具体的事务处理,如果涉及节点状态的更新，则交给 Raft 模块进行仲裁和日志的记录，然后再同步给别的 etcd 节点，只有当半数以上的节点确认了该节点状态的修改之后，才会进行数据的持久化。 etcd 集群的各个节点之间需要通过网络来传递数据，具体表现为如下几个方面： Leader Follower 发送心跳包， Follower Leader 回复消息 Leader Follower 发送日志追加信息 Leader Follower 发送 Snapshot 数据 Candidate 节点发起选举，向其他节点发起投票请求 Follower 将收到的写操作转发给 Leader ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:1:4","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"2. etcd数据通道 在etcd 的实现中， 根据不同的用途，定义了各种不同的消息类型些不同的消息，最终都将通过 protocol buffer 格式进行编码。 大消息如传输 Snapshot 的数据 就比较大，甚至会超过1GB ，而小消息则如 Leader Follower 节点之间的心跳消息可能只有几十 KB。 etcd 在实现中，对这些消息采取了分类处理的方式，它抽象出了两种类型的消息传输通道，即 Stream类型通道和 Pipeline 类型通道。 Stream: 用于处理数据量较少的消息,例如心跳、日志追加消息等。点到点之间维护一个HTTP长连接。 Pipeline:用于处理数据量大的消息，如Snapshot。不维护长连接。 Snapshot这种数据量大的消息必须和心跳分开传，否则会阻塞心跳消息。 Pipeline也能用于传小消息前提是Stream不能用了。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:2:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"3. 模块间交互 1. 网络层与Raft模块交互 etcd 通过 Raft 模块中抽象的 RaftNode 拥有一个消息盒子，RaftNode 将各种类型的消息都放入消息盒子中，由专门的 go routine 将消息盒子里的消息写入管道（Go 语言的 Channel ），而管道的另外一端就链接在网络层的不同类型的传输通道上，同样也有专门的 go routine 在等待(select)消息的到达。 网络层与Raft模块之间通过Go语言的Channel来完成数据通信。 2.Server与Client交互 etcd server 在启动之初 ，会监听服务端口，待服务端口收到客户端的请求之后，就会解析出消息体，然后通过管道传给 Raft 模块，当 Raft 模块按照Raft 协议完成操作时，会回复该请求(或者请求超时关闭了)。 3.Server之间的交互 etcd server 之间通过 peer 端口(初始化时可以手动指定)使用 HTTP 进行通信。 etcd server peer端口主要用来协调 Raft 的相关消息，包括各种提议的协商。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:3:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"4. etcd实现 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"1. 名字由来 etcd它是etc和distributed的结合体。 在类unix系统中/etc目录是用于存放配置文件的，二distributed则是分布式的意思。 那么etcd的意思就很明显了：大型分布式系统的配置中心。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:1","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"2. raft协议 每次写入都是在一个事务（tx）中完成的。 一个事务（tx）可以包含若干个写操作。 etcd集群有一个leader，写请求都会提交给它。 leader先将数据保存成日志形式，并定时的将日志发往其他节点保存。 当超过1/2节点成功保存了日志，则leader会将tx最终提交（也是一条日志）。 一旦leader提交tx，则会在下一次心跳时将提交记录发送给其他节点，其他节点也会提交。 leader宕机后，剩余节点协商找到拥有最大已提交tx ID（必须是被超过半数的节点已提交的）的节点作为新leader。 具体Raft协议可参考大神制作的Raft协议动画 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:2","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"3. mvcc多版本 每个tx事务有唯一事务ID，在etcd中叫做mainID，全局递增不重复。 一个tx可以包含多个修改操作（put和delete），每一个操作叫做一个revision(修订)，共享同一个mainID。 一个tx内连续的多个修改操作会被从0递增编号，这个编号叫做subID。 每个revision由（mainID，subID）唯一标识。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:3","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"4. 索引+存储 内存索引+磁盘存储value 在多版本中，每一次操作行为都被单独记录下来，保存到bbolt中。 在bbolt中，每个revision将作为key，即将序列化后的(revision.main+revision.sub)作为key; 在bbolt中存储的value是这样一个json序列化后的结构，包括key创建时的revision（对应某一代generation的created），本次更新版本，sub ID（Version ver），Lease ID（租约ID）如下： kv := mvccpb.KeyValue{ Key: key, Value: value, CreateRevision: c, ModRevision: rev, Version: ver, Lease: int64(leaseID), } 因此，我们先通过内存btree在keyIndex.generations[0].revs中找到最后一条revision，即可去bbolt中读取对应的数据。 相应的，etcd支持按key前缀查询，其实也就是遍历btree的同时根据revision去bbolt中获取用户的value。 type keyIndex struct { key []byte modified revision // 最后一次修改对应的revision信息。 generations []generation //记录多版本信息 } // mvcc多版本 type generation struct { ver int64 created revision // 引起本次key创建的revision信息 revs []revision } type revision struct { main int64 sub int64 } 内存索引(btree)中存放keyIndex，磁盘中存放对应的多版本数据(序列化后的(revision.main+revision.sub)作为key) 用户查询时去内存中(btree)中根据key找到对应的keyIndex,在keyIndex中找到最后一次revision信息 然后根据（revision.main+revision.sub）作为key去磁盘查询具体数据。 由于会存储下每个版本的数据，所以多次修改后会产生大量数据，可以使用compact 压缩清理掉太久的数据。compact(n)表示压缩掉revision.main \u003c= n的所有历史版本 多版本总结来说：内存btree维护的是用户key =\u003e keyIndex的映射，keyIndex内维护多版本的revision信息，而revision可以映射到磁盘bbolt中的用户value。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:4","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"5. watch etcd的事件通知机制是基于mvcc多版本实现的。 客户端可以提供一个要监听的revision.main作为watch的起始ID，只要etcd当前的全局自增事务ID \u003e watch起始ID，etcd就会将MVCC在bbolt中存储的所有历史revision数据，逐一顺序的推送给客户端。 zookeeper只会提示数据有更新，由用户主动拉取最新数据，中间多版本数据无法知道。 etcd会推送每一次修改的数据给用户。 实际是etcd根据mainID去磁盘查数据，磁盘中数据以revision.main+revision.sub为key(bbolt 数据库中的key)，所以就会依次遍历出所有的版本数据。同时判断遍历到的value中的key(etcd中的key)是不是用户watch的，是则推送给用户。 这里每次都会遍历数据库性能可能会很差，实际使用时一般用户只会关注最新的revision，不会去关注旧数据。 同时也不是每个watch都会去遍历一次数据库，将多个watch作为一个watchGroup，一次遍历可以处理多个watch，判断到value中的key属于watchGroup中的某个watch关注的则返回，从而有效减少遍历次数。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:4:5","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"5. etcd与zookeeper比较 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:5:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"1. CAP原则 zookeeper和etcd都是顺序一致性的（满足CAP的CP），意味着无论你访问任意节点，都将获得最终一致的数据视图。这里最终一致比较重要，因为zookeeper使用的paxos和etcd使用的raft都是quorum机制(大多数同意原则)，所以部分节点可能因为任何原因延迟收到更新，但数据将最终一致，高度可靠。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:5:1","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"2. 逻辑结构 zookeeper从逻辑上来看是一种目录结构，而etcd从逻辑上来看就是一个k-v结构。 但etcd的key可以是任意字符串同时在存储上实现了key有序排列。 所以仍旧可以模拟出父子目录关系，例如：key=/a/b/c、/a/b、/a 结论：etcd本质上是一个有序的k-v存储。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:5:2","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"3. 临时节点 在实现服务发现时，我们一般都会用到zookeeper的临时节点。当客户端掉线一段时间，对应的zookeeper session会过期，那么对应的临时节点就会被自动删除。 在etcd中对应的是lease租约机制，通过该机制实现了key的自动删除。 可以在set key的同时携带lease ID，当lease过期后所有关联的key都将被自动删除。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:5:3","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"4. 事件模型 在我们用zookeeper实现服务发现时，我们一般会getChildrenAndWatch来获取一个目录下的所有在线节点，这个API会先获取当前的孩子列表并同时原子注册了一个观察器。 每当zookeeper发现孩子有变动的时候，就会发送一个通知事件给客户端（同时关闭观察器），此时我们会再次调用getChildrenAndWatch再次获取最新的孩子列表并重新注册观察器。 简单的来说，zookeeper提供了一个原子API，它先获取当前状态，同时注册一个观察器，当后续变化发生时会发送一次通知到客户端：获取并观察-\u003e收到变化事件-\u003e获取并观察-\u003e收到变化事件-\u003e….，如此往复。 zookeeper的事件模型非常可靠，不会出现发生了更新而客户端不知道的情况，但是特点也很明显： 事件不包含数据，仅仅是通知变化。 多次连续的更新，通知会合并成一个；即，客户端收到通知再次拉取数据，会跳过中间的多个版本，只拿到最新数据。 这些特点并不是缺点，因为一般应用只关注最新状态，并不关注中间的连续变化。 相反etcd的事件是包含数据的，并且通常情况下连续的更新不会被合并通知，而是逐条通知到客户端。 ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:5:4","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["etcd"],"content":"6. 参考 《云原生分布式存储基石:etcd深入解析》 https://yuerblog.cc/2017/12/10/principle-about-etcd-v3/ http://www.wangjialong.cc/2017/09/27/etcd\u0026zookeeper/#more https://www.cnblogs.com/jasontec/p/9651789.html http://jolestar.com/etcd-architecture/ ","date":"2020-02-10","objectID":"/posts/etcd/04-etcd-architecture/:6:0","tags":["etcd"],"title":"etcd教程(四)---etcd架构及其实现简单分析","uri":"/posts/etcd/04-etcd-architecture/"},{"categories":["Redis"],"content":"redis常用命令汇总","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"本文主要介绍Redis高级数据结构，包括Bitmap(位图)、Hyperloglog、GEO以及使用场景。 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:0:0","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"1. Bitmap ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:0","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"1. 概述 BitMap，即位图，也就是 byte 数组，用二进制表示，只有 0 和 1 两个数字，底层使用SDS存储。 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:1","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"2. API #对key所存储的字符串值，获取指定偏移量上的位（bit） getbit key offset # 对key所存储的字符串值，设置或清除指定偏移量上的位（bit） 1. 返回值为该位在setbit之前的值 2. value只能取0或1 3. offset从0开始，即使原位图只能10位，offset可以取1000 setbit key offset value #获取位图指定范围中位值为1的个数 如果不指定start与end，则取所有 bitcount key [start end] #做多个BitMap的and（交集）、or（并集）、not（非）、xor（异或）操作并将结果保存在destKey中 bitop op destKey key1 [key2...] #计算位图指定范围第一个偏移量对应的的值等于targetBit的位置 1. 找不到返回-1 2. start与end没有设置，则取全部 3. targetBit只能取0或者1 bitpos key tartgetBit [start end] ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:2","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"3. 使用场景 有的场景用Bitmap会有奇效，比如: 日活跃用户 如果是日活跃用户的话只需要创建一个Bitmap即可，每个用户根据ID对应Bitmap中的一位，当某个用户满足活跃用户条件后，就在Bitmap中把标识此用户的位置为1。 假设有1000W用户，那么只需要1000W/8/1024/1024 差不多1.2M的空间，简直完美 那周活跃用户、月活跃用户呢？ 同理，多建几个Bitmap即可。 同样的，每日签到、用户在线状态什么的也可以这么实现。 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:3","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"4. 实例 假设用来统计日活跃用户。 用户满足条件则写入缓存，假设用户ID1满足活跃条件了，那么： #setbit key offset value 这里直接把id当做offset就更方便了 setbit key 1 value 周活跃用户则创建7个Bitmap，最后统计一周7天每天都活跃的用户呢? 直接把7个Bitmap进行与运算求交集，最后还为1的就是7天都活跃的，毕竟7个1与之后才能为1。 #bitop op destKey key1 [key2...] 大概是这样的 bitop and destKey Monday [Tuesday...] 如果是7天中登录过的(1天及其以上)那就是求并集，或运算。 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:4","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"5. bloomfilter 还可以用来实现布隆过滤(在官方提供布隆过滤之前，一般都是用的Bitmap实现)，具体逻辑如下： 1.根据hash算法确定key要映射到哪些bit上(一般为多个,越多冲突越小) 2.setbit 将对应的bit全置为1 3.查询时同样先hash,如果对应的映射不是都为1则说明该key一定不存在，都为1则说明可能存在。 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:1:5","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"2. Hyperloglog ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:2:0","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"1. 概述 HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 基数不大，数据量不大就用不上，会有点大材小用浪费空间 有局限性，就是只能统计基数数量，而没办法去知道具体的内容是什么 和bitmap相比，属于两种特定统计情况，简单来说，HyperLogLog 去重比 bitmap 方便很多 一般可以bitmap和hyperloglog配合使用，bitmap标识哪些用户活跃，hyperloglog计数 ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:2:1","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"2. API #添加指定元素到 HyperLogLog 中 #影响基数估值则返回1否则返回0 PFADD key element [element ...] #返回给定 HyperLogLog 的基数估算值。 #白话就叫做去重值 带有 0.81% 标准错误（standard error）的近似值 PFCOUNT key [key ...] #将多个 HyperLogLog 合并为一个 HyperLogLog PFMERGE destkey sourcekey [sourcekey ...] ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:2:2","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"3. 使用场景 统计 APP或网页 的一个页面，每天有多少用户点击进入的次数。同一个用户的反复点击进入记为 1 次。 // 判定当前元素是否存在 直接添加PFADD 如果影响基数估值则返回1否则返回0 func RedisHyperLogLog() { var ( key = \"clickStatic\" userId = 10010 ) // 删除旧测试数据 rc.Del(key) for i := 10000; i \u003c 10010; i++ { rc.PFAdd(key, i) } // 判定当前元素是否存在 // PFAdd添加后对基数值产生影响则返回1 否则返回0 res := rc.PFAdd(key, userId) if err := res.Err(); err != nil { logrus.Errorf(\"err :%v\", ) return } if res.Val() != 1 { logrus.Println(\"该用户已统计\") } else { logrus.Println(\"该用户未统计\") } } ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:2:3","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"3. GEO 支持存储地理位置信息用来实现诸如附近位置、摇一摇这类依赖于地理位置信息的功能。 暂时没用到，一般用的也不是很多，到时候在写。 //TODO ","date":"2020-01-15","objectID":"/posts/redis/03-advanced-datastructure/:3:0","tags":["Redis"],"title":"Redis教程(三)---redis高级数据结构","uri":"/posts/redis/03-advanced-datastructure/"},{"categories":["Redis"],"content":"redis常用命令汇总","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"redis常用命令汇总，包括Key、String、Hash、List、Set、ZSet、发布订阅等待。 ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:0:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"0. Key 基本操作 DEL key EXISTS key EXPIRE key seconds #模糊查询所有匹配的key KEYS pattern #移除key的过期时间 PERSIST key #以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 TTL key #从当前数据库中随机返回一个key RANDOMKEY #仅当 newkey 不存在时，将 key 改名为 newkey 。 RENAMENX key newkey #返回key的类型 TYPE key ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:1:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"1. String 作为常规的key-value缓存应用 一个键最大能存储512MB 常用的大概有get、set、inc等 # 可以在设置的同时指定过期时间 SET key value (expire) #SET+Expire SETEX key second value GET key #设置新值返回旧值 GETSET key value STRLEN key #返回 key 中字符串值的子字符 GETRANGE key start end #key不存在才设置 分布式锁 SETNX key value #所有key都不存在才设置 MSETNX key value[key value...] #批量get set MGET key1[key2...] MSET key1[key2...] #在原来的值上加或减 key不存在则会初始化为0之后执行 会返回命令执行后的值 只能在integer上操作 INCR key INCRBY key inc INCRBYFLOAT key inc #增加浮点数 DECR key DECRBY key inc #字符串末尾追加数据 APPEEND key value ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:2:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"2. Hash 主要用来存储对象信息。 比如存储用户信息 key为用户名或id field则为各种字段(name,age) value就是对应的信息(illusory,23) 命令都差不多 主要多了个HLen、HKeys、HVals、HScan等 常用的hset、hget、hgetall、HINCRBY、hscan等 HSET key field value HSETNX key field value HGET key field HGETALL key field HMSET key field value[field value...] HMGET key field[field...] HDEL key field[field...] HEXISTS key field HINCRBY key field inc HINCRBYFLOAT key field inc # 获取hash表中字段的数量 HLEN key # 获取hash表中所有字段 HKEYS key # 获取hash表中所有值 HVALS key # 扫描整个hash表 直接用keys命令可能产生阻塞 # cursor填0则表示第一次遍历 从头开始 # MATCH pattern可以模糊匹配 # count 可以看成是每次返回的数量 虽然不完全是这样 HSCAN key cursor [MATCH pattern][Count count] ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:3:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"3. List 主要用于各种列表数据，比如关注列表、粉丝列表、最新消息列表 等 也可以做各种数据收集的工作。 比如统计数据按时间戳以分钟数区别写入redis 然后后续任务从redis中读取写入到db 当然list能做的zset也能做 但是同样的数据zset占用的空间会比list多很多很多。 常用命令 lpush、rpush、lpop、rpop、lrange 等 #将值插入到表头(最左边) #list不存在时会新建 LPUSH key value1[value2] #list不存在时什么都不做 LPUSHX key value #将值插入到表尾(最右边) #list不存在时会新建 RPUSH key value1[value2] #list不存在时什么都不做 RPUSHX key value # 弹出表中元素 L表头 R表尾 # 非阻塞 LPOP key RPOP key # 指定多个表 弹出第一个非空表中的元素 # 阻塞 B=blocking L=left R=right BLPOP key1[key2] timeout BRPOP key1[key2] timeout #将source表尾最后一个元素弹出并写入destination 作为表头第一个元素 RPOPPUSH source destination BRPOPPUSH source destination timeout #返回表中下标为index的元素 LINDEX key index #在表key的元素pivot前|后插入元素value # key或者pivot不存在则不执行任何操作 LINSERT key BEFORE|AFTER pivot value #获取列表长度 LLEN key #返回列表指定范围内的元素 LRANGE key start stop #根据参数 count 的值，移除列表中与参数 value 相等的元素。 #count \u003e 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。 #count \u003c 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。 #count = 0 : 移除表中所有与 value 相等的值。 LREM key count value #通过索引设置列表元素的值 LSET key index value #对一个列表进行修剪(trim)，只保留指定区间内的元素，区间外元素都将被删除 LTRIM key start stop ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:4:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"4. Set 无序集合，元素唯一(自动去重)，可以方便的去交、并、差集， 以非常方便的实现如共同关注、共同喜好、二度好友等功能 。 常用命令 sadd、spop、smembers、sunion SADD key member1[member2] SREM key member1 [member2] #获取集合的成员数 SCARD key #返回给定所有集合的差集 SDIFF key1[key2] #将差集存到指定集合 SDIFFSTORE destination key1 [key2] #交集 SINTER key1 [key2] SINTERSTORE destination key1 [key2] #并集 SUNION key1 [key2] SUNIONSTORE destination key1 [key2] #判断元素是否是集合的成员 SISMEMBER key member #返回集合中所有成员 SMEMBERS key #将 member 元素从 source 集合移动到 destination 集合 SMOVE source destination member #随机移除并返回集合中的一个元素 SPOP key #返回集合中一个或多个随机数 SRANDMEMBER key [count] #迭代集合中的元素 SSCAN key cursor [MATCH pattern] [COUNT count] ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:5:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"5. Sorted Set sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。 比如用来存成绩,自动根据score排序。 很方便的取指定分数的成员或者指定名次的成员 常用命令 zadd、zrange、zrem、zcard ZADD key score1 member1 [score2 member2] ZCARD key #计算在有序集合中指定区间分数的成员数 ZCOUNT key min max #有序集合中对指定成员的分数加上增量 increment ZINCRBY key increment member #计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 ZINTERSTORE destination numkeys key [key ...] #返回集合中指定成员之间的成员数量 根据名字排序 A-Z a-z这样 #eg ZLEXCOUNT scoreSet [a [f 返回a到f之间的成员数 如果有的话 #用来存电话号码吧。。 ZLEXCOUNT phoneSet [133 [134 返回133-144号段的记录 #通过[ (来设置开闭区间 ZLEXCOUNT key min max # 根据索引返回区间内的成员 ZRANGE key start stop [WITHSCORES] #同ZLEXCOUNT根据名字排序 返回区间内的成员 必须要集合中所有成员分数相同时结果才准确 ZRANGEBYLEX key min max [LIMIT offset count] # 通过分数排序 返回min到max分的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] #返回指定成员的索引 ZRANK key member #移除指定成员 ZREM key member [member ...] #通过名字排序移除 ZREMRANGEBYLEX key min max #通过索引，分数从高到低 ZREMRANGEBYRANK key start stop #Rev=reverse 颠倒了排序方式 其他是一样的 #通过索引返回有序集中指定区间内的成员，按照分数从高到低排 ZREVRANGE key start stop [WITHSCORES] #通过分数返回有序集中指定区间内的成员，按照分数从高到低排 ZREVRANGEBYSCORE key max min [WITHSCORES] #返回指定成员的排序 按分数值递减(从大到小)排序 ZREVRANK key member #返回成员的分数 ZSCORE key member #迭代有序集合中的元素（包括元素成员和元素分值） ZSCAN key cursor [MATCH pattern] [COUNT count] ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:6:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"6. 发布订阅 Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 #订阅给定的一个或多个频道的信息。 SUBSCRIBE channel [channel ...] #指退订给定的频道。 UNSUBSCRIBE [channel [channel ...]] #将信息发送到指定的频道。 PUBLISH channel message #订阅一个或多个符合给定模式的频道 PSUBSCRIBE pattern [pattern ...] #退订所有给定模式的频道。 PUNSUBSCRIBE [pattern [pattern ...]] #查看订阅与发布系统状态 PUBSUB subcommand [argument [argument ...]] ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:7:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"7. 文档 https://redis.io/commands http://doc.redisfans.com/key/scan.html ","date":"2020-01-05","objectID":"/posts/redis/02-common-cmd/:8:0","tags":["Redis"],"title":"Redis教程(二)---redis常用命令","uri":"/posts/redis/02-common-cmd/"},{"categories":["Redis"],"content":"Linux下通过编译方式安装redis和通过docker-compose一键安装redis","date":"2020-01-02","objectID":"/posts/redis/01-install/","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"本文主要记录了如何在 Linux(CentOS 7) 下安装redis，包括源码编译安装和通过docker compose一键安装。 ","date":"2020-01-02","objectID":"/posts/redis/01-install/:0:0","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"1. 编译安装 ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:0","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"1. 环境准备 由于 redis 是用 C 语言开发，安装之前必先确认是否安装 gcc 环境（gcc -v），如果没有安装，执行以下命令进行安装 $ yum install -y gcc ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:1","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"2. 编译安装 1. 下载源码 直接去官网下载最新的版本即可 $ wget http://download.redis.io/releases/redis-5.0.7.tar.gz $ tar -zxvf redis-5.0.7.tar.gz 2. 编译 $ cd redis-5.0.7 $ make 3. 安装 # PREFIX指定安装路径 make --PREFIX=/usr/local/redis install ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:2","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"3. 修改配置文件 可以在源码中复制一份配置文件出来。 一般只需要修改下面这三个地方 #设置密码 requirepass password #开放远程访问 bind 0.0.0.0 # 后台运行 daemonize yes ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:3","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"4. 使用 # 启动 /usr/local/redis/bin/redis-server /usr/local/redis/redis.conf # 停止 记得替换密码 /usr/local/redis/bin/redis-cli -a '{password}' -h 127.0.0.1 -p 6379 shutdown ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:4","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"5. 添加systemd管理 编写service文件 vi /usr/lib/systemd/system/redisd.service 内容如下 [Unit] Description=redis-server Documentation=https://redis.io/ After=network.target [Service] Type=forking ExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/redis.conf # 记得替换密码 ExecStop=/usr/local/redis/bin/redis-cli -a '{password}' -h 127.0.0.1 -p 6379 shutdown #Restart=always #Restart=on-failure [Install] WantedBy=multi-user.target 重新加载并添加到自启动 systemctl daemon-reload # 添加到开机自启动 systemctl enable redisd.service systemctl start redisd.service systemctl stop redisd.service ","date":"2020-01-02","objectID":"/posts/redis/01-install/:1:5","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"2. Docker一键安装 需要先安装docker和docker-compose ","date":"2020-01-02","objectID":"/posts/redis/01-install/:2:0","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"1. 目录结构 /usr/local/docker/redis /data /conf/redis.conf --docker-compose.yml ","date":"2020-01-02","objectID":"/posts/redis/01-install/:2:1","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"2. docker-compose.yml version: '3' services: redis: hostname: redis image: redis container_name: redis restart: unless-stopped command: redis-server /etc/redis.conf # 启动redis命令 environment: - TZ=Asia/Shanghai - LANG=en_US.UTF-8 volumes: - /etc/localtime:/etc/localtime:ro # 设置容器时区与宿主机保持一致 - ./data:/data - ./conf/redis.conf:/etc/redis/redis.conf ports: - \"6379:6379\" ","date":"2020-01-02","objectID":"/posts/redis/01-install/:2:2","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"3. 配置文件 去官网上下载一份最新的配置文件 然后改一改就好了。 官网地址:https://redis.io/topics/config **daemonize yes必须改成daemonize no ** 不然容器启动不了 bind 127.0.0.1改成bind 0.0.0.0或注释掉 不然其他机器访问不了 ","date":"2020-01-02","objectID":"/posts/redis/01-install/:2:3","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["Redis"],"content":"4. 启动 #在docker-compose.yml文件所在目录执行才可以 # 启动 -d参数后台启动 docker-compose up -d #停止 docker-compose down ","date":"2020-01-02","objectID":"/posts/redis/01-install/:2:4","tags":["Redis"],"title":"Redis教程(一)---通过docker-compose安装redis","uri":"/posts/redis/01-install/"},{"categories":["etcd"],"content":"etcd v3的简单分析及其和v2版本的对比","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"本文主要记录了对etcd v3的简单分析及其和v2版本的对比及改进的地方。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:0:0","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"2. v3与v2简单对比 etcdv3在v2基础上进行了改进和优化： 使用 gRPC+protobuf 取代 HTTP+JSON 通信，提高通信效率；另外通过gRPC gateway 来继续保持对 HTTPJSON 接口的支持。 使用更轻 级的基于租约(lease)的 key 自动过期机制，取代了基于TTL key 的自动过期机制 观察者(watcher)机制也进行了重新设计。etcd v2 的观察者机制是基于HTTP 长连接的事件驱动机制；而 etcd v3 的观察者机制是基于HTTP/2的server push ，并且对事件进行了多路复用（ multiplexing ）优化。 etcd v3 的数据模型也发生了较大的改变， v2 是一个简单的 key value 的内存数据库，而 v3 则是支持事务和多版本并发控制的磁盘数据库。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:0","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"1. gRPC+protobuf 序列化和反序列化速度是v2的两倍多。 gRPC支持HTTP/2,对HTTP 通信进行了多路复用，可以共享一个 TCP 连接，每个客户端只需要和服务器建立一个TCP连接即可，v2版本则每个HTTP请求都要建立一个连接。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:1","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"2. 租约机制 etcd v2 key 的自动过期机制是基于 TTL 的：客户端可以为一个 key设置自动过期时间， 一旦 TTL 到了，服务端就会自动删除该 key。 如果客户端不想服务器端删除某个 key ，就需要定期去更新这个 key TTL 。 也就是说，即使整个集群都处于 闲状态，也会有很多客户端需要与服务器端进行定期通信以保证某个 key 不被自动删除。而且 TTL 是设置在 key 上的，那么对于客户想保留的 key ，客户端需要对每个 key 都进行定期更新，即使这些 key过期时间是一样的。 etcd v3 使用租约(lease)机制，替代了 TTL 的自动过期机制 用户可以创建 个租约，然后将这个租约与 key 关联起来 一旦一个租约过期， etcd v3 服务器端就会删除与这个租约关联的所有的 key。 如果多个 key的过期时间是一样的，那么这些 key 就可以共享一个租约。这就大大减小了客户端请求的数量， 对于过期时间相同 ，共享了一个租约的所有 key ，客户端只 需要更新这个租约的过期时间即可 而不是像 etcd v2 样更新所有 key 的过期时间 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:2","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"3. 观察者模式 观察者机制使得客户端可以监控一个 key 的变化，当 key 发生变化时，服务器端将通知客户端，而不是让客户端定期向服务器端发送请求去轮询 key的变化。 etcd v2 的服务端对每个客户端的每个 watch 请求都维持着一 HTTP 长连接 如果数千个客户端 watch 了数千个 key ，那么 etcd v2 服务器端的 socket 和内存等资源很快就会被耗尽。 etcd v3 的改进方法是对来自于同一个客户端的 watch 请求进行了多路复用(multiplexing) 这样的话，同一个客户端只需要与服务器端维护一个 TCP 连接即可，这就大大减轻了服务器端的压力。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:3","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"4. 数据存储模型 etcd 是一个 key-value 数据库， etcd v2 只保存了 key 的最新的 value ，之前value 直接被覆盖了。 同时 v2 维护了一个全局的 key 的历史记录变更的窗口，默认保存最新的 1000 个变更，整个数据库全局的历史变更记录。因此在很短的时间内如果有频繁的写操作的话，那么变更记录会很快超过 1000 ；如果watch 过慢就会无法得到之前的变更，带来的后果就是 watch 丢失事件。 etcd v3则通过引入MVCC(多版本并发控制),采用了从历史记录为主索引的存储结构，保存了key的所有历史变更记录。etcd 可以存储上十万个纪录进行快速查询，并且支持根据用户的要求进行压缩合并。 由于etcd v3 实现了 MVCC ，保存了每个 key value pair 的历史版本，数据量大了很多，不能将整个数据库都放在内存里了 因此 etcd v3 摒弃了内存数据库，转为磁盘数据库，整个数据库都存储在磁盘上，底层的存储引擎使用的是BoltDB。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:4","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"5. 迷你事务 etcd v3 引人了迷你事务（ mini transaction ）的概念 每个迷你事务都可以包含一系列的条件语句，只有在还有条件满足时事务才会执行成功。 // 开启事务 txn := kv.Txn(context.Background()) getOwner := clientv3.OpGet(Prefix+Suffix, clientv3.WithFirstCreate()...) // 如果/illusory/cloud的值为hello则获取/illusory/cloud的值 否则获取/illusory/wind的值 txnResp, err := txn.If(clientv3.Compare(clientv3.Value(Prefix+Suffix), \"=\", \"hello\")). Then(clientv3.OpGet(Prefix+\"/equal\"), getOwner). Else(clientv3.OpGet(Prefix+\"/unequal\"), getOwner). Commit() if err != nil { return } if txnResp.Succeeded { // 事务成功 } else { } ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:5","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"6. 快照 一致性系统都采用了基于 log 的复制 log 不能无限增长，所以在某一时刻系统需要做完整的快照，并且将快照存储到磁盘中，在存储快照之后才能将之前的 log 丢弃。 每次存储完整的快照是一件非常没有效率的事情，但是对于一致性系统来说，设计增量快照以及传输同步大数据都是非常烦琐的。 etcd 通过对 Raft 和存储系统的重构，能够很好地支持增量快照和传输相对较大的快照。目前 etcd 可以存储百万到千万级别的 key。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:1:6","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"3. gRPC 发送至 etcd v3服务器 每一个API 请求均为 gRPC 过程调用。 根据 etcd v3 所定义的不同服务，其 API 可分为键值 KV 、集群（ Cluster ）、维护（ Ma ntenance）、 认证／鉴权（ Auth ）、观察（ Watch）与租约（ Lease) 6 大类。 KV键值相关API KV Service:键值对创建、更新、获取和删除操作。 Watch Service:用于检测Key的变化。 Lease Service：用于消耗客户端Keep-Alive消息的原语。 Cluster Service：集群相关，增删成员、更新配置和获取成员列表。 Auth Service：可使能或失能某项鉴定过程以及处理鉴定的请求。如增删用户、修改密码、授予用户角色等等。 Matintenance Service：提供了启动或停止警报以及查询警报的功能。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:2:0","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"4. KV对象 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:0","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"1. revision etcd revision ，本质上就是 etcd 维护的一个在集群范围内有效的 64计数器(单调递增)。只要 etcd 的键空间发生变化， revision 的值就会相应地增加。 也可以revision 看成是全局的逻辑时钟，即将所有针对后端存储的修改操作进行连续的排序。 对于 etcd 的多版本并发性控制（ multi-version concurrency control, MVCC) 后端而言revision 的价值更是不言而喻。 MVCC 模型是指由于保存了键的历史，因此可以查过去某个 revision （时刻）的 key value 存储。 为了实现细粒度的存储管理，集群管理者可自定义配置键空间历史保存策略。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:1","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"2. 键区间 etcd v3 数据模型采用了扁平 key 空间，为所有 key 都建立了索引。 该模型有别于其他常见的采用层级系统将 key 组建为目录(directory)的 key-value储系统（即v2)。 key 不再以目录的形式列出，而代之以新的方式一左闭右开的 key 区间（interval ），如[Key1,KeyN)。 区间左端的字段为 key ，表示 range 的非空首 key ，而右端的字段则为range_end ，表示紧接 range key 的后一个 key 即[1,20)=[1,19]，其中[a,b)在表示了a为前缀的所有key key或range_end字段为\\0则表示所有 [a,\\0)表示该区间包含所有大于a的 [\\0,\\0)则代表key [\\0,a) 应该没有这种写法 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:2","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"3. 事务 etcd v3 中，事务就是一个原子的、针对 key-value 存储操作的 If/Then/Else 结构。 事务提供了一个原语，用于将请求归并到一起放在原子块中（例then/else ），这些原子块的执行条件（例如 if）以 key value 存储里的内容为依据。 事务可以用来保护 key 不受其他并发更新操作的修改，也可构建CAS(Compare And Swap ）操作，并以此作为更高层次（应用层）并发控制的基础。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:3","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"4. Compact调用 Compact 远程调用压缩 etcd 键值存储的事件历史中。 键值存储应该定期压缩，否则事件历史会无限制地持续增长。 var rev int64 = 10 // rev:会压缩指定版本之前的记录 // clientv3.WithCompactPhysical(): RPC 将会等待直 压缩物理性地应用到数据库，之后被压缩的项将完全从后端数据库中移除 kv.Compact(context.Background(), rev, clientv3.WithCompactPhysical()) ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:4","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"5. Watch Watch API 提供了 基于事件（ event ）的接口，用于异步监测 key 的变化。 watchChan := client.Watch(context.Background(),\"mykey\") for wr := range watchChan { for _, e := range wr.Events { switch e.Type { case clientv3.EventTypePut: fmt.Printf(\"watch event put-current: %#v \\n\", string(e.Kv.Value)) case clientv3.EventTypeDelete: fmt.Printf(\"watch event delete-current: %#v \\n\", string(e.Kv.Value)) default: } } } 1. event 对于每个 key 而言，发生的每一个变化都以 Event 消息进行表示。 message Event { enum EventType { PUT = 0; DELETE = 1; } EventType type = 1; KeyValue kv = 2; KeyValue prev_kv = 3; } type：Event类型包括PUT和DELETE两种。 kv：与当前event关联的key-value。 prev_kv：该 key 在发生此 Event 之前最近一刻 revision key value对。 2. 流式watch watch 操作是长期持续存在的请求，并且它使用 gRPC 流来传输 Event数据 etcd3 watch 机制确保了监测到的 Event 有有序、可靠与原子化的特点。 有序：Event 按照 revision 排序，后发 Event 不会在前面的 Event之前出现在 watch 流中。 可靠：某个事件序列不会遗漏其中任意的子序列，假设有 Event,按发生的时间依次排序分别为a\u003cb\u003cc ，而如果 watch 接收到 a和c ，那么就能保证b也已经被接收了。 原子性：Event列表确保包含完整的revision,在相同revision的多个key上，更新不会分裂为几个事件列表。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:5","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"6. Lease 租约(Lease)是一种检查客户端活跃度的机制，Lease机制被用于授权进行同步等操作，分布式锁等场景。 1. 获取租约 通过LeaseGrant API获取租约。 type LeaseGrantResponse struct { *pb.ResponseHeader ID LeaseID TTL int64 Error string } ID：服务端授予的ID TTL：服务端为该Lease选取的time-to-live值，单位是秒 2. KeepAlives 可以通过KeepAlive为Lease续期。 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:3:6","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"5. 参考 《云原生分布式存储基石:etcd深入解析》 ","date":"2019-09-10","objectID":"/posts/etcd/03-v3-analyze/:4:0","tags":["etcd"],"title":"etcd教程(三)---etcd之v3API简单分析","uri":"/posts/etcd/03-v3-analyze/"},{"categories":["etcd"],"content":"etcd v3版本入门教程,常见命令的简单使用","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"本文主要介绍了etcd v3版本的基本使用。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:0:0","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"1. 初始化Client cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"localhost:2379\"}, DialTimeout: 5 * time.Second, }) 要访问etcd第一件事就是创建client，它需要传入一个Config配置，这里传了2个选项： Endpoints：etcd的多个节点服务地址，因为我是单点测试，所以只传1个。 DialTimeout：创建client的首次连接超时，这里传了5秒，如果5秒都没有连接成功就会返回err；值得注意的是，一旦client创建成功，我们就不用再关心后续底层连接的状态了，client内部会重连。 当然，如果上述err != nil，那么一般情况下我们可以选择重试几次，或者退出程序。 这里重点需要了解一下client到底长什么样： type Client struct { Cluster KV Lease Watcher Auth Maintenance // Username is a user name for authentication. Username string // Password is a password for authentication. Password string // contains filtered or unexported fields } Cluster、KV、Lease…，你会发现它们其实就代表了整个客户端的几大核心功能板块，分别用于： Cluster：向集群里增加etcd服务端节点之类，属于管理员操作。 KV：我们主要使用的功能，即操作K-V。 Lease：租约相关操作，比如申请一个TTL=10秒的租约。 Watcher：观察订阅，从而监听最新的数据变化。 Auth：管理etcd的用户和权限，属于管理员操作。 Maintenance：维护etcd，比如主动迁移etcd的leader节点，属于管理员操作。 我们需要使用什么功能，就去获取对应的对象即可。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:1:0","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2. 获取KV对象 实际上client.KV是一个interface，提供了关于k-v操作的所有方法： type KV interface { Put(ctx context.Context, key, val string, opts ...OpOption) (*PutResponse, error) Get(ctx context.Context, key string, opts ...OpOption) (*GetResponse, error) Delete(ctx context.Context, key string, opts ...OpOption) (*DeleteResponse, error) Compact(ctx context.Context, rev int64, opts ...CompactOption) (*CompactResponse, error) Do(ctx context.Context, op Op) (OpResponse, error) Txn(ctx context.Context) Txn } 但是我们并不是直接获取client.KV来使用，而是通过一个方法来获得一个经过装饰的KV实现（内置错误重试机制的高级KV）： kv := clientv3.NewKV(cli) func NewKV(c *Client) KV { api := \u0026kv{remote: RetryKVClient(c)} if c != nil { api.callOpts = c.callOpts } return api } // RetryKVClient implements a KVClient. func RetryKVClient(c *Client) pb.KVClient { return \u0026retryKVClient{ kc: pb.NewKVClient(c.conn), retryf: c.newAuthRetryWrapper(c.newRetryWrapper()), } } 接下来，我们将通过kv对象操作etcd中的数据。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:0","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.1 Put if putResp, err = kv.Put(ctx, \"/illusory/cloud\", \"hello\", clientv3.WithPrevKV()); err != nil { fmt.Println(err) return } 第一个参数context用于设置超时返回，后面2个参数分别是key和value， 其函数原型如下： Put(ctx context.Context, key, val string, opts ...OpOption) (*PutResponse, error) 除了我们传递的参数，还支持一个可变参数，主要是传递一些控制项来影响Put的行为，例如可以携带一个lease ID来支持key过期，这个后面再说。 上述Put操作返回的是PutResponse，不同的KV操作对应不同的response结构，这里顺便一提。 type ( CompactResponse pb.CompactionResponse PutResponse pb.PutResponse GetResponse pb.RangeResponse DeleteResponse pb.DeleteRangeResponse TxnResponse pb.TxnResponse ) 你可以通过IDE跳转到PutResponse，详细看看有哪些可用的信息： type PutResponse struct { Header *ResponseHeader `protobuf:\"bytes,1,opt,name=header\" json:\"header,omitempty\"` // if prev_kv is set in the request, the previous key-value pair will be returned. PrevKv *mvccpb.KeyValue `protobuf:\"bytes,2,opt,name=prev_kv,json=prevKv\" json:\"prev_kv,omitempty\"` } Header里保存的主要是本次更新的revision信息，而PrevKv可以返回Put覆盖之前的value是什么（目前是nil，后面会说原因），打印给大家看看： cluster_id:16331561280905954307 member_id:9359753661018847437 revision:6 raft_term:7 记得，我们需要判断err来确定操作是否成功。 我们再Put其他2个key，用于后续演示： // 再写一个孩子 kv.Put(context.TODO(),\"/illusory/wind\"\", \"world\") // 再写一个同前缀的干扰项 kv.Put(context.TODO(), \"/illusoryxxx, \"干扰\") 现在理论上来说，illusory目录下有2个孩子：cloud与wind，而/illusoryxxx并不是。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:1","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.2 Get 我们可以先来读取一下/illusory/cloud： // 用kv获取key if putResp, err = kv.Put(con, \"/illusory/wind\", \"world\"); err != nil { fmt.Println(err) } 其函数原型如下： Get(ctx context.Context, key string, opts ...OpOption) (*GetResponse, error) 和Put类似，函数注释里提示我们可以传递一些控制参数来影响Get的行为，比如：WithFromKey表示读取从参数key开始递增的所有key，而不是读取单个key。 在上面的例子中，我没有传递opOption，所以就是获取key=/test/a的最新版本数据。 这里err并不能反馈出key是否存在（只能反馈出本次操作因为各种原因异常了），我们需要通过GetResponse（实际上是pb.RangeResponse）判断key是否存在： type RangeResponse struct { Header *ResponseHeader `protobuf:\"bytes,1,opt,name=header\" json:\"header,omitempty\"` Kvs []*mvccpb.KeyValue `protobuf:\"bytes,2,rep,name=kvs\" json:\"kvs,omitempty\"` More bool `protobuf:\"varint,3,opt,name=more,proto3\" json:\"more,omitempty\"` Count int64 `protobuf:\"varint,4,opt,name=count,proto3\" json:\"count,omitempty\"` } Kvs字段，保存了本次Get查询到的所有k-v对，因为上述例子只Get了一个单key，所以只需要判断一下len(Kvs)是否==1即可知道是否存在。 而mvccpb.KeyValue在etcd原理分析中有所提及，它就是etcd在bbolt中保存的K-v对象： type KeyValue struct { Key []byte `protobuf:\"bytes,1,opt,name=key,proto3\" json:\"key,omitempty\"` CreateRevision int64 `protobuf:\"varint,2,opt,name=create_revision,json=createRevision,proto3\" json:\"create_revision,omitempty\"` ModRevision int64 `protobuf:\"varint,3,opt,name=mod_revision,json=modRevision,proto3\" json:\"mod_revision,omitempty\"` Value []byte `protobuf:\"bytes,5,opt,name=value,proto3\" json:\"value,omitempty\"` Lease int64 `protobuf:\"varint,6,opt,name=lease,proto3\" json:\"lease,omitempty\"` } 至于RangeResponse.More和Count，当我们使用withLimit()选项进行Get时会发挥作用，相当于翻页查询。 接下来，我们通过一个特别的Get选项，获取/illusory目录下的所有孩子： // 用kv获取Key 获取前缀为/illusory/的 即 /illusory/的所有孩子 if getResp, err = kv.Get(con, \"/illusory/\",clientv3.WithPrefix()); err != nil { fmt.Println(err) } 我们知道etcd是一个有序的k-v存储，因此/test/为前缀的key总是顺序排列在一起。 withPrefix实际上会转化为范围查询，它根据前缀/illusory/生成了一个key range，[“/illusory/”, “/illusory0”)，为什么呢？因为比/大的字符是’0’，所以以/illusory0作为范围的末尾，就可以扫描到所有的/illusory/打头的key了。 在之前，我Put了一个/illusoryxxx干扰项，因为不符合/illusory/前缀（注意末尾的/），所以就不会被这次Get获取到。但是，如果我查询的前缀是/illusory，那么/illusoryxxx也会被扫描到，这就是etcd k-v模型导致的，编程时一定要特别注意。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:2","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.3 获取Lease对象 和获取KV对象一样，通过下面代码获取它： lease := clientv3.NewLease(client) type Lease interface { Grant(ctx context.Context, ttl int64) (*LeaseGrantResponse, error) Revoke(ctx context.Context, id LeaseID) (*LeaseRevokeResponse, error) TimeToLive(ctx context.Context, id LeaseID, opts ...LeaseOption) (*LeaseTimeToLiveResponse, error) Leases(ctx context.Context) (*LeaseLeasesResponse, error) KeepAlive(ctx context.Context, id LeaseID) (\u003c-chan *LeaseKeepAliveResponse, error) KeepAliveOnce(ctx context.Context, id LeaseID) (*LeaseKeepAliveResponse, error) Close() error } Lease提供了几个功能： Grant：分配一个租约。 Revoke：释放一个租约。 TimeToLive：获取剩余TTL时间。 Leases：列举所有etcd中的租约。 KeepAlive：自动定时的续约某个租约。 KeepAliveOnce：为某个租约续约一次。 Close：貌似是关闭当前客户端建立的所有租约。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:3","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.4 Lease 要想实现key自动过期，首先得创建一个租约，它有10秒的TTL： grantResp, err := lease.Grant(context.TODO(), 10) grantResp中主要使用到了ID，也就是租约ID： type LeaseGrantResponse struct { *pb.ResponseHeader ID LeaseID TTL int64 Error string } 接下来，我们用这个租约来Put一个会自动过期的Key： // 用client也可以设置key，kv是client的一个结构，因此可以使用其方法 if putResp, err = kv.Put(context.TODO(), \"/illusory/cloud/x\", \"ok\", clientv3.WithLease(leaseResp.ID)); err != nil { fmt.Println(err) } 这里特别需要注意，有一种情况是在Put之前Lease已经过期了，那么这个Put操作会返回error，此时你需要重新分配Lease。 当我们实现服务注册时，需要主动给Lease进行续约，这需要调用KeepAlive/KeepAliveOnce， KeepAlive:自动定时的续约某个租约。 KeepAliveOnce:为某个租约续约一次 // 主动给Lease进行续约 if keepAliveChan, err := client.KeepAlive(context.TODO(), leaseResp.ID); err != nil { // 有协程来帮自动续租，每秒一次。 fmt.Println(err) } keepResp结构如下： // LeaseKeepAliveResponse wraps the protobuf message LeaseKeepAliveResponse. type LeaseKeepAliveResponse struct { *pb.ResponseHeader ID LeaseID TTL int64 } KeepAlive和Put一样，如果在执行之前Lease就已经过期了，那么需要重新分配Lease。Etcd并没有提供API来实现原子的Put with Lease。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:4","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.5 OP Op字面意思就是”操作”，Get和Put都属于Op，只是为了简化用户开发而开放的特殊API。 实际上，KV有一个Do方法接受一个Op： // Do applies a single Op on KV without a transaction. // Do is useful when creating arbitrary operations to be issued at a // later time; the user can range over the operations, calling Do to // execute them. Get/Put/Delete, on the other hand, are best suited // for when the operation should be issued at the time of declaration. Do(ctx context.Context, op Op) (OpResponse, error) 其参数Op是一个抽象的操作，可以是Put/Get/Delete…；而OpResponse是一个抽象的结果，可以是PutResponse/GetResponse… 可以通过一些函数来分配Op： func OpDelete(key string, opts ...OpOption) Op func OpGet(key string, opts ...OpOption) Op func OpPut(key, val string, opts ...OpOption) Op func OpTxn(cmps []Cmp, thenOps []Op, elseOps []Op) Op 其实和直接调用KV.Put，KV.GET没什么区别。 下面是一个例子： // 给key设置新的value并返回设置之前的值 op := clientv3.OpPut(\"/illusory/cloud\", \"newKey\", clientv3.WithPrevKV()) response, err := kv.Do(context.TODO(), op) 把这个op交给Do方法执行，返回的opResp结构如下： type OpResponse struct { put *PutResponse get *GetResponse del *DeleteResponse txn *TxnResponse } 你的操作是什么类型，你就用哪个指针来访问对应的结果，仅此而已。 ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:5","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.6 Txn事务 etcd中事务是原子执行的，只支持if … then … else …这种表达，能实现一些有意思的场景。 首先，我们需要开启一个事务，这是通过KV对象的方法实现的： // 开启事务 txn := kv.Txn(context.TODO()) 我写了如下的测试代码，Then和Else还比较好理解，If是比较陌生的。 // 如果/illusory/cloud的值为hello则获取/illusory/cloud的值 否则获取/illusory/wind的值 txnResp, err := txn.If(clientv3.Compare(clientv3.Value(\"/illusory/cloud\"), \"=\", \"hello\")). Then(clientv3.OpGet(\"/illusory/cloud\")). Else(clientv3.OpGet(\"/illusory/wind\", clientv3.WithPrefix())). Commit() 我们先看下Txn支持的方法： type Txn interface { // If takes a list of comparison. If all comparisons passed in succeed, // the operations passed into Then() will be executed. Or the operations // passed into Else() will be executed. If(cs ...Cmp) Txn // Then takes a list of operations. The Ops list will be executed, if the // comparisons passed in If() succeed. Then(ops ...Op) Txn // Else takes a list of operations. The Ops list will be executed, if the // comparisons passed in If() fail. Else(ops ...Op) Txn // Commit tries to commit the transaction. Commit() (*TxnResponse, error) } Txn必须是这样使用的：If(满足条件) Then(执行若干Op) Else(执行若干Op)。 If中支持传入多个Cmp比较条件，如果所有条件满足，则执行Then中的Op（上一节介绍过Op），否则执行Else中的Op。 在我的例子中只传入了1个比较条件： txn.If(clientv3.Compare(clientv3.Value(\"/illusory/cloud\"), \"=\", \"hello\")) Value(“/illusory/cloud”)是指key=/illusory/cloud对应的value，它是条件表达式的”主语”，类型是Cmp： func Value(key string) Cmp { return Cmp{Key: []byte(key), Target: pb.Compare_VALUE} } 这个Value(“/illusory/cloud”)返回的Cmp表达了：”/illusory/cloud这个key对应的value”。 接下来，利用Compare函数来继续为”主语”增加描述，形成了一个完整条件语句，即”/illusory/cloud\"i这个key对应的value”必须等于”hello”。 Compare函数实际上是对Value返回的Cmp对象进一步修饰，增加了”=”与”hello”两个描述信息： func Compare(cmp Cmp, result string, v interface{}) Cmp { var r pb.Compare_CompareResult switch result { case \"=\": r = pb.Compare_EQUAL case \"!=\": r = pb.Compare_NOT_EQUAL case \"\u003e\": r = pb.Compare_GREATER case \"\u003c\": r = pb.Compare_LESS default: panic(\"Unknown result op\") } cmp.Result = r switch cmp.Target { case pb.Compare_VALUE: val, ok := v.(string) if !ok { panic(\"bad compare value\") } cmp.TargetUnion = \u0026pb.Compare_Value{Value: []byte(val)} case pb.Compare_VERSION: cmp.TargetUnion = \u0026pb.Compare_Version{Version: mustInt64(v)} case pb.Compare_CREATE: cmp.TargetUnion = \u0026pb.Compare_CreateRevision{CreateRevision: mustInt64(v)} case pb.Compare_MOD: cmp.TargetUnion = \u0026pb.Compare_ModRevision{ModRevision: mustInt64(v)} case pb.Compare_LEASE: cmp.TargetUnion = \u0026pb.Compare_Lease{Lease: mustInt64orLeaseID(v)} default: panic(\"Unknown compare type\") } return cmp } Cmp可以用于描述”key=xxx的yyy属性，必须=、!=、\u003c、\u003e，kkk值”，比如： key=xxx的value，必须!=，hello。 key=xxx的create版本号，必须=，11233。 key=xxx的lease id，必须=，12319231231238。 经过Compare函数修饰的Cmp对象，内部包含了完整的条件信息，传递给If函数即可。 类似于Value的函数用于指定yyy属性，有这么几个方法： func CreateRevision(key string) Cmp:key=xxx的创建版本必须满足… func LeaseValue(key string) Cmp:key=xxx的Lease ID必须满足… func ModRevision(key string) Cmp:key=xxx的最后修改版本必须满足… func Value(key string) Cmp:key=xxx的创建值必须满足… func Version(key string) Cmp:key=xxx的累计更新次数必须满足… 最后Commit提交整个Txn事务，我们需要判断txnResp获知If条件是否成立： if txnResp.Succeeded { // If = true fmt.Println(\"~~~\", txnResp.Responses[0].GetResponseRange().Kvs) } else { // If =false fmt.Println(\"!!!\", txnResp.Responses[0].GetResponseRange().Kvs) } Succeed=true表示If条件成立，接下来我们需要获取Then或者Else中的OpResponse列表（因为可以传多个Op），可以看一下txnResp的结构： type TxnResponse struct { Header *ResponseHeader `protobuf:\"bytes,1,opt,name=header\" json:\"header,omitempty\"` // succeeded is set to true if the compare evaluated to true or false otherwise. Succeeded bool `protobuf:\"varint,2,opt,name=succeeded,proto3\" json:\"succeeded,omitempty\"` // responses is a list of responses corresponding to the results from applying // success if succeeded is true or failure if succeeded is false. Responses []*ResponseOp `protobuf:\"bytes,3,rep,name=responses\" json:\"responses,omitempty\"` } ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:6","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"2.7 watch watch := client.Watch(context.Background(), \"maxProcess\") select { case \u003c-watch: fmt.Println(client.Get(context.Background(), \"maxProcess\")) } ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:2:7","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"3. 参考 https://etcd.io/docs/v3.4.0/learning/api/ https://yuerblog.cc/2017/12/12/etcd-v3-sdk-usage/ ","date":"2019-09-07","objectID":"/posts/etcd/02-v3-getting-started/:3:0","tags":["etcd"],"title":"etcd教程(二)---clientv3简单使用","uri":"/posts/etcd/02-v3-getting-started/"},{"categories":["etcd"],"content":"`docker-compose`来搭建`etcd`，包括单节点和集群模式","date":"2019-09-05","objectID":"/posts/etcd/01-install/","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"本文主要记录了如何通过docker-compose来搭建etcd，包括单节点和集群模式及其web监控。 ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:0:0","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"1. 单节点 ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:1:0","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"1. 目录结构 /usr/local/docker/etcd --/data --docker-compose.yml ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:1:1","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"2. docker-compose.yml version: '3' networks: myetcd_single: services: etcd: image: quay.io/coreos/etcd container_name: etcd_single command: etcd -name etcd1 -advertise-client-urls http://0.0.0.0:2379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 ports: - 12379:2379 - 12380:2380 volumes: - ./data:/etcd-data networks: - myetcd_single etcdkeeper: image: deltaprojects/etcdkeeper container_name: etcdkeeper_single ports: - 8088:8080 networks: - myetcd_single ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:1:2","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"3. 相关参数 参数 作用 name 节点名称 data-dir 指定节点的数据存储目录 listen-client-urls 对外提供服务的地址：比如 http://ip:2379,http://127.0.0.1:2379 ，客户端会连接到这里和 etcd 交互 listen-peer-urls 监听URL，用于与其他节点通讯 advertise-client-urls 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点 initial-advertise-peer-urls 该节点同伴监听地址，这个值会告诉集群中其他节点 initial-cluster 集群中所有节点的信息，格式为 node1=http://ip1:2380,node2=http://ip2:2380,… 。注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值 initial-cluster-state 新建集群的时候，这个值为 new ；假如已经存在的集群，这个值为 existing initial-cluster-token 创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误 ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:1:3","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"4. 启动 docker-compose up #后台启动增加`-d`参数 docker-compose up -d ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:1:4","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"2. 伪集群 ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:2:0","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"1. 目录结构 /usr/local/docker/etcd --/data --/etcd1 --/etcd2 --/etcd3 --docker-compose.yml ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:2:1","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"2. docker-compose.yml version: '3' networks: myetcd: services: etcd1: image: quay.io/coreos/etcd container_name: etcd1 command: etcd -name etcd1 -advertise-client-urls http://0.0.0.0:2379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster \"etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380\" -initial-cluster-state new ports: - 12379:2379 - 12380:2380 volumes: - ./data/etcd1:/etcd-data networks: - myetcd etcd2: image: quay.io/coreos/etcd container_name: etcd2 command: etcd -name etcd2 -advertise-client-urls http://0.0.0.0:2379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster \"etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380\" -initial-cluster-state new ports: - 22379:2379 - 22380:2380 volumes: - ./data/etcd2:/etcd-data networks: - myetcd etcd3: image: quay.io/coreos/etcd container_name: etcd3 command: etcd -name etcd3 -advertise-client-urls http://0.0.0.0:2379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster \"etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380\" -initial-cluster-state new ports: - 32379:2379 - 32380:2380 volumes: - ./data/etcd3:/etcd-data networks: - myetcd etcdkeeper: image: deltaprojects/etcdkeeper container_name: etcdkeeper ports: - 8088:8080 networks: - myetcd ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:2:2","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"3. 启动 docker-compose up #后台启动增加`-d`参数 docker-compose up -d ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:2:3","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"4. 查看集群信息 [root@localhost etcd3]# curl -L http://127.0.0.1:32379/v2/members {\"members\":[{\"id\":\"ade526d28b1f92f7\",\"name\":\"etcd1\",\"peerURLs\":[\"http://etcd1:2380\"],\"clientURLs\":[\"http://0.0.0.0:2379\"]},{\"id\":\"bd388e7810915853\",\"name\":\"etcd3\",\"peerURLs\":[\"http://etcd3:2380\"],\"clientURLs\":[\"http://0.0.0.0:2379\"]},{\"id\":\"d282ac2ce600c1ce\",\"name\":\"etcd2\",\"peerURLs\":[\"http://etcd2:2380\"],\"clientURLs\":[\"http://0.0.0.0:2379\"]}]} ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:2:4","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"3. web监控 启动etcd的同时也会启动web监控etcdkeeper 访问路径 localhost:8088 etcdKeeper访问etcd好像是通过外网访问的(也可能是我配置问题) 如果是线上服务器的话需要配置一下安全组才能访问 ","date":"2019-09-05","objectID":"/posts/etcd/01-install/:3:0","tags":["etcd"],"title":"etcd教程(一)---通过docker安装etcd集群","uri":"/posts/etcd/01-install/"},{"categories":["etcd"],"content":"主要记录了etcd的使用过程中遇到的问题及其解决方案","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["etcd"],"content":"本文主要记录了etcd的使用过程中遇到的问题及其解决方案。 ","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/:0:0","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["etcd"],"content":"1. 空间不足 ","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/:1:0","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["etcd"],"content":"1.1 问题描述 使用的时候报错，提示空间不足 但是看了下没存多少数据呀… Server register error: etcdserver: mvcc: database space exceeded ","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/:1:1","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["etcd"],"content":"1.2 原因分析 前几章说了etcd采用的是MVCC来进行多版本并发控制，会存储数据的每一次变化，如果不进行压缩(删除)则空间会越变越大。 1条数据修改了1W次就相当于存了1W条记录。 ","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/:1:2","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["etcd"],"content":"1.3 解决方案 etcd 默认不会自动 compact，需要设置启动参数，或者通过命令进行compact，如果变更频繁建议设置，否则会导致空间和内存的浪费以及错误。 etcd v3 的默认的 backend quota 2GB，如果不 compact，boltdb 文件大小超过这个限制后，就会报错：”Error: etcdserver: mvcc: database space exceeded”，导致数据无法写入。 命令如下： 获取当前etcd数据的修订版本(revision) rev=$(ETCDCTL_API=3 etcdctl –endpoints=:2379 endpoint status –write-out=”json” | egrep -o ‘”revision”:[0-9]*’ | egrep -o ‘[0-9]*’) 整合压缩旧版本数据 ETCDCTL_API=3 etcdctl compact $rev 执行碎片整理 ETCDCTL_API=3 etcdctl defrag 解除告警 ETCDCTL_API=3 etcdctl alarm disarm 备份以及查看备份数据信息 ETCDCTL_API=3 etcdctl snapshot save backup.db ETCDCTL_API=3 etcdctl snapshot status backup.db ","date":"2019-09-01","objectID":"/posts/etcd/00-etcd-faq/:1:3","tags":["etcd"],"title":"etcd教程(零)---etcd使用过程中遇到的问题","uri":"/posts/etcd/00-etcd-faq/"},{"categories":["gRPC"],"content":"主要记录在使用gRPC时遇到的一些问题及其解决方案","date":"2019-07-16","objectID":"/posts/grpc/00-faq/","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"本文章主要记录在使用gRPC时遇到的一些问题及其解决方案。 ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:0:0","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"1. Method Not Found ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:1:0","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"1. 错误描述 在使用gPRC时遇到的问题。 其中客户端由golang编写，服务端由python编写。 测试时一直报如下这个错： rpc error:Code=Unimplemented desc = Method Not Found！ 下面是官网上的错误列表，其中GRPC_STATUS_UNIMPLEMENTED对应的case也是Method not found on server。 Case Status code Client application cancelled the request GRPC_STATUS_CANCELLED Deadline expired before server returned status GRPC_STATUS_DEADLINE_EXCEEDED Method not found on server GRPC_STATUS_UNIMPLEMENTED Server shutting down GRPC_STATUS_UNAVAILABLE Server threw an exception (or did something other than returning a status code to terminate the RPC) GRPC_STATUS_UNKNOWN 但是 单独测试时 python自己写的客户端服务端可以正常交互 golang写的客户端 服务端也可以正常交互。 就是两个互相调用时会出现这个错误Method Not Found 仔细检查代码之后并没有什么问题。 各种google之后总算找到了原因。 https://www.itread01.com/content/1547029280.html ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:1:1","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"2. 原因 这是由于.proto文件中的package name 被修改，和 server 端的package 不一致导致的,双方同步.proto文件 packagename 重新编译生成对应的代码即可。 由于python写时没有加package xxx;这就 然后go这边加了。。。怪不得会出错，果然在修改.proto文件从新编译后就能成功运行了。 ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:1:2","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"2. 数据传输限制 ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:2:0","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"1. 错误描述 details = \"Received message larger than max (6194304 vs. 4194304)\" ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:2:1","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"2. 原因 接收消息超过了最大限制(默认4M) ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:2:2","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"3. 解决方案 可以在建立连接的时候修改这个限制。 server maxSize := 20 * 1024 * 1024 s := grpc.NewServer(grpc.MaxRecvMsgSize(maxSize), grpc.MaxSendMsgSize(maxSize)) PbDownMaxSize.RegisterPbDownMaxSizeServer(s, \u0026server{}) s.Serve(listener) client maxSize := 20 * 1024 * 1024 diaOpt := grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxSize), grpc.MaxCallSendMsgSize(maxSize)) conn, err := grpc.Dial(endpoint, grpc.WithInsecure(), diaOpt) ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:2:3","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["gRPC"],"content":"4. 相关源码 RecvMsg func (p *parser) recvMsg(maxReceiveMessageSize int) (pf payloadFormat, msg []byte, err error) { if _, err := p.r.Read(p.header[:]); err != nil { return 0, nil, err } pf = payloadFormat(p.header[0]) length := binary.BigEndian.Uint32(p.header[1:]) if length == 0 { return pf, nil, nil } if int64(length) \u003e int64(maxInt) { return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max length allowed on current machine (%d vs. %d)\", length, maxInt) } if int(length) \u003e maxReceiveMessageSize { return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max (%d vs. %d)\", length, maxReceiveMessageSize) } // TODO(bradfitz,zhaoq): garbage. reuse buffer after proto decoding instead // of making it for each message: msg = make([]byte, int(length)) if _, err := p.r.Read(msg); err != nil { if err == io.EOF { err = io.ErrUnexpectedEOF } return 0, nil, err } return pf, msg, nil } 可以看到在Recv的时候判定了两次数据长度。 除了设置的长度外还有一个最大长度限制int64(length) \u003e int64(maxInt) if int64(length) \u003e int64(maxInt) { return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max length allowed on current machine (%d vs. %d)\", length, maxInt) } if int(length) \u003e maxReceiveMessageSize { return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max (%d vs. %d)\", length, maxReceiveMessageSize) } 最大限制const maxInt = int(^uint(0) \u003e\u003e 1)应该是2G大小。 不会有人拿gPRC传这么大的数据吧… SendMsg func (cs *clientStream) SendMsg(m interface{}) (err error) { defer func() { if err != nil \u0026\u0026 err != io.EOF { // Call finish on the client stream for errors generated by this SendMsg // call, as these indicate problems created by this client. (Transport // errors are converted to an io.EOF error in csAttempt.sendMsg; the real // error will be returned from RecvMsg eventually in that case, or be // retried.) cs.finish(err) } }() if cs.sentLast { return status.Errorf(codes.Internal, \"SendMsg called after CloseSend\") } if !cs.desc.ClientStreams { cs.sentLast = true } // load hdr, payload, data hdr, payload, data, err := prepareMsg(m, cs.codec, cs.cp, cs.comp) if err != nil { return err } // TODO(dfawley): should we be checking len(data) instead? if len(payload) \u003e *cs.callInfo.maxSendMessageSize { return status.Errorf(codes.ResourceExhausted, \"trying to send message larger than max (%d vs. %d)\", len(payload), *cs.callInfo.maxSendMessageSize) } msgBytes := data // Store the pointer before setting to nil. For binary logging. op := func(a *csAttempt) error { err := a.sendMsg(m, hdr, payload, data) // nil out the message and uncomp when replaying; they are only needed for // stats which is disabled for subsequent attempts. m, data = nil, nil return err } err = cs.withRetry(op, func() { cs.bufferForRetryLocked(len(hdr)+len(payload), op) }) if cs.binlog != nil \u0026\u0026 err == nil { cs.binlog.Log(\u0026binarylog.ClientMessage{ OnClientSide: true, Message: msgBytes, }) } return } 同理发送的时候也有这个限制 if len(payload) \u003e *cs.callInfo.maxSendMessageSize { return status.Errorf(codes.ResourceExhausted, \"trying to send message larger than max (%d vs. %d)\", len(payload), *cs.callInfo.maxSendMessageSize) } ","date":"2019-07-16","objectID":"/posts/grpc/00-faq/:2:4","tags":["gRPC"],"title":"gRPC(Go)教程(零)---使用gRPC时遇到的问题","uri":"/posts/grpc/00-faq/"},{"categories":["Go-Micro"],"content":"go语言微服务框架 go-micro框架简单分析","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"Go语言微服务系列文章，使用golang实现微服务，这里选用的是go-micro框架,本文主要是对该框架的一个架构简单介绍。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:0:0","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"1. 概述 go-micro是go语言下的一个很好的微服务框架。 1.服务间传输格式为protobuf，效率上没的说，非常的快，也很安全。 2.go-micro的服务注册和发现是多种多样的。我个人比较喜欢etcdv3的服务服务发现和注册。 3.主要的功能都有相应的接口，只要实现相应的接口，就可以根据自己的需要订制插件。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:1:0","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"2. 通信流程 go-micro的通信流程大至如下 Server端需要向Register注册自己的存在或消亡，这样Client才能知道自己的状态。 同时Server监听客户端的调用和Brocker推送过来的信息进行处理。 Client端从Register中得到Server的信息，然后每次调用都根据算法选择一个的Server进行通信，当然通信是要经过编码/解码，选择传输协议等一系列过程的。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:2:0","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"3. 框架结构 go-micro 之所以可以高度订制和他的框架结构是分不开的，go-micro 由8个关键的 interface组成，每一个interface 都可以根据自己的需求重新实现，这8个主要的inteface也构成了go-micro的框架结构。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:0","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"1.Transort 服务之间通信的接口。 也就是服务发送和接收的最终实现方式，是由这些接口定制的 type Socket interface { Recv(*Message) error Send(*Message) error Close() error } type Client interface { Socket } type Listener interface { Addr() string Close() error Accept(func(Socket)) error } type Transport interface { Dial(addr string, opts ...DialOption) (Client, error) Listen(addr string, opts ...ListenOption) (Listener, error) String() string } Transport 的Listen方法是一般是Server端进行调用的，他监听一个端口，等待客户端调用。 Transport 的Dial就是客户端进行连接服务的方法。他返回一个Client接口，这个接口返回一个Client接口，这个Client嵌入了Socket接口，这个接口的方法就是具体发送和接收通信的信息。 是go-micro默认的同步通信机制是http传输。当然还有很多其他的插件：grpc,nats,tcp,udp,rabbitmq,都是目前已经实现了的方式。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:1","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"2. Codec 有了传输方式，下面要解决的就是传输编码和解码问题，go-micro有很多种编码解码方式，默认的实现方式是protobuf,当然也有其他的实现方式，json、protobuf、jsonrpc、mercury等等。 type Codec interface { ReadHeader(*Message, MessageType) error ReadBody(interface{}) error Write(*Message, interface{}) error Close() error String() string } type Message struct { Id uint64 Type MessageType Target string Method string Error string Header map[string]string } Codec接口的Write方法就是编码过程，两个Read是解码过程。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:2","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"3. Registry 服务的注册和发现，目前实现的consul,mdns, etcd,etcdv3,zookeeper,kubernetes.等 type Registry interface { Register(*Service, ...RegisterOption) error Deregister(*Service) error GetService(string) ([]*Service, error) ListServices() ([]*Service, error) Watch(...WatchOption) (Watcher, error) String() string Options() Options } 简单来说就是Service 进行Register，来进行注册，Client 使用watch方法进行监控，当有服务加入或者删除时这个方法会被触发，以提醒客户端更新Service信息。 默认的是服务注册和发现是consul， 我个人比较喜欢etcdv3集群。大家可以根据自己的喜好选择。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:3","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"4. Selector 以Registry为基础，Selector 是客户端级别的负载均衡，当有客户端向服务发送请求时， selector根据不同的算法从Registery中的主机列表，得到可用的Service节点，进行通信。目前实现的有循环算法和随机算法，默认的是随机算法 type Selector interface { Init(opts ...Option) error Options() Options // Select returns a function which should return the next node Select(service string, opts ...SelectOption) (Next, error) // Mark sets the success/error against a node Mark(service string, node *registry.Node, err error) // Reset returns state back to zero for a service Reset(service string) // Close renders the selector unusable Close() error // Name of the selector String() string } 默认的是实现是本地缓存，当前实现的有blacklist,label,named等方式。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:4","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"5. Broker Broker是消息发布和订阅的接口。很简单的一个例子，因为服务的节点是不固定的，如果有需要修改所有服务行为的需求，可以使服务订阅某个主题，当有信息发布时，所有的监听服务都会收到信息，根据你的需要做相应的行为。 type Broker interface { Options() Options Address() string Connect() error Disconnect() error Init(...Option) error Publish(string, *Message, ...PublishOption) error Subscribe(string, Handler, ...SubscribeOption) (Subscriber, error) String() string } Broker默认的实现方式是http方式，但是这种方式不要在生产环境用。go-plugins里有很多成熟的消息队列实现方式，有kafka、nsq、rabbitmq、redis等等。 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:5","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"6. Client Client是请求服务的接口。 他封装 Transport 和 Codec 进行rpc调用，也封装了Brocker进行信息的发布。 type Client interface { Init(...Option) error Options() Options NewMessage(topic string, msg interface{}, opts ...MessageOption) Message NewRequest(service, method string, req interface{}, reqOpts ...RequestOption) Request Call(ctx context.Context, req Request, rsp interface{}, opts ...CallOption) error Stream(ctx context.Context, req Request, opts ...CallOption) (Stream, error) Publish(ctx context.Context, msg Message, opts ...PublishOption) error String() string } 当然他也支持双工通信 Stream 这些具体的实现方式和使用方式，默认的是rpc实现方式，他还有grpc和http方式，在go-plugins里可以找到 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:6","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"7. Server Server看名字大家也知道是做什么的了。监听等待rpc请求。监听broker的订阅信息，等待信息队列的推送等。 type Server interface { Options() Options Init(...Option) error Handle(Handler) error NewHandler(interface{}, ...HandlerOption) Handler NewSubscriber(string, interface{}, ...SubscriberOption) Subscriber Subscribe(Subscriber) error Register() error Deregister() error Start() error Stop() error String() string } 默认的是rpc实现方式，他还有grpc和http方式，在go-plugins里可以找到 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:7","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"8. Service Service是Client和Server的封装，他包含了一系列的方法使用初始值去初始化Service和Client，使我们可以很简单的创建一个rpc服务。 type Service interface { Init(...Option) Options() Options Client() client.Client Server() server.Server Run() error String() string } ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:3:8","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["Go-Micro"],"content":"4. 参考 https://blog.csdn.net/mi_duo/article/details/82701732 ","date":"2019-05-22","objectID":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/:4:0","tags":["Go","Micro"],"title":"Go-Micro框架入门教程(一)---框架结构","uri":"/posts/go-micro/00-go-micro%E7%AE%80%E4%BB%8B/"},{"categories":["MongoDB"],"content":"MongoDB安装与可视化工具选择","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"本文主要记录了如何通过Docker方便快捷的安装MongoDB数据库。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:0:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"1. 概述 MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:1:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"2. 安装 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:2:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"2.1 拉取镜像 docker pull mongo ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:2:1","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"2.2 准备环境 /usr/local/docker/mongodb/data //数据 /usr/local/docker/mongodb/backup //备份 /usr/local/docker/mongodb/conf //配置文件 准备3个文件夹用来存放相应数据。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:2:2","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"2.3 配置文件 mongodb.conf配置文件如下，建立好配置文件放到前面建好的目录中。 # mongodb.conf logappend=true # bind_ip=127.0.0.1 port=27017 fork=false noprealloc=true # 是否开启身份认证 auth=false ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:2:3","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"2.4 启动容器 docker run --name mongodb -v \\ /usr/local/docker/mongodb/data:/data/db -v \\ /usr/local/docker/mongodb/backup:/data/backup -v \\ /usr/local/docker/mongodb/conf:/data/configdb -p \\27017:27017 -d mongo \\ -f /data/configdb/mongodb.conf \\ --auth # 命令说明 容器命名mongodb， 数据库数据文件挂载到/usr/local/docker/mongodb/data 备份文件挂载到/usr/local/docker/mongodb/backup 启动的配置文件目录挂载到容器的/usr/local/docker/mongodb/conf --auth开启身份验证。 -f /data/configdb/mongodb.conf 以配置文件启动 # mongod启动命令是在容器内执行的，因此使用的配置文件路径是相对于容器的内部路径。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:2:4","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"3. 添加用户 使用MongoDB之前需要先创建用户。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:3:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"3.1 进入容器 docker exec -it mongodb bash 执行该命令进入到刚才启动的容器中。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:3:1","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"3.2 进入 MongoDB mongo 执行该命令进入到MongoDB客户端。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:3:2","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"3.3 创建用户 # 进入 admin 的数据库 use admin # 创建管理员用户 db.createUser( { user: \"admin\", pwd: \"123456\", roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } ) # 创建有可读写权限的用户. 对于一个特定的数据库, 比如'demo' db.createUser({ user: 'test', pwd: '123456', roles: [{role: \"readWrite\", db: \"demo\"}] }) ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:3:3","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"4. 使用 到此为止MongoDB就安装完成了，可以远程连接了。连接之前先关闭Linux防火墙。 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:4:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"4.1 可视化工具 可视化工具暂时用的Robo3T,官网：https://robomongo.org/ ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:4:1","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"4.2 连接 使用前面创建的用户就可以远程连接到MongoDB了. 其中管理员用户是用来管理用户的。 每个用户只能访问指定的db。 比如上面的test账号只能访问demo这个db。 url:192.168.1.1:27017 username:test password:123456 ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:4:2","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["MongoDB"],"content":"5. 参考 https://www.runoob.com/mongodb/mongodb-tutorial.html ","date":"2019-05-02","objectID":"/posts/mongodb/01-install-by-docker/:5:0","tags":["MongoDB"],"title":"MongoDB教程(一)---基于Docker安装MongoDB","uri":"/posts/mongodb/01-install-by-docker/"},{"categories":["Docker"],"content":"容器和Docker介绍,虚拟机与容器技术简单对比","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"本文主要介绍了什么是容器，接着对 Docker 做了详细介绍，最后对虚拟机与容器技术做了简单的对比 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:0:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"1. 什么是容器 一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性　，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:1:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"2. 什么是 Docker? 说实话关于Docker是什么并太好说，下面我通过四点向你说明Docker到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 的cgroup，namespace，以及AUFS类的UnionFS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进 程，因此也称其为容器。Docke最初实现是基于 LXC. Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:2:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"2.1 Docker思想 集装箱 标准化： ①运输方式 ② 存储方式 ③ API接口 隔离 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:2:1","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"2.2 Docker 容器特点 轻量 在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准 Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全 Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:2:2","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"3. 为什么要用 Docker ? 一致的运行环境 : Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题。 更快速的启动时间 : 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 隔离性 ： 避免公用的服务器，资源会容易受到其他用户的影响。 弹性伸缩，快速扩展： 善于处理集中爆发的服务器使用压力。 迁移方便 ： 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 持续交付和部署 ： 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:3:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"4. 虚拟机与容器 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。 通过Docker官网，我们知道了这么多Docker的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:4:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"5. Docker概念 Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:5:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"5.1 镜像(Image):一个特殊的文件系统 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。　比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:5:1","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"5.2 容器(Container):镜像运行时的实体 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:5:2","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"5.3 仓库(Repository):集中存放镜像文件的地方 镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过\u003c仓库名\u003e:\u003c标签\u003e的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:5:3","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"6. Securely Build Share and Run 如果你搜索Docker官网，会发现如下的字样：“Securely build, share and run any application, anywhere”。那么Build, Ship, and Run到底是在干什么呢？ Securely Build（安全构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Share（分享镜像） ：可以把镜像放到镜像仓库用于分享。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。 Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将Docker称为码头工人或码头装卸工，这和Docker的中文翻译搬运工人如出一辙。 ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:6:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Docker"],"content":"参考 https://github.com/Snailclimb/JavaGuide/ ","date":"2019-04-15","objectID":"/posts/docker/01-what-is-docker/:7:0","tags":["Docker"],"title":"Docker教程(一)---什么是Docker","uri":"/posts/docker/01-what-is-docker/"},{"categories":["Java"],"content":"Java中的ReentrantLock源码简单分析","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"本文主要对ReentrantLock的源码进行了简单的分析，具体包括ReentrantLock的初始化(公平锁和非公平锁)，加锁过程和解锁过程等。 ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:0:0","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"1. AbstractQueuedSynchronizer ReentrantLock的实现依赖于AbstractQueuedSynchronizer所以需要了解一下AQS。 ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:1:0","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"1.1 简介 类如其名，抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，是java.util.concurrent的核心，CountDownLatch、FutureTask、Semaphore、ReentrantLock等都有一个内部类是这个抽象类的子类。 AQS定义两种资源共享方式： Exclusive: 独占，只有一个线程能执行,如ReentrantLock Share: 共享，多个线程可同时执行，如Semaphore/CountDownLatch ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:1:1","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"1.2 AQS的4个属性 // 头结点，大概可以看做是当前持有锁的线程 private transient volatile Node head; // 阻塞的尾节点，每个新的节点进来，都插入到最后 private transient volatile Node tail; //当前锁的状态，0代表没有被占用，大于0代表有线程持有当前锁 //是可重入锁 每次获取都活加1 private volatile int state; // 代表当前持有独占锁的线程 锁重入时用这个来判断当前线程是否已经拥有了锁 //继承自AbstractOwnableSynchronizer private transient Thread exclusiveOwnerThread; ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:1:2","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"1.3 阻塞队列Node节点的属性 Node 的数据结构其实也挺简单的，就是 thread + waitStatus + pre + next 四个属性而已。 static final class Node { /** Marker to indicate a node is waiting in shared mode */ // 标识节点当前在共享模式下 static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ // 标识节点当前在独占模式下 static final Node EXCLUSIVE = null; // ======== 下面的几个int常量是给waitStatus用的 =========== /** waitStatus value to indicate thread has cancelled */ // 表示此线程取消了争抢这个锁 static final int CANCELLED = 1; /** waitStatus value to indicate successor's thread needs unparking */ //被标识为该等待唤醒状态的后继结点，当其前继结点的线程释放了同步锁或被取消， //将会通知该后继结点的线程执行。 //就是处于唤醒状态，只要前继结点释放锁，就会通知标识为SIGNAL状态的后继结点的线程执行。 static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ //该标识的结点处于等待队列中，结点的线程等待在Condition上,等待其他线程唤醒 //当其他线程调用了Condition的signal()方法后，CONDITION状态的结点将 //从等待队列转移到同步队列中，等待获取同步锁。 static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ // 与共享模式相关，在共享模式中，该状态标识结点的线程处于可运行状态。 static final int PROPAGATE = -3; // ===================================================== // 节点的等待状态 // 取值为上面的1、-1、-2、-3，或者0 // 这么理解，暂时只需要知道如果这个值 大于0 代表此线程取消了等待， // 也许就是说半天抢不到锁，不抢了，ReentrantLock是可以指定timeouot的 //AQS在判断状态时，通过用waitStatus\u003e0表示取消状态，而waitStatus\u003c0表示有效状态。 volatile int waitStatus; // 前驱节点的引用 volatile Node prev; // 后继节点的引用 volatile Node next; // 这个就是线程对象 volatile Thread thread; } ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:1:3","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"2. ReentrantLock的使用 /** * Server层 * 模拟ReentrantLock使用 * * @author illusoryCloud */ public class UserServer { /** * 默认是非公平锁 传入参数true则创建的是公平锁 */ private static ReentrantLock reentrantLock = new ReentrantLock(true); public void updateUser() { //加锁 同一时刻只能有一个线程更新User reentrantLock.lock(); try { //do something } finally { //释放锁放在finally代码块中 保证出现异常等情况也能释放锁 reentrantLock.unlock(); } } } ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:2:0","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"3. ReentrantLock源码分析 ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:3:0","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"1. 初始化 ReentrantLock reentrantLock = new ReentrantLock(true); /** *默认是非公平锁 */ public ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:3:1","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"2. 加锁过程 reentrantLock.lock(); 公平锁实现如下(JDK1.8)： /** * Sync object for fair locks */ static final class FairSync extends Sync { private static final long serialVersionUID = -3000897897090466540L; //争锁 final void lock() { //1 acquire(1); } /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() \u0026\u0026 compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u003c 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false; } } 1. acquire(1); /** * 尝试获取锁 */ public final void acquire(int arg) { //tryAcquire(1) 首先尝试获取一下锁 //若成功则不需要进入等待队列了 //1.1 if (!tryAcquire(arg) \u0026\u0026 //1.2 // tryAcquire(arg)没有成功，这个时候需要把当前线程挂起，放到阻塞队列中。 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //1.3 selfInterrupt(); } 1.1 tryAcquire(1) /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. * 尝试直接获取锁，返回值是boolean，代表是否获取到锁 * 返回true：1.没有线程在等待锁；2.重入锁，线程本来就持有锁，也就可以理所当然可以直接获取 */ protected final boolean tryAcquire(int acquires) { //获取当前线程 final Thread current = Thread.currentThread(); //查看锁的状态 int c = getState(); //state == 0 此时此刻没有线程持有锁 可以直接获取锁 if (c == 0) { //由于是公平锁 则在获取锁之前先看一下队列中还有没有其他等待的线程 //讲究先来后到 所以是公平锁 这也是和非公平锁的差别 //非公平锁在这里会直接尝试获取锁 //1.1.1 if (!hasQueuedPredecessors() \u0026\u0026 // 如果没有线程在等待，那就用CAS尝试获取一下锁 // 不成功的话，只能说明几乎同一时刻有个线程抢先获取到了锁 //因为刚才hasQueuedPredecessors判断是前面没有线程在等待的 //1.1.2 compareAndSetState(0, acquires)) { //获取到锁后把当前线程设置为锁的拥有者 //1.1.3 setExclusiveOwnerThread(current); //获取锁成功直接返回true return true; } } //到这里说明当前锁已经被占了 //然后判断如果当前线程就是持有锁的线程 //那么这次就是锁的重入 else if (current == getExclusiveOwnerThread()) { //把state加1 int nextc = c + acquires; if (nextc \u003c 0) throw new Error(\"Maximum lock count exceeded\"); //1.1.4 setState(nextc); return true; } //上面两个条件都不满足就返回false //获取锁失败了 回到上一个方法继续看 return false; } 1.1.1 hasQueuedPredecessors() /** * 通过判断\"当前线程\"是不是在CLH队列的队首 * 来返回AQS中是不是有比“当前线程”等待更久的线程 */ public final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t \u0026\u0026 ((s = h.next) == null || s.thread != Thread.currentThread()); } 1.1.2 compareAndSetState(0, acquires)) /** * 通过CAS设置锁的状态 */ protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 1.1.3 setExclusiveOwnerThread(current) /** * 设置锁的拥有者 */ protected final void setExclusiveOwnerThread(Thread thread) { exclusiveOwnerThread = thread; } 1.1.4 setState(nextc) /** * 设置锁的状态 */ protected final void setState(int newState) { state = newState; } 回到前面的方法 /** * 尝试获取锁 */ public final void acquire(int arg) { //tryAcquire(1) 首先尝试获取一下锁 //若成功则不需要进入等待队列了 //1.1 if (!tryAcquire(arg) \u0026\u0026 //1.2 // tryAcquire(arg)没有成功，这个时候需要把当前线程挂起，放到阻塞队列中。 //addWaiter(Node.EXCLUSIVE) 1.2.1 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //1.3 selfInterrupt(); } 1.1tryAcquire返回false则继续执行后面的 1.2acquireQueued(addWaiter(Node.EXCLUSIVE), arg) 1.2.1 addWaiter(Node.EXCLUSIVE) /** * 此方法的作用是把线程包装成node，同时进入到队列中 * 参数mode此时是Node.EXCLUSIVE，代表独占模式 */ private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 以下几行代码想把当前node加到链表的最后面去，也就是进到阻塞队列的最后 Node pred = tail; // tail!=null --\u003e 队列不为空 if (pred != null) { // 设置自己的前驱 为当前的队尾节点 node.prev = pred; // 用CAS把自己设置为队尾, 如果成功后，tail == node了 //1.2.1.1 if (compareAndSetTail(pred, node)) { // 进到这里说明设置成功，当前node==tail, 将自己与之前的队尾相连， // 上面已经有 node.prev = pred // 加上下面这句，也就实现了和之前的尾节点双向连接了 pred.next = nod","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:3:2","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"3. 解锁过程 reentrantLock.unlock() 解锁的代码比较相比加锁的要简单不少 /** * 解锁 */ public void unlock() { //1 sync.release(1); } 1. sync.release(1) /** * 释放锁 * */ public final boolean release(int arg) { //1.1 //这里尝试释放锁如果成功则进入if里面 if (tryRelease(arg)) { // h赋值为当前的head节点 Node h = head; //如果head节点不是null //并且head节点的waitStatus不等于0 即head节点不是刚初始化的 //因为刚初始化是waitStatus是等于0的 if (h != null \u0026\u0026 h.waitStatus != 0) //1.2 //唤醒后继节点 unparkSuccessor(h); return true; } return false; } 1.1 tryRelease(1) /** * 尝试释放锁 */ protected final boolean tryRelease(int releases) { //可重入锁 所以state可以大于1 每次释放时state减1 int c = getState() - releases; //如果当前线程不是拥有锁的线程直接抛出异常 这肯定嘛 都没获取到锁你释放什么 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); // 是否完全释放锁 boolean free = false; // state==0了 说明可以完全释放锁了 if (c == 0) { free = true; //把锁的拥有者设置为null setExclusiveOwnerThread(null); } //锁的状态设置为0 即没有被获取 setState(c); //到这里 锁已经释放了 //回到上边的release(1)方法 return free; } 1.2 unparkSuccessor(h) /** * Wakes up node's successor, if one exists. * 唤醒后继节点 如果有的话 * @param node the node 参数node是head头结点 */ private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // 如果head节点当前waitStatus\u003c0, 将其修改为0 if (ws \u003c 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 下面的代码就是唤醒后继节点，但是有可能后继节点取消了等待（waitStatus==1） Node s = node.next; //如果直接后继节点是null或者 waitStatus \u003e 0即取消了等待 //那么就直接从队尾往前找，找到waitStatus\u003c=0的所有节点中排在最前面的 if (s == null || s.waitStatus \u003e 0) { s = null; // 从后往前找，不必担心中间有节点取消(waitStatus==1)的情况 for (Node t = tail; t != null \u0026\u0026 t != node; t = t.prev) if (t.waitStatus \u003c= 0) s = t; } //如果直接后继节点不是空的就直接唤醒 if (s != null) // 唤醒线程 LockSupport.unpark(s.thread); } 唤醒线程以后，被唤醒的线程将从以下代码中继续往前走： private final boolean parkAndCheckInterrupt() { LockSupport.park(this); // 刚刚线程被挂起在这里了 return Thread.interrupted(); } // 又回到这个方法了：acquireQueued(final Node node, int arg)，这个时候，node的前驱是head了 ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:3:3","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["Java"],"content":"4. 参考 https://javadoop.com/post/AbstractQueuedSynchronizer#toc0 https://blog.csdn.net/chen77716/article/details/6641477 https://www.cnblogs.com/waterystone/p/4920797.html ","date":"2019-03-15","objectID":"/posts/java/08-reentrantlock/:4:0","tags":["Java"],"title":"ReentrantLock源码分析","uri":"/posts/java/08-reentrantlock/"},{"categories":["MySQL"],"content":"通过docker compose 一键搭建 MySQL 开发环境","date":"2019-03-05","objectID":"/posts/mysql/01-install-by-docker/","tags":["MySQL"],"title":"MySQL教程(一)---通过docker 一键搭建 MySQL 开发环境","uri":"/posts/mysql/01-install-by-docker/"},{"categories":["MySQL"],"content":"本文主要记录了如何通过 docker compose 一键搭建一个 MySQL 开发环境。 ","date":"2019-03-05","objectID":"/posts/mysql/01-install-by-docker/:0:0","tags":["MySQL"],"title":"MySQL教程(一)---通过docker 一键搭建 MySQL 开发环境","uri":"/posts/mysql/01-install-by-docker/"},{"categories":["MySQL"],"content":"1. 环境准备 首先需要安装 docker 即 docker-compose。 具体安装步骤看这里 然后准备一下目录结构 mysql/ --/data --docker-compose.yml 其中 data 目录用于存放 MySQL 数据。 ","date":"2019-03-05","objectID":"/posts/mysql/01-install-by-docker/:1:0","tags":["MySQL"],"title":"MySQL教程(一)---通过docker 一键搭建 MySQL 开发环境","uri":"/posts/mysql/01-install-by-docker/"},{"categories":["MySQL"],"content":"2. docker-compose.yml docker-compose.yml 具体内容如下： version: '3.1' services: db: # 默认会拉最新的镜像 image: mysql:latest restart: always environment: # 指定 root 账户密码 开发环境就随便设置了一个 MYSQL_ROOT_PASSWORD: 123456 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3306:3306 volumes: - ./data:/var/lib/mysql 默认安装的 MySQL 最新版本,需要其他版本的话请自行修改镜像。 更多docker-compose.yml点击这里 ","date":"2019-03-05","objectID":"/posts/mysql/01-install-by-docker/:2:0","tags":["MySQL"],"title":"MySQL教程(一)---通过docker 一键搭建 MySQL 开发环境","uri":"/posts/mysql/01-install-by-docker/"},{"categories":["MySQL"],"content":"3. 启动 真·一键启动 # -d 参数指定后台启动 也可以不加 docker-compose up -d ","date":"2019-03-05","objectID":"/posts/mysql/01-install-by-docker/:3:0","tags":["MySQL"],"title":"MySQL教程(一)---通过docker 一键搭建 MySQL 开发环境","uri":"/posts/mysql/01-install-by-docker/"},{"categories":["Java"],"content":"Java运行时数据区，包括线程私有的程序计数器，虚拟机栈，本地方法栈和线程共享的堆，方法区等简单介绍","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"本文主要讲的是Java运行时数据区，包括线程私有的程序计数器，虚拟机栈，本地方法栈和线程共享的堆，方法区等。 线程私有的：程序计数器 、虚拟机栈、本地方法栈 线程共享的： 堆、方法区 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:0","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.1 程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。 字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现OutOfMemoryError的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:1","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.2 虚拟机栈 Java虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口。 每个方法在执行时都会创建一个栈帧,每一个方法从调用到执行完成的过程就是一个栈帧在虚拟机中的入栈到出栈的过程。 局部变量表 存放了编译时期可知的各种基本类型（Boolean，byte，char,short,int.float.long,double）、对象引用（reference类型）和returnAddress（指向了一条字节码指令的地址）。局部变量表的创建是在方法被执行的时候,随着栈帧的创建而创建.而且,局部变量表的大小在编译时期就可以确定下来了,在创建的时候只需要分配实现规定好的大小即可.此外,在方法运行过程中局部变量表的大小是不会发生改变的。 操作数栈 后进先出LIFO，最大深度由编译期确定。栈帧刚建立时，操作数栈为空，执行方法操作时，操作数栈用于存放JVM从局部变量表复制的常量或者变量，提供提取，及结果入栈，也用于存放调用方法需要的参数及接受方法返回的结果。 操作数栈可以存放一个jvm中定义的任意数据类型的值。在任意时刻，操作数栈都一个固定的栈深度，基本类型除了long、double占用两个深度，其它占用一个深度. 动态链接 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接。Class文件的常量池中存在有大量的符号引用，字节码中的方法调用指令就以常量池中指向方法的符号引用为参数。这些符号引用，一部分会在类加载阶段或第一次使用的时候转化为直接引用（如final、static域等），称为静态解析，另一部分将在每一次的运行期间转化为直接引用，这部分称为动态连接。 方法返回地址 当一个方法被执行后，有两种方式退出该方法：执行引擎遇到了任意一个方法返回的字节码指令或遇到了异常，并且该异常没有在方法体内得到处理。无论采用何种退出方式，在方法退出之后，都需要返回到方法被调用的位置，程序才能继续执行。方法返回时可能需要在栈帧中保存一些信息，用来帮助恢复它的上层方法的执行状态。一般来说，方法正常退出时，调用者的PC计数器的值就可以作为返回地址，栈帧中很可能保存了这个计数器值，而方法异常退出时，返回地址是要通过异常处理器来确定的，栈帧中一般不会保存这部分信息。 方法退出的过程实际上等同于把当前栈帧出栈，因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈，如果有返回值，则把它压入调用者栈帧的操作数栈中，调整PC计数器的值以指向方法调用指令后面的一条指令。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:2","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.3 本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:3","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.4 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。**Java虚拟机规范中说的是：所有的对象实例以及数组都要在堆上分配内存。但是随着JIT(just in time)编译器的发展与逃逸分析技术的成熟，栈上分配，标量替换优化技术将会导致一些微妙的变化，所有对象都分配在堆上也变得不是那么绝对了。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor空间、To Survivor空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:4","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.5 方法区 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 HotSpot 虚拟机中方法区也常被称为 “永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:5","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.6 常量池 全局字符串池 全局字符串池里的内容是在类加载完成，经过验证，准备阶段之后在堆中生成字符串对象实例，然后将该字符串对象实例的引用值存到string pool中（记住：string pool中存的是引用值而不是具体的实例对象，具体的实例对象是在堆中开辟的一块空间存放的。）。 在HotSpot VM里实现的string pool功能的是一个StringTable类，它是一个哈希表，里面存的是驻留字符串(也就是我们常说的用双引号括起来的)的引用（而不是驻留字符串实例本身），也就是说在堆中的某些字符串实例被这个StringTable引用之后就等同被赋予了”驻留字符串”的身份。这个StringTable在每个HotSpot VM的实例只有一份，被所有的类共享。 静态常量池 也叫class文件常量池（class constant pool）,Class文件中除了有类的版本，字段，方法，接口等描述信息外还有一项信息是常量池,用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。 字面量就是我们所说的常量概念，如文本字符串、被声明为final的常量值等。 符号引用是一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可（它与直接引用区分一下，直接引用一般是指向方法区的本地指针，相对偏移量或是一个能间接定位到目标的句柄）。一般包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 运行时常量池 运行时常量池是方法区的一部分。Class文件中除了有类的版本，字段，方法，接口等描述信息外还有一项信息是常量池,用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。 既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 JDK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 JDK1.8后放在一个独立空间里面，叫做“元空间” jvm在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。而当类加载到内存中后，jvm就会将class常量池中的内容存放到运行时常量池中，由此可知，运行时常量池也是每个类都有一个。在上面我也说了，class常量池中存的是字面量和符号引用，也就是说他们存的并不是对象的实例，而是对象的符号引用值。而经过解析（resolve）之后，也就是把符号引用替换为直接引用，解析的过程会去查询全局字符串池，也就是我们上面所说的StringTable，以保证运行时常量池所引用的字符串与全局字符串池中所引用的是一致的。 小结 1.全局常量池在每个VM中只有一份，存放的是字符串常量的引用值。 2.class常量池是在编译的时候每个class都有的，在编译阶段，存放的是常量的符号引用。 3.运行时常量池是在类加载完成之后，将每个class常量池中的符号引用值转存到运行时常量池中，也就是说，每个class都有一个运行时常量池，类在解析之后，将符号引用替换成直接引用，与全局常量池中的引用值保持一致。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:6","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"1.7 直接内存 直接内存并不是虚拟机运行时数据区的一部分，但是也频繁被用到，也可能导致OOM,虚拟机内存+直接内存超过物理内存时。 在JDK1.4出现的NIO类中引入了一个基于Channel和Buffer的IO方式，它可以直接使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，能在一些场合中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:7","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Java"],"content":"参考 《深入理解Java虚拟机》 https://blog.csdn.net/qq_26222859/article/details/73135660 ","date":"2019-02-24","objectID":"/posts/java/07-jvm-runtime-area/:0:8","tags":["Java"],"title":"Java运行时数据区","uri":"/posts/java/07-jvm-runtime-area/"},{"categories":["Nginx"],"content":"Nginx反向代理和负载均衡配置与使用","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"本章主要通过实例简单介绍了Nginx反向代理和负载均衡功能。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:0:0","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"1. 反向代理(proxy) ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:1:0","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"1.1 简介 如果您的内容服务器具有必须保持安全的敏感信息，如信用卡号数据库，可在防火墙外部设置一个代理服务器作为内容服务器的替身。 当外部客户机尝试访问内容服务器时，会将其送到代理服务器。实际内容位于内容服务器上，在防火墙内部受到安全保护。代理服务器位于防火墙外部，在客户机看来就像是内容服务器。 这样，代理服务器就在安全数据库和可能的恶意攻击之间提供了又一道屏障。与有权访问整个数据库的情况相对比，就算是侥幸攻击成功，作恶者充其量也仅限于访问单个事务中所涉及的信息。未经授权的用户无法访问到真正的内容服务器，因为防火墙通路只允许代理服务器有权进行访问。 就是客户端先访问Nginx服务器，Nginx收到请求后再去请求内容服务器,这样中间多了一个Nginx服务器中转，会更加安全。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:1:1","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"1.2 配置 1. 修改配置文件 首先需要修改Nginx服务器配置文件``nginx.conf`。 配置文件大概是这样的，在server中添加一个location用于中转。 http{ keepalive_timeout 65; server{ listen 80; //端口号 server_name localhost; //域名 location \\ { root html; //网站根目录 index index.html; //网站首页 } access_log logs/host.access.log main; //访问日志 error page 500 error.html; //错误页面 #这里就是代理 通过正则表达式来匹配 #后缀以.jsp结尾的请求都会跳转到 http://192.168.5.154:8080; location ~ \\.jsp$ { proxy_pass http://192.168.5.154:8080; } } } 2. 开启内容服务器 然后在192.168.5.154的8080端口开启了一个``tomcat,当做是真正的内容服务器，在tomcat默认的index.jsp`中添加了一句显示IP地址的。 \u003c!--测试Nginx反向代理新增--\u003e remote ip:\u003c%=request.getRemoteAddr()%\u003e ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:1:2","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"1.3 测试 然后开始访问： 首先直接访问内容服务器(Tomcat)：192.168.5.154:8080 remote ip:192.168.5.199 然后访问Nginx通过代理来访问内容服务器：192.168.5.154/index.jsp remote ip:192.168.5.154 显示远程 IP是192.168.5.154，这个刚好就是Nginx服务器的IP； 反向代理成功。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:1:3","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"1.4 问题 前面设置后反向代理已经成功了,但是这样设置后，每次访问内容服务器都显示的是Nginx服务器的IP,内容服务器无法获取用户的真实IP，所以还需要进行一点修改。 1. 修改 http{ keepalive_timeout 65; server{ listen 80; //端口号 server_name localhost; //域名 location \\ { root html; //网站根目录 index index.html; //网站首页 } access_log logs/host.access.log main; //访问日志 error page 500 error.html; //错误页面 #这里就是代理 通过正则表达式来匹配 #后缀以.jsp结尾的请求都会跳转到 http://192.168.5.154:8080; location ~ \\.jsp$ { #在请求头中添加上真实的IP #具体格式为 proxy_set_header 属性名 数据 proxy_set_header X-real-ip $remote_addr proxy_pass http://192.168.5.154:8080; } } } proxy_set_header X-real-ip $remote_addr :Nginx服务器是知道客户端真实IP的，所以为了让内容服务器知道真实IP，只需要将真实IP添加到请求头中就可以了。 其中X-real-ip 是自定义的，内容服务器取数据时也使用这个X-real-ip $remote_addr 则是获取远程客户端IP。 2. 测试： 修改jsp，添加了一句代码。 \u003c!--测试Nginx反向代理新增--\u003e \u003c!--获取请求头中的真实IP--\u003e Real remote ip:\u003c%=request.getHeader(\"X-real-ip\")%\u003e \u003cbr /\u003e remote ip/Nginx ip:\u003c%=request.getRemoteAddr()%\u003e 然后开始访问： 首先直接访问内容服务器(Tomcat)：192.168.5.154:8080 Real remote ip:null remote ip/Nginx ip:192.168.5.199 然后访问Nginx通过代理来访问内容服务器：192.168.5.154/index.jsp Real remote ip:192.168.5.199 remote ip/Nginx ip:192.168.5.154 成功获取到真实IP，问题解决。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:1:4","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"2. 负载均衡(upstream) ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:2:0","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"2.1 简介 可以在一个组织内使用多个代理服务器来平衡各 Web 服务器间的网络负载。 对于客户机发往真正服务器的请求，代理服务器起着中间调停者的作用。客户机每次都使用同一个 URL，但请求所采取的路由每次都可能经过不同的代理服务器。 同样是客户端先访问Nginx服务器，然后Nginx服务器再根据负载均衡算法将请求分发到不同的内容服务器上。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:2:1","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"2.2 配置 同意需要修改Nginx服务器配置文件``nginx.conf`。 配置文件大概是这样的，在server中添加一个location用于中转。 http{ keepalive_timeout 65; #upstream 负载均衡 与server同级 #tomcat_server 负载均衡名字 自定义的 #要用在下面location反向代理处 #poxy_pass http://tomcat_server; upstream tomcat_server{ #weight权重 max_fails 最大失败次数 超过后就认为该节点down掉了 fail_timeout 超时时间 #192.168.5.154:8080 IP地址或者域名都可以 server 192.168.5.154:8080 weight=1 max_fails=2 fail_timeout=30s; server 192.168.5.155:8080 weight=1 max_fails=2 fail_timeout=30s; } server{ listen 80; //端口号 server_name localhost; //域名 location \\ { root html; //网站根目录 index index.html; //网站首页 } access_log logs/host.access.log main; //访问日志 error page 500 error.html; //错误页面 #proxy_pass 反向代理 通过正则表达式来匹配 #后缀以.jsp结尾的请求都会跳转到 http://192.168.5.154:8080; #proxy_set_header 将真实IP添加到请求头中 传递到内容服务器 location ~ \\.jsp$ { proxy_set_header X-real-ip $remote_addr #proxy_pass http://192.168.5.154:8080; #反向代理这里不光可以写IP 还可以写上面配置的负载均衡 proxy_pass http://tomcat_server; } } } ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:2:2","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"2.3 测试 开启两个tomcat，一个是192.168.5.154,一个是``192.168.5.155`. 然后浏览器访问nginx服务器：192.168.5.154/index.jsp； 会随机跳转到两个tomcat服务器中的一个就说明负载均衡配置成功了。 ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:2:3","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"3. 参考 http://www.runoob.com/linux/nginx-install-setup.html https://www.cnblogs.com/javahr/p/8318728.html ","date":"2019-02-17","objectID":"/posts/nginx/04-proxy-upstream/:3:0","tags":["Nginx"],"title":"Nginx教程(四)---反向代理与负载均衡","uri":"/posts/nginx/04-proxy-upstream/"},{"categories":["Nginx"],"content":"通过cron定时任务实现Nginx的日志文件切割","date":"2019-02-16","objectID":"/posts/nginx/03-log/","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"本章主要对Nginx服务器的日志文件分析，包括日志文件切割与cron定时任务语法详解。 ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:0:0","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"1. 日志文件 再看一下Nginx目录结构 /usr/local/nginx --conf 配置文件 --html 网页文件 --logs 日志文件 --sbin 主要二进制文件 ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:1:0","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"1.1 查看日志 前面看了conf配置文件，这里看下logs日志文件; /usr/local/nginx/logs -- access.log #访问日志 -- error.log #错误日志 -- nginx.pid #存放Nginx当前进程的pid nginx.pid 存放Nginx当前进程的pid [root@localhost logs]# cat nginx.pid 98830 [root@localhost logs]# ps aux|grep nginx root 98830 0.0 0.0 20552 616 ? Ss 09:57 0:00 nginx: master process /usr/local/nginx/sbin/nginx nobody 98831 0.0 0.1 23088 1636 ? S 09:57 0:00 nginx: worker process root 105254 0.0 0.0 112708 976 pts/1 R+ 11:02 0:00 grep --color=auto nginx access.log 访问日志 [root@localhost logs]# tail -f -n 20 access.log 192.168.5.199 - - [04/Mar/2019:10:02:10 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" 192.168.5.199 - - [04/Mar/2019:10:02:10 +0800] \"GET /favicon.ico HTTP/1.1\" 404 555 \"http://192.168.5.154/\" \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:1:1","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"1.2 日志分割 Nginx日志都会存在一个文件里，随着时间推移，这个日志文件会变得非常大，分析的时候很难操作，所以需要对日志文件进行分割，可以根据访问量来进行选择：如按照天分割、或者半天、小时等。 建议使用shell脚本方式进行切割日志 。 1. 编写脚本 脚本如下： #!/bin/sh #根路径 BASE_DIR=/usr/local/nginx #最开始的日志文件名 BASE_FILE_NAME_ACCESS=access.log BASE_FILE_NAME_ERROR=error.log BASE_FILE_NAME_PID=nginx.pid #默认日志存放路径 DEFAULT_PATH=$BASE_DIR/logs #日志备份根路径 BASE_BAK_PATH=$BASE_DIR/datalogs BAK_PATH_ACCESS=$BASE_BAK_PATH/access BAK_PATH_ERROR=$BASE_BAK_PATH/error #默认日志文件路径+文件名 DEFAULT_FILE_ACCESS=$DEFAULT_PATH/$BASE_FILE_NAME_ACCESS DEFAULT_FILE_ERROR=$DEFAULT_PATH/$BASE_FILE_NAME_ERROR #备份时间 BAK_TIME=`/bin/date -d yesterday +%Y%m%d%H%M` #备份文件 路径+文件名 BAK_FILE_ACCESS=$BAK_PATH_ACCESS/$BAK_TIME-$BASE_FILE_NAME_ACCESS BAK_FILE_ERROR=$BAK_PATH_ERROR/$BAK_TIME-$BASE_FILE_NAME_ERROR # 打印一下备份文件 echo access.log备份成功：$BAK_FILE_ACCESS echo error.log备份成功：$BAK_FILE_ERROR #移动文件 mv $DEFAULT_FILE_ACCESS $BAK_FILE_ACCESS mv $DEFAULT_FILE_ERROR $BAK_FILE_ERROR #向nginx主进程发信号重新打开日志 kill -USR1 `cat $DEFAULT_PATH/$BASE_FILE_NAME_PID` 其实很简单，主要步骤如下： 1.移动日志文件：这里已经将日志文件移动到``datalogs`目录下了，但Nginx还是会继续往这里面写日志 2.发送USR1命令：告诉Nginx把日志写到``Nginx.conf`中配置的那个文件中，这里会重新生成日志文件 具体如下： 第一步:就是重命名日志文件，不用担心重命名后nginx找不到日志文件而丢失日志。在你未重新打开原名字的日志文件前(即执行第二步之前)，nginx还是会向你重命名的文件写日志，Linux是靠文件描述符而不是文件名定位文件。 第二步:向nginx主进程发送USR1信号。nginx主进程接到信号后会从配置文件中读取日志文件名称，重新打开日志文件(以配置文件中的日志名称命名)，并以工作进程的用户作为日志文件的所有者。重新打开日志文后，nginx主进程会关闭重名的日志文件并通知工作进程使用新打开的日志文件。(就不会继续写到前面备份的那个文件中了)工作进程立刻打开新的日志文件并关闭重名名的日志文件。然后你就可以处理旧的日志文件了。 2. 赋权 [root@localhost sbin]# chmod 777 log.sh 将log.sh脚本设置为可执行文件 3. 执行 设置一个定时任务用于周期性的执行该脚本 cron是一个linux下的定时执行工具，可以在无需人工干预的情况下运行作业。 service crond start //启动服务 service crond stop //关闭服务 service crond restart //重启服务 service crond reload //重新载入配置 service crond status //查看服务状态 设置定时任务： [root@localhost datalogs]# crontab -e */1 * * * * sh /usr/local/nginx/sbin/log.sh */1 * * * *： 为定时时间 这里为了测试 是设置的每分钟执行一次； 0 2 * * * :每天凌晨两点执行 sh ：为任务类型 这里是一个sh脚本 /usr/local/nginx/sbin/log.sh ：为脚本路径 4. Nginx信号量 Nginx支持以下几种信号选项： TERM，INT : 快速关闭 QUIT ：从容关闭（优雅的关闭进程,即等请求结束后再关闭) HUP ：平滑重启，重新加载配置文件 （平滑重启，修改配置文件之后不用重启服务器。直接kill -PUT 进程号即可） USR1 ：重新读取日志文件，在切割日志时用途较大（停止写入老日志文件，打开新日志文件，之所以这样是因为老日志文件就算修改的文件名，由于inode的原因，nginx还会一直往老的日志文件写入数据） USR2 ：平滑升级可执行程序 ，nginx升级时候用 WINCH ：从容关闭工作进程 ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:1:2","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"2.cron表达式 ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:2:0","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"2.1 基本语法 cron表达式代表一个时间的集合，使用6个空格分隔的字段表示： 字段名 是否必须 允许的值 允许的特定字符 秒(Seconds) 是 0-59 * / , - 分(Minute) 是 0-59 * / , - 时(Hours) 是 0-23 * / , - 日(Day of month) 是 1-31 * / , - ? 月(Month) 是 1-12 或 JAN-DEC * / , - 星期(Day of week) 否 0-6 或 SUM-SAT * / , - ? 注：月(Month)和星期(Day of week)字段的值不区分大小写，如：SUN、Sun 和 sun 是一样的。 星期字段没提供相当于* 一般只需要写5位就行了。即 分 时 日 月 周 # ┌───────────── min (0 - 59) # │ ┌────────────── hour (0 - 23) # │ │ ┌─────────────── day of month (1 - 31) # │ │ │ ┌──────────────── month (1 - 12) # │ │ │ │ ┌───────────────── day of week (0 - 6) (0 to 6 are Sunday to # │ │ │ │ │ Saturday, or use names; 7 is also Sunday) # │ │ │ │ │ # │ │ │ │ │ # * * * * * command to execute ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:2:1","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"2.2 特定字符 星号(*):表示 cron 表达式能匹配该字段的所有值。如在第2个字段使用星号(hour)，表示每小时 斜线(/):表示增长间隔，如第1个字段(minutes) 值是 3/1，表示每小时的第3分钟开始执行一次，之后每隔1分钟执行一次（1,2,3,4….59都执行一次） 逗号(,):用于枚举值，如第6个字段值是 MON,WED,FRI，表示 星期一、三、五 执行。 连字号(-):表示一个范围，如第3个字段的值为 9-17 表示 9am 到 5pm 之间每个小时（包括9和17） 问号(?):只用于 日(Day of month) 和 星期(Day of week)，表示不指定值，可以用于代替 * ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:2:2","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"3. 参考 https://www.cnblogs.com/crazylqy/p/6891929.html http://www.runoob.com/linux/nginx-install-setup.html ","date":"2019-02-16","objectID":"/posts/nginx/03-log/:3:0","tags":["Nginx"],"title":"Nginx教程(三)---日志文件切割","uri":"/posts/nginx/03-log/"},{"categories":["Nginx"],"content":"Nginx服务器的常用配置文件介绍","date":"2019-02-14","objectID":"/posts/nginx/02-conf/","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"本章主要对Nginx服务器的常用配置文件，包括虚拟主机配置，location配置级语法等。 ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:0:0","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"1. 虚拟主机配置 在前面启动Nignx后，Nginx目录下会多出几个文件夹 /usr/local/nginx --conf 配置文件 --html 网页文件 --logs 日志文件 --sbin 主要二进制文件 --client_body_temp --fastcgi_temp --proxy_temp --scgi_temp --uwsgi_temp 不过这些temp文件夹都不是重点。 ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:1:0","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"1.1 配置文件 这里讲解一下conf里的配置文件，有很多配置文件，重点看 nginx.conf. /usr/local/nginx/conf -- fastcgi.conf -- fastcgi.conf.default -- fastcgi_params -- fastcgi_params.default -- koi-utf -- koi-win -- mime.types -- mime.types.default -- nginx.conf # 重点关心这个 -- nginx.conf.default -- scgi_params -- scgi_params.default -- uwsgi_params -- uwsgi_params.default --win-utf ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:1:1","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"1.2 nginx.conf 看一下默认的nginx.conf [root@localhost conf]# vim nginx.conf //默认配置如下： #可以指定用户 不过无所谓 #user nobody; #nginx工作进程,一般设置为和cpu核数一样 worker_processes 1; #错误日志存放目录 #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #进程pid存放位置 #pid logs/nginx.pid; events { # 单个CPU最大连接数 worker_connections 1024; } # http 这里重点 http { #文件扩展名与类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #设置日志模式 #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; #开启高效传输模式 sendfile on; # 激活tcp_nopush参数可以允许把httpresponse header和文件的开始放在一个文件里发布 # 积极的作用是减少网络报文段的数量 #tcp_nopush on; #连接超时时间，单位是秒 #keepalive_timeout 0; keepalive_timeout 65; #开启gzip压缩功能 #gzip on; #基于域名的虚拟主机 server { #监听端口 listen 80; #域名 server_name localhost; #字符集 #charset koi8-r; #nginx访问日志 这里的main就是上面配置的那个log_format main #access_log logs/host.access.log main; #location 标签 #这里的/表示匹配根目录下的/目录 location / { #站点根目录，即网站程序存放目录 #就是上面的四个文件夹中的html文件夹 root html; #首页排序 默认找index.html 没有在找index.htm index index.html index.htm; } # 错误页面 #error_page 404 /404.html; # redirect server error pages to the static page /50x.html #错误页面 错误码为500 502 503 504时 重定向到50x.html error_page 500 502 503 504 /50x.html; #location 标签 #这里的表示匹配根目录下的/50x.html location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:1:2","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"1.3 基本配置 上面的配置文件好像挺长的，其实最重要的就那么几个。 http{ keepalive_timeout 65; server{ listen 80; //端口号 server_name localhost; //域名 location \\ { root html; //网站根目录 index index.html; //网站首页 } access_log logs/host.access.log main; //访问日志 error page 500 error.html; //错误页面 } } ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:1:3","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"2. location ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:2:0","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"2.1 简介 nginx.conf大概内容如下： http{ keepalive_timeout 65; server{ listen 80; #端口号 server_name localhost; #域名 location \\ { root html; #网站根目录 index index.html; #网站首页 } access_log logs/host.access.log main; #访问日志 error page 500 error.html; #错误页面 } } 其中server代表虚拟主机，一个虚拟主机可以配置多个location location表示uri方法定位 基本语法如下： 1.location=pattern{} 静准匹配 2.location pattern{} 一般匹配 3.location~pattern{} 正则匹配 Nginx可以对数据进行压缩，对一些图片、css、js、html等文件进行缓存，从而实现动静分离等待优化功能。 动态的就去访问tomcat服务器，静态的就直接访问Nginx服务器。 基本语法： location [=|~|~*|^~|@] /uri/ { .... } 〖=〗 表示精确匹配，如果找到，立即停止搜索并立即处理此请求。 〖~ 〗 表示区分大小写匹配 〖~*〗 表示不区分大小写匹配 〖^~ 〗 表示只匹配字符串,不查询正则表达式。 〖@〗 指定一个命名的location，一般只用于内部重定向请求。 ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:2:1","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"2.2 正则表达式 1.语法格式： location [=|~|~*|^~|@] /uri/ { ..... } 1.依据不同的前缀=，^~,~ ，~* ”和不带任何前缀(因为[ ] 表示可选，可以不要的)表达不同的含义。 简单的说尽管location 的/uri/ 配置一样，但前缀不一样，表达的是不同的指令含义。 注意：查询字符串不在URI范围内。例如：/films.htm?fid=123 的URI 是/films.htm。 2.对这些不同前缀，分下类，就2 大类： 正则location : ~ 和~*前缀表示正则location ，~ 区分大小写，~* 不区分大小写。 普通location : =，^~ 和@ 和 无任何前缀, 都属于普通location 。 详细说明： ~ : 区分大小写匹配 ~* : 不区分大小写匹配 !~ : 区分大小写不匹配 !~* : 不区分大小写不匹配 ^ : 以什么开头的匹配 $ : 以什么结尾的匹配 ***** : 代表任意字符 ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:2:2","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"3. 参考 http://www.runoob.com/linux/nginx-install-setup.html ","date":"2019-02-14","objectID":"/posts/nginx/02-conf/:3:0","tags":["Nginx"],"title":"Nginx教程(二)---配置文件详解","uri":"/posts/nginx/02-conf/"},{"categories":["Nginx"],"content":"Nginx安装及其与Apache简单对比","date":"2019-02-13","objectID":"/posts/nginx/01-install/","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"本章主要对Nginx服务器进行了介绍，同时对Nginx与Apache之间做出了对比，最后记录了如何在Linux下通过解压方式安装Nginx，也对Nginx基本使用做出了说明。 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:0:0","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"1. Nginx简介 Nginx是一款轻量级的Web服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。其特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好。 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:1:0","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"1.1 Nginx模块架构 Nginx 由内核和模块组成。Nginx 的模块从结构上分为核心模块、基础模块和第三方模块。 核心模块：HTTP 模块、 EVENT 模块和 MAIL 模块 基础模块： HTTP Access 模块、HTTP FastCGI 模块、HTTP Proxy 模块和 HTTP Rewrite模块 第三方模块：HTTP Upstream Request Hash 模块、 Notice 模块和 HTTP Access Key模块 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:1:1","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"1.2 Nignx与Appache Nginx的高并发得益于其采用了epoll模型，与传统的服务器程序架构不同epoll 是linux内核2.6以后才出现的。 **Nginx采用epoll模型，异步非阻塞，而Apache采用的是select 模型 **。 Select模型：select 选择句柄的时候是遍历所有句柄，也就是说句柄有事件响应时，select 需要遍历所有句柄才能获取到哪些句柄有事件通知，因此效率是非常低。 epoll 模型：epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高。 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:1:2","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"2. 安装 注：这里用的是CentOS 7 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:2:0","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"2.1 安装包下载 官网：http://nginx.org/en/download.html 这里下载的时nginx-1.15.9.tar.gz 上传到服务器上，这里放在了usr/software目录下 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:2:1","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"2.2 环境准备 安装编译源码所需要的工具和库: # yum install gcc gcc-c++ ncurses-devel perl 安装HTTP rewrite module模块: # yum install pcre pcre-devel 安装HTTP zlib模块: # yum install zlib gzip zlib-devel ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:2:2","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"2.3 编译安装 解压： [root@localhost software]# tar -zxvf nginx-1.15.9.tar.gz -C /usr/local //解压到/usr/local目录下 配置: 进行configure配置，检查是否报错。 [root@localhost nginx-1.15.9]# ./configure --prefix=/usr/local/nginx //出现下面的配置摘要就算配置ok Configuration summary + using system PCRE library + OpenSSL library is not used + using system zlib library nginx path prefix: \"/usr/local/nginx\" nginx binary file: \"/usr/local/nginx/sbin/nginx\" ..... nginx http uwsgi temporary files: \"uwsgi_temp\" nginx http scgi temporary files: \"scgi_temp\" 编译安装: [root@localhost nginx-1.15.9]# make\u0026\u0026make install //出现下面的提示就算编译安装ok make[1]: Leaving directory `/usr/local/nginx-1.15.9' 编译安装后多了一个Nginx文件夹,在/usr/local/nginx 内部又分为四个目录 /usr/local/nginx --conf 配置文件 --html 网页文件 --logs 日志文件 --sbin 主要二进制文件 查看Nginx版本: [root@localhost nginx]# /usr/local/nginx/sbin/nginx -v nginx version: nginx/1.15.9 //这里是Nginx 1.15.9 到这里Nginx安装就结束了。 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:2:3","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"3. 基本操作 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:3:0","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"3.1 启动 [root@localhost sbin]# /usr/local/nginx/sbin/nginx //这里如果没有报错就说明启动成功了 查看 [root@localhost sbin]# ps aux|grep nginx root 98830 0.0 0.0 20552 616 ? Ss 09:57 0:00 nginx: master process /usr/local/nginx/sbin/nginx nobody 98831 0.0 0.1 23088 1392 ? S 09:57 0:00 nginx: worker process root 98839 0.0 0.0 112708 976 pts/1 R+ 09:57 0:00 grep --color=auto nginx 可以看到Nginx有两个进程，一个master进程一个worker进程. 同时浏览器已经可以访问了:直接访问IP地址即可http://192.168.5.154/ 显示如下： Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 说明Nginx确实已经启动了。 ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:3:1","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"3.2 常用命令 # 重新载入配置文件 [root@localhost sbin]# /usr/local/nginx/sbin/nginx -s reload # 重新打开日志文件 [root@localhost sbin]# /usr/local/nginx/sbin/nginx -s reopen # 停止 [root@localhost sbin]# /usr/local/nginx/sbin/nginx -s stop ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:3:2","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Nginx"],"content":"4. 参考 http://www.runoob.com/linux/nginx-install-setup.html ","date":"2019-02-13","objectID":"/posts/nginx/01-install/:4:0","tags":["Nginx"],"title":"Nginx教程(一)---安装与配置","uri":"/posts/nginx/01-install/"},{"categories":["Java"],"content":"看《并发编程的艺术》时的笔记","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"最近在看并发编程的艺术这本书，希望加深对并发这块的理解。毕竟并发相关还是十分重要的。本文主要是关于第二章Java并发机制的底层实现原理的相关笔记。主要包括volatile,synchronized,原子操作等实现原理的分析。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:0:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"1. 上下文切换 多线程 即使是单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。 什么是上下文切换 CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。 所以任务从保存到再加载的过程就是一次上下文切换。 上下文切换也会影响多线程的执行速度 因为线程有创建和上下文切换的开销，所以有时候并发不一定比串行快。 减少上下文切换的办法 减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一 些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这 样会造成大量线程都处于等待状态。 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:1:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"2. volatile关键字 如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，其中就包括了Lock前缀.Lock前缀的指令在多核处理器下会引发了两件事情。 1）将当前处理器缓存行的数据写回到系统内存。 Lock前缀指令导致在执行指令期间，声言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存。 如果访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反，它会锁定这块内存区 域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁 定”，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:2:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"3. synchronized原理与应用 Java SE 1.6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁，以及锁的存储结构和升级过程。 Java中的每一个对象都可以作为锁。具体表现 为以下3种形式。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:3:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"3.1 底层实现 JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。 代码块同步是使用monitorenter和monitorexit指令实现的. 而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现。 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:3:1","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"3.2 Java对象头 synchronized用的锁是存在Java对象头里的。 java的对象头由以下三部分组成： 1，Mark Word 2，指向类的指针 3，数组长度（只有数组对象才有） ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:4:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"3.3 锁的升级与对比 Java SE 1.6中，锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状 态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。 偏向锁 HotSpot的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁解除 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程. 轻量级锁 （1）轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 （2）轻量级锁解锁 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。 优缺点比较 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:4:1","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"4. 原子操作的实现原理 原子（atomic）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为“不可被中断的一个或一系列操作”。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:5:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"4.1 处理器如何实现原子操作 处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。 第一个机制是通过总线锁保证原子性。 所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。 第二个机制是通过缓存锁定来保证原子性。 总线锁定的开销比较大，目前处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。 所谓“缓存锁定”是指内存区域如果被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时，会使缓存行无效. ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:5:1","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"4.2 Java如何实现原子操作 使用循环CAS实现原子操作 JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止。 CAS实现原子操作的三大问题 1.ABA问题 但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。 ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1。 2.循环时间长开销大 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 3.只能保证一个共享变量的原子操作 操作多个共享变量时无法使用CAS操作，这个时候就可以用锁。还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。 使用锁机制实现原子操作 锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:5:2","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Java"],"content":"参考 本文内容来自Java并发编程的艺术 ","date":"2019-02-06","objectID":"/posts/java/06-two-volatile-synchronized/:6:0","tags":["Java"],"title":"Synchronize和Volatile底层实现原理","uri":"/posts/java/06-two-volatile-synchronized/"},{"categories":["Network"],"content":"网上常见的一道面试题","date":"2019-01-29","objectID":"/posts/network/05-connection-process/","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"本文主要对用户从浏览器输入URL到页面加载的这一过程进行了具体分析与叙述。包括：DNS解析、发送HTTP请求、TCP连接、服务器响应、浏览器解析渲染页面等。 总体来说分为以下几个过程: 1.DNS解析 2.TCP连接 3.发送HTTP请求 4.服务器处理请求并返回HTTP报文 5.浏览器解析渲染页面 6.连接结束 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:0:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"1. DNS解析 解析域名，找到主机IP。如百度对应的IP为180.97.33.108 ,浏览器输入IP也可以访问到百度。 （1）浏览器会缓存DNS一段时间，一般2-30分钟不等。如果有缓存，直接返回IP，否则下一步。 （2）缓存中无法找到IP，浏览器会进行一个系统调用，查询hosts文件。如果找到，直接返回IP，否则下一步。（在计算机本地目录etc下有一个hosts文件，hosts文件中保存有域名与IP的对应解析，通常也可以修改hosts科学上网或破解软件。） （3）进行了（1）（2）本地查询无果，只能借助于网络。路由器一般都会有自己的DNS缓存，ISP服务商DNS缓存，这时一般都能够得到相应的IP。如果还是无果，只能借助于DNS递归解析了。 （4）这时，ISP的DNS服务器就会开始从根域名服务器开始递归搜索，从.com顶级域名服务器，到baidu的域名服务器。 到这里，浏览器就获得了IP。在DNS解析过程中，常常会解析出不同的IP。比如，电信的是一个IP，网通的是另一个IP。这是采取了智能DNS的结果，降低运营商间访问延时，在多个运营商设置主机房，就近访问主机。电信用户返回电信主机IP，网通用户返回网通主机IP。当然，劫持DNS，也可以屏蔽掉一部分网点的访问，某防火长城也加入了这一特性。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:1:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"2. TCP连接 浏览器与网站建立TCP连接 浏览器利用IP直接与网站主机通信。浏览器发出TCP（SYN标志位为1）连接请求，主机返回TCP（SYN，ACK标志位均为1）应答报文，浏览器收到应答报文发现ACK标志位为1，表示连接请求确认。浏览器返回TCP（）确认报文，主机收到确认报文，三次握手，TCP链接建立完成。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:2:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"3. 发送HTTP请求 浏览器发起HTTP请求 其实这部分又可以称为前端工程师眼中的HTTP，它主要发生在客户端。发送HTTP请求的过程就是构建HTTP请求报文并通过TCP协议中发送到服务器指定端口(HTTP协议80/8080, HTTPS协议443)。HTTP请求报文是由三部分组成: 请求行, 请求报头和请求正文。 请求行 请求行包括：请求方法，URL ， 协议版本 请求行：请求方法 URL 协议版本 eg: GET index.html HTTP/1.1 请求报头 请求报头允许客户端向服务器传递请求的附加信息和客户端自身的信息。 PS: 客户端不一定特指浏览器，有时候也可使用Linux下的CURL命令以及HTTP客户端测试工具等。 常见的请求报头有: Accept, Accept-Charset, Accept-Encoding, Accept-Language, Content-Type, Authorization, Cookie, User-Agent等。 请求正文 当使用POST, PUT等方法时，通常需要客户端向服务器传递数据。这些数据就储存在请求正文中。在请求包头中有一些与请求正文相关的信息，例如: 现在的Web应用通常采用Rest架构，请求的数据格式一般为json。这时就需要设置Content-Type: application/json 浏览器向主机发起一个HTTP请求。请求中包含访问的URL，也就是https://www.lixueduan.com/ ，还有User-Agent用户浏览器操作系统信息，编码等。值得一提的是Accep-Encoding和Cookies项。Accept-Encoding一般采用gzip，压缩之后传输html文件。Cookies如果是首次访问，会提示服务器建立用户缓存信息，如果不是，可以利用Cookies对应键值，找到相应缓存，缓存里面存放着用户名，密码和一些用户设置项。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:3:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"4. 服务器响应 服务器对请求做出响应并返回HTTP响应报文。自然而然这部分对应的就是后端工程师眼中的HTTP。后端从在固定的端口接收到TCP报文开始，这一部分对应于编程语言中的socket。它会对TCP连接进行处理，对HTTP协议进行解析，并按照报文格式进一步封装成HTTP Request对象，供上层使用。这一部分工作一般是由Web服务器去进行，例如Tomcat。 HTTP响应报文也是由三部分组成: 响应行, 响应报头和响应报文。 响应行 响应行包括：协议版本 状态码 状态码描述 响应行包括：协议版本 状态码 状态码描述 eg: HTTP/1.1 200 OK 响应报头 常见的响应报头字段有: Server, Connection…。 响应报文 服务器返回给浏览器的文本信息，通常HTML, CSS, JS, 图片等文件就放在这一部分。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:4:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"5. 浏览器解析渲染页面 返回状态码200 OK，表示服务器可以相应请求，返回报文，由于在报头中Content-type:“text/html”，浏览器以HTML形式呈现，而不是下载文件。 浏览器在收到HTML,CSS,JS文件后，它是如何把页面呈现到屏幕上的? 浏览器是一个边解析边渲染的过程。首先浏览器解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。这个过程比较复杂，涉及到两个概念: reflow(回流)和repain(重绘)。DOM节点中的各个元素都是以盒模型的形式存在，这些都需要浏览器去计算其位置和大小等，这个过程称为relow;当盒模型的位置,大小以及其他属性，如颜色,字体,等确定下来之后，浏览器便开始绘制内容，这个过程称为repain。页面在首次加载时必然会经历reflow和repain。reflow和repain过程是非常消耗性能的，尤其是在移动设备上，它会破坏用户体验，有时会造成页面卡顿。所以我们应该尽可能少的减少reflow和repain。 重定向 负载均衡 但是，对于大型网站存在多个主机站点，往往不会直接返回请求页面，而是重定向。返回的状态码就不是200 OK，而是301,302以3开头的重定向码，浏览器在获取了重定向响应后，在响应报文中Location项找到重定向地址，浏览器重新第一步访问即可。 补充一点的就是，重定向是为了负载均衡或者导入流量，提高SEO排名。利用一个前端服务器接受请求，然后负载到不同的主机上，可以大大提高站点的业务并发处理能力；重定向也可将多个域名的访问，集中到一个站点；由于lixueduan.com，www.lixueduan.com会被搜索引擎认为是两个网站，照成每个的链接数都会减少从而降低排名，永久重定向会将两个地址关联起来，搜索引擎会认为是同一个网站，从而提高排名。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:5:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"6. 连接结束 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:6:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"7. 计算机网络常见问题 看完系列文章，下面这些问题应该也不是问题了。 1.TCP三次握手和四次挥手 2.在浏览器中输入url地址-»显示主页的过程 3.HTTP和HTTPS的区别 4.TCP、UDP协议的区别 5.常见的状态码。 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:7:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"8. 参考 https://segmentfault.com/a/1190000006879700 ","date":"2019-01-29","objectID":"/posts/network/05-connection-process/:8:0","tags":["Network"],"title":"计算机网络(五)---从输入URL到页面加载的过程中发生了什么","uri":"/posts/network/05-connection-process/"},{"categories":["Network"],"content":"HTTP与HTTPS基本概念及SSL工作原理","date":"2019-01-24","objectID":"/posts/network/04-http-https/","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"本文主要介绍了HTTP、HTTPS的基本概念及两者的区别，HTTPS的工作原理及优缺点，最后介绍了HTTP的响应状态码。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:0:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"1. HTTP和HTTPS基本概念 HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。 HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 HTTP--\u003e HTTP--\u003eTCP--\u003eIP HTTPS-\u003e HTTP--\u003eSSL--\u003eTCP--\u003eIP HTTPS协议的主要作用：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:1:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"2. HTTP与HTTPS的区别 HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比HTTP协议安全。 HTTPS和HTTP的区别: https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 这里推荐一下https://letsencrypt.org/已经推出了免费SSL证书 有效期90天，不过可以免费续期。 不想自己弄得话https://user.vaptcha.com/ssl这里提供了比较方便的生成方式也是免费的。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:2:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"3. HTTPS工作原理 我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取，所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。 SSL协议的握手过程 第一步，客户端给出SSL协议版本号、一个客户端生成的随机数1（Client random），以及客户端支持的加密方法。 第二步，服务端根据客服端支持的加密方法选出双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数2（Server random）。 第三步，客户端确认数字证书有效，然后生成一个新的随机数3（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给服务端。 第四步，服务端使用自己的私钥，获取客户端发来的随机数3（即Premaster secret）。到这里双方都拥有三个随机数了，为什么要使用三个随机数呢？这是因为 SSL/TLS 握手过程的数据都是明文传输的，并且多个随机数种子来生成秘钥不容易被暴力破解出来。 第五步，客户端和服务端根据约定的加密方法，使用前面的三个随机数，生成\"对话密钥\"（session key），用来加密接下来的整个对话过程。 第六步，客户端将前面的握手消息生成摘要再用协商好的秘钥加密，这是客户端发出的第一条加密消息。服务端接收后会用秘钥解密，能解出来说明前面协商出来的秘钥是一致的。 第七步，服务端也会将握手过程的消息生成摘要再用秘钥加密，这是服务端发出的第一条加密消息。客户端接收后会用秘钥解密，能解出来说明协商的秘钥是一致的。 到这里，双方已安全地协商出了同一份秘钥，所有的应用层数据都会用这个秘钥加密后再通过 TCP 进行可靠传输。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:3:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"4. HTTPS的优缺点 优点: （1）使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器； （2）HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 （3）HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 （4）谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高” 缺点: （1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电； （2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响； （3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。 （4）SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗。 （5）HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:4:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"5. HTTP响应状态码 状态码以3位数字和原因短语组成，例如 200 OK 。 数字的第一位指定了响应类型，后两位无分类。响应类别一共有5种： 1XX Informational(信息性状态码) 2XX Success(成功状态码) 3XX Redirection(重定向状态码) 4XX Client Error(客户端错误状态码) 5XX Server Error(服务器错误状态码) 200：请求成功 处理方式：获得响应的内容，进行处理 201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到 202：请求被接受，但处理尚未完成 处理方式：阻塞等待 204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃 300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃 301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源 处理方式：重定向到分配的URL 302：请求到的资源在一个不同的URL处临时保存 处理方式：重定向到临时的URL 304 请求的资源未更新 处理方式：丢弃 400 非法请求 处理方式：丢弃 401 未授权 处理方式：丢弃 403 禁止 处理方式：丢弃 404 没有找到 处理方式：丢弃 5XX 回应代码以“5”开头的状态码表示服务器端发现自己出现错误，不能继续执行请求 处理方式：丢弃 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:5:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"6. HTTP长连接、短连接 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 https加密是在传输层 https报文在被包装成tcp报文的时候完成加密的过程，无论是https的header域也好，body域也罢都是会被加密的。 当使用tcpdump或者wireshark之类的tcp层工具抓包，获取是加密的内容，而如果用应用层抓包，使用**Charels(Mac)、Fildder(Windows)**抓包工具，那当然看到是明文的。 ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:6:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Network"],"content":"7. 参考 https://www.cnblogs.com/qiangxia/p/5261813.html https://www.cnblogs.com/wqhwe/p/5407468.html http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html ","date":"2019-01-24","objectID":"/posts/network/04-http-https/:7:0","tags":["Network"],"title":"计算机网络(四)---HTTP与HTTPS","uri":"/posts/network/04-http-https/"},{"categories":["Linux"],"content":"通过解压方式在Linux下安装RabbitMQ和Erlang和安装过程中的一些问题","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"本章主要讲了如何通过解压方式在Linux下安装RabbitMQ和Erlang，超级详细的安装过程，和安装过程中遇到的一些问题。 更多文章欢迎访问我的个人博客–\u003e幻境云图 软件统一放在/usr/software下 解压后放在单独的文件夹下/usr/locac/opt/rabbitmq,/usr/local/opt/erlang RabbitMQ ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:0:0","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"0. 环境准备 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:1:0","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.版本问题 Erlang和RabbitMQ版本必须对应才行，不然可能会出错。 官网信息如下 RabbitMQ Erlang Version Requirements Erlang/OTP versions older than 20.3 are not supported by RabbitMQ versions released in 2019. RabbitMQ versions prior to 3.7.7 do not support Erlang/OTP 21 or newer. RabbitMQ version3.7.7–3.7.10 需要的Erlang版本最低为20.3.X,最高为21.X RabbitMQ version Minimum required Erlang/OTP Maximum supported Erlang/OTP 3.7.7—3.7.10 20.3.X 21.X 3.7.0–3.7.6 19.3 20.3.X 具体信息在这里http://www.rabbitmq.com/which-erlang.html 这里选择的版本是 Erlang:21.2,RabbitMQ3.7.10,Linux:CentOS 7 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:1:1","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2. 依赖下载 安装rabbitmq需要下载以下依赖，这里可以提前下载上。 # yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel # yum install xmlto -y ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:1:2","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1. Erlang安装 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:0","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.1 下载 安装RabbitMQ之前需要先安装Erlang. 下载地址：http://www.erlang.org/downloads 文件otp_src_21.2.tar.gz ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:1","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.2 解压 将压缩包上传到虚拟机中，我是放在/usr/software目录下的 # tar xvf otp_src_21.2.tar.gz 解压文件 复制一份到/usr/local/opt/erlang-software # cp otp_src_21.2 /usr/local/opt/erlang-software -r 创建erlang安装目录： /usr/local/opt/erlang ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:2","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.3 编译 进入到/usr/local/opt/erlang-software目录下 # cd /usr/local/opt/erlang-software 配置安装路径编译代码：# ./configure --prefix=/usr/local/opt/erlang # make \u0026\u0026 make install 执行编译 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:3","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.4 环境变量配置 配置Erlang环境变量,# vi /etc/profile 添加以下内容 export PATH=$PATH:/usr/local/opt/erlang/bin # source /etc/profile 使得文件生效 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:4","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"1.5 验证 验证erlang是否安装成功：# erl 进入如下界面就说明 配置好了 [root@localhost bin]# erl Erlang/OTP 21 [erts-10.2] [source] [64-bit] [smp:1:1] [ds:1:1:10] [async-threads:1] Eshell V10.2 (abort with ^G) 1\u003e ` ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:2:5","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2. RabbitMQ安装 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:0","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.1 下载 官网：http://www.rabbitmq.com/releases/rabbitmq-server 这里下载3.7.10 :http://www.rabbitmq.com/install-generic-unix.html 文件：rabbitmq-server-generic-unix-3.7.10.tar.xz ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:1","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.2 解压 文件是xz格式的，解压后得到tar格式文件。 # xz -d rabbitmq-server-generic-unix-3.7.10.tar.xz # tar -xvf rabbitmq-server-generic-unix-3.7.10.tar 复制到/usr/local/opt/rabbitmq目录下# cp -r rabbitmq_server-3.7.10/ /usr/local/opt/rabbitmq ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:2","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.3 环境变量配置 配置rabbitmq环境变量,# vi /etc/profile 添加以下内容 export PATH=$PATH:/usr/local/opt/rabbitmq/sbin 环境变量生效：source /etc/profile ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:3","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.4 使用 进入/usr/local/opt/rabbitmq/sbin目录 启动服务：# ./rabbitmq-server -detached 查看服务状态：# ./rabbitmqctl status 关闭服务：# ./rabbitmqctl stop ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:4","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.5 配置网页插件 首先创建目录，否则可能报错：# mkdir /etc/rabbitmq 启用插件：# ./rabbitmq-plugins enable rabbitmq_management 启动mq：# ./rabbitmq-server -detached 配置linux 端口： 15672 网页管理， 5672 AMQP端口 然后访问http://192.168.5.154:15672/ 这里是需要登录了。 rabbitmq默认会创建guest账号，只能用于localhost登录页面管理员，需要自己创建账号。 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:5","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"2.6 添加账户 查看mq用户：# rabbitmqctl list_users 查看用户权限：# rabbitmqctl list_user_permissions guest 新增用户： # rabbitmqctl add_user root root 用户名root,密码root 赋予管理员权限： rabbitmqctl set_user_tags root administrator rabbitmqctl set_permissions -p \"/\" root \".*\" \".*\" \".*\" ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:3:6","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Linux"],"content":"3. 问题 1.启动报错 [root@localhost sbin]# ./rabbitmq-server start BOOT FAILED =========== =INFO REPORT==== 21-Jan-2019::20:49:29.302765 === Error description: noproc Log files (may contain more information): /usr/local/opt/rabbitmq/var/log/rabbitmq/rabbit@localhost.log /usr/local/opt/rabbitmq/var/log/rabbitmq/rabbit@localhost-sasl.log Stack trace: [{gen,do_for_proc,2,[{file,\"gen.erl\"},{line,228}]}, {gen_event,rpc,2,[{file,\"gen_event.erl\"},{line,239}]}, {rabbit,ensure_working_log_handlers,0, [{file,\"src/rabbit.erl\"},{line,856}]}, {rabbit,'-boot/0-fun-0-',0,[{file,\"src/rabbit.erl\"},{line,288}]}, {rabbit,start_it,1,[{file,\"src/rabbit.erl\"},{line,424}]}, {init,start_em,1,[]}, {init,do_boot,3,[]}] {\"init terminating in do_boot\",noproc} init terminating in do_boot (noproc) Crash dump is being written to: erl_crash.dump...done 这个问题网上查了一下，有的说是权限问题，也有说是erlang和rabbitmq版本对应不上，暂时没解决。 以解决，确实是版本问题，erlang版本和rabbitmq的版本对应不上，最前面单独写了这个关于版本的问题。 ","date":"2019-01-21","objectID":"/posts/linux/05-install-rabbitmq/:4:0","tags":["Linux"],"title":"Linux下安装RabbitMQ","uri":"/posts/linux/05-install-rabbitmq/"},{"categories":["Java"],"content":"常见排序算法的Java实现","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"本文主要记录了几种常见的排序算法的Java实现，如冒泡排序、快速排序、直接插入排序、希尔排序、选择排序等等。 ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:0:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"1. 冒泡排序 将序列中所有元素两两比较，将最大的放在最后面。 将剩余序列中所有元素两两比较，将最大的放在最后面。 重复第二步，直到只剩下一个数。 /** * 冒泡排序：两两比较，大者交换位置,则每一轮循环结束后最大的数就会移动到最后. * 时间复杂度为O(n²) 空间复杂度为O(1) */ private static void bubbleSort(int[] arr) { //外层循环length-1次 for (int i = 0; i \u003c arr.length-1; i++) { //外层每循环一次最后都会排好一个数 //所以内层循环length-1-i次 for (int j = 0; j \u003c arr.length - 1 - i; j++) { if (arr[j] \u003e arr[j + 1]) { int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; } } } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:1:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"2. 快速排序 快速排序（Quicksort）是对冒泡排序的一种改进，借用了分治的思想，由C. A. R. Hoare在1962年提出。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列. 具体步骤 快速排序使用分治策略来把一个序列（list）分为两个子序列（sub-lists）。 ①. 从数列中挑出一个元素，称为”基准”（pivot）。 ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 ③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 /** * 快速排序 * 时间复杂度为O(nlogn) 空间复杂度为O(1) */ public static void quickSort(int[] arr, int start, int end) { if (start \u003c end) { int baseNum = arr[start];//选基准值 int midNum;//记录中间值 int left = start;//左指针 int right = end;//右指针 while(left\u003cright){ while ((arr[left] \u003c baseNum) \u0026\u0026 left \u003c end) { left++; } while ((arr[right] \u003e baseNum) \u0026\u0026 right \u003e start) { right--; } if (left \u003c= right) { midNum = arr[left]; arr[left] = arr[right]; arr[right] = midNum; left++; right--; } } if (start \u003c right) { quickSort(arr, start, right); } if (end \u003e left) { quickSort(arr, left, end); } } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:2:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"3. 直接插入排序 直接插入排序（Straight Insertion Sorting）的基本思想：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过为止。 首先设定插入次数，即循环次数，for(int i=1;i\u003clength;i++)，1个数的那次不用插入。 设定插入数和得到已经排好序列的最后一个数的位数。insertNum和j=i-1。 从最后一个数开始向前循环，如果插入数小于当前数，就将当前数向后移动一位。 将当前数放置到空着的位置，即j+1。 /** * 直接插入排序 * 时间复杂度O(n²) 空间复杂度O(1) */ public static void straightInsertion(int[] arr) { int current;//要插入的数 for (int i = 1; i \u003c arr.length; i++) { //从1开始 第一次一个数不需要排序 current = arr[i]; int j = i - 1;//序列元素个数 while (j \u003e= 0 \u0026\u0026 arr[j] \u003e current) {//从后往前循环，将大于当前插入数的向后移动 arr[j + 1] = arr[j];//元素向后移动 j--; } arr[j + 1] = current;//找到位置，插入当前元素 } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:3:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"4. 希尔排序 是插入排序的一种高速而稳定的改进版本。 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 /** * 希尔排序 * 时间复杂度O(n²) 空间复杂度O(1) */ public static void shellSort(int[] arr) { int gap = arr.length / 2; for (; gap \u003e 0; gap = gap / 2) { //不断缩小gap，直到1为止 for (int j = 0; (j + gap) \u003c arr.length; j++) { //使用当前gap进行组内插入排序 for (int k = 0; (k + gap) \u003c arr.length; k += gap) { if (arr[k] \u003e arr[k + gap]) { //交换操作 int temp = arr[k]; arr[k] = arr[k + gap]; arr[k + gap] = temp; } } } } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:4:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"5. 选择排序 遍历整个序列，将最小的数放在最前面。 遍历剩下的序列，将最小的数放在最前面。 重复第二步，直到只剩下一个数。 /** * 选择排序 * 时间复杂度O(n²) 空间复杂度O(1) */ public static void selectSort(int[] arr) { for (int i = 0; i \u003c arr.length; i++) { //循环次数 int min = arr[i];//等会用来放最小值 int index = i;//用来放最小值的索引 for (int j = i + 1; j \u003c arr.length; j++) { //找到最小值 if (arr[j] \u003c min) { min = arr[j]; index = j; } } //内层循环结束后进行交换 arr[index] = arr[i];//当前值放到最小值所在位置 arr[i] = min;//当前位置放最小值 } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:5:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"6. 堆排序 对简单选择排序的优化。 将序列构建成大顶堆。 将根节点与最后一个节点交换，然后断开最后一个节点。 重复第一、二步，直到所有节点断开。 public void heapSort(int[] a){ int len=a.length; //循环建堆 for(int i=0;i\u003clen-1;i++){ //建堆 buildMaxHeap(a,len-1-i); //交换堆顶和最后一个元素 swap(a,0,len-1-i); } } //交换方法 private void swap(int[] data, int i, int j) { int tmp=data[i]; data[i]=data[j]; data[j]=tmp; } //对data数组从0到lastIndex建大顶堆 private void buildMaxHeap(int[] data, int lastIndex) { //从lastIndex处节点（最后一个节点）的父节点开始 for(int i=(lastIndex-1)/2;i\u003e=0;i--){ //k保存正在判断的节点 int k=i; //如果当前k节点的子节点存在 while(k*2+1\u003c=lastIndex){ //k节点的左子节点的索引 int biggerIndex=2*k+1; //如果biggerIndex小于lastIndex，即biggerIndex+1代表的k节点的右子节点存在 if(biggerIndex\u003clastIndex){ //若果右子节点的值较大 if(data[biggerIndex]\u003cdata[biggerIndex+1]){ //biggerIndex总是记录较大子节点的索引 biggerIndex++; } } //如果k节点的值小于其较大的子节点的值 if(data[k]\u003cdata[biggerIndex]){ //交换他们 swap(data,k,biggerIndex); //将biggerIndex赋予k，开始while循环的下一次循环，重新保证k节点的值大于其左右子节点的值 k=biggerIndex; }else{ break; } } } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:6:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"7. 归并排序 速度仅次于快速排序，内存少的时候使用，可以进行并行计算的时候使用。 选择相邻两个数组成一个有序序列。 选择相邻的两个有序序列组成一个有序序列。 重复第二步，直到全部组成一个有序序列。 public void mergeSort(int[] a, int left, int right) { int t = 1;// 每组元素个数 int size = right - left + 1; while (t \u003c size) { int s = t;// 本次循环每组元素个数 t = 2 * s; int i = left; while (i + (t - 1) \u003c size) { merge(a, i, i + (s - 1), i + (t - 1)); i += t; } if (i + (s - 1) \u003c right) merge(a, i, i + (s - 1), right); } } private static void merge(int[] data, int p, int q, int r) { int[] B = new int[data.length]; int s = p; int t = q + 1; int k = p; while (s \u003c= q \u0026\u0026 t \u003c= r) { if (data[s] \u003c= data[t]) { B[k] = data[s]; s++; } else { B[k] = data[t]; t++; } k++; } if (s == q + 1) B[k++] = data[t++]; else B[k++] = data[s++]; for (int i = p; i \u003c= r; i++) data[i] = B[i]; } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:7:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"8. 基数排序 用于大量数，很长的数进行排序时。 将所有的数的个位数取出，按照个位数进行排序，构成一个序列。 将新构成的所有的数的十位数取出，按照十位数进行排序，构成一个序列。 public void baseSort(int[] a) { //首先确定排序的趟数; int max = a[0]; for (int i = 1; i \u003c a.length; i++) { if (a[i] \u003e max) { max = a[i]; } } int time = 0; //判断位数; while (max \u003e 0) { max /= 10; time++; } //建立10个队列; List\u003cArrayList\u003cInteger\u003e\u003e queue = new ArrayList\u003cArrayList\u003cInteger\u003e\u003e(); for (int i = 0; i \u003c 10; i++) { ArrayList\u003cInteger\u003e queue1 = new ArrayList\u003cInteger\u003e(); queue.add(queue1); } //进行time次分配和收集; for (int i = 0; i \u003c time; i++) { //分配数组元素; for (int j = 0; j \u003c a.length; j++) { //得到数字的第time+1位数; int x = a[j] % (int) Math.pow(10, i + 1) / (int) Math.pow(10, i); ArrayList\u003cInteger\u003e queue2 = queue.get(x); queue2.add(a[j]); queue.set(x, queue2); } int count = 0;//元素计数器; //收集队列元素; for (int k = 0; k \u003c 10; k++) { while (queue.get(k).size() \u003e 0) { ArrayList\u003cInteger\u003e queue3 = queue.get(k); a[count] = queue3.get(0); queue3.remove(0); count++; } } } } ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:8:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"9. 总结 排序法 平均时间 最小时间 最大时间 稳定度 额外空间 备注 冒泡排序 O(n2) O(n) O(n2) 稳定 O(1) n小时较好 选择排序 O(n2) O(n2) O(n2) 不稳定 O(1) n小时较好 插入排序 O(n2) O(n) O(n2) 稳定 O(1) 大部分已排序时较好 基数排序 O(logRB) O(n) O(logRB) 稳定 O(n) B是真数(0-9)，R是基数(个十百) Shell排序 O(nlogn) - O(ns) 1\u003cs\u003c2 不稳定 O(1) s是所选分组 快速排序 O(nlogn) O(n2) O(n2) 不稳定 O(logn) n大时较好 归并排序 O(nlogn) O(nlogn) O(nlogn) 稳定 O(n) 要求稳定性时较好 堆排序 O(nlogn) O(nlogn) O(nlogn) 不稳定 O(1) n大时较好 ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:9:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Java"],"content":"10. 参考 https://www.cnblogs.com/shixiangwan/p/6724292.html ","date":"2019-01-20","objectID":"/posts/java/05-java-sorts/:10:0","tags":["Java"],"title":"几种常见排序算法的Java实现","uri":"/posts/java/05-java-sorts/"},{"categories":["Markdown"],"content":"Markdown语法记录","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"本文主要介绍了关于markdown的一些常用语法和技巧，让大家更好的写文章。自从17年前开始在 GitHub 玩耍，接触到 Markdown 之后，就感觉很有意思。不过也仅仅是了解一下基本语法，所以找了一下Markdown的语法用法来学习学习。 更多文章欢迎访问我的个人博客–\u003e幻境云图 如下： 注：如下技巧大多是利用 Markdown 兼容部分 HTML 标签的特性来完成，不一定在所有网站和软件里都完全支持，主要以 GitHub 支持为准。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:0:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"标题 # This is an \u003ch1\u003e tag ## This is an \u003ch2\u003e tag ###### This is an \u003ch6\u003e tag ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:1:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"重点 *This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_ This text will be italic This will also be italic This text will be bold This will also be bold You can combine them ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:2:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"清单 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:3:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"无序 * Item 1 * Item 2 * Item 2a * Item 2b Item 1 Item 2 Item 2a Item 2b ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:3:1","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"有序 1. Item 1 1. Item 2 1. Item 3 1. Item 3a 1. Item 3b Item 1 Item 2 Item 3 Item 3a Item 3b ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:3:2","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"图片 ![GitHub Logo](/images/logo.png) Format: ![Alt Text](url) ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:4:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"链接 http://github.com - automatic! [GitHub](http://github.com) http://github.com - automatic! GitHub ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:5:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"引用文字 As Kanye West said: \u003e We're living the future so \u003e the present is our past. As Kanye West said: We’re living the future so the present is our past. ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:6:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"内联代码 I think you should use an `\u003caddr\u003e` element here instead. I think you should use an \u003caddr\u003e element here instead. ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:7:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"删除线 用两个波浪线（如~~this~~）包裹的任何单词都会显示为划掉。 这是被删除的内容 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:8:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"在表格单元格里换行 借助于 HTML 里的 \u003cbr /\u003e 实现。 示例代码： | Header1 | Header2 | |---------|----------------------------------| | item 1 | 1. one\u003cbr /\u003e2. two\u003cbr /\u003e3. three | 示例效果： Header1 Header2 item 1 1. one2. two3. three ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:9:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"引用 在引用的文字前加\u003e即可。引用也可以嵌套，如加两个»三个»\u003e 这是引用的内容 这是引用的内容 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:10:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"分割线 三个或者三个以上的 - 或者 * 都可以。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:11:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"流程图 ​```flow st=\u003estart: 开始 op=\u003eoperation: My Operation cond=\u003econdition: Yes or No? e=\u003eend st-\u003eop-\u003econd cond(yes)-\u003ee cond(no)-\u003eop \u0026``` ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:12:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"表格 | 左对齐标题 | 右对齐标题 | 居中对齐标题 | | :------| ------: | :------: | | 短文本 | 中等文本 | 稍微长一点的文本 | | 稍微长一点的文本 | 短文本 | 中等文本 | //语法： 1）|、-、:之间的多余空格会被忽略，不影响布局。 2）默认标题栏居中对齐，内容居左对齐。 3）-:表示内容和标题栏居右对齐，:-表示内容和标题栏居左对齐，:-:表示内容和标题栏居中对齐。 4）内容和|之间的多余空格会被忽略，每行第一个|和最后一个|可以省略，-的数量至少有一个。 表格在渲染之后很整洁好看，但是在文件源码里却可能是这样的： |Header1|Header2| |---|---| |a|a| |ab|ab| |abc|abc| 不知道你能不能忍，反正我是不能忍。 好在广大网友们的智慧是无穷的，在各种编辑器里为 Markdown 提供了表格格式化功能，比如我使用 Vim 编辑器，就有 vim-table-mode 插件，它能帮我自动将表格格式化成这样： | Header1 | Header2 | |---------|---------| | a | a | | ab | ab | | abc | abc | 是不是看着舒服多了？ 如果你不使用 Vim，也没有关系，比如 Atom 编辑器的 markdown-table-formatter 插件，Sublime Text 3 的 MarkdownTableFormatter 等等，都提供了类似的解决方案。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:13:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"使用 Emoji 这个是 GitHub 对标准 Markdown 标记之外的扩展了，用得好能让文字生动一些。 示例代码： 我和我的小伙伴们都笑了。:smile: 示例效果： 我和我的小伙伴们都笑了。:smile: Github支持的表情在这里哟 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:14:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"行首缩进 直接在 Markdown 里用空格和 Tab 键缩进在渲染后会被忽略掉，需要借助 HTML 转义字符在行首添加空格来实现，\u0026ensp; 代表半角空格，\u0026emsp; 代表全角空格。 示例代码： \u0026emsp;\u0026emsp;春天来了，又到了万物复苏的季节。 示例效果： 春天来了，又到了万物复苏的季节。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:15:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"展示数学公式 如果是在 GitHub Pages，可以参考 http://wanguolin.github.io/mathmatics_rending/ 使用 MathJax 来优雅地展示数学公式（非图片）。 如果是在 GitHub 项目的 README 等地方，目前我能找到的方案只能是贴图了，以下是一种比较方便的贴图方案： 在 https://www.codecogs.com/latex/eqneditor.php 网页上部的输入框里输入 LaTeX 公式，比如 $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$； 在网页下部拷贝 URL Encoded 的内容，比如以上公式生成的是 https://latex.codecogs.com/png.latex?%24%24x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D%24%24； 在文档需要的地方使用以上 URL 贴图，比如 ![](https://latex.codecogs.com/png.latex?%24%24x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D%24%24) 示例效果： ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:16:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"任务列表 在 GitHub 和 GitLab 等网站，除了可以使用有序列表和无序列表外，还可以使用任务列表，很适合要列出一些清单的场景。 示例代码： **购物清单** - [ ] 一次性水杯 - [x] 西瓜 - [ ] 豆浆 - [x] 可口可乐 - [ ] 小茗同学 示例效果： 购物清单 一次性水杯 西瓜 豆浆 可口可乐 小茗同学 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:17:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"自动维护目录 有时候维护一份比较长的文档，希望能够自动根据文档中的标题生成目录（Table of Contents），并且当标题有变化时自动更新目录，能减轻工作量，也不易出错。比如 Atom 编辑器的 markdown-toc 插件，Sublime Text 的 MarkdownTOC 插件等。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:18:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"后话 希望自己，也希望大家在了解这些之后能有所收获，更好地排版，专注写作。 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:19:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Markdown"],"content":"参考 https://raw.githubusercontent.com/matiassingers/awesome-readme/master/readme.md https://www.zybuluo.com/songpfei/note/247346 支持的表情 ","date":"2019-01-18","objectID":"/posts/markdown/markdown-skills/:20:0","tags":["Markdown"],"title":"Markdown语法与小技巧","uri":"/posts/markdown/markdown-skills/"},{"categories":["Network"],"content":"TCP保证传输可靠性的几种手段介绍","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"本文主要叙述了TCP协议是如何保证传输的可靠性的，主要保证手段包括：序列号、校验和、流量控制、拥塞控制、停止等待协议、超时重传、连接管理等。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:0:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"1. 主要保证方式 **序列号:**应用数据被分割成 TCP 认为最适合发送的数据块,同时给每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到端的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 停止等待协议(确认应答) 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 连接管理: 三次握手四次挥手,保证可靠的连接，是保证可靠性的前提。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:1:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"2. 停止等待协议 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组； 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认； 1) 无差错情况: 发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。 2) 出现差错情况（超时重传）: [ 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3) 确认丢失和确认迟到 确认丢失：当确认消息在传输过程丢失 A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施： 丢弃这个重复的M1消息，不向上层交付。 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到 [A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下： A收到重复的确认后，直接丢弃。 B收到重复的M1后，也直接丢弃重复的M1。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:2:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"3. ARQ协议 即自动重传请求 ARQ 协议(Automatic Repeat reQuest )，停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。 优点： 简单 缺点： 信道利用率低 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:3:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"4. 连续ARQ协议 连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:4:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"5. 流量控制 滑动窗口（Sliding window）是一种流量控制技术。早期的网络通信中，通信双方不会考虑网络的拥挤情况直接发送数据。由于大家不知道网络拥塞状况，同时发送数据，导致中间节点阻塞掉包，谁也发不了数据，所以就有了滑动窗口机制来解决此问题。 TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 TCP 利用滑动窗口实现流量控制的机制。 流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:5:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"6. 拥塞控制 防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。 拥塞：即对资源的需求超过了可用的资源。若网络中许多资源同时供应不足，网络的性能就要明显变坏，整个网络的吞吐量随之负荷的增大而下降 拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。 几种拥塞控制方法： 慢开始( slow-start ) 拥塞避免( congestion avoidance ) 快重传( fast retransmit ) 快恢复( fast recovery )。 慢开始和拥塞避免 ​ 发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞。 ​ 发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 **慢开始算法：**当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是 先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。 **拥塞避免算法：**让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 快重传与快恢复 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:6:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Network"],"content":"7.参考 https://blog.csdn.net/liuchenxia8/article/details/80428157 https://blog.csdn.net/yangbodong22011/article/details/48473183 ","date":"2019-01-18","objectID":"/posts/network/03-tcp-reliability/:7:0","tags":["Network"],"title":"计算机网络(三)--TCP如何保证传输可靠性","uri":"/posts/network/03-tcp-reliability/"},{"categories":["Linux"],"content":"如何通过解压方式在Linux下安装MySQL，以及如何设置让我们可以远程连接到服务器上的MySQL","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"本章主要讲了如何通过解压方式在Linux下安装MySQL，以及如何设置让我们可以远程连接到服务器上的mysql. 软件统一放在/usr/software下 解压后放在单独的文件夹下/usr/local/java//usr/local/mysql 其中：#为Linux命令，mysql则是mysql下的命令 软件统一放在/usr/software下 解压后放在单独的文件夹下/usr/local/java//usr/local/mysql 安装包下载mysql-5.7.24-linux-glibc2.12-x86_64.tar 网址https://dev.mysql.com/downloads/mysql/5.7.html#downloads ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:0:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"1. 安装依赖 # yum install -y cmake make gcc gcc-c++ libaio ncurses ncurses-devel ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:1:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"2. 解压文件 压缩包上传到虚拟机/usr/software目录下,进入这个目录 解压文件 # tar zxvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz 将解压后的文件移动到/usr/local/mysql # mv mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/mysql ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:2:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"3. 添加用户和赋权 1.添加用户和用户组 给mysql赋权的用户必须对当前目录具有读写权限，但是一般不用root账户，所以创建一个用户mysql。 执行命令：创建用户组mysql`groupadd mysql`` 创建用户也叫mysql // 命令中第一个mysql是用户，第二个mysql是用户组。 # useradd -r -g mysql mysql 2.给用户赋权限 一定保证当前是在/usr/local/mysql 目录下 给用户组赋权限 //mysql是用户组名 # chgrp -R mysql. 给用户赋权限 //这个mysql是用户名 # chown -R mysql. ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:3:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"4. 数据库初始化 安装数据库 : // 这里会生成临时密码，后边有用 # bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 执行以下命令创建RSA private key ： # bin/mysql_ssl_rsa_setup --datadir=/usr/local/mysql/data ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:4:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"5. 配置my.cnf # vim /etc/my.cnf 内容如下： [mysqld] character_set_server=utf8 init_connect='SET NAMES utf8' basedir=/usr/local/mysql datadir=/usr/local/mysql/data socket=/tmp/mysql.sock #不区分大小写 (sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 这个简单来说就是sql语句是否严格) lower_case_table_names = 1 log-error=/var/log/mysqld.log pid-file=/usr/local/mysql/data/mysqld.pid # cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld # vim /etc/init.d/mysqld 添加以下内容，在46行 basedir=/usr/local/mysql datadir=/usr/local/mysql/data ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:5:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"6. 修改密码 启动mysql # service mysqld start 加入开机起动 # chkconfig --add mysqld 进入客户端 登录修改密码 # mysql -uroot -p 上面初始化时的密码 如果出现错误 需要添加软连接 # ln -s /usr/local/mysql/bin/mysql /usr/bin 如果出现Access denied for user 'root'@'localhost' (using password: YES)应该是密码错了，直接强行修改密码好了。先停掉mysql. # service mysql stop 然后修改配置文件 # vim /etc/my.cnf 在[mysqld]后面任意一行添加skip-grant-tables用来跳过密码验证的过程 接下来我们需要重启MySQL # /etc/init.d/mysqld restart 重启之后输入命令mysql即可进入mysql了，然后开始修改密码。 mysql\u003e use mysql; # 这里修改密码的命令在5.7以上和5.7以下是不同的 需要注意 mysql\u003e update user set authentication_string=passworD(\"你的密码\") where user='root'; flush privileges; mysql\u003e quit 完成后可以把配置文件中的跳过密码验证去掉。 然后就可以正常使用啦。 ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:6:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"7. 外部访问 首先进入mysql， # mysql -u root -p 接着创建远程连接 MySQL 的用户 mysql命令 -- 创建用户、密码及权限范围 第一个 roo t为用户名 @后为适用的主机，‘%’表示所有电脑都可以访问连接，第二个 root 为密码 mysql\u003e GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.1.3' IDENTIFIED BY 'root' WITH GRANT OPTION; -- 立即生效 mysql\u003e flush privileges; 查看数据库用户： -- 使用 mysql 库 mysql\u003e use mysql; -- 查看用户 mysql\u003e SELECT DISTINCT CONCAT('User: [', user, '''@''', host, '];') AS USER_HOST FROM user; -- 查看端口 mysql\u003e show global variables like 'port'; --mysql 默认端口为3306 解决防火墙问题 防火墙默认只开放了22端口，要访问数据库要么关掉防火墙要么修改配置文件，开放3306端口 修改防火墙配置： 命令 # vim /etc/sysconfig/iptables 添加以下内容 -A INPUT -m state --state NEW -m tcp -p tcp --dport 3306 -j ACCEPT 然后重启防火墙 # service iptables restart 最后查看服务器IP # ip a 到这里应该就可以通过IP和端口号远程连接服务器上的MySQL了。 ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:7:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"8. 问题 mysql中执行命令出现以下错误： ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement. 解决： 修改用户密码 mysql\u003e alter user 'root'@'localhost' identified by '你的密码'; ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:8:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"参考 https://blog.csdn.net/z13615480737/article/details/80019881 https://www.cnblogs.com/goodcheap/p/7103049.html ","date":"2019-01-16","objectID":"/posts/linux/04-install-mysql/:9:0","tags":["Linux"],"title":"Linux安装MySQL教程","uri":"/posts/linux/04-install-mysql/"},{"categories":["Linux"],"content":"如何通过解压方式在Linux下安装JDK和Tomcat等软件","date":"2019-01-15","objectID":"/posts/linux/03-install-jdk-tomcat/","tags":["Linux"],"title":"Linux下安装jdk和Tomcat","uri":"/posts/linux/03-install-jdk-tomcat/"},{"categories":["Linux"],"content":"本章主要讲了如何通过解压方式在Linux下安装JDK和Tomcat等软件。 软件统一放在/usr/software下 解压后放在单独的文件夹下/usr/local/java//usr/local/mysql ","date":"2019-01-15","objectID":"/posts/linux/03-install-jdk-tomcat/:0:0","tags":["Linux"],"title":"Linux下安装jdk和Tomcat","uri":"/posts/linux/03-install-jdk-tomcat/"},{"categories":["Linux"],"content":"1.JDK 安装包下载jdk-8u191-linux-x64.tar.gz 注意32位和64位的别下载错了。 命令uname -a 查看Linux系统位数。 网址：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 1.首先将压缩包传到虚拟机。放在/usr/software下 2.然后解压文件tar zxvf jdk-8u191-linux-x64.tar.gz 按tab会自动补全。 3.将解压得到的文件移动到/usr/local/java,命令`mv jdk1.8.0_191/ /usr/local/jdk8 4.配置环境变量 命令vim /etc/profile 添加以下内容 export JAVA_HOME=/usr/local/jdk8/ export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 5.解析该文件 命令source /etc/profile 6.测试 命令 java -version 输出版本信息就说明配好了。 ","date":"2019-01-15","objectID":"/posts/linux/03-install-jdk-tomcat/:1:0","tags":["Linux"],"title":"Linux下安装jdk和Tomcat","uri":"/posts/linux/03-install-jdk-tomcat/"},{"categories":["Linux"],"content":"2.Tomcat 安装包下载apache-tomcat-8.5.37.tar.gz 网址https://tomcat.apache.org/download-80.cgi 1.压缩包上传到虚拟机/usr/software目录下 2.解压文件 tar zxvf apache-tomcat-8.5.37.tar.gz 3.将解压后的文件移动到/usr/local/tomcat,命令mv apache-tomcat-8.5.37 /usr/local/tomcat 4.配置环境变量 命令vim /etc/profile 添加以下内容 export TOMCAT_HOME=/usr/local/tomcat export CATANILA_HOME=/usr/local/tomcat 5.解析该文件 命令source /etc/profile ","date":"2019-01-15","objectID":"/posts/linux/03-install-jdk-tomcat/:2:0","tags":["Linux"],"title":"Linux下安装jdk和Tomcat","uri":"/posts/linux/03-install-jdk-tomcat/"},{"categories":["Linux"],"content":"linux如何配置网络，让虚拟机能够连上外网，如何让虚拟机和主机联通","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"本章主要讲了linux如何配置网络，让虚拟机能够连上外网，如何让虚拟机和主机联通，同时介绍了ssh客户端工具连接虚拟机。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:0:0","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"1. Xshell 在安装好虚拟机后就可以正常使用了。但是在正常工作中不可能真的在服务器上操作，一般都是通过ssh客户端工具连接服务器进行操作。 这里用到的客户端工具是Xshell,通过该工具连上服务器后就可以在自己的电脑上操作了。而且还可以开多个窗口，比较方便。 这里新建连接时需要输入要连接的服务器的IP和端口号，账户和密码，端口号默认是22，一般不用改。 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:1:0","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"2. 网络配置 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:2:0","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"2.1 桥接模式和NAT模式 按照上面的方法就可以连上虚拟机了，但是现在虚拟机的IP是自动获取的，每次重启后都IP都会变，这肯定不行呀，所以我们需要为虚拟机设置静态IP. 由于我们这里使用的是NAT模式。这个模式下虚拟机可以上网，但是无法和主机联通。 桥接模式和NAT模式的区别： 桥接模式下虚拟机可以看做一台真正的独立的电脑，所以桥接模式下需要为虚拟机分配独立的IP，在家里到时无所谓，在公司的话由于IP和电脑绑定的，所以需要网络管理人员给你的虚拟机分配一个IP才行。 NAT模式下，虚拟机会动态获取IP,虽然有自己的IP但是最终上网还是通过主机上网的。所以NAT模式下不用分配独立的IP,但是NAT模式下主机和虚拟机无法联通。 为了主机和虚拟机联通，我们必须让主机和虚拟机在同一个网段下。 为了主机和虚拟机联通，我们必须让主机和虚拟机在同一个网段下。 为了主机和虚拟机联通，我们必须让主机和虚拟机在同一个网段下。 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:2:1","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"2.2 设置静态IP 在设置静态IP前我们需要知道主机的IP. windows下命令行输入 ipconfig 即可获取到本机IP. ![ipconfig]( https://github.com/lixd/blog/raw/master/images/linux/network-set/ip-query.png ip-query.png) 然后通过VMware软件对网络进行配置。 ![vmware]( https://github.com/lixd/blog/raw/master/images/linux/network-set/ip-query.png ip-set-way.png) ![static ip]( https://github.com/lixd/blog/raw/master/images/linux/network-set/ip-query.png vm-ip-set.png) 接着在虚拟机中配置具体网络信息。 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:2:2","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Linux"],"content":"2.3 网络配置 2.3.1 网卡配置 网络配置文件在/etc/sysconfig/network-scripts/ifcfg-ens33目录下，一般是叫ifcfg-ens33 编辑配置文件 命令：vi /etc/sysconfig/network-scripts/ifcfg-ens33 配置如下 ： 其中ip地址必须和主机在同一网段下，网关就是上边的那个网关。DNS可填可不填。 BOOTPROTO=\"static\" # 手动分配ip ONBOOT=\"yes\" # 该网卡是否随网络服务启动 IPADDR=\"192.168.1.111\" # 该网卡ip地址就是你要配置的固定IP GATEWAY=\"192.168.1.2\" # 网关 NETMASK=\"255.255.255.0\" # 子网掩码 固定值 DNS1=\"8.8.8.8\" # DNS，8.8.8.8为Google提供的免费DNS服务器的IP地址 DNS2=\"192.168.1.2\" 2.3.2 网络配置 命令：vi /etc/sysconfig/network 添加以下内容 NETWORKING=yes # 网络是否工作，此处一定不能为no NETWORKING_IPV6=no HOSTNAME=localhost.localdomain GATEWAY=192.168.1.2 2.3.3 配置公共DNS服务 vi /etc/resolv.conf search localdomain nameserver 8.8.8.8 nameserver 192.168.1.2 2.3.4 关闭防火墙 systemctl stop firewalld # 临时关闭防火墙 systemctl disable firewalld # 禁止开机启动 2.3.5 重启网络服务 service network restart 到此为止网络配置就完成了，现在虚拟机的IP重启后不会变了，也可以连上外网了，还可以和主机联通了。 ","date":"2019-01-14","objectID":"/posts/linux/02-conf-network/:2:3","tags":["Linux"],"title":"Linux下的网络配置","uri":"/posts/linux/02-conf-network/"},{"categories":["Network"],"content":"TCP三次握手四次挥手具体步骤及其原因分析","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"本文主要介绍了TCP的三次握手和四次挥手具体步骤及其原因分析。 ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:0","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"1. 三次握手 step1:第一次握手 建立连接时，客户端发送SYN包到服务器，其中包含客户端的初始序号seq=x，并进入SYN_SENT状态，等待服务器确认。（其中，SYN=1，ACK=0，表示这是一个TCP连接请求数据报文；序号seq=x，表明传输数据时的第一个数据字节的序号是x）。 step2:第二次握手 服务器收到请求后，必须确认客户的数据包。同时自己也发送一个SYN包，即SYN+ACK包，此时服务器进入SYN_RCVD状态。（其中确认报文段中，标识位SYN=1，ACK=1，表示这是一个TCP连接响应数据报文，并含服务端的初始序号seq(服务器)=y，以及服务器对客户端初始序号的确认号ack(服务器)=seq(客户端)+1=x+1）。 step3:第三次握手 客户端收到服务器的SYN+ACK包，向服务器发送一个序列号(seq=x+1)，确认号为ack(客户端)=y+1，此包发送完毕，客户端和服务器进入**ESTABLISHED **(TCP连接成功)状态，完成三次握手。 建立连接前要确认客户端和服务端的接收和发送功能是否正常。 第一次客户端发送SYN时 什么也确认不了 第二次服务端发送SYN+ACK 可以确认服务端发送功能正常 第三次 客户端收到服务端发送的YSN+ACK 可以确认客户端发送接收功能正常 最后客户端发送ACK 服务端接收到后 可以确认服务端发送功能正常 到此就确认完毕了。 ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:1","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"2. 四次挥手 step1：第一次挥手 首先，客户端发送一个FIN，用来关闭客户端到服务器的数据传送，然后等待服务器的确认。其中终止标志位FIN=1，序列号seq=u。 客户端进入FIN_WAIT1状态 我（Client端）没有数据要发给你（Server端）了\"，但是如果你（Server端）还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK step2：第二次挥手 服务器收到这个FIN进入CLOSE_WAIT状态，然后它给客户端发送一个ACK，确认ack为收到的序号加一。 客户端收到ACK应答后进入FIN_WAIT2状态 告诉Client端，你的请求我收到了，但是我（Server端）还没准备好，请继续你等我的消息\" step3：第三次挥手 服务端关闭服务器到客户端的连接，发送一个FIN给客户端。服务端进入LAST_ACK状态 告诉Client端，好了，我（Server端）这边数据发完了，准备好关闭连接了 step4：第四次挥手 客户端收到FIN后，进入TIME_WAIT状态 并发回一个ACK报文确认，并将确认序号seq设置为收到序号加一。 服务端收到客户端回复的ACK后立即关闭，服务端进入CLOASED状态 而客户端要等待2MSL后关闭 进入CLOASED状态 Client端收到FIN报文后，\"就知道可以关闭连接了，所以发送ACK。但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后没有立即，而是进入TIME_WAIT状态，如果Server端没有收到ACK那么自己还可以重传。Server端收到ACK后，\"就知道可以断开连接了\"。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:2","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"3. TIME-WAIT状态详解 为什么Client端要先进入TIME-WAIT状态，等待2MSL时间后才进入CLOSED状态？ 保证TCP协议的全双工连接能够可靠关闭，保证这次连接的重复数据段从网络中消失 假设由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，Client端在等待2MSL时间后都没收到信息，说明Server端已经收到自己发送的ACK并且成功关闭了。 假设CLient端直接关闭了： 1.由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。 2.如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。 2MSL:Maximum Segment Lifetime 即数据在网络中保存的最大时间。 简单易懂的说法: 假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说\"我Client端没有数据要发给你了\"，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，\"告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息\"。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，\"告诉Client端，好了，我这边数据发完了，准备好关闭连接了\"。Client端收到FIN报文后，\"就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，\"就知道可以断开连接了\"。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:3","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"4. TCP 的有限状态机 红色为客户端 蓝色为服务端 细箭头为异常变化 ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:4","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Network"],"content":"5. 参考 https://www.baidu.com/link?url=_mlor11BLttd1jmMU4k9OP0gqcjNKhZQ9fJuvbMOhkuH9-lVeB-y3VIVK1neZURi_tmR3rg1lj2lfgvvGhTV-q\u0026wd=\u0026eqid=d0144c250007b69c000000035bfdfafc ","date":"2019-01-08","objectID":"/posts/network/02-tcp-connection/:0:5","tags":["Network"],"title":"计算机网络(二)---TCP三次握手四次挥手","uri":"/posts/network/02-tcp-connection/"},{"categories":["Linux"],"content":"记录了如何通过VMware虚拟机安装Linux，从软件下载到虚拟机安装等等","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"这是一个十分详细的CentOS7的安装教程，对自己的安装过程做了一个记录。主要记录了如何通过VMware虚拟机安装Linux，从软件下载到虚拟机安装等等。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:0:0","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"1. 准备工作 ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:1:0","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"1.1 VMware下载 百度网盘下载（内含注册机） 链接: https://pan.baidu.com/s/1wz4hdNQBikTvyUMNokSVYg提取码: yed7 怎么安装就不用写了吧。 ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:1:1","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"1.2 CentOS下载 http://mirrors.163.com/centos/7.6.1810/isos/x86_64/ ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:1:2","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"2. CentOS 7安装 创建虚拟机，这里我们选择自定义安装类型。 然后选择版本，需要注意兼容问题，一般是向下兼容，14上的虚拟机复制到15上可以用，15的复制到14上可能会用不了。 这里选择稍后再安装。 接着选择系统，这里是CentOS 7 64位。 这个是保存的文件名字。 这里一般默认的就行了,电脑配置好的可以调高点。 网络这里,如果仅仅是让虚拟机能上网，两种模式都可以的，用桥接的话只要你在局域网内有合法的地址，比如你的ADSL猫是带路由功能的，如果是在单位，那就要网络管理人员给你合法IP才行。NAT模式下，虚拟机从属于主机，也就是访问外部网络必须通过主机来访问，因此虚拟机的IP只有主机才能识别。而桥接模式下，虚拟机和主机是平行关系，共享一张网卡（使用网卡的多个接口），可以直接访问外部网络。 这些都默认的就行了。 这个是虚拟机文件的名字。 这里选择自定义硬件。 选择镜像文件。 到这里就结束了，点击开启虚拟机后会自动开始安装。 选择安装CentOS 7 语言选择 调一下时间和地区。 选择要安装的软件，新手还是安装一个GUI比较好。 查看一下网络连接 开始安装。 安装过程中可以设置一下账号密码，一个root账户，一个普通账户。 然后耐心等待安装完成就好了。 安装完成后重启就可以登录系统了。 ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:2:0","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Linux"],"content":"3. 快照 快照相当于windows中的还原点。在安装好后可以拍摄一张快照，方便恢复或者克隆虚拟机。 ","date":"2019-01-08","objectID":"/posts/linux/01-install-centos/:3:0","tags":["Linux"],"title":"超详细的VMware虚拟机安装CentOS7教程","uri":"/posts/linux/01-install-centos/"},{"categories":["Network"],"content":"计算机网络OSI七层模型介绍","date":"2019-01-03","objectID":"/posts/network/01-network-model/","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"本文主要通过OSI七层模型与常用TCP/IP5层模型介绍了各层的主要作用，包括应用层，运输层，网络层，数据链路层，物理层等。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:0:0","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"OSI与TCP/IP模型 应用层:通过应用进程间的交互来完成特定网络应用。 运输层：向用户提供可靠的、端到端的差错和流量控制，保证报文的正确传输。 网络层：通过路由算法，为报文或分组通过通信子网选择最适当的路径。 数据链路层：其最基本的服务是将源自网络层来的数据可靠地传输到相邻节点的目标机网络层。 物理层：利用传输介质为数据链路层提供屋里连接，实现比特流的透明传输。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:0","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"1. 应用层 主要作用:通过应用进程间的交互来完成特定网络应用。 应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:1","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"2. 运输层 主要任务：向用户提供可靠的、端到端的差错和流量控制，保证报文的正确传输。 主要作用：向高层屏蔽下层数据通信的具体细节，即向用户透明的传送报文。 主要用到的协议： 传输控制协议 TCP（Transmisson Control Protocol）–提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 2.1 UDP UDP 是无连接的； UDP 使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP 是面向报文的； UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如 直播，实时视频会议等）； UDP 支持一对一、一对多、多对一和多对多的交互通信； UDP 的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 2.2 TCP TCP 是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条 TCP 连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP 提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:2","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"3. 网络层 **主要任务：通过路由算法，为报文或分组通过通信子网选择最适当的路径。**该层控制数据链路层与物理层之间的信息转发，建立、维持与终止网络的连接。具体的说，数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。 一般的，数据链路层是解决统一网络内节点之间的通信，而网络层主要解决不同子网之间的通信。例如路由选择问题。 在实现网络层功能时，需要解决的主要问题如下： 寻址：数据链路层中使用的物理地址（如MAC地址）仅解决网络内部的寻址问题。在不同子网之间通信时，为了识别和找到网络中的设备，每一子网中的设备都会被分配一 个唯一的地址。由于各个子网使用的物理技术可能不同，因此这个地址应当是逻辑地址（如IP地址） **交换：**规定不同的交换方式。常见的交换技术有：线路交换技术和存储转发技术，后者包括报文转发技术和分组转发技术。 **路由算法：**当源节点和路由节点之间存在多条路径时，本层可以根据路由算法，通过网络为数据分组选择最佳路径，并将信息从最合适的路径，由发送端传送的接受端。 **连接服务：**与数据链路层的流量控制不同的是，前者控制的是网络相邻节点间的流量，后者控制的是从源节点到目的节点间的流量。其目的在于防止阻塞，并进行差错检测 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:3","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"4. 数据链路层 其最基本的服务是将源自网络层来的数据可靠地传输到相邻节点的目标机网络层。 主要功能：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。 具体工作：接受来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上一层的数据帧，拆装为位流形式的数据转发到物理层；并且还负责处理接受端发回的确认帧的信息，以便提供可靠的数据传输。 该层通常又被分为 介质访问控制(MAC)和逻辑链路控制(LLC)两个子层： MAC子层的主要任务是解决共享型网络中多用户对信道竞争的问题，完成网络介质的访问控制。 LLC子层的主要任务是建立和维护网络连接，执行差错校验、流量控制和链路控制。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:4","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"5. 物理层 主要功能：利用传输介质为数据链路层提供屋里连接，实现比特流的透明传输。 作用：实现相邻计算机节点之间比特流的透明传输，尽可能屏蔽掉具体传输介质与物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。 透明传输的意义就是：不管传的是什么，所采用的设备只是起一个通道作用，把要传输的内容完好的传到对方！ 在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:5","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Network"],"content":"6. 参考 https://blog.csdn.net/yaopeng_2005/article/details/7064869 ","date":"2019-01-03","objectID":"/posts/network/01-network-model/:1:6","tags":["Network"],"title":"计算机网络(一)---OSI七层模型","uri":"/posts/network/01-network-model/"},{"categories":["Java"],"content":"观察者模式的具体实现及其在JDK中的应用","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的观察者模式，并结合实例描述了观察者模式的具体实现和优缺点分析。 ","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/:0:0","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Java"],"content":"1. 简介 让多个观察者对象同时监听某一个主题对象，这个主题对象在状态上发生变化时，会通知所有观察者对象，使他们能够自动更新自己。 在对象之间定义了一对多的依赖，这样一来，当一个对象改变状态，依赖它的对象会收到通知并自动更新。其实就是发布订阅模式，发布者发布信息，订阅者获取信息，订阅了就能收到信息，没订阅就收不到信息。 该模式包含四个角色 抽象被观察者角色：也就是一个抽象主题，它把所有对观察者对象的引用保存在一个集合中，每个主题都可以有任意数量的观察者。抽象主题提供一个接口，可以增加和删除观察者角色。一般用一个抽象类和接口来实现。 抽象观察者角色：为所有的具体观察者定义一个接口，在得到主题通知时更新自己。 具体被观察者角色：也就是一个具体的主题，在集体主题的内部状态改变时，所有登记过的观察者发出通知。 具体观察者角色：实现抽象观察者角色所需要的更新接口，一边使本身的状态与制图的状态相协调。 ","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/:1:0","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Java"],"content":"2. 代码实现 /** * 抽象观察者角色 * 定义了一个update()方法，当被观察者调用notifyObservers()方法时，观察者的update()方法会被回调。 * * @author illusoryCloud */ public interface Observer { /** * 更新消息 由被观察者调用 * * @param o 被观察者 即消息来源 * @param message 收到的消息 */ void update(Observable o, Message message); } /** * 抽象被观察者接口 * 声明了添加、删除、通知观察者方法 * * @author illusoryCloud */ public class Observable { /** * 被观察者是否有变化 * 在通知观察者时做判断 若没有发生变化则不通知 */ private boolean changed = false; /** * Vector集合 线程安全的 * 用于存放已注册的观察者 */ private Vector\u003cObserver\u003e obs; public Observable() { obs = new Vector\u003c\u003e(); } /** * 注册观察者 * * @param o 需要注册的观察者 */ public synchronized void addObserver(Observer o) { if (o == null) { throw new NullPointerException(); } if (!obs.contains(o)) { obs.addElement(o); } } /** * 移除观察者 * * @param o 被移除的观察者 */ public synchronized void deleteObserver(java.util.Observer o) { obs.removeElement(o); } /** * 发通知 */ public void notifyObservers() { notifyObservers(null); } /** * 循环遍历 通知注册的所有的观察者 * * @param message 发送的消息 */ public void notifyObservers(Message message) { Object[] arrLocal; synchronized (this) { //判断若没有变化则直接返回 if (!changed) { return; } arrLocal = obs.toArray(); clearChanged(); } for (int i = arrLocal.length - 1; i \u003e= 0; i--) { ((Observer) arrLocal[i]).update(this, message); } } /** * 移除所有观察者 */ public synchronized void deleteObservers() { obs.removeAllElements(); } /** * set clear has 设置 清除 获取 * 观察者状态是否变化 true/false */ protected synchronized void setChanged() { changed = true; } protected synchronized void clearChanged() { changed = false; } public synchronized boolean hasChanged() { return changed; } /** * 已注册观察者的个数 * * @return count */ public synchronized int countObservers() { return obs.size(); } } /** * 具体观察者角色 * 实现update方法 * * @author illusoryCloud */ public class Client implements Observer { private String clientName; private int id; public Client(String clientName, int id) { this.clientName = clientName; this.id = id; } @Override public void update(Observable o, Message message) { System.out.println(id + \"号\" + clientName + \"收到\u003c\" + ((Server)o).getName() + \"\u003e推送的消息：\" + message.toString()); } } /** * 具体被观察者角色 * * @author illusoryCloud */ public class Server extends Observable { /** * 被观察者name 用于区分多个被观察者 */ private String name; public Server(String name) { this.name = name; } @Override public void notifyObservers(Message message) { //发送消息 super.notifyObservers(message); //发送后取消change标志 clearChanged(); } public String getName(){ return this.name; } } /** * 观察者模式 测试类 * * @author illusoryCloud */ public class ObserverTest { @Test void observerTest() { //发送的消息对象 Message message = null; //1个被观察者 Server s1 = new Server(\"幻境\"); Server s2 = new Server(\"云图\"); //4个观察者 Client c1 = new Client(\"大佬\", 1); Client c2 = new Client(\"萌新\", 2); Client c3 = new Client(\"菜鸟\", 3); Client c4 = new Client(\"咸鱼\", 4); //将4个观察者分别注册到两个被观察者上 s1.addObserver(c1); s1.addObserver(c2); s2.addObserver(c3); s2.addObserver(c4); message = Message.newBuilder().setTitle(\"欢迎\") .setContent(\"欢迎关注 \u003c幻境云图\u003e\") .build(); //消息变化后 将被观察者设置为已变化状态 s1.setChanged(); s2.setChanged(); //发送消息 s1.notifyObservers(message); s2.notifyObservers(message); //再次发送消息无效 因为change=false s1.notifyObservers(message); s2.notifyObservers(message); } } //输出 2号萌新收到\u003c幻境\u003e推送的消息：Message{title='欢迎', content='欢迎关注 \u003c幻境云图\u003e'} 1号大佬收到\u003c幻境\u003e推送的消息：Message{title='欢迎', content='欢迎关注 \u003c幻境云图\u003e'} 4号咸鱼收到\u003c云图\u003e推送的消息：Message{title='欢迎', content='欢迎关注 \u003c幻境云图\u003e'} 3号菜鸟收到\u003c云图\u003e推送的消息：Message{title='欢迎', content='欢迎关注 \u003c幻境云图\u003e'} ","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/:2:0","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Java"],"content":"3. 总结 优点： 1.降低重复代码，使得代码更清晰、更易读、更易扩展 2.解耦，使得代码可维护性更好，修改代码的时候可以尽量少改地方 应用场景： 1.对一个对象状态的更新需要其他对象同步更新 2.对象仅需要将自己的更新通知给其他对象而不需要知道其他对象的细节，如消息推送. 观察者模式在Java中的应用及解读 JDK是有直接支持观察者模式的，就是java.util.Observer这个接口： public interface Observer { /** * This method is called whenever the observed object is changed. An * application calls an \u003ctt\u003eObservable\u003c/tt\u003e object's * \u003ccode\u003enotifyObservers\u003c/code\u003e method to have all the object's * observers notified of the change. * * @param o the observable object. * @param arg an argument passed to the \u003ccode\u003enotifyObservers\u003c/code\u003e * method. */ void update(Observable o, Object arg); } 这就是观察者的接口，定义的观察者只需要实现这个接口就可以了。update()方法，被观察者对象的状态发生变化时，被观察者的notifyObservers()方法就会调用这个方法： public class Observable { private boolean changed = false; private Vector\u003cObserver\u003e obs; public Observable() { obs = new Vector\u003c\u003e(); } public synchronized void addObserver(Observer o) { if (o == null) throw new NullPointerException(); if (!obs.contains(o)) { obs.addElement(o); } } public synchronized void deleteObserver(Observer o) { obs.removeElement(o); } public void notifyObservers() { notifyObservers(null); } public void notifyObservers(Object arg) { Object[] arrLocal; synchronized (this) { if (!changed) return; arrLocal = obs.toArray(); clearChanged(); } for (int i = arrLocal.length-1; i\u003e=0; i--) ((Observer)arrLocal[i]).update(this, arg); } public synchronized void deleteObservers() { obs.removeAllElements(); } protected synchronized void setChanged() { changed = true; } protected synchronized void clearChanged() { changed = false; } public synchronized boolean hasChanged() { return changed; } public synchronized int countObservers() { return obs.size(); } } 这是被观察者的父类，也就是主题对象，用的Vector集合,方法也加了synchronized关键字，是多线程安全的。 ","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/:3:0","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Java"],"content":"4. 参考 https://www.cnblogs.com/xrq730/p/4908686.html ","date":"2018-12-25","objectID":"/posts/java/design-pattern/10-observer/:4:0","tags":["设计模式"],"title":"Java常用设计模式(十)---观察者模式","uri":"/posts/java/design-pattern/10-observer/"},{"categories":["Blog"],"content":"通过gulp工具压缩静态文件提升网站加载速度","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Blog"],"content":"本章主要记录了如何通过gulp工具压缩压缩博客静态文件以加快网站加载速度。 在本系列文章的第二章中也有类似静态资源压缩的教程，是用的hexo-neat插件，最近用着用着出现了一点点问题，无奈之下换用了gulp。这个工具也可以很方便的压缩静态资源。 ","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/:0:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Blog"],"content":"1. 插件安装 首先需要安装gulp工具 命令：npm install gulp 接着安装功能模块，包括 # 清理html gulp-htmlclean # 压缩html gulp-htmlmin # 压缩css gulp-minify-css # 混淆js gulp-uglify # 一键安装 npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify --save ","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/:1:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Blog"],"content":"2. 创建任务 在站点根目录下，新建gulpfile.js文件，文件内容如下: var gulp = require('gulp'); //Plugins模块获取 var minifycss = require('gulp-minify-css'); var uglify = require('gulp-uglify'); var htmlmin = require('gulp-htmlmin'); var htmlclean = require('gulp-htmlclean'); //压缩css gulp.task('minify-css', function () { return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public')); }); //压缩html gulp.task('minify-html', function () { return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public')) }); //压缩js 不压缩min.js gulp.task('minify-js', function () { return gulp.src(['./public/**/*.js', '!./public/**/*.min.js']) .pipe(uglify()) .pipe(gulp.dest('./public')); }); //4.0以前的写法 //gulp.task('default', [ // 'minify-html', 'minify-css', 'minify-js' //]); //4.0以后的写法 // 执行 gulp 命令时执行的任务 gulp.task('default', gulp.parallel('minify-html', 'minify-css', 'minify-js', function() { // Do something after a, b, and c are finished. })); ","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/:2:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Blog"],"content":"3. 使用 使用时按照以下顺序就可以了： # 先清理文件 hexo clean # 编译生成静态文件 hexo g # gulp插件执行压缩任务 gulp # 开启服务 hexo s ","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/:3:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Blog"],"content":"4. 问题 刚开始弄这个的时候也是各种百度，Google，大部分的文章也是这么写的但是，第二部的js 代码却都有问题，也不能说有问题吧，大部分都是4.0以前的写法，导致现在gulp更新到4.0之后全都无法使用了。可能在看到这篇文章之前也试了各种办法。然后每次都出现这样的问题： assert.js:85 throw new assert.AssertionError({ ^ AssertionError: Task function must be specified at Gulp.set [as _setTask] (/home/hope/web/node_modules/undertaker/lib/set-task.js:10:3) at Gulp.task (/home/hope/web/node_modules/undertaker/lib/task.js:13:8) ................. 在看了下gulp相关资料后才发现了问题，接着把js代码稍微改了改终于能用了。不过运行的时候好像也有点问题，不过不影响使用，对这些工具还是不太了解。 [21:35:20] The following tasks did not complete: default, \u003canonymous\u003e [21:35:20] Did you forget to signal async completion? # TODO ","date":"2018-12-24","objectID":"/posts/blog/hexo/05-compress-statstic/:4:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(五)---压缩篇","uri":"/posts/blog/hexo/05-compress-statstic/"},{"categories":["Java"],"content":"模板方法模式的具体实现和优缺点分析","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的模板方法模式，并结合实例描述了模板方法模式的具体实现和优缺点分析。 ","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/:0:0","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"1. 简介 模板方法模式是类的行为模式。 准备一个抽象类，将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。 这就是模板方法模式的用意。 抽象模板(Abstract Template)角色有如下责任： 定义了一个或多个抽象操作，以便让子类实现。这些抽象操作叫做基本操作，它们是一个顶级逻辑的组成步骤。 定义并实现了一个模板方法。这个模板方法一般是一个具体方法，它给出了一个顶级逻辑的骨架，而逻辑的组成步骤在相应的抽象操作中，推迟到子类实现。顶级逻辑也有可能调用一些具体方法。 具体模板(Concrete Template)角色又如下责任： 实现父类所定义的一个或多个抽象方法，它们是一个顶级逻辑的组成步骤。 每一个抽象模板角色都可以有任意多个具体模板角色与之对应，而每一个具体模板角色都可以给出这些抽象方法（也就是顶级逻辑的组成步骤）的不同实现，从而使得顶级逻辑的实现各不相同。 ","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/:1:0","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"2. 代码实现 假设泡茶喝咖啡都需有四个步骤：1.烧水 2.泡茶/冲咖啡 3.倒入杯子 4.添加调味品 那么可以写一个抽象类，因为大多数饮料都可以看成这四个步骤。 然后烧水和倒入杯子这两个步骤都是相同的，那么在抽象类中可以直接实现，然后其他特殊操作则由子类具体实现。 /** * 模板方法模式 * 抽象模板角色 * * @author illusoryCloud */ public abstract class BaseCreatDrink { /** * 按顺序调用其他方法 */ public void doCreate() { boilWater(); brew(); pourInCup(); if (isNeedCondiments()) { addCondiments(); } } /** * 烧开水 * 通用的方法 直接实现 */ private void boilWater() { System.out.println(\"烧开水~\"); } /** * 特殊操作，在子类中具体实现 */ public abstract void brew(); /** * 倒入杯中 * 通用的方法 直接实现 */ private void pourInCup() { System.out.println(\"倒入杯中~\"); } /** * 添加调味品 茶里面加柠檬 咖啡中加糖等等 * 特殊操作 * 具体由子类实现 */ public abstract void addCondiments(); /** * 钩子方法，决定某些算法步骤是否挂钩在算法中 * 子类可以重写该类来改变算法或者逻辑 */ public boolean isNeedCondiments() { return true; } } /** * 具体模板角色 * 纯茶 * * @author illusoryCloud */ public class CreatTea extends BaseCreatDrink { @Override public void brew() { System.out.println(\"泡茶~\"); } @Override public void addCondiments() { System.out.println(\"加柠檬~\"); } } /** * 具体模板角色 * 茶 * * @author illusoryCloud */ public class CreatPureTea extends BaseCreatDrink { @Override public void brew() { System.out.println(\"泡茶~\"); } @Override public void addCondiments() { System.out.println(\"加柠檬~\"); } /** * 通过重写钩子方法来改变算法 * 返回true则添加调味品 * 返回false则不加 * 默认为true * * @return isNeedCondiments */ @Override public boolean isNeedCondiments() { return false; } } /** * 具体模板角色 * 咖啡 * * @author illusoryCloud */ public class CreatCoffee extends BaseCreatDrink { @Override public void brew() { System.out.println(\"冲咖啡~\"); } @Override public void addCondiments() { System.out.println(\"加糖~\"); } } /** * 模板方法模式 测试类 * * @author illusoryCloud */ public class TemplateTest { @Test public void templateTest() { System.out.println(\"-------茶-------\"); CreatTea tea = new CreatTea(); tea.doCreate(); System.out.println(\"-------咖啡-------\"); CreatCoffee coffee = new CreatCoffee(); coffee.doCreate(); System.out.println(\"-------纯茶-------\"); CreatPureTea pureTea = new CreatPureTea(); pureTea.doCreate(); } } //输出 往锅里加的是白菜~ 炒啊炒啊炒~ 菜炒好了，起锅~ 往锅里加的是肉~ 炒啊炒啊炒~ 菜炒好了，起锅~ ","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/:2:0","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"3. 总结 模板方法模式在Java中的应用 最常见的就是Servlet了。 HttpServlet担任抽象模板角色 模板方法：由service()方法担任。 基本方法：由doPost()、doGet()等方法担任。 MyServlet担任具体模板角色 自定义的servlet置换掉了父类HttpServlet中七个基本方法中的其中两个，分别是doGet()和doPost()。 ","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/:3:0","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"4. 参考 https://www.cnblogs.com/qiumingcheng/p/5219664.html https://www.cnblogs.com/yanlong300/p/8446261.html ","date":"2018-12-23","objectID":"/posts/java/design-pattern/09-template/:4:0","tags":["设计模式"],"title":"Java常用设计模式(九)---模板方法模式","uri":"/posts/java/design-pattern/09-template/"},{"categories":["Java"],"content":"策略模式的具体实现及其优缺点分析","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的策略模式，并结合实例描述了策略模式的具体实现和策略模式的优缺点分析。 ","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/:0:0","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"1. 简介 策略模式是对算法的包装 策略模式定义了一系列的算法，并将每一个算法封装起来，而且它们还可以相互替换。策略模式让算法独立于使用它的客户而独立变化。 这个模式涉及到三个角色： ●　环境(Context)角色：持有一个Strategy的引用。 ●　抽象策略(Strategy)角色：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。 ●　具体策略(ConcreteStrategy)角色：包装了相关的算法或行为。 ","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/:1:0","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"2. 代码实现 /** * 策略模式 抽象策略角色 * 定义一个两个整数间的计算方法 * * @author illusoryCloud */ public interface Strategy { /** * 两个整数间的计算方法 * * @param a * @param b * @return */ int calculate(int a, int b); } /** * 策略模式 具体策略角色 * 加法 * * @author illusoryCloud */ public class AddStrategy implements Strategy { @Override public int calculate(int a, int b) { return a + b; } } /** * 策略模式 具体策略角色 * 减法 * * @author illusoryCloud */ public class SubtractionStrategy implements Strategy { @Override public int calculate(int a, int b) { return a - b; } } /** * 策略模式 具体策略角色 * 乘法 * * @author illusoryCloud */ public class MultiplyStrategy implements Strategy { @Override public int calculate(int a, int b) { return a * b; } } /** * 策略模式 具体策略角色 * 除法 * * @author illusoryCloud */ public class DivisionStrategy implements Strategy { @Override public int calculate(int a, int b) { if (b != 0) { return a / b; } else { throw new RuntimeException(\"除数不能为零\"); } } } /** * 策略模式 环境角色 * * @author illusoryCloud */ public class Context { /** * 持有Strategy的引用 */ private Strategy strategy; public Context(Strategy strategy) { super(); this.strategy = strategy; } public Strategy getStrategy() { return strategy; } /** * set方法可以完成策略更换 */ public void setStrategy(Strategy strategy) { this.strategy = strategy; } public int calculate(int a, int b) { return strategy.calculate(a, b); } } /** * 策略模式 测试类 * * @author illusoryCloud */ public class StrategyTest { @Test public void strategyTest() { //加法 Context context = new Context(new AddStrategy()); System.out.println(context.calculate(5, 5)); //减法 Context context2 = new Context(new SubtractionStrategy()); System.out.println(context2.calculate(5, 5)); //乘法 Context context3 = new Context(new MultiplyStrategy()); System.out.println(context3.calculate(5, 5)); //除法 Context context4 = new Context(new DivisionStrategy()); System.out.println(context4.calculate(5, 5)); } } ","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/:2:0","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"3. 总结 策略模式的重心不是如何实现算法（就如同工厂模式的重心不是工厂中如何产生具体子类一样），而是如何组织、调用这些算法，从而让程序结构更灵活，具有更好的维护性和扩展性。 策略模式与状态模式 策略模式与状态模式及其相似，但是二者有其内在的差别，策略模式将具体策略类暴露出去，调用者需要具体明白每个策略的不同之处以便正确使用。而状态模式状态的改变是由其内部条件来改变的，与外界无关，二者在思想上有本质区别。 优点 1.让代码更优雅，避免了多重条件if…else语句。 2.策略模式提供了管理相关算法簇的办法，恰当使用继承可以把公共代码移到父类，从而避免了代码重复。 缺点 1.客户端必须知道所有的策略类，并自行决定使用 哪一个策略，这意味着客户端必须理解这些算法的区别，以便选择恰当的算法 2.如果备选策略很多，对象的数据会很多 ","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/:3:0","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"4. 参考 https://www.cnblogs.com/xrq730/p/4906313.html ","date":"2018-12-19","objectID":"/posts/java/design-pattern/08-strategy/:4:0","tags":["设计模式"],"title":"Java常用设计模式(八)---策略模式","uri":"/posts/java/design-pattern/08-strategy/"},{"categories":["Java"],"content":"外观模式的实现及其在Tomcat中的使用","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的外观模式，并结合实例描述了 模式的具体实现和性能分析测试。 ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:0:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"1. 简介 它通过引入一个外观角色来简化客户端与子系统之间的交互，为复杂的子系统调用提供一个统一的入口，降低子系统与客户端的耦合度，且客户端调用非常方便。 外观模式结构： SubSystem: 子系统角色。表示一个系统的子系统或模块。 Facade: 外观角色，客户端通过操作外观角色从而达到控制子系统角色的目的。对于客户端来说，外观角色好比一道屏障，对客户端屏蔽了子系统的具体实现。 ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:1:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"2. 具体实现 /** * 子系统角色类 * 电脑CPU * * @author illusoryCloud */ public class CPU { public void startUp(){ System.out.println(\"cpu is startUp...\"); } public void shutDown(){ System.out.println(\"cpu is shutDown...\"); } } /** * 子系统角色类 * 电脑硬盘 * * @author illusoryCloud */ public class Disk { public void startUp() { System.out.println(\"disk is startUp...\"); } public void shutDown() { System.out.println(\"disk is shutDown...\"); } } /** * 子系统角色类 * 电脑内存 * * @author illusoryCloud */ public class Memory { public void startUp() { System.out.println(\"memory is startUp...\"); } public void shutDown() { System.out.println(\"memory is shutDown...\"); } } /** * 外观角色 * 电脑 * 用户通过操作当前类即可达到操作所有子系统的目的 * * @author illusoryCloud */ public class Computer { private CPU cpu; private Disk disk; private Memory memory; public Computer() { cpu = new CPU(); disk = new Disk(); memory = new Memory(); } public void startUp() { cpu.startUp(); disk.startUp(); memory.startUp(); } public void shutDown() { cpu.shutDown(); disk.shutDown(); memory.shutDown(); } } /** * 外观模式 测试类 * * @author illusoryCloud */ public class FacedeTest { @Test public void facedeTest() { Computer computer = new Computer(); computer.startUp(); System.out.println(\"------------------\"); computer.shutDown(); } } ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:2:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"3. 总结 外观模式的优点 外观模式有如下几个优点： 1、松散耦合 外观模式松散了客户端和子系统的耦合关系，让子系统内部的模块能更容易扩展和维护 2、简单易用 客户端不需要了解系统内部的实现，也不需要和众多子系统内部的模块交互，只需要和外观类交互就可以了 3、更好地划分层次 通过合理使用Facade，可以帮助我们更好地划分层次。有些方法是系统对内的，有些方法是对外的，把需要暴露给外部的功能集中到Facade中，这样既方便客户端使用，也很好地隐藏了内部的细节 ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:3:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"4. Tomcat中的外观模式 Tomcat中有很多场景都使用到了外观模式，因为Tomcat中有很多不同的组件，每个组件需要相互通信，但又不能将自己内部数据过多地暴露给其他组件。用外观模式隔离数据是个很好的方法，比如Request上使用外观模式。 比如Servlet，doGet和doPost方法，参数类型是接口HttpServletRequest和接口HttpServletResponse，那么Tomcat中传递过来的真实类型到底是什么呢？ 在真正调用Servlet前，会经过很多Tomcat方法，传递给Tomcat的request和response的真正类型是一个Facade类。 Request类 public HttpServletRequest getRequest() { if (facade == null) { facade = new RequestFacade(this); } return facade; } Response类 public HttpServletResponse getResponse() { if (facade == null) { facade = new ResponseFacade(this); } return (facade); } 因为Request类中很多方法都是组件内部之间交互用的，比如setComet、setReuqestedSessionId等方法，这些方法并不对外公开，但又必须设置为public，因为还要和内部组件交互使用。最好的解决方法就是通过使用一个Facade类，屏蔽掉内部组件之间交互的方法，只提供外部程序要使用的方法。 如果不使用Facade，直接传递的是HttpServletRequest和HttpServletResponse，那么熟悉容器内部运作的开发者可以分别把ServletRequest和ServletResponse向下转型为HttpServletRequest和HttpServletResponse，这样就有安全性的问题了。 ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:4:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Java"],"content":"5. 参考 https://www.cnblogs.com/xrq730/p/4908822.html ","date":"2018-12-17","objectID":"/posts/java/design-pattern/07-facade/:5:0","tags":["设计模式"],"title":"Java常用设计模式(七)---外观模式","uri":"/posts/java/design-pattern/07-facade/"},{"categories":["Android"],"content":"一个Android平台自动发送邮件的小demo","date":"2018-12-15","objectID":"/posts/android/01-email-sender/","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"一个Android平台自动发送邮件的小demo，记录了一下实现的过程。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:0:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"1. 导包 使用邮件发送功能,需要导入3个jar包. additional.jar mail.jar activation.jar //用的是AndroidStudio //1.切换到Project视图 //2.将这3个jar包放到app下的lib文件夹中 //3.选择这个三个jar包右键 Add As Library //4.如果导入成功 在Module 的build.gradle中就能看到这个 和平常引入第三方库一样 implementation files('libs/activation.jar') implementation files('libs/additionnal.jar') implementation files('libs/mail.jar') ","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:1:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"2. 创建Helper工具类 package lillusory.com.androidemail; import android.os.AsyncTask; import android.util.Log; import java.util.Date; import java.util.List; import java.util.Properties; import javax.activation.CommandMap; import javax.activation.MailcapCommandMap; import javax.mail.Authenticator; import javax.mail.Message; import javax.mail.Multipart; import javax.mail.PasswordAuthentication; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeBodyPart; import javax.mail.internet.MimeMessage; import javax.mail.internet.MimeMultipart; public class MyEmailHelper { private static final String TAG = MyEmailHelper.class.getSimpleName(); private int port = 25; //smtp协议使用的端口 private String host = \"smtp.163.com\"; // 发件人邮件服务器 //TODO 需要改成自己的账号和授权密码 private String user = \"xxx@163.com\"; // 使用者账号 private String password = \"xxx\"; //使用者SMTP授权密码 private List\u003cString\u003e emailTos; private List\u003cString\u003e emailCCs; private String title; private String context; private List\u003cString\u003e paths; public enum SendStatus { SENDING, UNDO, SENDOK, SENDFAIL, BADCONTEXT } private SendStatus sendStatus; public interface EmailInfterface { void startSend(); void SendStatus(SendStatus sendStatus); } private EmailInfterface EmailInfterface; public void setJieEmailInfterface(EmailInfterface EmailInfterface) { this.EmailInfterface = EmailInfterface; } public MyEmailHelper() { sendStatus = SendStatus.UNDO; } //构造发送邮件帐户的服务器，端口，帐户，密码 public MyEmailHelper(String host, int port, String user, String password) { this.port = port; this.user = user; this.password = password; this.host = host; sendStatus = SendStatus.UNDO; } /** * @param emailTos 主要接收人的电子邮箱列表 * @param emailCCs 抄送人的电子邮箱列表 * @param title 邮件标题 * @param context 正文内容 * @param paths 发送的附件路径集合 */ public void setParams(List\u003cString\u003e emailTos, List\u003cString\u003e emailCCs, String title, String context, List\u003cString\u003e paths) { this.emailTos = emailTos; this.emailCCs = emailCCs; this.title = title; this.context = context; this.paths = paths; } public void sendEmail() { new MyAsynTask().execute(); } private void sendEmailBg() throws Exception { Properties properties = new Properties(); properties.put(\"mail.smtp.host\", host); properties.put(\"mail.smtp.port\", port); properties.put(\"mail.smtp.auth\", \"true\");//true一定要加引号 properties.put(\"mail.transport.protocol\", \"smtp\"); MyAuthenticator jieAuth = new MyAuthenticator(user, password); Session session = Session.getInstance(properties, jieAuth); //创建一个消息 MimeMessage msg = new MimeMessage(session); //设置发送人 msg.setFrom(new InternetAddress(user)); //设置主要接收人 if (emailTos != null \u0026\u0026 !emailTos.isEmpty()) { int size = emailTos.size(); InternetAddress[] addresses = new InternetAddress[size]; for (int i = 0; i \u003c size; i++) { addresses[i] = new InternetAddress(emailTos.get(i)); } msg.setRecipients(Message.RecipientType.TO, addresses); } //设置抄送人的电子邮件 if (emailCCs != null \u0026\u0026 !emailCCs.isEmpty()) { int size = emailCCs.size(); InternetAddress[] addresses = new InternetAddress[size]; for (int i = 0; i \u003c size; i++) { addresses[i] = new InternetAddress(emailCCs.get(i)); } msg.setRecipients(Message.RecipientType.CC, addresses); } msg.setSubject(title); //创建一个消息体 MimeBodyPart msgBodyPart = new MimeBodyPart(); msgBodyPart.setText(context); //创建Multipart增加其他的parts Multipart mp = new MimeMultipart(); mp.addBodyPart(msgBodyPart); //创建文件附件 if (paths != null) { for (String path : paths) { MimeBodyPart fileBodyPart = new MimeBodyPart(); fileBodyPart.attachFile(path); mp.addBodyPart(fileBodyPart); } } //增加Multipart到消息体中 msg.setContent(mp); //设置日期 msg.setSentDate(new Date()); //设置附件格式 MailcapCommandMap mc = (MailcapCommandMap) CommandMap.getDefaultCommandMap(); mc.addMailcap(\"text/html;; x-java-content-handler=com.sun.mail.handlers.text_html\"); mc.addMailcap(\"text/xml;; x-java-content-handler=com.sun.mail.handlers.text_xml\"); mc.addMailcap(\"text/plain;; x-java-content-handler=com.sun.mail.handlers.text_plain\"); mc.addMailcap(\"multipart/*;; x-java-cont","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:2:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"3. 具体发送方法 public void sendMail(String from, String to, String title, String context) { // 附件 // List\u003cString\u003e files = new ArrayList\u003cString\u003e(); // files.add(\"/mnt/sdcard/test.txt\"); //主要接收人的电子邮箱列表 List\u003cString\u003e toEmail = new ArrayList\u003cString\u003e(); toEmail.add(to); List\u003cString\u003e ccEmail = new ArrayList\u003cString\u003e(); //抄送人的电子邮箱列表 抄送给自己 防止被检测为垃圾邮件 ccEmail.add(from); helper.setParams(toEmail, ccEmail, title, context, null); Log.v(TAG, \"toEmail:\" + toEmail + \" ccEmail:\" + ccEmail + \" EMAIL_TITLE_APP:\" + title + \" appEmailContext:\" + context); helper.setJieEmailInfterface(new MyEmailHelper.EmailInfterface() { @Override public void startSend() { Toast.makeText(MainActivity.this, \"邮件发送中~\", Toast.LENGTH_LONG).show(); } @Override public void SendStatus(MyEmailHelper.SendStatus sendStatus) { switch (sendStatus) { case SENDOK: Toast.makeText(MainActivity.this, \"发送邮件成功~\", Toast.LENGTH_LONG).show(); break; case SENDFAIL: Toast.makeText(MainActivity.this, \"发送邮件失败~\", Toast.LENGTH_LONG).show(); break; case SENDING: Toast.makeText(MainActivity.this, \"邮件正在发送中，请稍后重试~\", Toast.LENGTH_LONG).show(); break; case BADCONTEXT: Toast.makeText(MainActivity.this, \"邮件内容或标题被识别为垃圾邮件，请修改后重试~\", Toast.LENGTH_LONG).show(); break; } } }); helper.sendEmail(); } ","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:3:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"4. 发送失败原因检查 package lillusory.com.androidemail; import android.util.Log; public class CheckErrorUtils { public static MyEmailHelper.SendStatus checkExcption(String message){ if(message.contains(\"554 DT:SPM\")){ //发送失败原因有很多 这个是比较常见的问题 Log.v(\"Az\",\"邮件被识别为垃圾邮件了~\"); } return MyEmailHelper.SendStatus.BADCONTEXT; } } ","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:4:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Android"],"content":"5. 网络权限 记得添加网络权限 github ","date":"2018-12-15","objectID":"/posts/android/01-email-sender/:5:0","tags":["Android"],"title":"Android平台自动发送邮件demo","uri":"/posts/android/01-email-sender/"},{"categories":["Java"],"content":"各种代理模式的具体实现和对比及代理模式与装饰者模式区别","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的代理模式，并结合实例描述了各种代理模式的具体实现和对比。包括：JDK静态代理，JDK动态代理，cglib动态代理. ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:0:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"1. 简介 给某一对象提供一个代理对象，并由代理对象控制对原对象的引用。 代理模式的结构 有些情况下，一个客户不想或者不能够直接引用一个对象，可以通过代理对象在客户端和目标对象之间起到中介作用。代理模式中的角色有： 1、抽象对象角色 声明了目标对象和代理对象的共同接口，这样一来在任何可以使用目标对象的地方都可以使用代理对象 2、目标对象角色 定义了代理对象所代表的目标对象 3、代理对象角色 代理对象内部含有目标对象的引用，从而可以在任何时候操作目标对象；代理对象提供一个与目标对象相同的接口，以便可以在任何时候替代目标对象 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:1:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"2. 静态代理 由程序员创建或特定工具自动生成源代码，也就是在编译时就已经将接口，被代理类，代理类等确定下来。在程序运行之前，代理类的.class文件就已经生成。 代理类和被代理类必须实现同一个接口 /** * 抽象对象角色 * * @author illusoryCloud */ public interface Human { void work(); } /** * 目标对象角色 * * @author illusoryCloud */ public class Singer implements Human { @Override public void work() { System.out.println(\"歌手在唱歌~\"); } } /** * 代理对象角色 * * @author illusoryCloud */ public class ProxyMan implements Human { /** * 持有目标对象的引用 */ private Human human; /** * 通过构造方法注入 * * @param human 目标对象 */ public ProxyMan(Human human) { this.human = human; } @Override public void work() { System.out.println(\"经纪人为歌手安排好时间~\"); human.work(); System.out.println(\"经纪人为歌手联系下一场演出~\"); } } /** * 静态代理模式 测试类 * * @author illusoryCloud */ public class StaticProxyTest { @Test public void staticProxyTest() { Human singer = new ProxyMan(new Singer()); singer.work(); } } //输出结果 经纪人为歌手安排好时间~ 歌手在唱歌~ 经纪人为歌手联系下一场演出~ ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:2:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"3. JDK动态代理 代理类在程序运行时创建的代理方式被成为动态代理。 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:3:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"1. 具体实现 /** * 回调方法 * * @author illusoryCloud */ public class MyInvocationHandler implements InvocationHandler { public static final String PROXY_METHOD = \"work\"; /** * 持有一个被代理对象的引用 */ private Human human; public MyInvocationHandler(Human human) { this.human = human; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 判断是否是需要代理的方法 if (PROXY_METHOD.equals(method.getName())) { System.out.println(\"经纪人为歌手安排好时间~\"); Object invoke = method.invoke(human, args); System.out.println(\"经纪人为歌手联系下一场演出~\"); return invoke; } else { return null; } } } /** * JDK动态代理 测试类 * * @author illusoryCloud */ public class JDKProxyTest { @Test public void JDKProxyTest() { Singer singer = new Singer(); //参数1：类加载器 参数2：被代理类实现的接口 参数3：回调 由自己实现 Human human = (Human) Proxy.newProxyInstance(singer.getClass().getClassLoader() , singer.getClass().getInterfaces() , new MyInvocationHandler(singer)); human.work(); } } ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:3:1","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"2. InvocationHandler InvocationHandler是一个接口，官方文档解释说，每个代理的实例都有一个与之关联的 InvocationHandler 实现类，如果代理的方法被调用，那么代理便会通知和转发给内部的 InvocationHandler 实现类，由它决定处理。 public interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; } 接口内部只是一个 invoke() 方法，正是这个方法决定了怎么样处理代理传递过来的方法调用。对代理对象的增强就在这里进行。实现该接口 重写此方法 可以用匿名内部类或者直接用生成代理的那个类实现该接口。 方法参数 1.proxy 代理对象，可以使用反射获取代理对象proxy.getClass().getName() 2.method 代理对象调用的方法 3.args 调用的方法中的参数 因为Proxy 动态产生的代理会调用 InvocationHandler实现类，所以 InvocationHandler是实际执行者。 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:3:2","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"3. 生成代理对象 Proxy.newProxyInstance(classLoader, interfaces, dynamicInvocationHandler); 方法参数 1.classLoader 类加载器,告诉虚拟机用哪个字节码加载器加载内存中创建出来的字节码文件 一般是application类加载器.(增强哪个对象就写哪个类的类加载器) 2.interfaces 字节码数组 告诉虚拟机内存中正在你被创建的字节码文件中应该有哪些方法(被代理类实现的所有接口的字节码数组 ) 3.一个InvocationHandler对象,表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上,告诉虚拟机字节码上的那些方法如何处理 （用户自定义增强操作等 写在实现InvocationHandler接口的那个类中. 小结： 1.通过 Proxy.newProxyInstance(classLoader, interfaces, dynamicInvocationHandler);生成代理对象 2.创建InvocationHandler接口实现类 重写invoke方法 实现具体的方法增强 3.调用对象的方法最后都是调用InvocationHandler接口的invoke方法 4.只能增强接口中有的方法 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:3:3","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"4. CGLIB动态代理 JDK代理要求被代理的类必须实现接口，有很强的局限性。 而CGLIB动态代理则没有此类强制性要求。简单的说，CGLIB会让生成的代理类继承被代理类，并在代理类中对代理方法进行强化处理(前置处理、后置处理等)。在CGLIB底层，其实是借助了ASM这个非常强大的Java字节码生成框架。 cglib原理 通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。由于是通过子类来代理父类，因此不能代理被final字段修饰的方法。 需要引入两个jar包 cglib-3.2.10.jar //cglib包 asm-7.0.jar //底层用到的asm包 /** * 被代理类 没有实现接口 无法使用JDK动态代理 * * @author illusoryCloud */ public class Dancer { public void dance() { System.out.println(\"跳舞者翩翩起舞~\"); } } /** * @author illusoryCloud */ public class MyMethodInterceptor implements MethodInterceptor { public static final String PROXY_METHOD = \"work\"; /** * @param o cglib生成的代理对象 * @param method 目标对象的方法 * @param objects 方法入参 * @param methodProxy 代理方法 * @return 返回值 * @throws Throwable 异常 */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\"经纪人为舞蹈演员安排好时间~\"); //注意 这里是invokeSuper 若是invoke则会循环调用最终堆栈溢出 Object o1 = methodProxy.invokeSuper(o, objects); System.out.println(\"经纪人为舞蹈演员联系下一场演出~\"); return o1; } } /** * CGLib动态代理 测试类 * * @author illusoryCloud */ public class CglibProxyTest { @Test public void cglibProxyTest(){ Enhancer enhancer=new Enhancer(); //设置父类 即被代理类 cglib是通过生成子类的方式来代理的 enhancer.setSuperclass(Dancer.class); //设置回调 enhancer.setCallback(new MyMethodInterceptor()); Dancer dancer= (Dancer) enhancer.create(); dancer.dance(); } } ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:4:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"5. 代理模式比较 代理方式 实现 优点 缺点 特点 JDK静态代理 代理类与委托类实现同一接口，并且在代理类中需要硬编码接口 实现简单，容易理解 代理类需要硬编码接口，在实际应用中可能会导致重复编码，浪费存储空间并且效率很低 好像没啥特点 JDK动态代理 代理类与委托类实现同一接口，主要是通过代理类实现InvocationHandler并重写invoke方法来进行动态代理的，在invoke方法中将对方法进行增强处理 不需要硬编码接口，代码复用率高 只能够代理实现了接口的委托类 底层使用反射机制进行方法的调用 CGLIB动态代理 代理类将委托类作为自己的父类并为其中的非final委托方法创建两个方法，一个是与委托方法签名相同的方法，它在方法中会通过super调用委托方法；另一个是代理类独有的方法。在代理方法中，它会判断是否存在实现了MethodInterceptor接口的对象，若存在则将调用intercept方法对委托方法进行代理 可以在运行时对类或者是接口进行增强操作，且委托类无需实现接口 不能对final类以及final方法进行代理 底层将方法全部存入一个数组中，通过数组索引直接进行方法调用 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:5:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"6. 代理模式与装饰器模式 代理模式和装饰者模式有着很多的应用，这两者具有一定的相似性，都是通过一个新的对象封装原有的对象。 二者之间的差异在于代理模式是为了实现对象的控制，可能被代理的对象难以直接获得或者是不想暴露给客户端，而装饰者模式是继承的一种替代方案，在避免创建过多子类的情况下为被装饰者提供更多的功能。 装饰器模式应当为所装饰的对象提供增强功能，而代理模式对所代理对象的使用施加控制，并不提供对象本身的增强功能。 代理模式侧重于不能直接访问一个对象，只能通过代理来间接访问，比如对象在另外一台机器上， 或者对象被持久化了，对象是受保护的。对象在另外一台机器上，其实就是rpc，感兴趣的可以看看dubbo的源码本地访问的其实就是远程对象的代理，只不过代理帮你做了访问这个对象之前和之后的很多事情，但是对使用者是透明的了。对象被持久化了，比如mybatis的mapperProxy。通过mapper文件自动生成代理类。第三种，对内核对象的访问。 装饰器模式是因为没法在编译期就确定一个对象的功能，需要运行时动态的给对象添加职责，所以只能把对象的功能拆成一一个个的小部分，动态组装，感兴趣的可以看看dubbo的源码，里面的mock，cluster，failover都是通过装饰器来实现的。因为这些功能是由使用者动态配置的。但是代理模式在编译期其实就已经确定了和代理对象的关系。 同时这两个设计模式是为了解决不同的问题而抽象总结出来的。是可以混用的。可以在代理的基础上在加一个装饰，也可以在装饰器的基础上在加一个代理。感兴趣的去看看dubbo源码，里面就是这么实现的。 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:6:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"7. 参考 https://www.cnblogs.com/xrq730/p/4907999.html https://www.zhihu.com/question/41988550/answer/567925484 ","date":"2018-12-05","objectID":"/posts/java/design-pattern/06-proxy/:7:0","tags":["设计模式"],"title":"Java常用设计模式(六)---代理模式","uri":"/posts/java/design-pattern/06-proxy/"},{"categories":["Java"],"content":"双重校验锁式单例模式中volatile关键字起什么作用？《并发编程的艺术》笔记","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"本文主要讲述了Java单例模式之双重校验锁中volatile关键字的作用。 上篇文章Java设计模式(一)–单例模式中讲了Java单例模式的几种写法，其中懒汉式和双重校验锁方式写法如下： ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:0:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"1. 懒汉式 public class Singleton { private static Singleton instance; private Singleton (){} public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 这种方式实现的单例：实现了lazy loading 使用时才创建实例。synchronized保证了线程安全，但效率低。 ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:1:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"2. 双重校验锁 public class Singleton { private static volatile Singleton singleton; private Singleton() { } public static Singleton getInstance() { if (singleton == null) { synchronized (Singleton.class) { //1 if (singleton == null) { //2 singleton = new Singleton(); //3 } } } return singleton; } } ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:2:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"3. 执行过程 双重校验锁方式的执行过程如下： 1.线程A进入 getInstance() 方法。 2.由于 singleton为 null，线程A在 //1 处进入 synchronized 块。 3.线程A被线程B预占。 4.线程B 进入 getInstance() 方法。 5.由于 singleton仍旧为 null，线程B试图获取 //1 处的锁。然而，由于线程A已经持有该锁，线程B在 //1 处阻塞。 6.线程B被线程A预占。 7.线程A执行，由于在 //2 处实例仍旧为 null，线程A还创建一个 Singleton 对象并将其引用赋值给 instance。 8.线程A退出 synchronized 块并从 getInstance() 方法返回实例。 9.线程A被线程B预占。 10.线程B获取 //1 处的锁并检查 instance 是否为 null。 11.由于 singleton是非 null 的，并没有创建第二个 Singleton 对象，由线程A所创建的对象被返回。 ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:3:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"4. 问题 双重检查锁定背后的理论是完美的。不幸地是，现实完全不同。双重检查锁定的问题是：并不能保证它会在单处理器或多处理器计算机上顺利运行。 双重检查锁定失败的问题并不归咎于 JVM 中的实现 bug，而是归咎于 Java 平台内存模型。内存模型允许所谓的“无序写入”，这也是这些习语失败的一个主要原因。 singleton = new Singleton(); 该语句非原子操作，实际是三个步骤。 1.给 singleton 分配内存； 2.调用 Singleton 的构造函数来初始化成员变量； 3.将给 singleton 对象指向分配的内存空间（此时 singleton 才不为 null ）； 虚拟机的指令重排序–\u003e 执行命令时虚拟机可能会对以上3个步骤交换位置 最后可能是132这种 分配内存并修改指针后未初始化 多线程获取时可能会出现问题。 当线程A进入同步方法执行singleton = new Singleton();代码时，恰好这三个步骤重排序后为1 3 2， 那么步骤3执行后 singleton 已经不为 null ,但是未执行步骤2，singleton 对象初始化不完全，此时线程B执行 getInstance() 方法，第一步判断时 singleton 不为null,则直接将未完全初始化的singleton对象返回了。 ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:4:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"5. 解决 如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的，同时还会禁止指令重排序 所以使用volatile关键字会禁止指令重排序,可以避免这种问题。使用volatile关键字后使得 singleton = new Singleton();语句一定会按照上面拆分的步骤123来执行。 ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:5:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"参考 https://blog.csdn.net/qq646040754/article/details/81327933 ","date":"2018-12-03","objectID":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/:6:0","tags":["Java"],"title":"volatile关键字在单例模式(双重校验锁)中的作用","uri":"/posts/java/design-pattern/01ex2-java-volatile-double-check-lock/"},{"categories":["Java"],"content":"装饰者模式实现及优缺点分析","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的装饰者模式，并结合实例描述了装饰者模式的具体实现和优缺点分析。 ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:0:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"1. 简介 在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。 它是通过创建一个包装对象，也就是装饰来包裹真实的对象。是继承关系的一个替代方案。 装饰模式由4种角色组成： （1）抽象构件（Component）角色：给出一个抽象接口，以规范准备接收附加职责的对象。 （2）具体构件（Concrete Component）角色：定义一个将要接收附加职责的类。 （3）装饰（Decorator）角色：持有一个构件（Component）对象的实例，并实现一个与抽象构件接口一致的接口，从外类来扩展Component类的功能，但对于Component类来说，是无需知道Decorato的存在的。 （4）具体装饰（Concrete Decorator）角色：负责给构件对象添加上附加的职责。 ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:1:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"2. 具体实现 /** * 抽象构件角色 * 人类 * * @author illusoryCloud */ public interface Human { void run(); } /** * 具体构件角色 * 男人 * * @author illusoryCloud */ public class Man implements Human { @Override public void run() { System.out.println(\"男人跑得很快\"); } } /** * 抽象装饰角色 * * @author illusoryCloud */ public class Decorator implements Human { /** * 持有一个具体构件的引用 */ private Human human; public Decorator(Human human) { this.human = human; } @Override public void run() { human.run(); } } /** * 具体装饰角色 * 飞人 * * @author illusoryCloud */ public class FlyMan extends Decorator { public FlyMan(Human human) { super(human); } @Override public void run() { super.run(); this.fly(); } /** * 扩展功能 */ private void fly() { System.out.println(\"变成飞人了，跑得更快了~\"); } } /** * 具体装饰角色 * 强壮的男人 * * @author illusoryCloud */ public class StrongMan extends Decorator { public StrongMan(Human human) { super(human); } @Override public void run() { super.run(); this.strong(); } public void strong() { System.out.println(\"变得强壮了，耐力提升了~\"); } } /** * 装饰者模式 测试类 * * @author illusoryCloud */ public class DecoratorTest { @Test public void decoratorTest() { //普通对象 Human man = new Man(); man.run(); System.out.println(\"--------------------\"); //装饰后的对象 Human flyMan = new FlyMan(man); flyMan.run(); System.out.println(\"--------------------\"); //装饰后的对象 Human strongMan = new StrongMan(man); strongMan.run(); System.out.println(\"--------------------\"); //装饰后的对象再次装饰 Human strongFlyMan = new StrongMan(flyMan); strongFlyMan.run(); } } //输出 男人在跑 -------------------- 男人在跑 变成飞人了，速度加快了~ -------------------- 男人在跑 变得强壮了，耐力提升了~ -------------------- 男人在跑 变成飞人了，速度加快了~ 变得强壮了，耐力提升了~ ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:2:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"3. 总结 优点 1.装饰者模式可以提供比继承更多的灵活性。装饰器模式允许系统动态决定贴上一个需要的装饰，或者除掉一个不需要的装饰。继承关系是不同，继承关系是静态的，它在系统运行前就决定了。 2.通过使用不同的具体装饰器以及这些装饰类的排列组合，设计师可以创造出很多不同的行为组合。 缺点 由于使用装饰器模式，可以比使用继承关系需要较少数目的类。使用较少的类，当然使设计比较易于进行。但是另一方面，由于使用装饰器模式会产生比使用继承关系更多的对象，更多的对象会使得查错变得困难，特别是这些对象看上去都很像。 装饰者模式和代理模式对比 装饰者模式主要对功能进行扩展，代理模式主要是添加一些无关业务的功能，比如日志，验证等。 使用代理模式,代理和真实对象之间的关系在编译时就已经确定了,而装饰器者能够在运行时递归的被构造.(代理模式会在代理类中创建真实处理类的一个实例,所以可以确定代理和真实对象的关系,而装饰器模式是将原始对象作为一个参数传给装饰器类) 装饰模式：以对客户端透明的方式扩展对象的功能，是继承关系的一个替代方案； 代理模式：给一个对象提供一个代理对象，并有代理对象来控制对原有对象的引用； ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:3:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"4. 装饰者模式在Java中的应用 装饰器模式在Java体系中的经典应用是Java I/O 抽象构件角色:InputStream 具体构建角色:ByteArrayInputStream、FileInputStream、ObjectInputStream、PipedInputStream等 装饰角色；FilterInputStream –\u003e实现了InputStream内的所有抽象方法并且持有一个InputStream的引用 具体装饰角色:InflaterInputStream、BufferedInputStream、DataInputStream等 ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:4:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"5. 参考 https://www.cnblogs.com/xrq730/p/4908940.html ","date":"2018-12-01","objectID":"/posts/java/design-pattern/05-decorator/:5:0","tags":["设计模式"],"title":"Java常用设计模式(五)---装饰者模式","uri":"/posts/java/design-pattern/05-decorator/"},{"categories":["Java"],"content":"适配器模式实现、原理及优缺点分析","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式之适配器模式，并结合实例描述了适配器模式的具体实现和优缺点分析。 ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:0:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"1. 简介 适配器模式将一个接口转换成客户希望的另外一个接口。它使得原来由于接口不兼容而不能在一起工作的那些类可以一起工作。 把一个类的接口变换成客户端所期待的另一种接口 用到的对象 Target —定义Client使用的与特定领域相关的接口。 Client —与符合Target接口的对象协同。 Adaptee —定义一个已经存在的接口，这个接口需要适配。 Adapter —对Adaptee的接口与Target接口进行适配 ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:1:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"2. 类适配器模式 原理： 通过继承来实现适配器功能。 当我们要访问的类A中没有我们想要的方法 ，却在另一个接口B中发现了合适的方法，但我们又不能改变类A。 在这种情况下，我们可以定义一个适配器p来进行中转，这个适配器p要继承我们访问类A，这样我们就能继续访问当前类A中的方法（虽然它目前不是我们的菜），然后再实现接口B，这样我们可以在适配器P中访问接口B的方法了。 /** * 目标类 * * @author illusoryCloud */ public interface Target { void Target(); } /** * 被适配类 * 只有Adaptee方法 但是目标接口要Target方法 * * @author illusoryCloud */ public class Adaptee { public void Adaptee() { System.out.println(\"这是现有的方法\"); } } /** * 适配器类 * 继承Adaptee类 使得此类保留了Adaptee方法 * 实现Target接口 使得此类同时也拥有Target方法 * 适配完成 * * @author illusoryCloud */ public class Adapter extends Adaptee implements Target { @Override public void Target() { System.out.println(\"这是目标方法\"); } } /** * 类适配器模式 测试类 * * @author illusoryCloud */ public class ClassAdapterTest { @Test public void classAdapterTest() { //Target类型的对象 同时拥有Target()和Adaptee()方法 Target target = new Adapter(); //这是目标方法 target.Target(); //这是现有的方法 ((Adapter) target).Adaptee(); } } ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:2:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"3. 对象适配器模式 原理 基本思路和类的适配器模式相同，只是将 Adapter 类作修改，这次不继承 Adaptee 类，而是持有 Adaptee 类的实例，以达到解决兼容性的问题。 //-------Target和Adaptee与上面一样---------- /** * 适配器类 持有Adaptee对象来代替继承Adaptee类 * 传入Adaptee对象 使得此类同样拥有Adaptee方法 * 实现Target接口 使得此类同时也拥有Target方法 * 适配完成 * * @author illusoryCloud */ public class Adapter implements Target { private Adaptee adaptee; public Adapter(Adaptee adaptee) { this.adaptee = adaptee; } @Override public void Target() { System.out.println(\"这是目标方法\"); } public void Adaptee() { adaptee.Adaptee(); } } /** * 对象适配器模式 测试类 * * @author illusoryCloud */ public class ObjectAdapterTest { @Test public void objectAdapterTest() { Target target = new Adapter(new Adaptee()); //这是目标方法 target.Target(); //这是现有的方法 ((Adapter) target).Adaptee(); } } ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:3:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"4. 总结 适配器模式优点 1、有更好的复用性。系统需要使用现有的类，但此类接口不符合系统需要，通过适配器模式让这些功能得到很好的复用 2、有更好的扩展性。实现适配器，可以调用自己开发的功能 缺点 过多使用适配器会使得系统非常凌乱，明明调用的是A接口，内部却被适配成了B接口。因此除非必要，不推荐使用适配器，而是作为一种补救措施，条件允许的情况下推荐直接对系统重构。 适配器模式在JDK中的应用 InputStreamReader/OutputStreanWriter 创建InputStreamReader对象的时候必须在构造函数中传入一个InputStream实例，然后InputStreamReader的作用就是将InputStream适配到Reader。很显然，适配器就是InputStreamReader，源角色就是InputStream代表的实例对象，目标接口就是Reader类。 ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:4:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"5. 参考 https://www.cnblogs.com/xrq730/p/4906487.html ","date":"2018-11-28","objectID":"/posts/java/design-pattern/04-adapter/:5:0","tags":["设计模式"],"title":"Java常用设计模式(四)---适配器模式","uri":"/posts/java/design-pattern/04-adapter/"},{"categories":["Java"],"content":"单例模式枚举式写法的序列化与反序列化安全问题分析","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"本文主要记录了单例模式中的枚举式写法的序列化与反序列化安全问题，通过分析源码说明了为什么枚举式单例是序列化安全的。 ","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/:0:0","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"1. 问题 Java中单例模式大概有五种：饿汉式、静态内部类、懒汉式、双重校验锁、枚举式。 静态内部类和双重校验锁已经这么优秀了为什么还要有第五种枚举式呢？ 因为前面4种都存在一个序列化和反序列化时的安全问题。将单例对象序列化后，在反序列化时会重新创建一个单例对象，违背了单例模式的初衷。而枚举式单例则没有这个问题。更多Java单例模式信息请阅读:Java设计模式(一)—单例模式。 ","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/:1:0","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"2. 分析 序列化可能会破坏单例模式，比较每次反序列化一个序列化的对象实例时都会创建一个新的实例,枚举类单例可以解决该问题。 枚举序列化是由jvm保证的，每一个枚举类型和定义的枚举变量在JVM中都是唯一的，在枚举类型的序列化和反序列化上，Java做了特殊的规定：在序列化时Java仅仅是将枚举对象的name属性输出到结果中，反序列化的时候则是通过java.lang.Enum的valueOf()方法来根据名字查找枚举对象。同时，编译器是不允许任何对这种序列化机制的定制的并禁用了writeObject、readObject、readObjectNoData、writeReplace和readResolve等方法，从而保证了枚举实例的唯一性. Enum类的valueOf方法: public static \u003cT extends Enum\u003cT\u003e\u003e T valueOf(Class\u003cT\u003e enumType, String name) { T result = enumType.enumConstantDirectory().get(name); if (result != null) return result; if (name == null) throw new NullPointerException(\"Name is null\"); throw new IllegalArgumentException( \"No enum constant \" + enumType.getCanonicalName() + \".\" + name); } 实际上通过调用enumType(Class对象的引用)的enumConstantDirectory()方法获取到的是一个Map集合，在该集合中存放了以枚举name为key和以枚举实例变量为value的Key\u0026Value数据，因此通过name的值就可以获取到枚举实例. enumConstantDirectory()方法： Map\u003cString, T\u003e enumConstantDirectory() { if (enumConstantDirectory == null) { //getEnumConstantsShared最终通过反射调用枚举类的values方法 T[] universe = getEnumConstantsShared(); if (universe == null) throw new IllegalArgumentException( getName() + \" is not an enum type\"); Map\u003cString, T\u003e m = new HashMap\u003c\u003e(2 * universe.length); //map存放了当前enum类的所有枚举实例变量，以name为key值 for (T constant : universe) m.put(((Enum\u003c?\u003e)constant).name(), constant); enumConstantDirectory = m; } return enumConstantDirectory; } private volatile transient Map\u003cString, T\u003e enumConstantDirectory = null; 到这里我们也就可以看出枚举序列化确实不会重新创建新实例，jvm保证了每个枚举实例变量的唯一性。 **通过反射获取构造器并创建枚举 **: public static void main(String[] args) throws IllegalAccessException, InvocationTargetException, InstantiationException, NoSuchMethodException { Constructor\u003cSingletonEnum\u003e constructor=SingletonEnum.class.getDeclaredConstructor(String.class,int.class); constructor.setAccessible(true); //创建枚举 SingletonEnum singleton=constructor.newInstance(\"otherInstance\",9); } 执行报错 Exception in thread \"main\" java.lang.IllegalArgumentException: Cannot reflectively create enum objects at java.lang.reflect.Constructor.newInstance(Constructor.java:417) at zejian.SingletonEnum.main(SingletonEnum.java:38) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 显然不能使用反射创建枚举类，这是为什么呢？在newInstance()方法中找找原因。 newInstance()方法： public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException { if (!override) { if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) { Class\u003c?\u003e caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); } } //这里判断Modifier.ENUM是不是枚举修饰符，如果是就抛异常 if ((clazz.getModifiers() \u0026 Modifier.ENUM) != 0) throw new IllegalArgumentException(\"Cannot reflectively create enum objects\"); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) { ca = acquireConstructorAccessor(); } @SuppressWarnings(\"unchecked\") T inst = (T) ca.newInstance(initargs); return inst; } 源码显示确实无法使用反射创建枚举实例，也就是说明了创建枚举实例只有编译器能够做到而已。 ","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/:2:0","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"3. 结论 显然枚举单例模式确实是很不错的选择，因此我们推荐使用它。 不过由于使用枚举时占用的内存常常是静态变量的两倍还多，因此android官方在内存优化方面给出的建议是尽量避免在android中使用enum。 但是不管如何，关于单例，我们总是应该记住：线程安全，延迟加载，序列化与反序列化安全，反射安全是很重重要的。 ","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/:3:0","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"4. 参考 https://blog.csdn.net/javazejian/article/details/71333103#%E6%9E%9A%E4%B8%BE%E4%B8%8E%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F ","date":"2018-11-24","objectID":"/posts/java/design-pattern/01ex1-java-enum-singleton/:4:0","tags":["设计模式"],"title":"枚举式单例模式与序列化","uri":"/posts/java/design-pattern/01ex1-java-enum-singleton/"},{"categories":["Java"],"content":"建造者模式优缺点及和工厂模式的差别","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"本文主要介绍了Java23种设计模式中的建造者模式，并结合实例描述了建造者模式的具体实现和优缺点及建造者模式和工厂模式的差别分析等。 ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:0:0","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"1. 简介 建造者模式是将一个复杂的对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。。 建造者模式通常包括下面几个角色： （1） Builder：给出一个抽象接口，以规范产品对象的各个组成成分的建造。这个接口规定要实现复杂对象的哪些部分的创建，并不涉及具体的对象部件的创建。 （2） ConcreteBuilder：实现Builder接口，针对不同的商业逻辑，具体化复杂对象的各部分的创建。 在建造过程完成后，提供产品的实例。 （3）Director：调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。 （4）Product：要创建的复杂对象 ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:1:0","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"2. 实现 ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:2:0","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"2.1 常见写法 以创建一个Person为例： Product（要创建的对象）： package builder.first; import java.util.Date; /** * 构建的消息对象 * 普通对象 * * @author illusoryCloud */ public class Message { /** * 标题 */ private String title; /** * 内容 */ private String content; /** * 发送者 */ private String from; /** * 接收者 */ private String to; /** * 时间 */ private Date time; public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } public String getContent() { return content; } public void setContent(String content) { this.content = content; } public String getFrom() { return from; } public void setFrom(String from) { this.from = from; } public String getTo() { return to; } public void setTo(String to) { this.to = to; } public Date getTime() { return time; } public void setTime(Date time) { this.time = time; } @Override public String toString() { return \"Message{\" + \"title='\" + title + '\\'' + \", content='\" + content + '\\'' + \", from='\" + from + '\\'' + \", to='\" + to + '\\'' + \", time=\" + time + '}'; } } Builder（给出一个抽象接口，以规范产品对象的各个组成成分的建造 ） /** * Builder接口 建造对象的标准 */ public interface Builder { void setTitle(); void setContent(); void setFrom(); void setTo(); void setTime(); Message build(); } ConcreteBuilder（实现Builder接口，针对不同的商业逻辑，具体化复杂对象的各部分的创建） /** * 具体的建造对象类 实现了Builder接口 * 可以根据不同需求有不同的实现 * * @author illusoryCloud */ public class CommonMessageBuilder implements Builder { private Message message; public CommonMessageBuilder() { this.message = new Message(); } @Override public void setTitle() { message.setTitle(\"常见的标题\"); } @Override public void setContent() { message.setContent(\"普通的内容\"); } @Override public void setFrom() { message.setFrom(\"未知的发送者\"); } @Override public void setTo() { message.setTo(\"未知的接收者\"); } @Override public void setTime() { message.setTime(new Date()); } @Override public Message build() { return this.message; } } /** * 具体的建造对象类 实现了Builder接口 * 可以根据不同需求有不同的实现 * * @author illusoryCloud */ public class OthersMessageBuilder implements Builder { private Message message; public OthersMessageBuilder() { this.message = new Message(); } @Override public void setTitle() { message.setTitle(\"不寻常的标题\"); } @Override public void setContent() { message.setContent(\"奇怪的内容\"); } @Override public void setFrom() { message.setFrom(\"神秘的发送者\"); } @Override public void setTo() { message.setTo(\"诡异的接收者\"); } @Override public void setTime() { message.setTime(new Date()); } @Override public Message build() { return this.message; } } Director（调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建 ） /** 指导者 *只负责保证对象各部分完整创建 * @author illusoryCloud */ public class Dreator { /** * * @param builder 参数是只要实现了Builder接口的类都可以 * @return */ public Message createMessage(Builder builder) { builder.setTitle(); builder.setContent(); builder.setFrom(); builder.setTo(); builder.setTime(); return builder.build(); } } 测试 public class Test { @org.junit.jupiter.api.Test public void testBuilder() { Message commonMessage = new Dreator().createMessage(new CommonMessageBuilder()); Message othersMessage = new Dreator().createMessage(new OthersMessageBuilder()); System.out.println(commonMessage); System.out.println(othersMessage); } } ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:2:1","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"2.2 静态内部类方式 静态内部类写法。 /** * 构建的消息对象 * 有个静态内部类 * * @author illusoryCloud */ public class Message { /** * 标题 */ private String title; /** * 内容 */ private String content; /** * 发送者 */ private String from; /** * 接收者 */ private String to; /** * 时间 */ private Date time; @Override public String toString() { return \"Message{\" + \"title='\" + title + '\\'' + \", content='\" + content + '\\'' + \", from='\" + from + '\\'' + \", to='\" + to + '\\'' + \", time=\" + time + '}'; } public static Builder newBuilder() { return new Builder(); } /** * 静态内部类 builder */ public static class Builder { /** * 设置默认值 */ private String title = \"未命名\"; private String content = \"暂无内容\"; private String from = \"unknow\"; private String to = \"unknow\"; private Date time = new Date(); /** * 设置消息标题 * * @param title 要设置的标题 * @return 返回Builder对象 以达到链式调用 */ public Builder setTitle(String title) { this.title = title; return this; } public Builder setContent(String content) { this.content = content; return this; } public Builder setFrom(String from) { this.from = from; return this; } public Builder setTo(String to) { this.to = to; return this; } public Builder setTime(Date time) { this.time = time; return this; } public Message build() { Message message = new Message(); message.title = title; message.content = content; message.from = from; message.to = to; message.time = time; return message; } } } 测试类 /** * 建造者模式 测试类 * * @author illusoryCloud */ public class Test { @org.junit.jupiter.api.Test public void testBuilder() { Message build = Message.newBuilder().setTitle(\"这是消息标题\") .setContent(\"这是消息内容\") .setFrom(\"这是消息发送者\") .setTo(\"这是消息接收者\") .setTime(new Date()) .Build(); System.out.println(build.toString()); } } ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:2:2","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"3. 总结 建造者模式优点： 1.将对象本身与对象的创建过程解耦，使得相同的创建过程可以创建不同的对象。 2.可以更加精细地控制产品的创建过程 3.增加新的具体建造者无须修改原有类库的代码，符合开闭原则 与工厂模式的区别： 工厂模式注重的是整体对象的创建方法，只为了获取对象，关注整体 建造者模式注重的是部件构建的过程，旨在通过一步一步地精确构造创建出一个复杂的对象，关注细节。建造者模式一般用来创建更为复杂的对象 ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:3:0","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Java"],"content":"4. 参考 https://blog.csdn.net/zhuhuitao_struggle/article/details/80489572 ","date":"2018-11-22","objectID":"/posts/java/design-pattern/03--builder/:4:0","tags":["设计模式"],"title":"Java常用设计模式(三)---建造者模式","uri":"/posts/java/design-pattern/03--builder/"},{"categories":["Android"],"content":"Android事件分发机制源码分析与实例测试","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"本文主要记录了Android中的事件分发机制。通过对源码进行分析和实例测试，对Android事件分发机制有了更深的了解。主要为学习Android时的笔记。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:0:0","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"1. 触发过程 ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:1:0","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"1.1 点击控件 ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:1:1","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"1.2 dispatchTouchEven 一定会执行dispatchTouchEvent方法，若当前类没有该方法，则向上往父类查找。 public boolean dispatchTouchEvent(MotionEvent event) { if (mOnTouchListener != null \u0026\u0026 (mViewFlags \u0026 ENABLED_MASK) == ENABLED \u0026\u0026 mOnTouchListener.onTouch(this, event)) { return true; } return onTouchEvent(event); } 条件1 mOnTouchListener != null public void setOnTouchListener(OnTouchListener l) { mOnTouchListener = l; } 给控件设置监听就会给mOnTouchListener赋值，则条件1成立。 条件2 (mViewFlags \u0026 ENABLED_MASK) == ENABLED 控件是否是可点击的 条件3 mOnTouchListener.onTouch(this, event) 回调onTouch方法，返回true 则成立 小结：dispatchTouchEvent 方法中一定会执行onTouch方法，如果onTouch方法返回true 则dispatchTouchEvent方法直接返回true 不会执行if外的 return onTouchEvent(event)。 title_bar.setOnTouchListener(new View.OnTouchListener() { @Override public boolean onTouch(View v, MotionEvent event) { Log.v(\"Az\",\"onTouch\"); return false; } }); 在setOnTouchListener时，onTouch方法默认返回false,所以才会执行后面的onTouchEvent方法； ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:1:2","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"1.3 onTouchEvent public boolean onTouchEvent(MotionEvent event) { final int viewFlags = mViewFlags; if ((viewFlags \u0026 ENABLED_MASK) == DISABLED) { // A disabled view that is clickable still consumes the touch // events, it just doesn't respond to them. return (((viewFlags \u0026 CLICKABLE) == CLICKABLE || (viewFlags \u0026 LONG_CLICKABLE) == LONG_CLICKABLE)); } if (mTouchDelegate != null) { if (mTouchDelegate.onTouchEvent(event)) { return true; } } if (((viewFlags \u0026 CLICKABLE) == CLICKABLE || (viewFlags \u0026 LONG_CLICKABLE) == LONG_CLICKABLE)) { switch (event.getAction()) { case MotionEvent.ACTION_UP: boolean prepressed = (mPrivateFlags \u0026 PREPRESSED) != 0; if ((mPrivateFlags \u0026 PRESSED) != 0 || prepressed) { // take focus if we don't have it already and we should in // touch mode. boolean focusTaken = false; if (isFocusable() \u0026\u0026 isFocusableInTouchMode() \u0026\u0026 !isFocused()) { focusTaken = requestFocus(); } if (!mHasPerformedLongPress) { // This is a tap, so remove the longpress check removeLongPressCallback(); // Only perform take click actions if we were in the pressed state if (!focusTaken) { // Use a Runnable and post this rather than calling // performClick directly. This lets other visual state // of the view update before click actions start. if (mPerformClick == null) { mPerformClick = new PerformClick(); } if (!post(mPerformClick)) { performClick(); } } } if (mUnsetPressedState == null) { mUnsetPressedState = new UnsetPressedState(); } if (prepressed) { mPrivateFlags |= PRESSED; refreshDrawableState(); postDelayed(mUnsetPressedState, ViewConfiguration.getPressedStateDuration()); } else if (!post(mUnsetPressedState)) { // If the post failed, unpress right now mUnsetPressedState.run(); } removeTapCallback(); } break; case MotionEvent.ACTION_DOWN: if (mPendingCheckForTap == null) { mPendingCheckForTap = new CheckForTap(); } mPrivateFlags |= PREPRESSED; mHasPerformedLongPress = false; postDelayed(mPendingCheckForTap, ViewConfiguration.getTapTimeout()); break; case MotionEvent.ACTION_CANCEL: mPrivateFlags \u0026= ~PRESSED; refreshDrawableState(); removeTapCallback(); break; case MotionEvent.ACTION_MOVE: final int x = (int) event.getX(); final int y = (int) event.getY(); // Be lenient about moving outside of buttons int slop = mTouchSlop; if ((x \u003c 0 - slop) || (x \u003e= getWidth() + slop) || (y \u003c 0 - slop) || (y \u003e= getHeight() + slop)) { // Outside button removeTapCallback(); if ((mPrivateFlags \u0026 PRESSED) != 0) { // Remove any future long press/tap checks removeLongPressCallback(); // Need to switch from pressed to not pressed mPrivateFlags \u0026= ~PRESSED; refreshDrawableState(); } } break; } return true; } return false; } 首先在第14行我们可以看出，如果该控件是可以点击的就会进入到第16行的switch判断中去，而如果当前的事件是抬起手指，则会进入到MotionEvent.ACTION_UP这个case当中。在经过种种判断之后，会执行到第38行的performClick()方法，那我们进入到这个方法里瞧一瞧： 若当前事件为抬手，则进入performClick方法 public boolean performClick() { sendAccessibilityEvent(AccessibilityEvent.TYPE_VIEW_CLICKED); if (mOnClickListener != null) { playSoundEffect(SoundEffectConstants.CLICK); mOnClickListener.onClick(this); return true; } return false; } 如果 mOnClickListener != null 则会执行onClick方法， public void setOnClickListener(OnClickListener l) { if (!isClickable()) { setClickable(true); } mOnClickListener = l; } 所以只要给控件设置了点击监听，setOnClickListener就会给mOnClickListener赋值，上面条件就成立，然后回调onClick方法。 到这儿差不多就清楚了分发流程。 这样View的整个事件分发的流程就让我们搞清楚了！不过别高兴的太早，现在还没结束，还有一个很重要的知识点需要说明，就是touch事件的层级传递。我们都知道如果给一个控件注册了touch事件，每次点击它的时候都会触发一系列的ACTION_DOWN，ACTION_MOVE，ACTION_UP等事件。这里需要注意，如果你在执行ACTION_DOWN的时候返回了false，后面一系列其它的action就不会再得到执行了。简单的说，就是当dispatchTouchEvent在进行事件分发的时候，只有前一个action返回true，才会触发后一个action。 public boolean onTouchEvent(MotionEvent event) { //省略... if (((viewFlags \u0026 CLICKABLE) == CLICKABLE || (viewFlags \u0026 LONG_CLICKABLE) == LONG_CLICKABLE)) { switch (event.getAction()) { case MotionEvent.ACTION_UP: break; case MotionEvent.ACTION_DOWN: break; case MotionEvent.ACTION_CANCEL: break; case MotionEvent.ACTION_MOVE: break; } return true; } } 可以看出在dispatchTouchEvent方法中，onTouch方法返回false,然后执行onTouchEvent方法，在进入","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:1:3","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"2. ViewGroup Android中touch事件的传递，绝对是先传递到ViewGroup，再传递到View的 上边说只要你触摸了任何控件，就一定会调用该控件的dispatchTouchEvent方法。这个说法没错，只不过还不完整而已。实际情况是，当你点击了某个控件，首先会去调用该控件所在布局的dispatchTouchEvent方法，然后在布局的dispatchTouchEvent方法中找到被点击的相应控件，再去调用该控件的dispatchTouchEvent方法。 ViewGroup的dispatchTouchEvent方法 public boolean dispatchTouchEvent(MotionEvent ev) { final int action = ev.getAction(); final float xf = ev.getX(); final float yf = ev.getY(); final float scrolledXFloat = xf + mScrollX; final float scrolledYFloat = yf + mScrollY; final Rect frame = mTempRect; boolean disallowIntercept = (mGroupFlags \u0026 FLAG_DISALLOW_INTERCEPT) != 0; if (action == MotionEvent.ACTION_DOWN) { if (mMotionTarget != null) { mMotionTarget = null; } if (disallowIntercept || !onInterceptTouchEvent(ev)) { ev.setAction(MotionEvent.ACTION_DOWN); final int scrolledXInt = (int) scrolledXFloat; final int scrolledYInt = (int) scrolledYFloat; final View[] children = mChildren; final int count = mChildrenCount; for (int i = count - 1; i \u003e= 0; i--) { final View child = children[i]; if ((child.mViewFlags \u0026 VISIBILITY_MASK) == VISIBLE || child.getAnimation() != null) { child.getHitRect(frame); if (frame.contains(scrolledXInt, scrolledYInt)) { final float xc = scrolledXFloat - child.mLeft; final float yc = scrolledYFloat - child.mTop; ev.setLocation(xc, yc); child.mPrivateFlags \u0026= ~CANCEL_NEXT_UP_EVENT; if (child.dispatchTouchEvent(ev)) { mMotionTarget = child; return true; } } } } } } boolean isUpOrCancel = (action == MotionEvent.ACTION_UP) || (action == MotionEvent.ACTION_CANCEL); if (isUpOrCancel) { mGroupFlags \u0026= ~FLAG_DISALLOW_INTERCEPT; } final View target = mMotionTarget; if (target == null) { ev.setLocation(xf, yf); if ((mPrivateFlags \u0026 CANCEL_NEXT_UP_EVENT) != 0) { ev.setAction(MotionEvent.ACTION_CANCEL); mPrivateFlags \u0026= ~CANCEL_NEXT_UP_EVENT; } return super.dispatchTouchEvent(ev); } if (!disallowIntercept \u0026\u0026 onInterceptTouchEvent(ev)) { final float xc = scrolledXFloat - (float) target.mLeft; final float yc = scrolledYFloat - (float) target.mTop; mPrivateFlags \u0026= ~CANCEL_NEXT_UP_EVENT; ev.setAction(MotionEvent.ACTION_CANCEL); ev.setLocation(xc, yc); if (!target.dispatchTouchEvent(ev)) { } mMotionTarget = null; return true; } if (isUpOrCancel) { mMotionTarget = null; } final float xc = scrolledXFloat - (float) target.mLeft; final float yc = scrolledYFloat - (float) target.mTop; ev.setLocation(xc, yc); if ((target.mPrivateFlags \u0026 CANCEL_NEXT_UP_EVENT) != 0) { ev.setAction(MotionEvent.ACTION_CANCEL); target.mPrivateFlags \u0026= ~CANCEL_NEXT_UP_EVENT; mMotionTarget = null; } return target.dispatchTouchEvent(ev); } 第二个if语句 if (disallowIntercept || !onInterceptTouchEvent(ev) 第一个条件disallowIntercept是否禁用掉事件拦截的功能，默认是false。所以是否进入if内部就由第二个条件决定了。 ViewGroup中有一个onInterceptTouchEvent方法 是否拦截触摸事件 默认返回false 即不拦截 public boolean onInterceptTouchEvent(MotionEvent ev) { return false; } 第二个条件 !onInterceptTouchEvent(ev)对返回值取反 即返回false不拦截触摸事件时进入if内部，返回true拦截时不进入if内部 //省略。。。 if (disallowIntercept || !onInterceptTouchEvent(ev)) { for (int i = count - 1; i \u003e= 0; i--) {//遍历当前ViewGroup下的所有子View if ((child.mViewFlags \u0026 VISIBILITY_MASK) == VISIBLE || child.getAnimation() != null) { if (frame.contains(scrolledXInt, scrolledYInt)) {//判断当前遍历的View是不是正在点击的View if (child.dispatchTouchEvent(ev)) {//是则调用子View的dispatchTouchEvent mMotionTarget = child; return true; } } } } } if内部对子View进行了遍历，最终调用子View的dispatchTouchEvent，然后控件可点击那么dispatchTouchEvent一定会返回true，所以后面的代码就执行不了。 即 ViewGroup 的onInterceptTouchEvent返回false,不拦截触摸事件时，最终会执行子View的dispatchTouchEvent。 ViewGroup的onInterceptTouchEvent返回true,拦截触摸事件，就不会进入if内部，则会执行到后面的程序 if (target == null) { ev.setLocation(xf, yf); if ((mPrivateFlags \u0026 CANCEL_NEXT_UP_EVENT) != 0) { ev.setAction(MotionEvent.ACTION_CANCEL); mPrivateFlags \u0026= ~CANCEL_NEXT_UP_EVENT; } return super.dispatchTouchEvent(ev); } 可以看到，最后会执行super.dispatchTouchEvent(ev)，执行父类即View的dispatchTouchEvent。 View的dispatchTouchEvent如下： public boolean dispatchTouchEvent(MotionEvent event) { if (mOnTouchListener != null \u0026\u0026 (mViewFlags \u0026 ENABLED_MASK)","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:2:0","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"3. 总结 ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:3:0","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"传递顺序 Activity －\u003e PhoneWindow －\u003e DecorView －\u003e ViewGroup －\u003e … －\u003e View 通俗语言总结一下，事件来的时候， Activity会询问Window，Window这个事件你能不能消耗， Window一看，你先等等，我去问问DecorView他能不能消耗， DecorView一看，onInterceptTouchEvent返回false啊，不让我拦截啊， (DecorView继承自FrameLayout,FrameLayout是ViewGroup的子类，所以DecorView也是ViewGroup的子类，事件从Activity传到了ViewGroup) 遍历一下子View吧，问问他们能不能消耗，那个谁，事件按在你的身上了，你看看你能不能消耗， 假如子View为RelativeLayout RelativeLayout一看，也没有让我拦截啊，我也得遍历看看这个事件发生在那个子View上面， 到这儿事件从ViewGroup传到View上了 那个TextView,事件在你身上，你能不能消耗了他。TextView一看，消耗不了啊， RelativeLayout一看TextView消耗不了啊，mFirstTouchTarget==null啊，得，我自己消耗吧，嗯！一看自己的onTouchEvent也消耗不了啊！那个DecorView事件我消耗不了， DecorView一看自己，我也消耗不了，继续往上传，那个Window啊。事件我消耗不了啊， Window再告诉Activity事件消耗不了啊。 Activity还得我自己来啊。调用自己的onTouchEvent，还是消耗不了，算了，不要了。 最后Activity的onTouchEvent无论返回什么，事件分发都结束。（如果事件在边界范围外默认会返回false） ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:3:1","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Android"],"content":"参考 https://blog.csdn.net/guolin_blog/article/details/9097463 https://blog.csdn.net/guolin_blog/article/details/9153747 ","date":"2018-11-20","objectID":"/posts/android/02-view-click-event/:4:0","tags":["Android"],"title":"Android事件分发机制详解","uri":"/posts/android/02-view-click-event/"},{"categories":["Java"],"content":"工厂模式的实现与使用场景分析","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"本章主要介绍了设计模式中的工厂模式，并结合实例描述了工厂模式的具体实现和使用场景。包括：普通工厂模式、工厂方法模式、抽象工厂模式等。 ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:0:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"1. 简介 工厂模式可以分为普通工厂模、工厂方法模式和抽象工厂模式。 简单工厂模式：建立一个工厂类，根据传入的参数对实现了同一接口的一些类进行实例的创建。如果传入的字符串错误就不能正确创建对象。 工厂方法模式：是对普通工厂方法模式的改进，提供多个工厂方法，分别创建对象。 抽象工厂模式：创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 工厂模式优点： (1) 解耦 ：把对象的创建和使用的过程分开 (2)减少重复代码: 若创建对象的过程很复杂，有一定的代码量，且很多地方都要用到，那么就会有很多重复代码。 (3) 降低维护成本 ：创建过程都由工厂统一管理，发生业务逻辑变化，只需要在工厂里修改即可。 适用场景 （1）需要创建的对象较少。 （2）客户端不关心对象的创建过程。 ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:1:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"2. 简单工厂模式 /** * 抽象产品类 水果 */ public interface Fruit { void show(); } /** * 具体产品类 * 苹果 实现了水果接口 * @author illusoryCloud */ public class Apple implements Fruit { @Override public void show() { System.out.println(\"This is Apple\"); } } /** * 具体产品类 * 橘子 实现了水果接口 * @author illusoryCloud */ public class Orange implements Fruit { @Override public void show() { System.out.println(\"This is Orange\"); } } /** * 工厂类 水果工厂 * 负责生产各种产品 * * @author illusoryCloud */ public class FruitFactory { public static final String FRUIT_APPLE = \"Apple\"; public static final String FRUIT_ORANGE = \"Orange\"; public static Fruit creatFruit(String fruit) { if (FRUIT_APPLE.equals(fruit)) { return new Apple(); } else if (FRUIT_ORANGE.equals(fruit)) { return new Orange(); } else { System.out.println(\"error unknown fruit ~\"); return null; } } } /** * 简单工厂模式 测试 * * @author illusoryCloud * */ public class EasyFactoryTest { @Test public void testEasyFactory() { Fruit apple = FruitFactory.creatFruit(FruitFactory.FRUIT_APPLE); if (apple != null) { apple.show(); } Fruit orange = FruitFactory.creatFruit(FruitFactory.FRUIT_ORANGE); if (orange != null) { orange.show(); } } } ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:2:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"3. 工厂方法模式 简单工厂模式中，如果创建对象时传入的字符串出现错误则不能正确创建产品。工厂方法模式为每种产品创建一个工厂，则不会出现这样的问题。 /** * 抽象产品工厂类 * @author illusoryCloud */ public interface FruitFactory { Fruit create(); } /** * 具体产品工厂 实现接口 * 苹果工厂 * @author illusoryCloud */ public class AppleFactory implements FruitFactory { @Override public Fruit create() { return new Apple(); } } /** * 具体产品工厂 实现接口 * 苹果工厂 * @author illusoryCloud */ public class AppleFactory implements FruitFactory { @Override public Fruit create() { return new Apple(); } } /** * 工厂方法模式 测试类 * * @author illusoryCloud */ public class FactoryMethodTest { @Test public void factoryMethodTest() { Fruit apple = new AppleFactory().create(); apple.show(); Fruit orange = new OrangeFactory().create(); orange.show(); } } ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:3:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"4. 抽象工厂模式 网上找的一个类图： 工厂方法模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则，所以，从设计角度考虑，有一定的问题，如何解决？就用到抽象工厂模式，创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 /** * 抽象产品类 果汁 * * @author illusoryCloud */ public interface Juice { void show(); } /** * 具体产品类 * 苹果汁 * * @author illusoryCloud */ public class AppleJuice implements Juice { @Override public void show() { System.out.println(\"AppleJuice\"); } } /** * 具体产品类 * 橘子汁 * * @author illusoryCloud */ public class OrangeJuice implements Juice { @Override public void show() { System.out.println(\"OrangeJuice\"); } } /** * 抽象工厂类 * * @author illusoryCloud */ public interface AbstractFactory { /** * 创建水果 * * @return 水果 */ Fruit createFruit(); /** * 创建果汁 * * @return 果汁 */ Juice createJuice(); } /** * 具体工厂类 * 苹果工厂 生产苹果和苹果汁 * * @author illusoryCloud */ public class AppleFactory implements AbstractFactory { @Override public Fruit createFruit() { return new Apple(); } @Override public Juice createJuice() { return new AppleJuice(); } } /** * 具体工厂类 * 橘子工厂 生产橘子和橘子汁 * * @author illusoryCloud */ public class OrangeFactory implements AbstractFactory { @Override public Fruit createFruit() { return new Orange(); } @Override public Juice createJuice() { return new OrangeJuice(); } } /** * 抽象工厂模式 测试类 * * @author illusoryCloud */ public class AbstractFactoryTest { @Test public void abstractFactoryTest() { //苹果产品簇 AbstractFactory appleFactory = new AppleFactory(); Fruit apple = appleFactory.createFruit(); Juice appleJuice = appleFactory.createJuice(); //橘子产品簇 AbstractFactory orangeFactory = new OrangeFactory(); Fruit orange = orangeFactory.createFruit(); Juice orangeJuice = orangeFactory.createJuice(); apple.show(); appleJuice.show(); orange.show(); orangeJuice.show(); } } 优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 工厂里加代码，又要在具体的工厂里面加代码。要增加一个新的系列时比较简单。 例如上面例子中在苹果系列增加一个苹果派就很困难得修改苹果工厂和抽象工厂，但是若增加一个菠萝系列就很简单，只需要添加一个菠萝工厂就行了。 ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:4:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"5. 总结 工厂模式的优点？为什么要使用工厂模式 工厂都是用来封装对象的具体创建过程，减少重复代码，降低对象变化时的维护成本，将对象创建过程和使用相解耦。 工厂方法模式使用继承，抽象工厂使用对象组合；两者利用抽象的原则，将具体的实例化过程延迟到子类。 工厂利用的最重要和基本的原则——依赖抽象，不要依赖具体类。 应用场景 简单工厂：适合创建同一级别的不同对象。 工厂方法：为每种产品提供一个工厂类，通过不同的工厂实例来创建不同的产品。 抽象工厂模式：一个对象族（或是一组没有任何关系的对象）都有相同的约束，则可以使用抽象工厂模式。 工厂模式在Java中的应用 简单工厂模式 JDK中的简单工厂模式有很多应用，比较典型的比如线程池。我们使用线程池的时候，可以使用ThreadPoolExecutor，根据自己的喜好传入corePoolSize、maximumPoolSize、keepAliveTimem、unit、workQueue、threadFactory、handler这几个参数，new出一个指定的ThreadPoolExecutor出来。 工厂方法模式 public interface ThreadFactory { /** * Constructs a new {@code Thread}. Implementations may also initialize * priority, name, daemon status, {@code ThreadGroup}, etc. * * @param r a runnable to be executed by new thread instance * @return constructed thread, or {@code null} if the request to * create a thread is rejected */ Thread newThread(Runnable r); } 这是一个生产线程的接口,具体的线程工厂可以implements这个接口并实现newThread(Runnable r)方法，来生产具体线程工厂想要生产的线程。 ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:5:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"6. 参考 https://blog.csdn.net/d1562901685/article/details/77623237 https://www.cnblogs.com/xrq730/p/4905578.html ","date":"2018-11-18","objectID":"/posts/java/design-pattern/02--factory/:6:0","tags":["设计模式"],"title":"Java设计模式(二)---工厂模式","uri":"/posts/java/design-pattern/02--factory/"},{"categories":["Java"],"content":"各种单例模式的实现与分析","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"本文主要介绍了设计模式的六大原则，并结合实例描述了各种单例模式的具体实现和性能分析测试。包括：饿汉式、静态内部类、懒汉式、双重校验锁、枚举等。 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:0:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"1. 设计模式的六大原则 1、开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 2、里氏代换原则（Liskov Substitution Principle） 其官方描述比较抽象，可自行百度。实际上可以这样理解： （1）子类的能力必须大于等于父类，即父类可以使用的方法，子类都可以使用。 （2）返回值也是同样的道理。假设一个父类方法返回一个List，子类返回一个ArrayList，这当然可以。如果父类方法返回一个ArrayList，子类返回一个List，就说不通了。这里子类返回值的能力是比父类小的。 （3）还有抛出异常的情况。任何子类方法可以声明抛出父类方法声明异常的子类。而不能声明抛出父类没有声明的异常。 3、依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。 4、接口隔离原则（Interface Segregation Principle） 这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 5、迪米特法则（最少知道原则）（Demeter Principle） 为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 6、合成复用原则（Composite Reuse Principle） 原则是尽量使用合成/聚合的方式，而不是使用继承。 Java 中一般认为有 23 种设计模式，总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录 模式、状态模式、访问者模式、中介者模式、解释器模式。 比较常用的有：工厂方法模式、抽象工厂模式、单例模式、建造者模式、适配器模式、代理模式、享元模式、策略模式、观察者模式。 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:1:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"2. 单例模式 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:2:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"2.1 单例模式介绍 作用：保证一个类仅有一个实例，并提供一个访问它的全局访问点。 主要解决：一个全局使用的类频繁地创建与销毁。 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例。 2、避免对资源的多重占用（比如写文件操作）。 缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 应用场景： 1.配置文件访问类，不用每次使用时都new一个 2.数据库连接池 保证项目中只有一个连接池存在。 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:2:1","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"2.2 单例模式实现 1. 饿汉式 /** * 饿汉式 * @author illusoryCloud */ public class FirstSingleton { /** * 类变量在类准备阶段就初始化了然后放在\u003cclinit\u003e构造方法中 * 一旦外部调用了静态方法，那么就会初始化完成。 * 一个类的\u003cclinit\u003e只会执行一次 保证多线程情况下不会创建多个实例 */ private static final FirstSingleton INSTANCE =new FirstSingleton(); /** * * 构造函数私有化 */ private FirstSingleton(){} /** * 提供公共方法以获取实例对象 * @return instance 实例对象 */ public static FirstSingleton getInstance(){ return INSTANCE ; } } 这种方式实现的单例：类加载时就创建实例。由classloder保证了线程安全。 2. 静态内部类 /** * 静态内部类方式 * * @author illusoryCloud */ public class SecondSingleton { private static class SingletonHolder { /** * 静态变量类加载时才会被创建 且只会创建一次 */ private static final SecondSingleton INSTANCE = new SecondSingleton(); } private SecondSingleton() { } public static SecondSingleton getInstance() { return SingletonHolder.INSTANCE; } } 这种方式实现的单例：实现了lazy loading 使用时才创建实例，由classloder保证了线程安全。 饿汉式/静态内部类是如何保证线程安全的： 在《深入理解JAVA虚拟机》中，有这么一句话: 虚拟机会保证一个类的\u003cclinit\u003e()方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的\u003cclinit\u003e()方法，其他线程都需要阻塞等待，直到活动线程执行\u003cclinit\u003e()方法完毕。 3. 懒汉式 /** * 懒汉式 * * @author illusoryCloud */ public class ThirdSingleton { private static ThirdSingleton instance; private ThirdSingleton() { } /** * synchronized 保证线程安全 但效率低 * * @return instance单例对象 */ public static synchronized ThirdSingleton getInstance() { if (instance == null) { instance = new ThirdSingleton(); } return instance; } } 这种方式实现的单例：实现了lazy loading 使用时才创建实例。synchronized保证了线程安全，但效率低。 4. 双重校验锁 /** * 双重校验锁式 * * @author illusoryCloud */ public class FourSingleton { /** * volatile关键字禁止指令重排序 * 保证多线程下不会获取到未完全初始化的实例 * 详细请阅读：https://www.lixueduan.com/posts/e7cef119.html */ private static volatile FourSingleton instance; private FourSingleton() { } /** * 双重if校验 缩小synchronized代码块范围 * 若instance不为空 就可直接return * * @return instance 实例对象 */ public static FourSingleton getInstance() { if (instance == null) { synchronized (FourSingleton.class) { if (instance == null) { //非原子操作 instance = new FourSingleton(); } } } return instance; } } 这种方式实现的单例：实现了lazy loading 使用时才创建实例。synchronized保证了线程安全，volatile禁止指令重排序保证了多线程获取时不为空，但要JDK1.5以上才行。详细信息请阅读volatile关键字在单例模式(双重校验锁)中的作用 5. 枚举 /** * 枚举式 * 序列化及反序列化安全 * @author illusoryCloud */ public enum FiveSingleton { //定义一个枚举的元素，它就是 singleton 的一个实例 INSTANCE; public void doSomeThing(FiveSingleton instance) { System.out.println(\"枚举方式实现单例\"); } } public class Test { public static void main(String[] args) { Singleton singleton = Singleton.INSTANCE; singleton.doSomeThing();//output:枚举方法实现单例 } } 这种方式也是《Effective Java 》以及《Java与模式》的作者推荐的方式。 静态内部类和双重校验锁已经这么优秀了为什么还要有第五种枚举式呢？ 因为前面4种都存在一个序列化和反序列化时的安全问题。将单例对象序列化后，在反序列化时会重新创建一个单例对象，违背了单例模式的初衷。而枚举式单例则没有这个问题，具体信息查看：枚举式单例模式与序列化 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:2:2","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"3. 性能测试 五种单例实现方式，在100个线程下，每个线程访问1千万次实例的用时. Tables 实现方式 用时(毫秒) 1 饿汉式 13 2 懒汉式 10778 3 双重检查 15 4 静态内部类 14 5 枚举 12 (*注意:由于不同电脑之间的性能差异，测试的结果可能不同) 根据不同场合选择具体的实现方式，一般情况下我是使用的静态内部类或者DCL双重校验锁方式。 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:3:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"4. 总结 为什么要使用单例模式？什么场景适合使用单例模式?单例模式有什么好处 1.单例模式能够保证一个类仅有唯一的实例，避免创建多个实例。并提供一个全局访问点，优化和共享资源访问。 2.当一个对象需要频繁创建和销毁时使用单例模式能节省系统资源。 应用场景： 1.配置文件访问类，不用每次使用时都new一个 2.数据库连接池 保证项目中只有一个连接池存在。 单例模式的缺点： 单例模式一般没有接口，扩展很困难，若要扩展只能修改代码。 单例模式在Java中的应用 public class Runtime { private static Runtime currentRuntime = new Runtime(); /** * Returns the runtime object associated with the current Java application. * Most of the methods of class \u003ccode\u003eRuntime\u003c/code\u003e are instance * methods and must be invoked with respect to the current runtime object. * * @return the \u003ccode\u003eRuntime\u003c/code\u003e object associated with the current * Java application. */ public static Runtime getRuntime() { return currentRuntime; } /** Don't let anyone else instantiate this class */ private Runtime() {} ... } ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:4:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Java"],"content":"5. 参考 http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html https://blog.csdn.net/qq_22706515/article/details/74202814 ","date":"2018-11-15","objectID":"/posts/java/design-pattern/01-singleton/:5:0","tags":["设计模式"],"title":"Java设计模式(一)---单例模式","uri":"/posts/java/design-pattern/01-singleton/"},{"categories":["Blog"],"content":"通过GitHub实现切换环境后实现快速恢复","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"本章主要记录了如何通过使用Github的分支功能解决更换电脑后博客更新不方便的问题，让你的博客能在各个电脑上灵活切换。在也不用担心换电脑后博客配置丢失等问题了。 到此为止，我们已经完成了差不多所有的步骤。 1.搭建博客 2.优化主题 3.部署收录 新问题： ​ 现在博客只能在自己的电脑上更新，如果换电脑了就很麻烦。配置文件主题什么的都要重新弄。所以网上找了找多台电脑同时操作的办法，我们可以利用Github的分支功能。 ​ 将博客文件夹下所有文件全push到Github。这样换电脑后直接pull就可以了。 ","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/:0:0","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"1. 新建分支 1.在Github的lillusory.github.io（hexo仓库）上新建一个分支，例如Hexo，并切换到该分支. 2.并在该仓库Settings-\u003eBranches-\u003eDefault branch中将默认分支设为Hexo.Hexo分支是博客的开发环境，用来写博客，保存原始文件,master分支用于显示，保存生产的静态文件。 3.新建分支后将博客目录下的所有文件上传到该分支，注意由于一个git仓库中不能包含其他仓库，所以需要删除掉主题文件夹中的.git目录。 4.如果按照前面的博文添加了背景，则需要删掉站点目录\\themes\\next\\source\\lib\\canvas-nest文件夹中的.git目录。以后需要更新主题时，可以先克隆到本地在复制到相应目录. ","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/:1:0","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"2. 写博客 在本地对博客进行修改（添加新博文、修改样式等等）后，通过下面的流程进行管理。 依次执行git add .、git commit -m \"这里写备注\"、git push origin 这里写分支名字指令将改动推送到GitHub（此时当前分支应为hexo）。 然后才执行hexo g -d发布网站到master分支上。 ","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/:2:0","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"3. 博客迁移 当重装电脑之后，或者想在其他电脑上修改博客，可以使用下列步骤： 克隆仓库 使用git clone git@github.com:illusorycloud/illusorycloud.github.io.git拷贝仓库（默认分支为hexo）；//修改成自己的 安装插件 在前面克隆下的项目中安装插件 执行命令npm install hexo、npm install、npm install hexo-deployer-git ","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/:3:0","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"4. 参考 如何在多台电脑上更新博客 ","date":"2018-11-10","objectID":"/posts/blog/hexo/04-transfer/:4:0","tags":["Hexo\""],"title":"基于Hexo搭建个人博客(四)---管理篇","uri":"/posts/blog/hexo/04-transfer/"},{"categories":["Blog"],"content":"通过GitHub Pages和域名设置 让博客被搜索引擎收录","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"本章主要记录了如何将博客部署至云端，怎么设置个性域名，怎么将自己的网站提交到百度Google。让自己的网站能够出现在各大搜索引擎的具体方法和过程，希望能对大家有帮助。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:0:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"1. 购买个性域名 估计大家折腾了这么久也就是为 了拥有一个自己的个性站点,所以强烈建议大家为自己的博客站点配置一个独一无二的个性域名.我这里选择阿里旗下的万网。我的域名是www.lixueduan.com 大家可以选择一个自己喜欢的域名。等部署完毕就可以通过域名访问自己的博客了。 问题： GithubPages/CodingPages Github Pages是Github免费提供给开发者的一款托管个人网站的产品。 Coding Pages也是Coding免费提供给开发者的一款托管个人网站的产品。 关于为什么要部署两次 虽然可以根据自定义域名来访问自己的博客了，但是百度谷歌上都搜索不到，那岂不是很难受╮(╯▽╰)╭。 所以接下来为了让自己的博客能够被搜索出来，就需要让百度谷歌收录我们的网站。在部署收录过程中发现，Github屏蔽了百度的爬虫，所以搭建上GithubPages的话无法提交至百度，只有Google可以收录。 所以为了让百度收录我们网站，就得在Coding上也搭建一个。 同时在搭建的过程中发现如果先搭建在Github上，然后再搭建Coding时会出现DNS解析冲突。所以需要：先搭建Coding上的，再搭建Github上的，国外的访问则走Github，国内的访问会走Coding，完美 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:1:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"2. 部署到CodingPages ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:2:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"2.1 注册coding账户 点击这里注册Coding](https://coding.net/) ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:2:1","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"2.2 创建新项目 注册好后创建一个项目用来部署个人博客，项目路径和项目名称最好和用户名一致 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:2:2","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"2.3 开启CodingPages 点击Pages服务，然后一键开启。 部署master分支 自定义域名 可以填两个 www.xxx.com 和xxx.com 绑定自定义域名的时候需要在买域名的地方(我这里是阿里的万网)配置DNS解析 添加两条CNAME解析 主机记录 一个@，一个www//@就是无前缀，xxx.com, www就是www.xxx.com 解析路线 默认就行 记录值 lillusory.coding.me //这里改成自己的 然后可以开启Https访问。 到这里就可以通过个性域名访问啦。不过现在博客代码还没有push到项目里。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:2:3","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"2.4 Push代码到Coding 配置SSH key 首先需要配置一个SSHkey，Git有Http协议和Git协议两种。我们这里使用Git协议就需要配置一个SSH key,等会部署到Github上也需要配置这个。 具体配置方法如下： Git 配置及SSH key 修改站点配置文件 这里只配置了Coding，可以先把Github的注释掉 # Deployment 部署到云端相关配置 ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repository: github: git@github.com:illusorycloud/illusorycloud.github.io.git coding: git@git.coding.net:illusorycloud/illusorycloud.git branch: master 地址在这里： 配置好后，运行hexo g时就可以把博客部署到Coding上了，也可以通过个性域名访问了。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:2:4","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"3. 收录到百度 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:3:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"3.1 网站添加 直接百度搜索你的域名,比如我的www.lixueduan.com ，如果没有收录就会提示暂未收录，点击提交网址。 点击这个链接进入百度站长平台，登录成功后选择`用户中心–\u003e站点管理–\u003e添加网站 输入自己的网站，如www.lixueduan.com 协议头如果开启了https就选https ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:3:1","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"3.2 网站验证 然后会验证这个网站是不是你的，选CNAME验证 然后去域名哪里添加一条解析即可。 记录类型–\u003eCNAME 主机记录—\u003e前面那一串l3rUDBLOMX 记录值–\u003e后面那个ziyuan.baidu.com 其他的都按默认的就行了，添加后别删除，需要一直留着。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:3:2","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"3.3 站点地图 接下来我们需要生成网站地图sitemap,使用sitemap方式向百度提交我们的网址 站点地图是一种文件，您可以通过该文件列出您网站上的网页，从而将您网站内容的组织架构告知Google和其他搜索引擎。搜索引擎网页抓取工具会读取此文件，以便更加智能地抓取您的网站。 先安装一下，打开你的hexo博客根目录，分别用下面两个命令来安装针对谷歌和百度的插件 npm install hexo-generator-sitemap --save #sitemap.xml适合提交给谷歌搜素引擎 npm install hexo-generator-baidu-sitemap --save #baidusitemap.xml适合提交百度搜索引擎 在站点配置文件中添加如下代码 Plugins: - hexo-generator-baidu-sitemap - hexo-generator-sitemap baidusitemap: path: baidusitemap.xml sitemap: path: sitemap.xml 在你的博客根目录的public下面发现生成了sitemap.xml以及baidusitemap.xml就表示成功了. 然后将博客重新部署后就可以直接访问站点地图了。如https://www.lixueduan.com/baidusitemap.xml 然后将这个站点地图提交到百度 站点管理--\u003e站点属性--\u003e链接提交--\u003e自动提交--\u003esitemap 完成后就算是提交成功了，百度比较慢，要好几天才能收录。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:3:3","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"4. 部署到GitHub 步骤和Coding差不多的。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:4:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"4.1 注册Github账号 点这里注册Github账号 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:4:1","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"4.2 创建新仓库 也是名字必须和用户名一样，必须按照这个格式username.github.io，例如lillusorycloud.github.io 创建好仓库后找到Setings 往下拉，找到Github Pages 设置Custom domain填下自定义域名，如www.lixueduan.com.如果有Enforce HTTPS 选项也可以勾上。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:4:2","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"4.3 Push代码到Github 配置SSH key 首先需要配置一个SSHkey，Git有Http协议和Git协议两种。我们这里使用Git协议就需要配置一个SSH key,等会部署到Github上也需要配置这个。 具体配置方法： Git 配置及SSH key 修改站点配置文件 repository中添加一个github # Deployment 部署到云端相关配置 ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repository: github: git@github.com:illusorycloud/illusorycloud.github.io.git coding: git@git.coding.net:illusorycloud/illusorycloud.git branch: master 配置好后，运行hexo g时就可以把博客同时部署到Coding和Github上了，也可以通过个性域名访问了。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:4:3","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"5. 收录到Google 和百度差不多。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:5:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"5.1 网站添加 首先进入Google站点平台 然后添加资源，注意http和https ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:5:1","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"5.2 验证所有权 然后验证所有权,选择DNS供应商 供应商选择其他，然后选择添加CNAME记录，在域名解析中添加一条记录。也是添加后不要删除。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:5:2","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"5.3 站点地图 验证后就可以添加站点地图了 提交成功后,我们的站点就已经被Google收录了.大概一天就能收录成功，比百度块一些。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:5:3","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"6. 总结 本文主要讲了怎么将博客部署到Coding和Github和怎么让百度,Google收录我们的网站。 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:6:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"7.参考 Hexo官方文档 基于Hexo的个人博客 Hex博客搭建 ","date":"2018-11-05","objectID":"/posts/blog/hexo/03-deploy/:7:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(三)---部署篇","uri":"/posts/blog/hexo/03-deploy/"},{"categories":["Blog"],"content":"博客主题相关配置","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"本章主要包含了博客主题优化相关内容，第三方服务和插件的配置与使用。如：炫酷头像动态背景、链接变色、鼠标点击效果、站点字数、访客数统计等。 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"0. 选择主题 你可以点击这里选择你喜欢的Themes,里面有大量美观的主题 我这里用的是简约著称的Next主题. 下载主题 使用git命令下载该主题到本地. git clone https://github.com/theme-next/hexo-theme-next themes/next clone成功后,你的Themes文件夹下就会有next主题文件了. Hexo配置文件: 都叫_config.yml 一份位于站点根目录下，主要包含 Hexo 本身的配置,称为 站点配置文件 另一份位于主题目录下主要用于配置主题相关的选项,称为主题配置文件 开启主题 站点配置文件进行修改: 将theme: landscape修改为 theme: next ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:1","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"1. 侧边栏头像设置 新版next注意引入了该功能,直接在主题配置文件修改即可,如下: # Sidebar Avatar 头像 avatar: url: /images/avatar.gif # 圆形头像 rounded: true # 透明度 0~1之间 opacity: 1 # 头像旋转 rotated: true ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:2","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"2. 设置个人社交图标链接 直接在主题配置文件修改即可,如下: # Social Links. 社交链接 前面为链接地址 后面是图标 social: GitHub: https://github.com/illusorycloud || github E-Mail: mailto:xueduan.li@gmail.com || envelope #Weibo: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype # 图标配置 social_icons: #是否显示图标 enable: true #是否只显示图标 icons_only: false #是否开启图标变化(就是刷新后会变颜色) transition: false ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:3","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"3. 添加菜单项 1.先在主题配置文件修改 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive AAAAA: /BBBBB/ || CCC 其中AAA 为菜单项的名字,BBB是路径,CCC是菜单项显示的图标 next 使用的是 Font Awesome 提供的图标 ,在这里可以选择自己喜欢的图标. 2.生成上述路径的文件 git命令行输入 hexo new page BBB –其中BBB替换为具体的名字,会在站点目录\\source下新增一个BBB文件夹,文件夹中有一个index.md文件，需要在文件头中增加一句type: XXX,例如type: categories。这样就会在这个页面显示所有的分类了。 3.修改主题文件下的对应语言的配置文件,这里是中文就修改zh-CN.yml menu: home: 首页 archives: 归档 AAAA : XXXX AAA为上边的菜单项名字,XXX为中文的名字 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:4","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"4. 添加RSS 1.安装插件 首先在Git中运行npm install --save hexo-generator-feed命令,安装插件,插件会放在 node_modules文件夹里面. 2.修改站点配置文件 安装好插件后,打开站点配置文件_config.yml`,在末尾加入以下代码: # Extensions ## Plugins: http://hexo.io/plugins/ plugins: hexo-generate-feed 3.修改主题配置文件 打开主题配置文件_config.yml,找到rss 添加配置:rss: /atom.xml ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:5","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"5. 设置酷炫动态背景 next主题提供了两种背景可以选择. 第一种背景（我是用的这种） 新版本的next主题的话直接在主题配置文件中,找到canvas-nest 修改为canvas-nest: true, # Canvas-nest # Dependencies: https://github.com/theme-next/theme-next-canvas-nest canvas_nest: enable: true onmobile: true # display on mobile or not color: '0,0,255' # RGB values, use ',' to separate opacity: 0.5 # the opacity of line: 0~1 zIndex: -1 # z-index property of the background count: 99 # the number of lines 进入theme/next目录 执行命令git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest 第二种背景 # JavaScript 3D library. # Dependencies: https://github.com/theme-next/theme-next-three # three_waves three_waves: false # canvas_lines canvas_lines: false # canvas_sphere canvas_sphere: false 也是需要下载依赖 进入theme/next目录 执行命令：git clone https://github.com/theme-next/theme-next-three source/lib/three 4个背景中只能开启一种背景,不然会出错 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:6","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"6. 设置网站logo 把你的图片放在themes/next/source/images里 打开主题配置文件_config.yml ,找到字段favicon: 都修改为对应路径 favicon: small: /images/favicon-16x16-next.png medium: /images/favicon-32x32-next.png apple_touch_icon: /images/apple-touch-icon-next.png safari_pinned_tab: /images/logo.svg ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:7","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"7. 实现点击出现桃心效果 themes/next/source/js/src里面 新建一个love.js, 复制下面的代码进去 !function(e,t,a){function n(){c(\".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}\"),o(),r()}function r(){for(var e=0;e\u003cd.length;e++)d[e].alpha\u003c=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=\"left:\"+d[e].x+\"px;top:\"+d[e].y+\"px;opacity:\"+d[e].alpha+\";transform:scale(\"+d[e].scale+\",\"+d[e].scale+\") rotate(45deg);background:\"+d[e].color+\";z-index:99999\");requestAnimationFrame(r)}function o(){var t=\"function\"==typeof e.onclick\u0026\u0026e.onclick;e.onclick=function(e){t\u0026\u0026t(),i(e)}}function i(e){var a=t.createElement(\"div\");a.className=\"heart\",d.push({el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()}),t.body.appendChild(a)}function c(e){var a=t.createElement(\"style\");a.type=\"text/css\";try{a.appendChild(t.createTextNode(e))}catch(t){a.styleSheet.cssText=e}t.getElementsByTagName(\"head\")[0].appendChild(a)}function s(){return\"rgb(\"+~~(255*Math.random())+\",\"+~~(255*Math.random())+\",\"+~~(255*Math.random())+\")\"}var d=[];e.requestAnimationFrame=function(){return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e){setTimeout(e,1e3/60)}}(),n()}(window,document); 然后打开\\themes\\next\\layout\\_layout.swig文件,在末尾 添加以下代码： \u003c!-- 页面点击小红心 --\u003e \u003cscript type=\"text/javascript\" src=\"/js/src/love.js\"\u003e\u003c/script\u003e ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:8","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"8. 修改文章内链接文本样式 鼠标移动到连接上变颜色 修改文件 themes\\next\\source\\css\\_common\\components\\post\\post.styl，在末尾添加如下css样式，： // 文章内链接文本样式 .post-body p a{ color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; \u0026:hover { color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; } } ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:9","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"9. 设置顶部滚动加载条 打开next\\layout\\_partials\\head文件，在文件末尾添加以下代码: \u003cscript src=\"//cdn.bootcss.com/pace/1.0.2/pace.min.js\"\u003e\u003c/script\u003e \u003clink href=\"//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css\" rel=\"stylesheet\"\u003e \u003cstyle\u003e .pace .pace-progress { background: #1E92FB; /*进度条颜色*/ height: 3px; } .pace .pace-progress-inner { box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ } .pace .pace-activity { border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ } \u003c/style\u003e ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:10","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"10. 在每篇文章末尾统一添加“本文结束”标记 在路径 \\themes\\next\\layout\\_macro 中新建 page-end-tag.swig 文件,并添加以下内容： \u003c!--文字可以自己修改--\u003e \u003cdiv\u003e {% if not is_index %} \u003cdiv style=\"text-align:center;color: #A2CD5A;font-size:15px;\"\u003e------------------本文到此结束\u003ci class=\"fa fa-paw\"\u003e\u003c/i\u003e感谢您的阅读------------------\u003c/div\u003e {% endif %} \u003c/div\u003e 接着打开\\themes\\next\\layout\\_macro\\post.swig文件，在post-body 之后， post-footer 之前添加下面的代码 \u003cdiv\u003e {% if not is_index %} {% include 'page-end-tag.swig' %} {% endif %} \u003c/div\u003e 然后打开主题配置文件（_config.yml),在末尾添加： # 文章末尾添加“本文结束”标记 page_end_tag: enabled: true ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:11","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"11. 静态资源压缩 Hexo自动生成的html中有很多空白的地方,会影响加载速度,所以最好还是压缩一下. 这里使用hexo-neat插件来压缩。 安装插件 npm install hexo-neat --save 在站点配置文件添加配置 # hexo-neat # 博文压缩 neat_enable: true # 压缩html neat_html: enable: true exclude: # 压缩css 跳过min.css neat_css: enable: true exclude: - '**/*.min.css' # 压缩js 跳过min.js neat_js: enable: true mangle: true output: compress: exclude: - '**/*.min.js' - '**/jquery.fancybox.pack.js' - '**/index.js' - '**/love.js' # 压缩博文配置结束 3.使用 以后再执行hexo g命令时就会自动压缩了 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:12","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"12. 主页文章添加阴影效果 打开\\themes\\next\\source\\css\\_custom\\custom.styl,向里面加入： // 主页文章添加阴影效果 .post { margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); } ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:13","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"13. 修改文章底部的的标签样式 打开模板文件/themes/next/layout/_macro/post.swig，找到rel=\"tag\"\u003e#字段， 将# 换成\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e,其中tag是你选择标签图标的名字,也是可以自定义的 \u003ca href=\"{{ url_for(tag.path) }}\" rel=\"tag\"\u003e \u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e {{ tag.name }}\u003c/a\u003e ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:14","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"14. 实现文章字数统计和预计阅读时间 1.在站点根目录下使用GitBash命令安装 hexo-wordcount插件: npm install hexo-symbols-count-time --save 2.在全局配置文件_config.yml中激活插件: symbols_count_time: symbols: true time: true total_symbols: true total_time: true 3.在主题的配置文件_config.yml中进行如下配置: #字数统计 symbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 4 wpm: 275 到此,我们就实现了文章字数统计和预估时间的显示功能 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:15","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"15. 在文章底部增加版权信息 修改主题配置文件,找到creative_commons字段 # Creative Commons 4.0 International License. # https://creativecommons.org/share-your-work/licensing-types-examples # Available: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero creative_commons: #选择一个License license: by-nc-sa #是否在侧边栏显示 sidebar: false #是否在文章末尾显示 post: true ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:16","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"16. 文章置顶 打开文件：node_modules/hexo-generator-index/lib/generator.js,将原来的代码用下面的代码替换掉 'use strict'; var pagination = require('hexo-pagination'); module.exports = function(locals){ var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) { if(a.top \u0026\u0026 b.top) { // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 } else if(a.top \u0026\u0026 !b.top) { // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; } else if(!a.top \u0026\u0026 b.top) { return 1; } else return b.date - a.date; // 都没定义按照文章日期降序排 }); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, { perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: { __index: true } }); }; 写文章的时候,在标题加上top值,数值越大排在越前面. tag: hexo copyright: true password: xxx top: 150 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:17","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"17. 在网站底部加上访问量 Next主题配置这个就比较方便了 打开主题配置文件，找到如下配置： busuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye 将enable的值由false改为true，便可以看到页脚出现访问量. 另外本地预览时访客数异常是正常的,部署至云端后就不会出现这样的问题. ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:18","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"18. 网站搜索功能 1.安装插件 ​ 站点目录下执行命令npm install hexo-generator-searchdb --save 2.修改站点配置文件 search: path: search.xml field: post format: html limit: 10000 3.修改主题配置文件 # Local search # Dependencies: https://github.com/theme-next/hexo-generator-searchdb local_search: enable: enable # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false 重新开启服务后即可看到效果。 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:19","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"TODO开启留言评论功能 //TODO 待更新 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:20","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"参考 Hexo官方文档 Next官方文档 ","date":"2018-11-03","objectID":"/posts/blog/hexo/02-config/:0:21","tags":["Hexo"],"title":"基于Hexo搭建个人博客(二)---主题优化篇","uri":"/posts/blog/hexo/02-config/"},{"categories":["Blog"],"content":"使用hexo搭建个人博客系列一 环境安装和本地运行","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Blog"],"content":"本文主要记录了如何搭建自己的博客。基于Hexo框架在本地搭建自己博客的全过程，包括了环境准备到Hexo初始化，再到服务的开启等。 ","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/:0:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Blog"],"content":"1. 环境准备 Git Git下载地址 Node.js Node.js下载地址 小白式安装，一直下一步就ok了。 都安装好后就可以开始安装Hexo啦. ","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/:1:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Blog"],"content":"2. 安装Hexo 1.新建一个文件夹,用于安装Hexo,以后这个就是放博客文件的. 2.在此文件夹右键,Git Bash Here,打开Git 3.安装Hexo 命令npm install -g hexo 4.初始化Hexo 命令hexo init 5.安装组件 命令npm install 到此为止,Hexo就算是安装完成了。 ","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/:2:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Blog"],"content":"3. 开启服务 1.hexo generate或者简写hexo g 编译,生成静态文件,就是生成一个个html文件. 2.开启服务hexo server或者hexo s 成功开启后就可以在本地访问了。 http://localhost:4000 假如页面一直无法跳转，那么可能端口被占用了。此时我们ctrl+c停止服务器，hexo server -p 端口号来改变端口号 如hexo server -p 5000 将端口号换为5000,默认是4000 3.常用命令 hexo clean 清除缓存文件 hexo deploy或者hexo d 部署网站到云端,这个后面再讲。 ","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/:3:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Blog"],"content":"4. 参考 Hexo官方文档 ","date":"2018-11-01","objectID":"/posts/blog/hexo/01-setup/:4:0","tags":["Hexo"],"title":"基于Hexo搭建个人博客(一)---基础篇","uri":"/posts/blog/hexo/01-setup/"},{"categories":["Java"],"content":"通过源码详细分析 Mybatis 框架中 SQL 语句的执行流程","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"本文主要通过源码详细分析了 Mybatis 框架中 SQL 语句的执行流程，包括加载解析核心配置文件，创建SqlSessionFactory对象，创建SqlSession对象，执行 SQL 操作。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:0:0","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"1. Mybatis工作流程 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:1:0","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"1.1 概述 1.读取mybatis全局配置文件：将定义好的mybatis全局配置文件进行读取，并包装成为一个InputStream对象 2.解析配置文件：由SqlSessionFactoryBuilder类的bulid方法驱动，对包装好的XML文件进行解析。很容易看到，其具体的解析任务是交给XMLConfigBuilder对象完成. 3.创建SqlSessionFactory对象 4.创建SqlSession的对象 5.执行SQL操作 Mybatis底层自定义了Executor执行器接口操作数据库，Executor接口有两个实现，一个是基本执行器BaseExecutor、一个是缓存执行器CachingExecutor。 Mybatis底层封装了 Mapped Statement对象，它包装了mybatis配置信息及sql映射信息等。mapper.xml文件中一个sql对应一个Mapped Statement对象，sql的id即是Mapped statement的id。 Mapped Statement对sql执行输入参数进行定义，包括HashMap、基本类型、pojo，Executor通过 Mapped Statement在执行sql前将输入的java对象映射至sql中， 输入参数映射就是jdbc编程中对preparedStatement设置参数。 Mapped Statement对sql执行输出结果进行定义，包括HashMap、基本类型、pojo，Executor通过 Mapped Statement在执行sql后将输出结果映射至java对象中， 输出结果映射过程相当于jdbc编程中对结果的解析处理过程。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:1:1","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"1.2 实例代码 @Test public void testMybaits() throws IOException { // 1. mybatis核心配置文件 以流的形式加载进来 String resources = \"mybatis-config.xml\"; InputStream resourceAsStream = Resources.getResourceAsStream(resources); // 2. 解析配置文件 根据配置文件创建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(resourceAsStream); // 3. 用SqlSessionFactory创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 直接执行SQL操作或者获取mapper对象都在操作 User user = sqlSession.selectOne(\"com.illusory.i.shiro.mapper.UserMapper.findUserByName\", \"张三\"); System.out.println(user); // 4. SqlSession获取mapper UserMapper mapper = sqlSession.getMapper(UserMapper.class); // 5. 执行CRUD操作 User userByName = mapper.findUserByName(\"username\"); } ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:1:2","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.原理分析 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:0","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.1 读取mybatis全局配置文件 将定义好的mybatis全局配置文件进行读取，并包装称为一个InputStream对象。 // 1. mybatis核心配置文件 以流的形式加载进来 String resources = \"mybatis-config.xml\"; InputStream resourceAsStream = Resources.getResourceAsStream(resources); Resources.class是 Mybatis 提供的一个加载资源文件的工具类。 getResourceAsStream(String resource) //Resources类 /* * Returns a resource on the classpath as a Stream object * * @param resource The resource to find * @return The resource * @throws java.io.IOException If the resource cannot be found or read */ public static InputStream getResourceAsStream(String resource) throws IOException { return getResourceAsStream(null, resource); } getResourceAsStream() /* * Returns a resource on the classpath as a Stream object * * @param loader The classloader used to fetch the resource * @param resource The resource to find * @return The resource * @throws java.io.IOException If the resource cannot be found or read */ public static InputStream getResourceAsStream(ClassLoader loader, String resource) throws IOException { InputStream in = classLoaderWrapper.getResourceAsStream(resource, loader); if (in == null) { throw new IOException(\"Could not find resource \" + resource); } return in; } 获取到自身的 ClassLoader 对象，然后交给 ClassLoade r(lang包下的)来加载: getResourceAsStream() //ClassLoaderWrapper /* * Get a resource from the classpath, starting with a specific class loader * * @param resource - the resource to find * @param classLoader - the first class loader to try * @return the stream or null */ public InputStream getResourceAsStream(String resource, ClassLoader classLoader) { return getResourceAsStream(resource, getClassLoaders(classLoader)); } /* * Try to get a resource from a group of classloaders * * @param resource - the resource to get * @param classLoader - the classloaders to examine * @return the resource or null */ InputStream getResourceAsStream(String resource, ClassLoader[] classLoader) { for (ClassLoader cl : classLoader) { if (null != cl) { // try to find the resource as passed InputStream returnValue = cl.getResourceAsStream(resource); // now, some class loaders want this leading \"/\", so we'll add it and try again if we didn't find the resource if (null == returnValue) { returnValue = cl.getResourceAsStream(\"/\" + resource); } if (null != returnValue) { return returnValue; } } } return null; } 值的注意的是，它返回了一个InputStream对象。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:1","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.2 解析配置文件 由SqlSessionFactoryBuilder类的bulid方法驱动，对包装好的XML文件进行解析。很容易看到，其具体的解析任务是交给XMLConfigBuilder对象完成. SqlSessionFactory.build() public SqlSessionFactory build(InputStream inputStream) { return build(inputStream, null, null); } SqlSessionFactoryBuilder.build() public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) { try { XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); return build(parser.parse()); } catch (Exception e) { throw ExceptionFactory.wrapException(\"Error building SqlSession.\", e); } finally { ErrorContext.instance().reset(); try { inputStream.close(); } catch (IOException e) { // Intentionally ignore. Prefer previous error. } } } 首先通过 Document 对象来解析，然后返回 InputStream 对象，然后交给 XMLConfigBuilder 构造成org.apache.ibatis.session.Configuration 对象， ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:2","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.3 创建方法构造成SqlSessionFactory对象 将前面解析配置文件构造出来的Configuration对象交给SqlSessionFactoryBuilder.build()方法构造成SqlSessionFactory。 build方法如下： public SqlSessionFactory build(Configuration config) { return new DefaultSqlSessionFactory(config); } 最终返回的是DefaultSqlSessionFactory对象 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:3","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.4 创建SqlSession SqlSession 完全包含了面向数据库执行 SQL 命令所需的所有方法。你可以通过 SqlSession 实例来直接执行已映射的 SQL 语句。 // 3. 用SqlSessionFactory创建SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); DefaultSqlSessionFactory.openSession() @Override public SqlSession openSession() { return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false); } 最终也是返回的一个DefaultSqlSession对象。 private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { //根据Configuration的Environment属性来创建事务工厂 final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //通过事务工厂创建事务，默认level=null autoCommit=false tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //创建执行器 真正执行sql语句的对象 final Executor executor = configuration.newExecutor(tx, execType); //根据执行器返回对象 SqlSess return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + e, e); } finally { ErrorContext.instance().reset(); } } 构建步骤： Environment–\u003eTransactionFactory+autoCommit+tx-level–\u003eTransaction+ExecType–\u003eExecutor+Configuration+autoCommit–\u003eSqlSession 其中，Environment是Configuration中的属性。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:4","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"2.5 执行SQL操作 SQL语句的执行才是MyBatis的重要职责，该过程就是通过封装JDBC进行操作，然后使用Java反射技术完成JavaBean对象到数据库参数之间的相互转换， 这种映射关系就是有TypeHandler对象来完成的，在获取数据表对应的元数据时，会保存该表所有列的数据库类型，大致逻辑如下所示： User user = sqlSession.selectOne(\"com.illusory.i.shiro.mapper.UserMapper.findUserByName\", \"张三\"); System.out.println(user); 调用selectOne方法进行SQL查询，selectOne方法最后调用的是selectList，在selectList中，会查询 configuration中存储的MappedStatement对象，mapper文件中一个sql语句的配置对应一个MappedStatement对象，然后调用执行器进行查询操作。 DefaultSqlSession.selectOne(); @Override public \u003cT\u003e T selectOne(String statement, Object parameter) { // Popular vote was to return null on 0 results and throw exception on too many. List\u003cT\u003e list = this.\u003cT\u003eselectList(statement, parameter); if (list.size() == 1) { return list.get(0); } else if (list.size() \u003e 1) { throw new TooManyResultsException(\"Expected one result (or null) to be returned by selectOne(), but found: \" + list.size()); } else { return null; } } DefaultSqlSession.selectList(); @Override public \u003cE\u003e List\u003cE\u003e selectList(String statement, Object parameter) { return this.selectList(statement, parameter, RowBounds.DEFAULT); } @Override public \u003cE\u003e List\u003cE\u003e selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception e) { throw ExceptionFactory.wrapException(\"Error querying database. Cause: \" + e, e); } finally { ErrorContext.instance().reset(); } } 执行器在query操作中，优先会查询缓存是否命中，命中则直接返回，否则从数据库中查询。 CachingExecutor.query() @Override public \u003cE\u003e List\u003cE\u003e query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameterObject); CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql); return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } @Override public \u003cE\u003e List\u003cE\u003e query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { Cache cache = ms.getCache(); if (cache != null) { flushCacheIfRequired(ms); if (ms.isUseCache() \u0026\u0026 resultHandler == null) { ensureNoOutParams(ms, boundSql); @SuppressWarnings(\"unchecked\") List\u003cE\u003e list = (List\u003cE\u003e) tcm.getObject(cache, key); if (list == null) { list = delegate.\u003cE\u003e query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 } return list; } } //BaseExecutor.query() return delegate.\u003cE\u003e query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } BaseExecutor.query() @SuppressWarnings(\"unchecked\") @Override public \u003cE\u003e List\u003cE\u003e query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(\"executing a query\").object(ms.getId()); if (closed) { throw new ExecutorException(\"Executor was closed.\"); } if (queryStack == 0 \u0026\u0026 ms.isFlushCacheRequired()) { clearLocalCache(); } List\u003cE\u003e list; try { queryStack++; list = resultHandler == null ? (List\u003cE\u003e) localCache.getObject(key) : null; if (list != null) { handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); } else { list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); } } finally { queryStack--; } if (queryStack == 0) { for (DeferredLoad deferredLoad : deferredLoads) { deferredLoad.load(); } // issue #601 deferredLoads.clear(); if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { // issue #482 clearLocalCache(); } } return list; } BaseExecutor.queryFromDatabase() private \u003cE\u003e List\u003cE\u003e queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { List\u003cE\u003e list; /** * 先往localCache中插入一个占位对象，这个地方 */ localCache.putObject(key, EXECUTION_PLACEHOLDER","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:2:5","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"3. MyBatis缓存 MyBatis提供查询缓存，用于减轻数据库压力，提高性能。MyBatis提供了一级缓存和二级缓存。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:3:0","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"3.1 一级缓存 一级缓存是 SqlSession 级别的缓存，每个 SqlSession 对象都有一个哈希表用于缓存数据，不同 SqlSession 对象之间缓存不共享。 同一个 SqlSession 对象对象执行2遍相同的 SQL 查询，在第一次查询执行完毕后将结果缓存起来，这样第二遍查询就不用向数据库查询了， 直接返回缓存结果即可。MyBatis 默认是开启一级缓存的。 简单说就是SQL语句作为key，查询结果作为value，根据key去查找value，如果查询语句相同就能直接返回value。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:3:1","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"3.2 二级缓存 二级缓存是 mapper 级别的缓存，二级缓存是跨 SqlSession 的，多个 SqlSession 对象可以共享同一个二级缓存。不同的 SqlSession 对象执行两次相同的 SQL 语句， 第一次会将查询结果进行缓存，第二次查询直接返回二级缓存中的结果即可。MyBatis 默认是不开启二级缓存的，可以在配置文件中使用如下配置来开启二级缓存： \u003csettings\u003e \u003csetting name=\"cacheEnabled\" value=\"true\"/\u003e \u003c/settings\u003e ​ 当SQL语句进行更新操作(删除/添加/更新)时，会清空对应的缓存，保证缓存中存储的都是最新的数据。 ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:3:2","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"4. 参考 https://www.cnblogs.com/dongying/p/4142476.html http://www.mybatis.org/mybatis-3/zh/getting-started.html ","date":"2018-10-25","objectID":"/posts/java/ssm/05-mybatis/:4:0","tags":["Mybatis"],"title":"SSM系列(五)---Mybatis SQL 执行流程分析","uri":"/posts/java/ssm/05-mybatis/"},{"categories":["Java"],"content":"通过源码分析SpringMVC 框架执行流程","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"本文主要通过源码详细分析了 SpringMVC 框架的执行流程，包括建立 url 和 controller 的关系，通过 url 找到具体的方法，通过反射执行 controller 中的方法等。 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:0:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"1. Servlet 执行流程 传统servlet的执行过程分为如下几步： 1、浏览器向服务器发送请求http://localhost:8080/demo/hello 2、服务器接受到请求，并从地址中得到项目名称webproject 3、然后再从地址中找到名称hello，并与webproject下的web.xml文件进行匹配 4、在web.xml中找到一个 \u003curl-pattern\u003ehello\u003c/url-pattern\u003e的标签，并且通过他找到servlet-name进而找到\u003cservlet-class\u003e 5、再拿到servlet-class之后，这个服务器便知道了这个servlet的全类名，通过反射创建这个类的对象，并且调用doGet/doPost方法 6、方法执行完毕，结果返回到浏览器。结束。 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:1:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"2. SpringMVC 执行流程 SpringMVC 中也配置了一个 Servlet,配置的是 org.springframework.web.servlet.DispatcherServlet，所有的请求过来都会找这个 servlet (前端控制器)，DispatcherServlet 继承了 HttpServlet。 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:2:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"运行过程分析 1、 用户发送请求至前端控制器DispatcherServlet。 2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)HandlerExcutorChain并返回给 DispatcherServlet。 4、 DispatcherServlet调用HandlerAdapter处理器适配器。 5、 HandlerAdapter经过适配调用具体的处理器(就是我们写的 Controller )。 6、 Controller执行完成返回ModelAndView。 7、 HandlerAdapter将 Controller 执行结果ModelAndView返回给DispatcherServlet。 8、 DispatcherServlet将 ModelAndView 传给ViewReslover视图解析器。 9、 ViewReslover解析后返回具体View(这就是为什么reurn \"index\"会自动找到 index.html) 10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。 11、 DispatcherServlet响应用户。 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:2:1","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"3. 具体过程分析 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:3:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"1. 建立 Map\u003curls,Controller\u003e 的关系 概述 在容器初始化时会建立所有url 和 controller的对应关系,保存到Map\u003curl,controller\u003e中。 DispatcherServlet--\u003einitApplicationContext初始化容器 建立Map\u003curl,controller\u003e关系的部分 Tomcat启动时会通知 Spring 初始化容器(加载 bean 的定义信息和初始化所有单例 bean ),然后 SpringMVC 会遍历容器中的bean,获取每一个 Controller 中的所有方法访问的 url,然后将 url和 Controller 保存到一个 Map 中; ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:3:1","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"2.根据访问url 找到对应 Controller 中处理请求的方法 概述 DispatcherServlet--\u003edoDispatch() 有了前面的 Map 就可以根据 Request快速定位到 Controller,因为最终处理 Request 的是 Controller 中的方法,Map 中只保留了 url 和 Controller 中的对应关系,所以要根据 Request 的 url 进一步确认 Controller 中的 Method. 原理 这一步工作的原理就是拼接 Controller 的 url(controller上@RequestMapping的值) 和方法的 url(method 上@RequestMapping的值),与 Request 的 url 进行匹配,找到匹配的那个方法;　","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:3:2","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"3. 参数绑定 确定处理请求的 Method 后,接下来的任务就是参数绑定,把 Request 中参数绑定到方法的形式参数上,这一步是整个请求处理过程中最复杂的一个步骤。SpringMVC 提供了两种 Request 参数与方法形参的绑定方法: 注解 使用注解进行绑定,我们只要在方法参数前面声明 @RequestParam(\"a\"),就可以将 Request 中参数 a 的值绑定到方法的该参数上。 参数名称 使用参数名称进行绑定的前提是必须要获取方法中参数的名称,Java 反射只提供了获取方法的参数的类型,并没有提供获取参数名称的方法。SpringMVC 解决这个问题的方法是用 asm 框架读取字节码文件,来获取方法的参数名称。asm 框架是一个字节码操作框架,关于a sm 更多介绍可以参考它的官网。 个人建议,使用注解来完成参数绑定,这样就可以省去 asm 框架的读取字节码的操作。 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:3:3","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"4. 源码分析 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"1. 建立Map\u003curl,controller\u003e的关系 我们首先看第一个步骤,也就是建立Map\u003curl,controller\u003e关系的部分.第一部分的入口类ApplicationObjectSupport的setApplicationContext方法.setApplicationContext方法中核心部分就是初始化容器initApplicationContext(context),子类AbstractDetectingUrlHandlerMapping实现了该方法,所以我们直接看子类中的初始化容器方法. //ApplicationObjectSupport类 @Override public final void setApplicationContext(@Nullable ApplicationContext context) throws BeansException { if (context == null \u0026\u0026 !isContextRequired()) { // Reset internal context state. this.applicationContext = null; this.messageSourceAccessor = null; } else if (this.applicationContext == null) { // Initialize with passed-in context. if (!requiredContextClass().isInstance(context)) { throw new ApplicationContextException( \"Invalid application context: needs to be of type [\" + requiredContextClass().getName() + \"]\"); } this.applicationContext = context; this.messageSourceAccessor = new MessageSourceAccessor(context); initApplicationContext(context); } else { // Ignore reinitialization if same context passed in. if (this.applicationContext != context) { throw new ApplicationContextException( \"Cannot reinitialize with different application context: current one is [\" + this.applicationContext + \"], passed-in one is [\" + context + \"]\"); } } } 其中initApplicationContext(context)由子类AbstractDetectingUrlHandlerMapping实现,具体如下: /** * Calls the {@link #detectHandlers()} method in addition to the * superclass's initialization. */ @Override public void initApplicationContext() throws ApplicationContextException { super.initApplicationContext(); detectHandlers(); } /** 建立当前ApplicationContext中的所有controller和url的对应关系 * Register all handlers found in the current ApplicationContext. * \u003cp\u003eThe actual URL determination for a handler is up to the concrete * {@link #determineUrlsForHandler(String)} implementation. A bean for * which no such URLs could be determined is simply not considered a handler. * @throws org.springframework.beans.BeansException if the handler couldn't be registered * @see #determineUrlsForHandler(String) */ protected void detectHandlers() throws BeansException { ApplicationContext applicationContext = obtainApplicationContext(); String[] beanNames = (this.detectHandlersInAncestorContexts ? BeanFactoryUtils.beanNamesForTypeIncludingAncestors(applicationContext, Object.class) : applicationContext.getBeanNamesForType(Object.class)); // 获取ApplicationContext容器中所有bean的Name // Take any bean name that we can determine URLs for. // 遍历beanNames,并找到这些bean对应的url for (String beanName : beanNames) { // 找bean上的所有url(controller上的url+方法上的url),该方法由对应的子类实现 String[] urls = determineUrlsForHandler(beanName); if (!ObjectUtils.isEmpty(urls)) { // URL paths found: Let's consider it a handler. // 保存urls和beanName的对应关系,put it to Map\u003curls,beanName\u003e,该方法在父类AbstractUrlHandlerMapping中实现 registerHandler(urls, beanName); } } if ((logger.isDebugEnabled() \u0026\u0026 !getHandlerMap().isEmpty()) || logger.isTraceEnabled()) { logger.debug(\"Detected \" + getHandlerMap().size() + \" mappings in \" + formatMappingName()); } } /** * Determine the URLs for the given handler bean. * @param beanName the name of the candidate bean * @return the URLs determined for the bean, or an empty array if none */ /** 获取controller中所有方法的url,由子类实现,典型的模板模式 **/ protected abstract String[] determineUrlsForHandler(String beanName); determineUrlsForHandler(String beanName)方法的作用是获取每个Controller中的url,不同的子类有不同的实现,这是一个典型的模板设计模式.因为开发中我们用的最多的就是用注解来配置Controller``中的url,BeanNameUrlHandlerMapping是AbstractDetectingUrlHandlerMapping的子类,我们看BeanNameUrlHandlerMapping是如何查beanName上所有映射的url. public class BeanNameUrlHandlerMapping extends AbstractDetectingUrlHandlerMapping { /** * Checks name and aliases of the given bean for URLs, starting with \"/\". * 找出名字或者别名是以 / 开头的bean */ @Override protected String[] determineUrlsForHandler(String beanName) { List\u003cString\u003e urls = new ArrayList\u003c\u003e(); if (beanName.startsWith(\"/\")) { urls.add(beanName); } String[] aliases = obtainApplicationContext().getAliases(beanName); for (String alias : aliases) { if (a","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:1","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"2. 根据访问url找到对应controller中处理请求的方法 下面我们开始分析第二个步骤,第二个步骤是由请求触发的,所以入口为DispatcherServlet.DispatcherServlet的核心方法为doService(),doService()中的核心逻辑由doDispatch()实现,我们查看doDispatch()的源代码. /** * Process the actual dispatching to the handler. * \u003cp\u003eThe handler will be obtained by applying the servlet's HandlerMappings in order. * The HandlerAdapter will be obtained by querying the servlet's installed HandlerAdapters * to find the first that supports the handler class. * \u003cp\u003eAll HTTP methods are handled by this method. It's up to HandlerAdapters or handlers * themselves to decide which methods are acceptable. * @param request current HTTP request * @param response current HTTP response * @throws Exception in case of any kind of processing failure */ /** 中央控制器,控制请求的转发 **/ protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { // 1.检查是否是文件上传的请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. // 2.取得处理当前请求的controller,这里也称为hanlder,处理器 // 第一个步骤的意义就在这里体现了.这里并不是直接返回controller, // 而是返回的HandlerExecutionChain请求处理器链对象, // 该对象封装了handler和interceptors. mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { noHandlerFound(processedRequest, response); return; } // Determine handler adapter for the current request. //3. 获取处理request的处理器适配器handler adapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. // 处理 last-modified 请求头 String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) \u0026\u0026 isGet) { return; } } // 4.拦截器的预处理方法 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // Actually invoke the handler. // 5.实际的处理器处理请求,返回结果视图对象 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } // 结果视图对象的处理 applyDefaultViewName(processedRequest, mv); // 6.拦截器的后处理方法 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } //将结果解析为ModelAndView processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { // 请求成功响应之后的方法 triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion if (mappedHandler != null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } } 第2步:getHandler(processedRequest)方法实际上就是从HandlerMapping中找到url和Controller的对应关系.这也就是第一个步骤:建立Map\u003curl,Controller\u003e的意义.我们知道,最终处理Request的是Controller中的方法,我们现在只是知道了Controller,还要进一步确认Controller中处理Request的方法.由于下面的步骤和第三个步骤关系更加紧密,直接转到第三个步骤. ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:2","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"3. 反射调用处理请求的方法,返回结果视图 上面的方法中,第2步其实就是从第一个步骤中的Map\u003curls,beanName\u003e中取得Controller,然后经过拦截器的预处理方法,到最核心的部分–第5步调用Controller的方法处理请求.在第2步中我们可以知道处理Request的Controller,第5步就是要根据url确定Controller中处理请求的方法,然后通过反射获取该方法上的注解和参数,解析方法和参数上的注解,最后反射调用方法获取ModelAndView结果视图。 第5步调用的就是RequestMappingHandlerAdapter的handle().handle()中的核心逻辑由invokeHandlerMethod(request, response, handler)实现。 handle().handle()–\u003ehandleInternal(request, response, (HandlerMethod) handler)–\u003einvokeHandlerMethod(request, response, handlerMethod) RequestMappingHandlerAdapter类 /** * Invoke the {@link RequestMapping} handler method preparing a {@link ModelAndView} * if view resolution is required. * @since 4.2 * @see #createInvocableHandlerMethod(HandlerMethod) */ @Nullable protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { ServletWebRequest webRequest = new ServletWebRequest(request, response); try { WebDataBinderFactory binderFactory = getDataBinderFactory(handlerMethod); ModelFactory modelFactory = getModelFactory(handlerMethod, binderFactory); //创建invocableMetho ServletInvocableHandlerMethod invocableMethod = createInvocableHandlerMethod(handlerMethod); if (this.argumentResolvers != null) { invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); } if (this.returnValueHandlers != null) { invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); } invocableMethod.setDataBinderFactory(binderFactory); invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); mavContainer.addAllAttributes(RequestContextUtils.getInputFlashMap(request)); modelFactory.initModel(webRequest, mavContainer, invocableMethod); mavContainer.setIgnoreDefaultModelOnRedirect(this.ignoreDefaultModelOnRedirect); AsyncWebRequest asyncWebRequest = WebAsyncUtils.createAsyncWebRequest(request, response); asyncWebRequest.setTimeout(this.asyncRequestTimeout); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.setTaskExecutor(this.taskExecutor); asyncManager.setAsyncWebRequest(asyncWebRequest); asyncManager.registerCallableInterceptors(this.callableInterceptors); asyncManager.registerDeferredResultInterceptors(this.deferredResultInterceptors); if (asyncManager.hasConcurrentResult()) { Object result = asyncManager.getConcurrentResult(); mavContainer = (ModelAndViewContainer) asyncManager.getConcurrentResultContext()[0]; asyncManager.clearConcurrentResult(); LogFormatUtils.traceDebug(logger, traceOn -\u003e { String formatted = LogFormatUtils.formatValue(result, !traceOn); return \"Resume with async result [\" + formatted + \"]\"; }); invocableMethod = invocableMethod.wrapConcurrentResult(result); } //执行ServletInvocableHandlerMethod的invokeAndHandle方法 invocableMethod.invokeAndHandle(webRequest, mavContainer); if (asyncManager.isConcurrentHandlingStarted()) { return null; } // 封装结果视图 return getModelAndView(mavContainer, modelFactory, webRequest); } finally { webRequest.requestCompleted(); } } invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers);为参数绑定，后面说 其中invokeAndHandle如下： /** * Invoke the method and handle the return value through one of the * configured {@link HandlerMethodReturnValueHandler HandlerMethodReturnValueHandlers}. * @param webRequest the current request * @param mavContainer the ModelAndViewContainer for this request * @param providedArgs \"given\" arguments matched by type (not resolved) */ public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { //执行请求对应的方法，并获得返回值 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); if (returnValue == null) { if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) { mavContainer.setRequestHandled(true); return; } } else if (StringUtils.hasText(","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:3","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"4. 参数绑定 resolveHandlerArguments方法实现代码比较长,它最终要实现的目的就是:完成request中的参数和方法参数上数据的绑定. springmvc中提供两种request参数到方法中参数的绑定方式: 注解 使用注解进行绑定,我们只要在方法参数前面声明 @RequestParam(\"a\"),就可以将 Request 中参数 a 的值绑定到方法的该参数上。 参数名称 使用参数名称进行绑定的前提是必须要获取方法中参数的名称,Java 反射只提供了获取方法的参数的类型,并没有提供获取参数名称的方法。SpringMVC 解决这个问题的方法是用 asm 框架读取字节码文件,来获取方法的参数名称。asm 框架是一个字节码操作框架,关于a sm 更多介绍可以参考它的官网。 个人建议,使用注解来完成参数绑定,这样就可以省去 asm 框架的读取字节码的操作。 private Object[] resolveHandlerArguments(Method handlerMethod, Object handler, NativeWebRequest webRequest, ExtendedModelMap implicitModel) throws Exception { // 1.获取方法参数类型的数组 Class[] paramTypes = handlerMethod.getParameterTypes(); // 声明数组,存参数的值 Object[] args = new Object[paramTypes.length]; //2.遍历参数数组,获取每个参数的值 for (int i = 0; i \u003c args.length; i++) { MethodParameter methodParam = new MethodParameter(handlerMethod, i); methodParam.initParameterNameDiscovery(this.parameterNameDiscoverer); GenericTypeResolver.resolveParameterType(methodParam, handler.getClass()); String paramName = null; String headerName = null; boolean requestBodyFound = false; String cookieName = null; String pathVarName = null; String attrName = null; boolean required = false; String defaultValue = null; boolean validate = false; int annotationsFound = 0; Annotation[] paramAnns = methodParam.getParameterAnnotations(); // 处理参数上的注解 for (Annotation paramAnn : paramAnns) { if (RequestParam.class.isInstance(paramAnn)) { RequestParam requestParam = (RequestParam) paramAnn; paramName = requestParam.value(); required = requestParam.required(); defaultValue = parseDefaultValueAttribute(requestParam.defaultValue()); annotationsFound++; } else if (RequestHeader.class.isInstance(paramAnn)) { RequestHeader requestHeader = (RequestHeader) paramAnn; headerName = requestHeader.value(); required = requestHeader.required(); defaultValue = parseDefaultValueAttribute(requestHeader.defaultValue()); annotationsFound++; } else if (RequestBody.class.isInstance(paramAnn)) { requestBodyFound = true; annotationsFound++; } else if (CookieValue.class.isInstance(paramAnn)) { CookieValue cookieValue = (CookieValue) paramAnn; cookieName = cookieValue.value(); required = cookieValue.required(); defaultValue = parseDefaultValueAttribute(cookieValue.defaultValue()); annotationsFound++; } else if (PathVariable.class.isInstance(paramAnn)) { PathVariable pathVar = (PathVariable) paramAnn; pathVarName = pathVar.value(); annotationsFound++; } else if (ModelAttribute.class.isInstance(paramAnn)) { ModelAttribute attr = (ModelAttribute) paramAnn; attrName = attr.value(); annotationsFound++; } else if (Value.class.isInstance(paramAnn)) { defaultValue = ((Value) paramAnn).value(); } else if (\"Valid\".equals(paramAnn.annotationType().getSimpleName())) { validate = true; } } if (annotationsFound \u003e 1) { throw new IllegalStateException(\"Handler parameter annotations are exclusive choices - \" + \"do not specify more than one such annotation on the same parameter: \" + handlerMethod); } if (annotationsFound == 0) {// 如果没有注解 Object argValue = resolveCommonArgument(methodParam, webRequest); if (argValue != WebArgumentResolver.UNRESOLVED) { args[i] = argValue; } else if (defaultValue != null) { args[i] = resolveDefaultValue(defaultValue); } else { Class paramType = methodParam.getParameterType(); // 将方法声明中的Map和Model参数,放到request中,用于将数据放到request中带回页面 if (Model.class.isAssignableFrom(paramType) || Map.class.isAssignableFrom(paramType)) { args[i] = implicitModel; } else if (SessionStatus.class.isAssignableFrom(paramType)) { args[i] = this.sessionStatus; } else if (HttpEntity.class.isAssignableFrom(paramType)) { args[i] = resolveHttpEntityRequest(methodParam, webRequest); } else if (Errors.class.isAssignableFrom(paramType)) { throw new IllegalStateException(\"Errors/BindingResult argument declared \" + \"without preceding model attribute. Check your handler method signature!\"); } else if (BeanUtils.isSimpleProperty(paramType)) { paramName = \"\"; } else { attrName = \"\"; } } } // 从request中取值,并进行赋值操作 if (paramName != null) { // 根据paramName从requ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:4","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"5. 反射源码分析 反射相关分析来源于：http://www.sczyh30.com/posts/Java/java-reflection-2/ 第三步中的invoke方法如下 @CallerSensitive public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException { if (!override) { //quickCheckMemberAccess 检查方法是否为public 如果是的话跳出本步 if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) { //如果不是public方法，那么用Reflection.getCallerClass()方法获取调用这个方法的Class对象，这是一个native方法: Class\u003c?\u003e caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); } } MethodAccessor ma = methodAccessor; // read volatile if (ma == null) { ma = acquireMethodAccessor(); } return ma.invoke(obj, args); } getCallerClass()是一个native方法 @CallerSensitive public static native Class\u003c?\u003e getCallerClass(); 在OpenJDK的源码中找到此方法的JNI入口(Reflection.c): JNIEXPORT jclass JNICALL Java_sun_reflect_Reflection_getCallerClass__ (JNIEnv *env, jclass unused) { return JVM_GetCallerClass(env, JVM_CALLER_DEPTH); } 获取了这个Class对象caller后用checkAccess方法做一次快速的权限校验，其实现为: volatile Object securityCheckCache; void checkAccess(Class\u003c?\u003e caller, Class\u003c?\u003e clazz, Object obj, int modifiers) throws IllegalAccessException { if (caller == clazz) { // 快速校验 return; // 权限通过校验 } Object cache = securityCheckCache; // read volatile Class\u003c?\u003e targetClass = clazz; if (obj != null \u0026\u0026 Modifier.isProtected(modifiers) \u0026\u0026 ((targetClass = obj.getClass()) != clazz)) { // Must match a 2-list of { caller, targetClass }. if (cache instanceof Class[]) { Class\u003c?\u003e[] cache2 = (Class\u003c?\u003e[]) cache; if (cache2[1] == targetClass \u0026\u0026 cache2[0] == caller) { return; // ACCESS IS OK } // (Test cache[1] first since range check for [1] // subsumes range check for [0].) } } else if (cache == caller) { // Non-protected case (or obj.class == this.clazz). return; // ACCESS IS OK } // If no return, fall through to the slow path. slowCheckMemberAccess(caller, clazz, obj, modifiers, targetClass); } 首先先执行一次快速校验，一旦调用方法的Class正确则权限检查通过。 若未通过，则创建一个缓存，中间再进行一堆检查（比如检验是否为protected属性）。 如果上面的所有权限检查都未通过，那么将执行更详细的检查，其实现为： // Keep all this slow stuff out of line: void slowCheckMemberAccess(Class\u003c?\u003e caller, Class\u003c?\u003e clazz, Object obj, int modifiers, Class\u003c?\u003e targetClass) throws IllegalAccessException { Reflection.ensureMemberAccess(caller, clazz, obj, modifiers); // Success: Update the cache. Object cache = ((targetClass == clazz) ? caller : new Class\u003c?\u003e[] { caller, targetClass }); // Note: The two cache elements are not volatile, // but they are effectively final. The Java memory model // guarantees that the initializing stores for the cache // elements will occur before the volatile write. securityCheckCache = cache; // write volatile } 大体意思就是，用Reflection.ensureMemberAccess方法继续检查权限，若检查通过就更新缓存，这样下一次同一个类调用同一个方法时就不用执行权限检查了，这是一种简单的缓存机制。由于JMM的happens-before规则能够保证缓存初始化能够在写缓存之前发生，因此两个cache不需要声明为volatile。 到这里，前期的权限检查工作就结束了。如果没有通过检查则会抛出异常，如果通过了检查则会到下一步。 调用MethodAccessor的invoke方法 Method.invoke()实际上并不是自己实现的反射调用逻辑，而是委托给sun.reflect.MethodAccessor来处理。 首先要了解Method对象的基本构成，每个Java方法有且只有一个Method对象作为root，它相当于根对象，对用户不可见。当我们创建Method对象时，我们代码中获得的Method对象都相当于它的副本（或引用）。root对象持有一个MethodAccessor对象，所以所有获取到的Method对象都共享这一个MethodAccessor对象，因此必须保证它在内存中的可见性。root对象其声明及注释为： private volatile MethodAccessor methodAccessor; // For sharing of MethodAccessors. This branching structure is // currently only two levels deep (i.e., one root Method and // potentially many Method objects pointing to it.) // // If this branching structure would ever contain cycles, deadlocks can // occur in annotation code. private Method root; 那么MethodAccessor到底是个啥玩意呢？ /** This interface provides the declaration for java.lang.reflect.Method.invoke(). Each Method object is configured with a (possibly dynamically-generated) class which implements this interface. */ public interface MethodAccessor { /** Matches specification in {@link java.lang.reflect.Method} */ public Object invoke(Object obj, Object[] args) throws IllegalArgumentException, InvocationTargetException; } 可以看到MethodAccessor是一个接口，定义了invoke方法。分析其Usage可得它的具体实现类有: sun.reflec","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:4:5","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"5. 小结 大致流程如下： 1.建立 Map\u003curl,comtroller\u003e 的关系 2.根据 url 找到具体的处理方法 3.通过反射调用 controller 中的方法 4.通过注解或参数名称实现参数绑定 ","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:5:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"参考 https://www.cnblogs.com/heavenyes/p/3905844.html#t1 sczyh30: http://www.sczyh30.com/posts/Java/java-reflection-2/　","date":"2018-10-17","objectID":"/posts/java/ssm/04-springmvc-process/:6:0","tags":["SpringMVC"],"title":"SSM系列(四)---SpringMVC执行流程分析","uri":"/posts/java/ssm/04-springmvc-process/"},{"categories":["Java"],"content":"Spring 框架中 BeanFactory 与 ApplicationContext 的区别","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"本文主要介绍了 Spring 框架中 BeanFactory 与 ApplicationContext 的区别。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:0:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1. BeanFactory ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1.1 概述 BeanFactory 是 Spring 的“心脏”。它就是 Spring IoC 容器的真面目。Spring 使用 BeanFactory 来实例化、配置和管理 Bean。 BeanFactory：是IOC容器的核心接口， 它定义了IOC的基本功能，我们看到它主要定义了getBean方法。getBean方法是IOC容器获取bean对象和引发依赖注入的起点。方法的功能是返回特定的名称的Bean。 BeanFactory 是初始化 Bean 和调用它们生命周期方法的“吃苦耐劳者”。注意，BeanFactory 只能管理单例（Singleton）Bean 的生命周期。它不能管理原型(prototype,非单例)Bean 的生命周期。这是因为原型 Bean 实例被创建之后便被传给了客户端,容器失去了对它们的引用。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:1","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1.2 源码分析 BeanFactory 源码如下： package org.springframework.beans.factory; public interface BeanFactory { /** * 用来引用一个实例，或把它和工厂产生的Bean区分开，就是说，如果一个FactoryBean的名字为a，那么，\u0026a会得到那个Factory */ String FACTORY_BEAN_PREFIX = \"\u0026\"; /* * 四个不同形式的getBean方法，获取实例 */ Object getBean(String name) throws BeansException; \u003cT\u003e T getBean(String name, Class\u003cT\u003e requiredType) throws BeansException; \u003cT\u003e T getBean(Class\u003cT\u003e requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; boolean containsBean(String name); // 是否存在 boolean isSingleton(String name) throws NoSuchBeanDefinitionException;// 是否为单实例 boolean isPrototype(String name) throws NoSuchBeanDefinitionException;// 是否为原型（多实例） boolean isTypeMatch(String name, Class\u003c?\u003e targetType) throws NoSuchBeanDefinitionException;// 名称、类型是否匹配 Class\u003c?\u003e getType(String name) throws NoSuchBeanDefinitionException; // 获取类型 String[] getAliases(String name);// 根据实例的名字获取实例的别名 } ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:2","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1.3 方法列表 4个获取实例的方法。getBean的重载方法。 4个判断的方法。判断是否存在，是否为单例、原型，名称类型是否匹配。 1个获取类型的方法、1个获取别名的方法。根据名称获取类型、根据名称获取别名。 这10个方法，很明显，这是一个典型的工厂模式的工厂接口。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:3","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1.4 实例演示 在 Spring 3.2 之前的版本中，BeanFactory最常见的实现类为XmlBeanFactory(已废弃)，建议使用 XmlBeanDefinitionReader 与 DefaultListableBeanFactory。可以从classpath或文件系统等获取资源。 拿前面的 Book 举例 public class Book { private String type; private String name; //省略Getter/Setter和构造方法 } xml 配置文件 \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003cproperty name=\"name\" value=\"think in java\"\u003e\u003c/property\u003e \u003cproperty name=\"type\" value=\"CS\"\u003e\u003c/property\u003e \u003c/bean\u003e 使用 DefaultListableBeanFactory 与 XmlBeanDefinitionReader 来启动 IoC 容器 public static void main(String[] args) { esourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); Resource resource = resolver.getResource(\"classpath:beans.xml\"); System.out.println(\"getURL:\" + resource.getURL()); DefaultListableBeanFactory factory = new DefaultListableBeanFactory(); XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(factory); reader.loadBeanDefinitions(resource); //ApplicationContext factory=new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Book book = factory.getBean(\"book\",Book.class); System.out.println(\"boook对象已经初始化完成\"); System.out.println(book.getName()); } ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:4","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"1.5 小结 XmlBeanFactory通过Resource装载Spring配置信息冰启动IoC容器，然后就可以通过factory.getBean从IoC容器中获取Bean了。 通过BeanFactory启动IoC容器时，并不会初始化配置文件中定义的Bean，初始化动作发生在第一个调用时。 对于单实例（singleton）的Bean来说，BeanFactory会缓存Bean实例，所以第二次使用getBean时直接从IoC容器缓存中获取Bean。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:1:5","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"2. ApplicationContext ApplicationContext由BeanFactory派生而来，提供了更多面向实际应用的功能。在BeanFactory中，很多功能需要以编程的方式实现，而在ApplicationContext中则可以通过配置实现。 BeanFactorty接口提供了配置框架及基本功能，但是无法支持spring的aop功能和web应用。而ApplicationContext接口作为BeanFactory的派生，因而提供BeanFactory所有的功能。而且ApplicationContext还在功能上做了扩展，相较于BeanFactorty，ApplicationContext还提供了以下的功能： （1）MessageSource, 提供国际化的消息访问 （2）资源访问，如URL和文件 （3）事件传播特性，即支持aop特性 （4）载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层 ApplicationContext：是IOC容器另一个重要接口， 它继承了BeanFactory的基本功能， 同时也继承了容器的高级功能，如：MessageSource（国际化资源接口）、ResourceLoader（资源加载接口）、ApplicationEventPublisher（应用事件发布接口）等。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:2:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"3. 二者区别 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:3:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"3.1 bean 加载时机 BeanFactroy 采用的是延迟加载形式来注入 Bean 的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化，这样，我们就不能发现一些存在的Spring的配置问题。 ApplicationContext则相反，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:3:1","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"3.2 Bean 注册 BeanFactory 和 ApplicationContext 都支持 BeanPostProcessor、BeanFactoryPostProcessor 的使用。 但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 Applicationcontext比 beanFactory 加入了一些更好使用的功能。而且 beanFactory 的许多功能需要通过编程实现而 Applicationcontext 可以通过配置实现。 比如后处理 bean，ApplicationContext 直接配置在配置文件即可而 BeanFactory 这要在代码中显示的写出来才可以被容器识别。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:3:2","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"3.3 使用场景 BeanFactory 主要是面对与 Spring 框架的基础设施，面对 Spring 自己。 ApplicationContext 主要面对与 Spring 使用的开发者。 基本都会使用 ApplicationContext 并非 BeanFactory 。 ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:3:3","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"4. 总结 1.BeanFactory 负责读取 bean 配置文档，管理 bean 的加载，实例化，维护 bean 之间的依赖关系，负责bean 的声明周期。 2.ApplicationContext 除了提供上述 BeanFactory 所能提供的功能之外，还提供了更完整的框架功能： a. 国际化支持(MessageSource) b. 资源访问(ResourceLoader) c.载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层 d.消息发送、响应机制（ApplicationEventPublisher） e.AOP（拦截器） ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:4:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"5. 参考 https://www.cnblogs.com/xiaoxi/p/5846416.html https://www.jianshu.com/p/2808f7c4a24f ","date":"2018-10-10","objectID":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/:5:0","tags":["Spring"],"title":"SSM系列(三)---BeanFactory 与 ApplicationContext","uri":"/posts/java/ssm/03-beanfactory-applicationcontext-diff/"},{"categories":["Java"],"content":"Spring 常用注解与配置记录","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"本文主要对 Spring 框架中经常用到的注解与配置进行了说明。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:0:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"1. Bean相关的注解 与SpringBean相关的注解有以下四大类： @Controller ：标注一个控制器组件类 Controller层 @Service：标注一个业务逻辑组件类 Service层 @Repository ：标注一个 DAO 组件类 DAO层 @Component ：标注一个普通的 Spring Bean 类 前面三个都不是但又想交给Spring管理就用这个 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:1:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"2. @Autowired与@Resource区别 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:2:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"2.1 相同点 @Resource的作用相当于@Autowired，均可标注在字段或属性的setter方法上。 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:2:1","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"2.2 不同点 1. 提供方 @Autowired 是 Spring 提供的注解； @Resource是J2EE提供的注解，javax.annotation 包下的注解，来自于JSR-250，需要JDK1.6及以上。 2. 注入方式 @Autowired只按照Type 注入； @Resource 默认按Name自动注入，也提供按照Type 注入； 3. 属性 @Autowired注解可用于为类的属性、构造器、方法进行注值。 默认情况下，其依赖的对象必须存在(bean可用)，如果需要改变这种默认方式，可以设置其 required 属性为false。 @Autowired注解默认按照类型装配，如果容器中包含多个同一类型的Bean，那么启动容器时会报找不到指定类型bean的异常，解决办法是结合 @Qualifier 注解进行限定，指定注入的bean名称。 @Resource有两个中重要的属性：name和type。 name 属性指定 byName，如果没有指定 name 属性: 当注解标注在字段上，即默认取字段的名称作为 bean 名称寻找依赖对象， 当注解标注在属性的setter方法上，即默认取属性名作为bean名称寻找依赖对象。 @Resource如果没有指定name属性，并且按照默认的名称仍然找不到依赖对象时， @Resource注解会回退到按类型装配。但一旦指定了name属性，就只能按名称装配了。 4. 其他 @Autowired注解进行装配容易抛出异常，特别是装配的 bean 类型有多个的时候,解决的办法是增加 @Qualifier 注解进行限定。 @Resource注解的使用性更为灵活，可指定名称，也可以指定类型； ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:2:2","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"3. context:annotation-config与context:component-scan ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:3:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"3.1 context:annotation-config 我们一般在含有 Spring 的项目中，可能会看到配置项中包含这个配置节点 \u003ccontext:annotation-config\u003e 这条配置会向 Spring 容器中注册以下4个 BeanPostProcessor AutowiredAnnotationBeanPostProcessor CommonAnnotationBeanPostProcessor PersistenceAnnotationBeanPostProcessor RequiredAnnotationBeanPostProcessor 注册这4个 BeanPostProcessor 的作用，就是为了你的系统能够识别相应的注解。 如果想使用 @Resource 、@PostConstruct、@PreDestroy等注解就必须声明CommonAnnotationBeanPostProcessor。 如果想使用 @PersistenceContext注解，就必须声明PersistenceAnnotationBeanPostProcessor的Bean。 如果想使用 @Autowired注解，那么就必须声明AutowiredAnnotationBeanPostProcessor的 Bean。 如果想使用 @Required的注解，就必须声明RequiredAnnotationBeanPostProcessor的Bean。 所以如果不加一句context:annotation-config那么上面的这些注解就无法识别。 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:3:1","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"3.2 context:component-scan context:component-scan包括了context:annotation-config的功能，即注册 BeanPostProcessor 使系统能够识别上面的注解。 同时还会自动扫描所配置的包下的 bean。即 扫描包下面有@Controller、@Service、@Repository、@Component这四个注解的类，自动放入 Spring 容器。 所以一般写context:component-scan就行了。 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:3:2","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"3. 实例演示 就拿前面的 student 和 book 举例 实体类这样写,使用注解进行属性注入 public class Student { @Value(value = \"illusory\") private String name; @Value(value = \"23\") private int age; @Autowired private Book book; } public class Book { @Value(value = \"defaultType\") private String type; @Value(value = \"defaultName\") private String name; } 配置文件就不用写各种 property 属性注入了。 \u003cproperty name=\"name\" value=\"illusory\"\u003e\u003c/property\u003e 使用@Autowired后也不用配置引用对象了。 \u003cproperty name=\"book\" ref=\"book\"\u003e\u003c/property\u003e 但是还是需要在 xml配置 bean 的基本信息 \u003cbean id=\"student\" class=\"spring.Student\"\u003e\u003c/bean\u003e \u003cbean id=\"book\" class=\"spring.Book\"\u003e\u003c/bean\u003e \u003ccontext:annotation-config /\u003e 如果在实体类加上@Component注解 @Component(value = \"student\") public class Student { @Value(value = \"illusory\") private String name; @Value(value = \"23\") private int age; @Autowired private Book book; } 就不用在xml中配置bean了,只需要在xml中配置 \u003ccontext:component-scan base-package=\"spring\"/\u003e 系统可以识别到前面的注解，同时还会自动扫描包下的 bean。 这样xml中只要要一行就搞定了。 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:3:3","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"4. 自定义初始化与销毁方法 init-method destroy-method属性对应的注解 @PostConstruct注解，在对象创建后调用 @PreDestroy注解，在对象销毁前调用 @PostConstruct public void init() { System.out.println(\"init\"); } @PreDestroy public void destory() { System.out.println(\"destory\"); } ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:4:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"5. @Component和@Configuration 作为配置类的区别 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:5:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"5.1 概述 @Component和@Configuration都可以作为配置类,但还是有一定差别的。 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Component //看这里！！！ public @interface Configuration { String value() default \"\"; Spring 中新的 Java 配置支持的核心就是 @Configuration 注解的类。 这些类主要包括 @Bean 注解的方法来为 Spring 的 IoC 容器管理的对象定义实例，配置和初始化逻辑。 使用 @Configuration 来注解类表示类可以被 Spring 的 IoC 容器所使用，作为 bean 定义的资源。 @Configuration public class AppConfig { @Bean public MyService myService() { return new MyServiceImpl(); } } 这和 Spring 的 XML 文件中的非常类似 \u003cbeans\u003e \u003cbean id=\"myService\" class=\"com.acme.services.MyServiceImpl\"/\u003e \u003c/beans\u003e ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:5:1","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"5.2 实例演示 @Configuration public static class Config { @Bean public SimpleBean simpleBean() { return new SimpleBean(); } @Bean public SimpleBeanConsumer simpleBeanConsumer() { return new SimpleBeanConsumer(simpleBean()); } } @Component public static class Config { @Bean public SimpleBean simpleBean() { return new SimpleBean(); } @Bean public SimpleBeanConsumer simpleBeanConsumer() { return new SimpleBeanConsumer(simpleBean()); } } 第一个代码正常工作，正如预期的那样，SimpleBeanConsumer 将会得到一个单例 SimpleBean 的链接。 第二个配置是完全错误的，虽然 Spring 会创建一个 SimpleBean 的单例bean，但是 SimpleBeanConsumer 将获得另一个SimpleBean实例（也就是相当于直接调用new SimpleBean() ， 这个bean是不归Spring管理的）。 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:5:2","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"5.3 原因 使用 @Configuration 所有标记为 @Bean的方法将被包装成一个 CGLIB包装器，它的工作方式就好像是这个方法的第一个调用，那么原始方法的主体将被执行，最终的对象将在 Spring上下文中注册。所有进一步的调用只返回从上下文检索的 bean。 public void enhanceConfigurationClasses(ConfigurableListableBeanFactory beanFactory) { Map\u003cString, AbstractBeanDefinition\u003e configBeanDefs = new LinkedHashMap\u003cString, AbstractBeanDefinition\u003e(); for (String beanName : beanFactory.getBeanDefinitionNames()) { BeanDefinition beanDef = beanFactory.getBeanDefinition(beanName); //判断是否被@Configuration标注 if (ConfigurationClassUtils.isFullConfigurationClass(beanDef)) { if (!(beanDef instanceof AbstractBeanDefinition)) { throw new BeanDefinitionStoreException(\"Cannot enhance @Configuration bean definition '\" + beanName + \"' since it is not stored in an AbstractBeanDefinition subclass\"); } else if (logger.isWarnEnabled() \u0026\u0026 beanFactory.containsSingleton(beanName)) { logger.warn(\"Cannot enhance @Configuration bean definition '\" + beanName + \"' since its singleton instance has been created too early. The typical cause \" + \"is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor \" + \"return type: Consider declaring such methods as 'static'.\"); } configBeanDefs.put(beanName, (AbstractBeanDefinition) beanDef); } } if (configBeanDefs.isEmpty()) { // nothing to enhance -\u003e return immediately return; } ConfigurationClassEnhancer enhancer = new ConfigurationClassEnhancer(); for (Map.Entry\u003cString, AbstractBeanDefinition\u003e entry : configBeanDefs.entrySet()) { AbstractBeanDefinition beanDef = entry.getValue(); // If a @Configuration class gets proxied, always proxy the target class beanDef.setAttribute(AutoProxyUtils.PRESERVE_TARGET_CLASS_ATTRIBUTE, Boolean.TRUE); try { // Set enhanced subclass of the user-specified bean class Class\u003c?\u003e configClass = beanDef.resolveBeanClass(this.beanClassLoader); //生成代理的class Class\u003c?\u003e enhancedClass = enhancer.enhance(configClass, this.beanClassLoader); if (configClass != enhancedClass) { if (logger.isDebugEnabled()) { logger.debug(String.format(\"Replacing bean definition '%s' existing class '%s' with \" + \"enhanced class '%s'\", entry.getKey(), configClass.getName(), enhancedClass.getName())); } //替换class，将原来的替换为CGLIB代理的class beanDef.setBeanClass(enhancedClass); } } catch (Throwable ex) { throw new IllegalStateException(\"Cannot load configuration class: \" + beanDef.getBeanClassName(), ex); } } } isFullConfigurationClass代码如下： //是否为配置类 public static boolean isConfigurationCandidate(AnnotationMetadata metadata) { return (isFullConfigurationCandidate(metadata) || isLiteConfigurationCandidate(metadata)); } //是否为完整配置类 public static boolean isFullConfigurationCandidate(AnnotationMetadata metadata) { return metadata.isAnnotated(Configuration.class.getName()); } //是否为精简配置类 public static boolean isLiteConfigurationCandidate(AnnotationMetadata metadata) { // Do not consider an interface or an annotation... if (metadata.isInterface()) { return false; } // Any of the typical annotations found? for (String indicator : candidateIndicators) { if (metadata.isAnnotated(indicator)) { return true; } } // Finally, let's look for @Bean methods... try { return metadata.hasAnnotatedMethods(Bean.class.getName()); } catch (Throwable ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to introspect @Bean methods on class [\" + metadata.getClassName() + \"]: \" + ex); } return false; } } //精简配置类包含的注解 static { candidateIndicators.add(Component.class.getName()); candidateIndicators.add(ComponentScan.class.getName()); candidateIndicators.add(Import.class.getName()); candidateIndicators.add(ImportResource.class.getName()); } ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:5:3","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"6. 参考 https://blog.csdn.net/long476964/article/details/80626930 https://www.jianshu.com/p/89f55286cf21 ","date":"2018-10-05","objectID":"/posts/java/ssm/02-spring-annotation/:6:0","tags":["Spring"],"title":"SSM系列(二)---Spring 常用注解分析","uri":"/posts/java/ssm/02-spring-annotation/"},{"categories":["Java"],"content":"Spring IoC具体流程分析","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"本文主要介绍了 Spring 框架，通过代码演示详细讲述了 Spring IoC 的具体流程。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:0:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"1. 概述 Spring 是一个开源容器框架，可以接管 web 层，业务层，dao 层，持久层的组件，并且可以配置各种bean,和维护 bean 与 bean 之间的关系。其核心就是控制反转(IoC),和面向切面(AOP),简单的说就是一个分层的轻量级开源框架。 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:1:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"2. Spring 中的 IoC IoC：(Inverse of Control )控制反转，容器主动将资源推送给它所管理的组件，组件所做的是选择一种合理的方式接受资源。 简单的理解：把创建对象和维护之间的关系的权利由程序中转移到Spring容器的配置文件中。 DI : (Dependency Injection) 依赖注入，IoC 的另一种表现方式，组件以一种预先定义好的方式来接受容器注入的资源。 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:2:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"3. IoC 例子 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:3:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"3.1 xml 配置文件方式 1. 准备 bean 对象 先准备两个简单的实体类 Student 和 Book，需要提供Getter/Setter 和有参数无参构造方法等。 Spring Bean是事物处理组件类和实体类（POJO）对象的总称，Spring Bean 被Spring IoC 容器初始化，装配和管理。 /** * @author illusory * @version 1.0.0 * @date 2019/4/18 0018 */ public class Student { private String name; private int age; private Book book; //省略Getter/Setter和构造方法 } /** * @author illusory * @version 1.0.0 * @date 2019/4/18 */ public class Book { private String type; private String name; //省略Getter/Setter和构造方法 } 2. 将 Bean 类添加到 Spring IoC 容器 将 Bean 类添加到 Spring IoC 容器有三种方式。 一种方式是基于XML的配置文件； 一种方式是基于注解的配置； 一种方式是基于 Java 的配置。 1. Spring Bean 类的配置项 Spring IoC 容器管理 Bean 时，需要了解 Bean 的类名、名称、依赖项、属性、生命周期及作用域等信息。为此，Spring IoC 提供了一系列配置项，用于 Bean 在 IoC 容器中的定义。 ① class 该配置项是强制项，用于指定创建 Bean 实例的 Bean 类的路径。 ② name 该配置项是强制项，用于指定 Bean 唯一的标识符，在基于 XML 的配置项中，可以使用 id和或 name 属性来指定 Bean 唯一标识符。 ③ scope 该配置项是可选项，用于设定创建 Bean 对象的作用域。 ④ constructor-arg 该配置项是可选项，用于指定通过构造函数注入依赖数据到 Bean。 ⑤ properties 该配置项是可选项，用于指定通过 set 方法注入依赖数据到 Bean。 ⑥ autowiring mode 该配置项是可选项，用于指定通过自动依赖方法注入依赖数据到 Bean。 ⑦ lazy-initialization mode 该配置项是可选项，用于指定 IoC 容器延迟创建 Bean，在用户请求时创建 Bean，而不要在启动时就创建 Bean。 ⑧ initialization 该配置项是可选项，用于指定 IoC 容器完成 Bean 必要的创建后，调用 Bean 类提供的回调方法对 Bean 实例进一步处理。 ⑨ destruction 该配置项是可选项，用于指定 IoC 容器在销毁 Bean 时，调用 Bean 类提供的回调方法。 2. Spring xml 配置文件 下面主要介绍基于XML的配置方式，基于注解和基于Java的配置放在后面进行讨论，放在后面讨论的原因是一些其它重要的Spring概念还需要掌握。 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cbeans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"\u003e \u003c!-- bean的配置文件 --\u003e \u003cbean id=\"student\" class=\"spring.Student\"\u003e \u003cproperty name=\"name\" value=\"illusory\"\u003e\u003c/property\u003e \u003cproperty name=\"age\" value=\"23\"\u003e\u003c/property\u003e \u003cproperty name=\"book\" ref=\"book\"\u003e\u003c/property\u003e \u003c/bean\u003e \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003cproperty name=\"name\" value=\"think in java\"\u003e\u003c/property\u003e \u003cproperty name=\"type\" value=\"CS\"\u003e\u003c/property\u003e \u003c/bean\u003e \u003c/beans\u003e ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:3:1","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"3.2 注解方式 使用注解需要在xml配置文件中开启组件扫描 \u003c!--配置组件扫描--\u003e \u003ccontext:component-scan base-package=\"spring\"/\u003e 1.定义 bean 定义一个 bean 实体类或组件 @Component(value = \"book\") public class Book { private String type; private String name; //省略Getter/Setter和构造方法 } 2. 配置 bean 基本配置 @Component(value = \"book\") public class Book { private String type; private String name; //省略Getter/Setter和构造方法 } 其中@Component(value = \"book\")相当于\u003cbean id=\"book\" class=\"spring.Book\"\u003e Bean实例的名称默认是Bean类的首字母小写，其他部分不变 属性注入 普通类型注入: @Value(value = \"illusory\") 引用类型注入: @Autowired/@Resources(name=\"\") @Component(value = \"student\") public class Student { @Value(value = \"illusory\") private String name; @Value(value = \"23\") private int age; @Autowired private Book book; } @Component(value = \"book\") public class Book { @Value(value = \"defaultType\") private String type; @Value(value = \"defaultName\") private String name; } 3. 获取 bean public static void main(String[] args) { // 根据配置文件创建 IoC 容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); // 从容器中获取 bean 实例 Student student = (Student) ac.getBean(\"student\"); // 使用bean System.out.println(student.getName()); //成功打印出 illusory } ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:3:2","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"3.3 测试 /** * @author illusory * @version 1.0.0 * @date 2019/4/18 */ public class SpringTest { public static void main(String[] args) { // 根据配置文件创建 IoC 容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); // 从容器中获取 bean 实例 这里的名称就是配置文件中的id=\"student\" Student student = (Student) ac.getBean(\"student\"); // 使用bean System.out.println(student.getName()); //成功打印出 illusory } } ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:3:3","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"4. 简要分析 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:4:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"4.1 创建Spring IoC 容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext .xml\") 执行这句代码时 Spring 容器对象被创建，同时 applicationContext .xml中配置的 bean 就会被创建到内存中。 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:4:1","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"4.2 Bean注入 Bean 注入的方式有两种； 一种是在XML中配置，此时分别有属性注入、构造函数注入和工厂方法注入； 另一种则是使用注解的方式注入: @Autowired、@Resource、@Required。 1. 在xml文件中配置依赖注入 属性注入 属性注入即通过setXxx()方法注入Bean的属性值或依赖对象，属性注入要求Bean提供一个默认的构造函数，并为需要注入的属性提供对应的Setter方法。Spring先调用Bean的默认构造函数实例化Bean对象，然后通过反射的方式调用Setter方法注入属性值。 由于属性注入方式具有可选择性和灵活性高的优点，因此属性注入是实际应用中最常采用的注入方式。 \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003cproperty name=\"name\" value=\"think in java\"\u003e\u003c/property\u003e \u003cproperty name=\"type\" value=\"CS\"\u003e\u003c/property\u003e \u003c/bean\u003e 例子中的这个就是属性注入。 构造方法注入 使用构造函数注入的前提是 Bean必须提供带参数的构造函数。 \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003cconstructor-arg name=\"name\" value=\"think in java\"\u003e\u003c/constructor-arg\u003e \u003cconstructor-arg name=\"type\" value=\"CS\"\u003e\u003c/constructor-arg\u003e \u003c/bean\u003e 工厂方法注入 有时候 bean 对象不能直接 new，只能通过工厂方法创建。 /** * @author illusory * @version 1.0.0 * @date 2019/4/18 0018 */ public class BookFactory { //非静态方法 public Book createBook() { Book book = new Book(); book.setName(\"图解HTTP\"); book.setType(\"HTTP\"); return book; } //静态方法 public static Book createBookStatic() { Book book = new Book(); book.setName(\"大话数据结构\"); book.setType(\"数据结构\"); return book; } } 非静态方法：必须实例化工厂类（factory-bean）后才能调用工厂方法 \u003cbean id=\"bookFactory\" class=\"spring.BookFactory\"\u003e\u003c/bean\u003e \u003cbean id=\"book\" class=\"spring.Book\" factory-bean=\"bookFactory\" factory-method=\"createBook\"\u003e\u003c/bean\u003e 静态方法：不用实例化工厂类（factory-bean）后才能调用工厂方法 \u003cbean id=\"book\" class=\"spring.Book\" factory-method=\"createBookStatic\"\u003e\u003c/bean\u003e ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:4:2","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"4.3 获取 bean 实例 接着通过从 Spring 容器中根据名字获取对应的 bean 。 Student student = (Student) ac.getBean(\"student\"); ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:4:3","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"5. 小结 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:5:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"5.1 大致流程 1.定义bean：定义一个 bean 实体类或组件 2.配置 bean 基本配置 xml 配置文件中注册这个 bean 属性注入 xml 配置文件中为这个 bean 注入属性 XML中配置 : 属性注入、构造方法注入、工厂方法注入 注解方式 : @Autowired、@Resource、@Required 3.获取 bean 实例：根据 name(即配置文件中的 bean id) 从 Spring 容器中获取 bean 实例 ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:5:1","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"5.2 具体代码 1. 定义 bean 定义一个 bean 实体类或组件 public class Book { private String type; private String name; //省略Getter/Setter和构造方法 } 2. 配置 bean 基本配置 \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003c/bean\u003e 属性注入 \u003cbean id=\"book\" class=\"spring.Book\"\u003e \u003cproperty name=\"name\" value=\"think in java\"\u003e\u003c/property\u003e \u003cproperty name=\"type\" value=\"CS\"\u003e\u003c/property\u003e \u003c/bean\u003e 3. 获取 bean // 根据配置文件创建 IoC 容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); // 从容器中获取 bean 实例 Student student = (Student) ac.getBean(\"student\"); ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:5:2","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"6. 参考 https://blog.csdn.net/u010648555/article/details/76299467 https://www.cnblogs.com/_popc/p/3972212.html https://www.cnblogs.com/wnlja/p/3907836.html ","date":"2018-09-30","objectID":"/posts/java/ssm/01-spring-ioc/:6:0","tags":["Spring"],"title":"SSM系列(一)---Spring IoC 分析","uri":"/posts/java/ssm/01-spring-ioc/"},{"categories":["Java"],"content":"通过实例源码与反编译详细分析Java中为什么内部类可以访问外部类的成员","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"本文主要通过实例源码与反编译详细分析了Java中为什么内部类可以访问外部类的成员。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:0:0","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"1. 概述 内部类就是定义在一个类内部的类。 定义在类内部的类有两种情况： 一种是被static关键字修饰的， 叫做静态内部类. 另一种是不被static关键字修饰的， 就是普通内部类。 注：在下文中所提到的内部类都是指这种不被static关键字修饰的普通内部类 静态内部类虽然也定义在外部类的里面， 但是它只是在形式上（写法上）和外部类有关系， 其实在逻辑上和外部类并没有直接的关系。而一般的内部类，不仅在形式上和外部类有关系（写在外部类的里面）， 在逻辑上也和外部类有联系。 这种逻辑上的关系可以总结为以下两点： 1.内部类对象的创建依赖于外部类对象 2.内部类对象持有指向外部类对象的引用。 上边的第二条可以解释为什么在内部类中可以访问外部类的成员。就是因为内部类对象持有外部类对象的引用。 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:1:0","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"2. 测试 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:2:0","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"2.1 测试类 public class Outer { int outerField = 0; class Inner{ void InnerMethod(){ int i = outerField; } } } 虽然这两个类写在同一个文件中， 但是编译完成后， 还是生成各自的class文件： ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:2:1","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"2.2 反编译 这里我们的目的是探究内部类的行为， 所以只反编译内部类的 class 文件 Outer$Inner.class 。 在命令行中， 切换到工程的bin目录， 输入以下命令反编译这个类文件： javap -classpath . -v Outer$Inner -classpath . : 说明在当前目录下寻找要反编译的class文件 -v : 加上这个参数输出的信息比较全面。包括常量池和方法内的局部变量表， 行号， 访问标志等等。 D:\\lillusory\\Java\\work_idea\\java-learning\\target\\classes\\jvm\\innerclass\u003ejavap -c lasspath . -v Outer$Inner 警告: 二进制文件Outer$Inner包含jvm.innerclass.Outer$Inner Classfile /D:/lillusory/Java/work_idea/java-learning/target/classes/jvm/innercla ss/Outer$Inner.class Last modified 2019-4-29; size 596 bytes MD5 checksum 1c7365a21e81dd01b3c6b115c1a72484 Compiled from \"Outer.java\" \u003c!--类信息--\u003e class jvm.innerclass.Outer$Inner minor version: 0 major version: 52 flags: ACC_SUPER \u003c!--常量池--\u003e Constant pool: #1 = Fieldref #4.#24 // jvm/innerclass/Outer$Inner.this$0:L jvm/innerclass/Outer; #2 = Methodref #5.#25 // java/lang/Object.\"\u003cinit\u003e\":()V #3 = Fieldref #26.#27 // jvm/innerclass/Outer.outerField:I #4 = Class #28 // jvm/innerclass/Outer$Inner #5 = Class #29 // java/lang/Object #6 = Utf8 this$0 #7 = Utf8 Ljvm/innerclass/Outer; #8 = Utf8 \u003cinit\u003e #9 = Utf8 (Ljvm/innerclass/Outer;)V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 LocalVariableTable #13 = Utf8 this #14 = Utf8 Inner #15 = Utf8 InnerClasses #16 = Utf8 Ljvm/innerclass/Outer$Inner; #17 = Utf8 MethodParameters #18 = Utf8 InnerMethod #19 = Utf8 ()V #20 = Utf8 i #21 = Utf8 I #22 = Utf8 SourceFile #23 = Utf8 Outer.java #24 = NameAndType #6:#7 // this$0:Ljvm/innerclass/Outer; #25 = NameAndType #8:#19 // \"\u003cinit\u003e\":()V #26 = Class #30 // jvm/innerclass/Outer #27 = NameAndType #31:#21 // outerField:I #28 = Utf8 jvm/innerclass/Outer$Inner #29 = Utf8 java/lang/Object #30 = Utf8 jvm/innerclass/Outer #31 = Utf8 outerField \u003c!--从这里开始看--\u003e { final jvm.innerclass.Outer this$0; descriptor: Ljvm/innerclass/Outer; flags: ACC_FINAL, ACC_SYNTHETIC jvm.innerclass.Outer$Inner(jvm.innerclass.Outer); descriptor: (Ljvm/innerclass/Outer;)V flags: Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ljvm/innerclass/Ou ter; 5: aload_0 6: invokespecial #2 // Method java/lang/Object.\"\u003cinit\u003e \":()V 9: return LineNumberTable: line 9: 0 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this Ljvm/innerclass/Outer$Inner; 0 10 1 this$0 Ljvm/innerclass/Outer; MethodParameters: Name Flags this$0 final mandated void InnerMethod(); descriptor: ()V flags: Code: stack=1, locals=2, args_size=1 0: aload_0 1: getfield #1 // Field this$0:Ljvm/innerclass/Ou ter; 4: getfield #3 // Field jvm/innerclass/Outer.oute rField:I 7: istore_1 8: return LineNumberTable: line 11: 0 line 12: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Ljvm/innerclass/Outer$Inner; 8 1 1 i I } SourceFile: \"Outer.java\" InnerClasses: #14= #4 of #26; //Inner=class jvm/innerclass/Outer$Inner of class jvm/inner class/Outer ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:2:2","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"2.3 解析 暂时不看常量池等其他信息，从50行开始，可以看到第一行信息如下： final jvm.innerclass.Outer this$0; 这句话的意思是， 在内部类Outer$Inner中， 存在一个名字为this$0 ， 类型为jvm.innerclass.Outer的成员变量， 并且这个变量是final的。 其实这个就是所谓的“在内部类对象中存在的指向外部类对象的引用”。 但是我们在定义这个内部类的时候， 并没有声明它， 所以这个成员变量是编译器加上的。 虽然编译器在创建内部类时为它加上了一个指向外部类的引用， 但是这个引用是怎样赋值的呢？毕竟必须先给他赋值，它才能指向外部类对象。 下面我们把注意力转移到构造函数上,下面这段输出是关于构造函数的信息: jvm.innerclass.Outer$Inner(jvm.innerclass.Outer); descriptor: (Ljvm/innerclass/Outer;)V flags: Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ljvm/innerclass/Outer; 5: aload_0 6: invokespecial #2 // Method java/lang/Object.\"\u003cinit\u003e\":()V 9: return LineNumberTable: line 9: 0 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this Ljvm/innerclass/Outer$Inner; 0 10 1 this$0 Ljvm/innerclass/Outer; 我们知道， 如果在一个类中， 不声明构造方法的话， 编译器会默认添加一个无参数的构造方法。 但是这句话在这里就行不通了， 因为我们明明看到， 这个构造函数有一个构造方法， 并且类型为Outer。 所以说，编译器会为内部类的构造方法添加一个参数， 参数的类型就是外部类的类型。 下面我们看看在构造参数中如何使用这个默认添加的参数。 我们来分析一下构造方法的字节码。 下面是每行字节码的意义： aload_0 ： 将局部变量表中的第一个引用变量加载到操作数栈。 这里有几点需要说明。 1.局部变量表中的变量在方法执行前就已经初始化完成； 2.局部变量表中的变量包括方法的参数； 3.操作数栈就是执行当前代码的栈； 4.成员方法的局部变量表中的第一个变量永远是this； 所以这句话的意思是： 将this引用从局部变量表加载到操作数栈。 aload_1： 将局部变量表中的第二个引用变量加载到操作数栈。 这里加载的变量就是构造方法中的Outer类型的参数 . putfield #1 // Field this$0:Ljvm/innerclass/Outer; 使用操作数栈顶端的引用变量为指定的成员变量赋值。 这里的意思是将外面传入的Outer类型的参数赋给成员变量this$0 。 这一句putfield字节码就揭示了， 指向外部类对象的这个引用变量是如何赋值的。 后面几句如下： 5: aload_0 6: invokespecial #2 // Method java/lang/Object.\"\u003cinit\u003e\":()V 9: return 大致就是使用this引用调用父类（Object）的构造方法然后返回。 这也印证了上面所说的内部类和外部类逻辑关系的第一条： 内部类对象的创建依赖于外部类对象。 在内部类的 InnerMethod 方法中， 访问了外部类的成员变量 outerField， 下面的字节码揭示了访问是如何进行的： void InnerMethod(); descriptor: ()V flags: Code: stack=1, locals=2, args_size=1 0: aload_0 1: getfield #1 // Field this$0:Ljvm/innerclass/Outer; 4: getfield #3 // Field jvm/innerclass/Outer.outerField:I 7: istore_1 8: return LineNumberTable: line 11: 0 line 12: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Ljvm/innerclass/Outer$Inner; 8 1 1 i I getfield #1 // Field this$0:Ljvm/innerclass/Outer; 将成员变量this$0加载到操作数栈上来 getfield #3 // Field jvm/innerclass/Outer.outerField:I 使用上面加载的this$0引用， 将外部类的成员变量outerField加载到操作数栈 istore_1 将操作数栈顶端的int类型的值保存到局部变量表中的第二个变量上（注意， 第一个局部变量被 this 占用， 第二个局部变量是 i）。操作数栈顶端的 int 型变量就是上一步加载的 outerField 变量。 所以， 这句字节码的含义就是： 使用outerField为i赋值。 上面三步就是内部类中是如何通过指向外部类对象的引用， 来访问外部类成员的。 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:2:3","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"3. 总结 本文通过反编译内部类的字节码， 说明了内部类是如何访问外部类对象的成员的，除此之外， 我们也对编译器的行为有了一些了解， 编译器在编译时会自动加上一些逻辑， 这正是我们感觉困惑的原因。 关于内部类如何访问外部类的成员， 分析之后其实也很简单， 主要是通过以下几步做到的： 1.编译器自动为内部类添加一个成员变量， 这个成员变量的类型和外部类的类型相同， 这个成员变量就是指向外部类对象的引用； 2.编译器自动为内部类的构造方法添加一个参数， 参数的类型是外部类的类型， 在构造方法内部使用这个参数为1中添加的成员变量赋值； 3.在调用内部类的构造函数初始化内部类对象时， 会默认传入外部类的引用。 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:3:0","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"4. 参考 https://blog.csdn.net/weixin_39214481/article/details/80372676 ","date":"2018-08-18","objectID":"/posts/java/04-java-innerclass-outerclass-membervariable/:4:0","tags":["Java"],"title":"Java中为什么内部类可以访问外部类的成员","uri":"/posts/java/04-java-innerclass-outerclass-membervariable/"},{"categories":["Java"],"content":"为什么方法内定义的内部类可以访问方法中的局部变量 通过反编译分析其中的原理","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"本文主要通过实例代码分析了 Java 中为什么方法内定义的内部类可以访问方法中的局部变量。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:0:0","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"1. 概述 匿名内部类和非匿名内部类。 在平时写代码的过程中， 我们经常会写类似下面的代码段： public class Test { public static void main(String[] args) { final int count = 0; new Thread(){ public void run() { int var = count; }; }.start(); } } 这段代码在 main 方法中定义了一个匿名内部类， 并且创建了匿名内部类的一个对象， 使用这个对象调用了匿名内部类中的方法。 所有这些操作都在new Thread(){}.start() 这一句代码中完成， 这不禁让人感叹 Java 的表达能力还是很强的。 上面的代码和以下代码等价： public class Test { public static void main(String[] args) { final int count = 0; //在方法中定义一个内部类 class MyThread extends Thread{ public void run() { int var = count; } } new MyThread().start(); } } 这里我们不关心方法中匿名内部类和非匿名内部类的区别， 我们只需要知道， 这两种方式都是定义在方法中的内部类， 他们的工作原理是相同的。 在本文中主要根据非匿名内部类讲解。 让我们仔细观察上面的代码都有哪些“奇怪”的行为： 1.在外部类的 main 方法中有一个局部变量 count， 并且在内部类的 run 方法中访问了这个 count 变量。 也就是说， 方法中定义的内部类， 可以访问方法中的局部变量（方法的参数也是局部变量）； 2.count 变量使用 final 关键字修饰， 如果去掉 final， 则编译失败。 也就是说被方法中的内部类访问的局部变量必须是final的。 由于我们经常这样做， 这样写代码， 久而久之养成了习惯， 就成了司空见惯的做法了。 但是如果要问为什么Java支持这样的做法， 恐怕很少有人能说的出来。 在下面， 我们就会分析为什么Java支持这种做法， 让我们不仅知其然， 还要知其所以然。 为什么定义在方法中的内部类可以访问方法中的局部变量？ ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:1:0","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"2. 原理分析 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:2:0","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"2.1 当被访问的局部变量是编译时可确定的字面常量时 我们首先看这样一段代码， 本文的以下部分会以这样的代码进行讲解。 public class Outer { void outerMethod(){ final String localVar = \"abc\"; /*定义在方法中的内部类*/ class Inner{ void innerMethod(){ String a = localVar; } } } } 在外部类的方法 outerMethod 中定义了成员变量 String localVar， 并且用一个编译时字面量 “abc” 给他赋值。在 outerMethod 方法中定义了内部类 Inner， 并且在内部类的方法 innerMethod 中访问了 localVar 变量。 接下来我们就根据这个例子来讲解为什么可以这样做。 首先看编译后的文件， 和普通的内部类一样， 定义在方法中的内部类在编译之后， 也有自己独立的 class 文件： 1. 反编译 执行以下命令反编译该文件 javap -classpath . -v Outer$1Inner -classpath . : 说明在当前目录下寻找要反编译的class文件 -v : 加上这个参数输出的信息比较全面。包括常量池和方法内的局部变量表， 行号， 访问标志等等。 2. 结果分析 D:\\lillusory\\Java\\work_idea\\java-learning\\target\\classes\\jvm\\localfiled\u003ejavap -c lasspath . -v Outer$1Inner Classfile /D:/lillusory/Java/work_idea/java-learning/target/classes/jvm/localfil ed/Outer$1Inner.class Last modified 2019-4-29; size 643 bytes MD5 checksum 12cea9ab1340856585960146078de1b3 Compiled from \"Outer.java\" \u003c!--版本号等信息--\u003e class jvm.localfiled.Outer$1Inner minor version: 0 major version: 52 flags: ACC_SUPER \u003c!--常量池--\u003e Constant pool: #1 = Fieldref #4.#27 // jvm/localfiled/Outer$1Inner.this$0: Ljvm/localfiled/Outer; #2 = Methodref #5.#28 // java/lang/Object.\"\u003cinit\u003e\":()V #3 = String #29 // abc #4 = Class #30 // jvm/localfiled/Outer$1Inner #5 = Class #31 // java/lang/Object #6 = Utf8 this$0 #7 = Utf8 Ljvm/localfiled/Outer; #8 = Utf8 \u003cinit\u003e #9 = Utf8 (Ljvm/localfiled/Outer;)V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 LocalVariableTable #13 = Utf8 this #14 = Utf8 Inner #15 = Utf8 InnerClasses #16 = Utf8 Ljvm/localfiled/Outer$1Inner; #17 = Utf8 MethodParameters #18 = Utf8 innerMethod #19 = Utf8 ()V #20 = Utf8 a #21 = Utf8 Ljava/lang/String; #22 = Utf8 SourceFile #23 = Utf8 Outer.java #24 = Utf8 EnclosingMethod #25 = Class #32 // jvm/localfiled/Outer #26 = NameAndType #33:#19 // outerMethod:()V #27 = NameAndType #6:#7 // this$0:Ljvm/localfiled/Outer; #28 = NameAndType #8:#19 // \"\u003cinit\u003e\":()V #29 = Utf8 abc #30 = Utf8 jvm/localfiled/Outer$1Inner #31 = Utf8 java/lang/Object #32 = Utf8 jvm/localfiled/Outer #33 = Utf8 outerMethod \u003c!--从这里开始看--\u003e { final jvm.localfiled.Outer this$0; descriptor: Ljvm/localfiled/Outer; flags: ACC_FINAL, ACC_SYNTHETIC jvm.localfiled.Outer$1Inner(jvm.localfiled.Outer); descriptor: (Ljvm/localfiled/Outer;)V flags: Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ljvm/localfiled/Ou ter; 5: aload_0 6: invokespecial #2 // Method java/lang/Object.\"\u003cinit\u003e \":()V 9: return LineNumberTable: line 11: 0 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this Ljvm/localfiled/Outer$1Inner; 0 10 1 this$0 Ljvm/localfiled/Outer; MethodParameters: Name Flags this$0 final mandated void innerMethod(); descriptor: ()V flags: Code: stack=1, locals=2, args_size=1 0: ldc #3 // String abc 2: astore_1 3: return LineNumberTable: line 13: 0 line 14: 3 LocalVariableTable: Start Length Slot Name Signature 0 4 0 this Ljvm/localfiled/Outer$1Inner; 3 1 1 a Ljava/lang/String; } SourceFile: \"Outer.java\" EnclosingMethod: #25.#26 // jvm.localfiled.Outer.outerMethod InnerClasses: #14= #4; //Inner=class jvm/localfiled/Outer$1Inner 其中 InnerMethod 相关如下： void innerMethod(); descriptor: ()V flags: Code: stack=1, locals=2, args_size=1 0: ldc #3 // String abc 2: astore_1 3: return LineNumberTable: line 13: 0 line 14: 3 LocalVariableTable: Start Length Slot Name Signature 0 4 0 this Ljvm/localfiled/Outer$1Inner; 3 1 1 a Ljava/lang/String; ldc #3 // String abc Idc 指令的意思是将索引指向的常量池中的项压入操作数栈。 这里的索引为3 ， 引用的常量池中的项为字符串“abc” 。 这句话就揭示了内部类访问方法局部变量的原理。 让我们从常量池第3项看起。 #3 = String #29 // abc 但是这个字符串 “abc” 明明是定义在外部类 Outer 中的， 因为出现在外部类的 outerMethod 方法中。 为了查看这个 “abc” 是否在外部类中， 我们继续反编译外部类 Outer.class 。 3. 反编译外部类 反编译外部类结果如下： D:\\lillusory\\Java\\work_idea\\java-learning\\target\\classes\\jvm\\localfiled\u003ejavap -c lasspath . -v Outer.class Classfile /D:/lillusory/Java/work_idea/java-learning/target/classes/jvm/localfil ed/Outer.class Last modified 2019-4-29; size 471 bytes MD5 checksum 4442fd25b31a0563253f16e275643d11 Compiled from \"Outer.jav","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:2:1","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"2.2 当被访问的局部变量的值在编译时不可确定时 那么当方法中定义的内部类访问的局部变量不是编译时可确定的字面常量， 又会怎么样呢？想要让这个局部变量变成编译时不可确定的， 只需要将源码修改如下： public class Outer { void outerMethod(){ final String localVar = getString(); /*定义在方法中的内部类*/ class Inner{ void innerMethod(){ String a = localVar; } } new Inner(); } String getString(){ return \"illusory\"; } } 由于使用 getString 方法的返回值为 localVar 赋值， 所以在编译时期， 编译器不可确定 localVar 的值， 必须在运行时执行了 getString 方法之后才能确定它的值。 既然编译时不不可确定， 那么像上面那样的处理就行不通了。 1. 反编译 执行以下命令反编译该文件 javap -classpath . -v Outer$1Inner -classpath . : 说明在当前目录下寻找要反编译的class文件 -v : 加上这个参数输出的信息比较全面。包括常量池和方法内的局部变量表， 行号， 访问标志等等。 那么在这种情况下， 内部类是通过什么机制访问方法中的局部变量的呢？ 让我们继续反编译内部类的字节码： 2. 结果分析 D:\\lillusory\\Java\\work_idea\\java-learning\\target\\classes\\jvm\\localfiled\u003ejavap -c lasspath . -v Outer$1Inner 警告: 二进制文件Outer$1Inner包含jvm.localfiled.Outer$1Inner Classfile /D:/lillusory/Java/work_idea/java-learning/target/classes/jvm/localfil ed/Outer$1Inner.class Last modified 2019-4-29; size 716 bytes MD5 checksum e63d82ebc8752469f0d30edde17e88a5 Compiled from \"Outer.java\" class jvm.localfiled.Outer$1Inner minor version: 0 major version: 52 flags: ACC_SUPER Constant pool: #1 = Fieldref #4.#29 // jvm/localfiled/Outer$1Inner.this$0: Ljvm/localfiled/Outer; #2 = Fieldref #4.#30 // jvm/localfiled/Outer$1Inner.val$loc alVar:Ljava/lang/String; #3 = Methodref #5.#31 // java/lang/Object.\"\u003cinit\u003e\":()V #4 = Class #32 // jvm/localfiled/Outer$1Inner #5 = Class #33 // java/lang/Object #6 = Utf8 val$localVar #7 = Utf8 Ljava/lang/String; #8 = Utf8 this$0 #9 = Utf8 Ljvm/localfiled/Outer; #10 = Utf8 \u003cinit\u003e #11 = Utf8 (Ljvm/localfiled/Outer;Ljava/lang/String;)V #12 = Utf8 Code #13 = Utf8 LineNumberTable #14 = Utf8 LocalVariableTable #15 = Utf8 this #16 = Utf8 Inner #17 = Utf8 InnerClasses #18 = Utf8 Ljvm/localfiled/Outer$1Inner; #19 = Utf8 MethodParameters #20 = Utf8 Signature #21 = Utf8 ()V #22 = Utf8 innerMethod #23 = Utf8 a #24 = Utf8 SourceFile #25 = Utf8 Outer.java #26 = Utf8 EnclosingMethod #27 = Class #34 // jvm/localfiled/Outer #28 = NameAndType #35:#21 // outerMethod:()V #29 = NameAndType #8:#9 // this$0:Ljvm/localfiled/Outer; #30 = NameAndType #6:#7 // val$localVar:Ljava/lang/String; #31 = NameAndType #10:#21 // \"\u003cinit\u003e\":()V #32 = Utf8 jvm/localfiled/Outer$1Inner #33 = Utf8 java/lang/Object #34 = Utf8 jvm/localfiled/Outer #35 = Utf8 outerMethod { final java.lang.String val$localVar; descriptor: Ljava/lang/String; flags: ACC_FINAL, ACC_SYNTHETIC final jvm.localfiled.Outer this$0; descriptor: Ljvm/localfiled/Outer; flags: ACC_FINAL, ACC_SYNTHETIC jvm.localfiled.Outer$1Inner(); descriptor: (Ljvm/localfiled/Outer;Ljava/lang/String;)V flags: Code: stack=2, locals=3, args_size=3 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ljvm/localfiled/Ou ter; 5: aload_0 6: aload_2 7: putfield #2 // Field val$localVar:Ljava/lang/S tring; 10: aload_0 11: invokespecial #3 // Method java/lang/Object.\"\u003cinit\u003e \":()V 14: return LineNumberTable: line 12: 0 LocalVariableTable: Start Length Slot Name Signature 0 15 0 this Ljvm/localfiled/Outer$1Inner; 0 15 1 this$0 Ljvm/localfiled/Outer; MethodParameters: Name Flags this$0 final mandated val$localVar final synthetic Signature: #21 // ()V void innerMethod(); descriptor: ()V flags: Code: stack=1, locals=2, args_size=1 0: aload_0 1: getfield #2 // Field val$localVar:Ljava/lang/S tring; 4: astore_1 5: return LineNumberTable: line 14: 0 line 15: 5 LocalVariableTable: Start Length Slot Name Signature 0 6 0 this Ljvm/localfiled/Outer$1Inner; 5 1 1 a Ljava/lang/String; } SourceFile: \"Outer.java\" EnclosingMethod: #27.#28 // jvm.localfiled.Outer.outerMethod InnerClasses: #16= #4; //Inner=class jvm/localfiled/Outer$1Inner 首先来看它的构造方法。 方法的签名为： jvm.localfiled.Outer$1Inner(); descriptor: (Ljvm/localfiled/Outer;Ljava/lang/String;)V flags: Code: 我们知道， 如果不定义构造方法， 那么编译器会为这个类自动生成一个无参数的构造方法。 这个说法在这里就行不通了， 因为我们看到， 这个内部类的构造方法又两个参数。 至于第一个参数， 是指向外部类对象的引用， 在前面一篇博客中已经详细的介绍过了， 不明白的可以先看上一篇博客， 这里就不再重复叙述。这也说明了方法中的内部类和类中定义的内部类有相同的地方， 既然他们都是内部类， 就都持有指向外部类对象的引用。 我们来分析第二个参数， 他是 String 类型的， 和在内部类中访问的局部变量 localVar 的类型相同","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:2:2","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"3. 为什么局部变量必须是final的 上面我们讲解了， 方法中的内部类访问方法局部变量是怎么实现的。 那么为什么这个局部变量必须是 final 的呢？ 我认为有以下两个原因： ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:3:0","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"1. 原因一 当局部变量的值为编译时可确定的字面常量时（ 如字符串 “abc” 或整数1 ）， 通过 final 修饰， 可以实现类似 C 语言的编译时宏替换功能。 这样的话， 外部类和内部类各自访问自己的常量池， 各自执行各自的字节码指令， 看起来就像共同访问外部类方法中的局部变量， 这样就可以达到语义上的一致性。 由于存在内部类和外部类中的常量值是一样的， 并且是不可改变的，这样就可以达到数值访问的一致性。 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:3:1","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"2. 原因二 当局部变量的值不是可在编译时确定的字面常量时（比如通过方法调用为它赋值）， 这种情况下， 编译器给内部类增加相同类型的成员变量， 并通过构造函数将外部类方法中的局部变量的值赋给这个新增的内部类成员量。 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:3:2","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"3. 基本数据类型 如果这个局部变量是基本数据类型时， 直接拷贝数值给内部类成员变量。这样的话， 内部类和外部类各自访问自己的基本数据类型的变量， 他们的变量值一样， 并且不可修改， 这样就保证了语义上和数值访问上的一致性。 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:3:3","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"4. 引用类型 如果这个局部变量是引用数据类型时， 拷贝外部类方法中的引用值给内部类对象的成员变量， 这样的话， 他们就指向了同一个对象。 由于这两个引用变量指向同一个对象， 所以通过引用访问的对象的数据是一样的， 由于他们都不能再指向其他对象（被 final 修饰）， 所以可以保证内部类和外部类数据访问的一致性。 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:3:4","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"4. 参考 https://blog.csdn.net/zhangjg_blog/article/details/19996629 ","date":"2018-08-17","objectID":"/posts/java/03-java-innerclass-localvariable/:4:0","tags":["Java"],"title":"Java中为什么方法内定义的内部类可以访问方法中的局部变量","uri":"/posts/java/03-java-innerclass-localvariable/"},{"categories":["Java"],"content":"学习SSM框架时的记录","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"本文主要讲了MVC和三层架构的关系，和SSM框架整合教程。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:0:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"1.三层架构 整体分为三层,表现层UI,业务逻辑层BLL,数据访问层DAL. 表现层 Controller 用户界面,负责与用户进行交互 业务逻辑层 Service 具体的业务操作 数据访问层 Dao 对数据库进行操作,为上层提供数据 为了更好的降低各层间的耦合度，在三层架构程序设计中，采用面向抽象编程。即上层对下层的调用，是通过接口实现的。而下层对上层的真正服务提供者，是下层接口的实现类。服务标准（接口）是相同的，服务提供者（实现类）可以更换。这就实现了层间的耦合。 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:1:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"2.MVC MVC全名是Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写 . Model（模型） - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。 View（视图） - 视图代表模型包含的数据的可视化。 Controller（控制器） - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:2:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"3.MVC与三层架构 经典三层架构和MVC的关系？—–\u003e 他们是两个毫无相关的东西 经典三层架构是一种分层思想，将开发模式分为了这三层 MVC是一种设计模式，目的是让HTML代码和业务逻辑代码分开，让代码看起来更加清晰，便于开发 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:3:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"4.SSM框架和三层架构 SSM即SpringMVC、Spring、Mybatis三个框架。它们在三层架构中所处的位置是不同的，即它们在三层架构中的功能各不相同，各司其职。 SpringMVC：作为View层的实现者，完成用户的请求接收功能。SpringMVC的Controller作为整个应用的控制器，完成用户请求的转发及对用户的响应。 MyBatis：作为 Dao层的实现者，完成对数据库的增、删、改、查功能。 Spring：以整个应用大管家的身份出现。整个应用中所有的Bean的生命周期行为，均由Spring来管理。即整个应用中所有对象的创建、初始化、销毁，及对象间关联关系的维护，均由Spring进行管理。 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:4:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.SSM框架配置 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.1 目录 Controller springmvc.xml 包扫描–controller 注解驱动 视图解析器 web.xml DispatcherServlet 监听器 Service applicationContext-service.xml 包扫描–service applicationContext-trans.xml 事务管理器 通知 切面 Dao SqlMapConfig.xml applicationContext-dao.xml dataSource SqlSessionFactory 包扫描–mapper ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:1","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.2 springmvc.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cbeans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\"\u003e \u003c!-- 配置Controller扫描 --\u003e \u003ccontext:component-scan base-package=\"com.lillusory.crm.controller\" /\u003e \u003c!-- 加载属性文件--\u003e \u003ccontext:property-placeholder location=\"classpath:crm.properties\"/\u003e \u003c!-- 配置注解驱动 --\u003e \u003cmvc:annotation-driven /\u003e \u003c!-- 配置视图解析器 --\u003e \u003cbean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"\u003e \u003c!-- 前缀 --\u003e \u003cproperty name=\"prefix\" value=\"/WEB-INF/jsp/\" /\u003e \u003c!-- 后缀 --\u003e \u003cproperty name=\"suffix\" value=\".jsp\" /\u003e \u003c/bean\u003e \u003c/beans\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:2","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.3 applicationContext-dao \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cbeans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd\"\u003e \u003c!-- 配置 读取properties文件 jdbc.properties --\u003e \u003ccontext:property-placeholder location=\"classpath:jdbc.properties\" /\u003e \u003c!-- 配置 数据源 --\u003e \u003cbean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"\u003e \u003cproperty name=\"driverClassName\" value=\"${jdbc.driver}\" /\u003e \u003cproperty name=\"url\" value=\"${jdbc.url}\" /\u003e \u003cproperty name=\"username\" value=\"${jdbc.username}\" /\u003e \u003cproperty name=\"password\" value=\"${jdbc.password}\" /\u003e \u003c/bean\u003e \u003c!-- 配置SqlSessionFactory --\u003e \u003cbean class=\"org.mybatis.spring.SqlSessionFactoryBean\"\u003e \u003c!-- 设置MyBatis核心配置文件 --\u003e \u003cproperty name=\"configLocation\" value=\"classpath:SqlMapConfig.xml\" /\u003e \u003c!-- 设置数据源 --\u003e \u003cproperty name=\"dataSource\" ref=\"dataSource\" /\u003e \u003cproperty name=\"typeAliasesPackage\" value=\"com.lillusory.crm.domain\"\u003e\u003c/property\u003e \u003c/bean\u003e \u003c!-- 配置Mapper扫描 --\u003e \u003cbean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"\u003e \u003c!-- 设置Mapper扫描包 --\u003e \u003cproperty name=\"basePackage\" value=\"com.lillusory.crm.mapper\" /\u003e \u003c/bean\u003e \u003c/beans\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:3","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.4 applicationContext-service.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cbeans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd\"\u003e \u003c!-- 包扫描 --\u003e \u003ccontext:component-scan base-package=\"com.lillusory.crm.service\"/\u003e \u003c/beans\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:4","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.5 applicationContext-trans.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cbeans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd\"\u003e \u003c!-- 事务管理器 --\u003e \u003cbean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"\u003e \u003c!-- 数据源 --\u003e \u003cproperty name=\"dataSource\" ref=\"dataSource\" /\u003e \u003c/bean\u003e \u003c!-- 通知 --\u003e \u003ctx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"\u003e \u003ctx:attributes\u003e \u003c!-- 传播行为 --\u003e \u003ctx:method name=\"save*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"insert*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"add*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"create*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"delete*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"update*\" propagation=\"REQUIRED\" /\u003e \u003ctx:method name=\"find*\" propagation=\"SUPPORTS\" read-only=\"true\" /\u003e \u003ctx:method name=\"select*\" propagation=\"SUPPORTS\" read-only=\"true\" /\u003e \u003ctx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"true\" /\u003e \u003ctx:method name=\"query*\" propagation=\"SUPPORTS\" read-only=\"true\" /\u003e \u003c/tx:attributes\u003e \u003c/tx:advice\u003e \u003c!-- 切面 --\u003e \u003caop:config\u003e \u003caop:advisor advice-ref=\"txAdvice\" pointcut=\"execution(* com.lillusory.crm.service.*.*(..))\" /\u003e \u003c/aop:config\u003e \u003c/beans\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:5","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.6 SqlMapConfig.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"\u003e \u003cconfiguration\u003e \u003c!-- 暂时什么都不用配置 --\u003e \u003c/configuration\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:6","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.7 web.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cweb-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" id=\"WebApp_ID\" version=\"2.5\"\u003e \u003cdisplay-name\u003eDemo-CRM\u003c/display-name\u003e \u003cwelcome-file-list\u003e \u003cwelcome-file\u003eindex.html\u003c/welcome-file\u003e \u003cwelcome-file\u003eindex.htm\u003c/welcome-file\u003e \u003cwelcome-file\u003eindex.jsp\u003c/welcome-file\u003e \u003cwelcome-file\u003edefault.html\u003c/welcome-file\u003e \u003cwelcome-file\u003edefault.htm\u003c/welcome-file\u003e \u003cwelcome-file\u003edefault.jsp\u003c/welcome-file\u003e \u003c/welcome-file-list\u003e \u003c!-- 配置spring --\u003e \u003ccontext-param\u003e \u003cparam-name\u003econtextConfigLocation\u003c/param-name\u003e \u003cparam-value\u003eclasspath:spring/applicationContext-*.xml\u003c/param-value\u003e \u003c/context-param\u003e \u003c!-- 配置监听器加载spring --\u003e \u003clistener\u003e \u003clistener-class\u003eorg.springframework.web.context.ContextLoaderListener\u003c/listener-class\u003e \u003c/listener\u003e \u003c!-- 配置过滤器，解决post的乱码问题 --\u003e \u003cfilter\u003e \u003cfilter-name\u003eencoding\u003c/filter-name\u003e \u003cfilter-class\u003eorg.springframework.web.filter.CharacterEncodingFilter\u003c/filter-class\u003e \u003c/filter\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eencoding\u003c/filter-name\u003e \u003curl-pattern\u003e/\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003c!-- 配置SpringMVC --\u003e \u003cservlet\u003e \u003cservlet-name\u003edemo-crm\u003c/servlet-name\u003e \u003cservlet-class\u003eorg.springframework.web.servlet.DispatcherServlet\u003c/servlet-class\u003e \u003cinit-param\u003e \u003cparam-name\u003econtextConfigLocation\u003c/param-name\u003e \u003cparam-value\u003eclasspath:spring/springmvc.xml\u003c/param-value\u003e \u003c/init-param\u003e \u003c!-- 配置springmvc什么时候启动，参数必须为整数 --\u003e \u003c!-- 如果为0或者大于0，则springMVC随着容器启动而启动 --\u003e \u003c!-- 如果小于0，则在第一次请求进来的时候启动 --\u003e \u003cload-on-startup\u003e1\u003c/load-on-startup\u003e \u003c/servlet\u003e \u003cservlet-mapping\u003e \u003cservlet-name\u003edemo-crm\u003c/servlet-name\u003e \u003c!-- 所有的请求都进入springMVC / 拦截所有除了jsp /* jsp也拦截 --\u003e \u003curl-pattern\u003e*.action\u003c/url-pattern\u003e \u003c/servlet-mapping\u003e \u003c/web-app\u003e ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:7","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"5.8其他常用配置 jdbc jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/kct?characterEncoding=utf-8 jdbc.username=root jdbc.password=root log4j # Global logging configuration log4j.rootLogger=debug, stdout # Console output... log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:5:8","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"参考 https://juejin.im/post/5929259b44d90400642194f3 ","date":"2018-08-15","objectID":"/posts/java/02-mvc-ssm-merge/:6:0","tags":["Java"],"title":"MVC和三层架构及SSM框架整合","uri":"/posts/java/02-mvc-ssm-merge/"},{"categories":["Java"],"content":"Java字符串相关使用及优化方案","date":"2018-08-10","objectID":"/posts/java/01-java-string/","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"本文主要分析了 Java中的 String 字符串相关使用与优化方案，包括 String 类型相加的本质、String 字符串相关编译器优化、 StringBuilder 与 StringBuffer 选择、字符串拼接方法、基本类型转 String 类型等。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:0:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"1. String对象 String 对象是 Java 中重要的数据类型，在大部分情况下我们都会用到 String 对象。其实在 Java 语言中，其设计者也对 String 做了大量的优化工作，这些也是 String 对象的特点，它们就是:不变性，常量池优化和String类的final定义。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:1:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"1.1 不变性 String对象的状态在其被创建之后就不在发生变化。为什么说这点也是 Java 设计者所做的优化，在 Java 中，有一种模式叫不变模式：在一个对象被多线程共享，而且被频繁的访问时，可以省略同步和锁的时间，从而提高性能。而 String 的不变性，可泛化为不变模式。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:1:1","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"1.2 常量池优化 常量池优化指的是什么呢？那就是当两个 String 对象拥有同一个值的时候，他们都只是引用了常量池中的同一个拷贝。所以当程序中某个字符串频繁出现时，这个优化技术就可以节省大幅度的内存空间了。例如： String s1 = \"123\"; String s2 = \"123\"; String s3 = new String(\"123\"); System.out.println(s1==s2); //true System.out.println(s1==s3); //false System.out.println(s1==s3.intern()); //true123456 以上代码中，s1 和 s2 引用的是相同的地址，故而第四行打印出的结果是 true ;而 s3 虽然只与 s1,s2 相等，但是 s3 是通过 new String(“123”) 创建的，重新开辟了内存空间，因引用的地址不同，所以第5行打印出 false ; String 的 intern() 方法返回的是 String 对象在常量池中的引用，所以最后一行打印出 true。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:1:2","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"1.3 final的定义 String 类以 final 进行了修饰，在系统中就不可能有 String 的子类，这一点也是出于对系统安全性的考虑。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:1:3","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"2. 字符串操作中的常见优化方法 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:2:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"2.1 split()方法优化 通常情况下，split() 方法带给我们很大的方便，但是其性能不是很好。建议结合使用 indexOf( )和 subString()方法进行自定义拆分，这样性能会有显著的提高。　","date":"2018-08-10","objectID":"/posts/java/01-java-string/:2:1","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"2.2 String常量的累加操作优化方法 示例代码: String s = \"\"; long sBeginTime = System.currentTimeMillis(); for (int i = 0; i \u003c 100000; i++) { s+=\"s\"; } long sEndTime = System.currentTimeMillis(); System.out.println(\"s拼接100000遍s耗时: \" + (sEndTime - sBeginTime) + \"ms\"); StringBuffer s1 = new StringBuffer(); long s1BeginTime = System.currentTimeMillis(); for (int i = 0; i \u003c 100000; i++) { s1.append(\"s\"); } long s1EndTime = System.currentTimeMillis(); System.out.println(\"s1拼接100000遍s耗时: \" + (s1EndTime - s1BeginTime) + \"ms\"); StringBuilder s2 = new StringBuilder(); long s2BeginTime = System.currentTimeMillis(); for (int i = 0; i \u003c 100000; i++) { s2.append(\"s\"); } long s2EndTime = System.currentTimeMillis(); System.out.println(\"s2拼接100000遍s耗时: \" + (s2EndTime - s2BeginTime) + \"ms\"); 结果如下： s拼接100000遍s耗时: 3426ms s1拼接100000遍s耗时: 3ms s2拼接100000遍s耗时: 1ms 上例所示，使用+号拼接字符串，其效率明显较低，而使用 StringBuffer 和 StringBuilder 的 append() 方法进行拼接，效率是使用+号拼接方式的百倍甚至千倍，而 StringBuffer 的效率比 StringBuilder 低些，这是由于StringBuffer 实现了线程安全，效率较低也是不可避免的。 所以在字符串的累加操作中，建议结合线程问题选择 StringBuffer 或 StringBuilder，应避免使用+号拼接字符串。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:2:2","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"2.3 基本数据类型转化为 String 类型的优化方案 示例代码: Integer num = 0; int loop = 10000000; // 将结果放大10000000倍，以便于观察结果 long beginTime = System.currentTimeMillis(); for (int i = 0; i \u003c loop; i++) { String s = num+\"\"; } long endTime = System.currentTimeMillis(); System.out.println(\"+\\\"\\\"的方式耗时: \" + (endTime - beginTime) + \"ms\"); beginTime = System.currentTimeMillis(); for (int i = 0; i \u003c loop; i++) { String s = String.valueOf(num); } endTime = System.currentTimeMillis(); System.out.println(\"String.valueOf()的方式耗时: \" + (endTime - beginTime) + \"ms\"); beginTime = System.currentTimeMillis(); for (int i = 0; i \u003c loop; i++) { String s = num.toString(); } endTime = System.currentTimeMillis(); System.out.println(\"toString()的方式耗时: \" + (endTime - beginTime) + \"ms\"); 1234567891011121314151617181920212223 以上示例中，String.valueOf() 直接调用了底层的 Integer.toString() 方法，不过其中会先判断是否为空；+”“由StringBuilder 实现，先 new StringBuilder() 然后调用了 append() 方法，最后调用了 toString() 方法返回 String 对象；num.toString() 直接调用了 Integer.toString() 方法。 以下是结果： +\"\"的方式耗时: 120ms String.valueOf()的方式耗时: 31ms toString()的方式耗时: 30ms 所以效率是： num.toString() 方法最快，其次是 String.valueOf(num )，num+”“的方式最满。 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:2:3","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"3. 编译器优化 /** * String类型优化测试 */ private static void StringTest() { String a = \"hello illusory\"; String b = \"hello \" + \"illusory\"; //true System.out.println(a == b); String c = \"hello \"; String d = \"illusory\"; String e = c + d; //false System.out.println(a == e); } Java 中的变量和基本类型的值存放于栈，而 new 出来的对象本身存放于堆内存，指向对象的引用还是放在栈内存。 String b = \"hello \" + \"illusory\"; 两个都是字符串，是固定值 所以编译器会自动优化为 String b = \"hello illusory\"; a、b 都指向常量池中的hello illusory所以 a==b 为 true 由于 String 的不可变性，对其进行操作的效率会大大降低，但对 “+”操作符,编译器也对其进行了优化 String c = \"hello \"; String d = \"illusory\"; String e = c + d; 其中的 String e = c + d 当+号两边存在变量时(两边或任意一边)，在编译期是无法确定其值的，所以要等到运行期再进行处理 Java中对String 的相加其本质是 new 了 StringBuilder 对象进行 append 操作，拼接后调用 toString() 返回 String 对象。 String e = new StringBuilder().append(\"hello \").append(\"illusory\").toString(); StringBuilder的toString方法如下： @Override public String toString() { // Create a copy, don't share the array return new String(value, 0, count); } 所以e是指向new出来的一个 String 对象,而a指向常量池中的对象，a==e 为 false 反编译后如下： D:\\lillusory\\Java\\work_idea\\java-learning\\target\\classes\\jvm\\string\u003ejavap -class path . -v StringTest.class Classfile /D:/lillusory/Java/work_idea/java-learning/target/classes/jvm/string/S tringTest.class Last modified 2019-5-5; size 946 bytes MD5 checksum 2d529fca114cf155ae7c647bfc733150 Compiled from \"StringTest.java\" public class jvm.string.StringTest minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #12.#35 // java/lang/Object.\"\u003cinit\u003e\":()V #2 = String #36 // hello illusory #3 = String #37 // hello #4 = String #38 // illusory #5 = Class #39 // java/lang/StringBuilder #6 = Methodref #5.#35 // java/lang/StringBuilder.\"\u003cinit\u003e\":() V #7 = Methodref #5.#40 // java/lang/StringBuilder.append:(Lja va/lang/String;)Ljava/lang/StringBuilder; #8 = Methodref #5.#41 // java/lang/StringBuilder.toString:() Ljava/lang/String; #9 = Fieldref #42.#43 // java/lang/System.out:Ljava/io/Print Stream; #10 = Methodref #44.#45 // java/io/PrintStream.println:(Z)V #11 = Class #46 // jvm/string/StringTest #12 = Class #47 // java/lang/Object 可以看到确实是用到了StringBuilder 加上 final 有会如何呢? /** * String类型优化测试 */ private static void StringTest() { String a = \"hello illusory\"; final String c2 = \"hello \"; final String d2 = \"illusory\"; String e2 = c2 + d2; // true System.out.println(a == e2); } 由于c2、d2都加了final修饰 所以被当作一个常量对待 此时+号两边都是常量，在编译期就可以确定其值了 类似于 String b = \"hello \" + \"illusory\"; 此时都指向常量池中的hello illusory所以a == e2为true 如果+号两边有一个不是常量那么结果都是false /** * String类型优化测试 */ private static void StringTest() { String a = \"hello illusory\"; final String c2 = \"hello \"; String f = c2 + getName(); // false System.out.println(a == f); } private static String getName() { return \"illusory\"; } 其中c2是final 被当成常量 其值是固定的 但是getName() 要运行时才能确定值 所以最后 f 也是 new 的一个对象 a == f结果为false /** * String类型优化测试 */ private static void StringTest() { String a = \"hello illusory\"; String g = a.intern(); System.out.println(a == g); } 当调用 String.intern() 方法时，如果常量池中已经存在该字符串，则返回池中的字符串引用；否则将此字符串添加到常量池中，并返回字符串的引用。 这里g和a是都是指向常量池中的hello illusory，所以a == g为true ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:3:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"4. 总结 最后总结一下 1.直接字符串相加，编译器会优化。 String a = \"hello \" + \"illusory\";---\u003e String a = \"hello illusory\"; 2.String 用加号拼接本质是new了StringBuilder对象进行append操作，拼接后调用toString()返回String对象 String c = \"hello \"; String d = \"illusory\"; String e = c + d; //实现如下 String e = new StringBuilder().append(\"hello \").append(\"illusory\").toString(); 3.+号两边都在编译期能确定的也会优化 /** * String类型优化测试 */ private static void StringTest() { String a = \"hello illusory\"; final String c2 = \"hello \"; final String d2 = \"illusory\"; String e2 = c2 + d2; // true System.out.println(a == e2); } 4.在字符串的累加操作中，建议结合线程问题选择 StringBuffe r或 StringBuilder，应避免使用+号拼接字符串 5.基本数据类型转化为 String 类型，效率是: num.toString() 方法最快，其次是 String.valueOf(num)，最后是num+”“的方式 ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:4:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Java"],"content":"5. 参考 https://blog.csdn.net/SEU_Calvin/article/details/52291082 https://www.cnblogs.com/vincentl/p/9600093.html https://www.cnblogs.com/will959/p/7537891.html ","date":"2018-08-10","objectID":"/posts/java/01-java-string/:5:0","tags":["Java"],"title":"String字符串详解","uri":"/posts/java/01-java-string/"},{"categories":["Git"],"content":"Git基本使用和常用命令","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"本文主要记录了Git常用的一些命令，和Git基本使用教学，包括了版本库的创建、代码提交、推送、拉取、版本回退、撤销等操作。 更多文章欢迎访问我的个人博客–\u003e幻境云图 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:0:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"1. 简介 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:1:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"1.1 Git简介 Git(读音为/gɪt/。)是一个开源的分布式版本控制系统。 可以有效、高速地处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:1:1","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"1.2 Git工作区概念 Git本地有四个工作区域： 工作目录(Working Directory) 暂存区(Stage/Index) 版本库(Repository或Commit History) 远程仓库(Remote Directory) 文件在这四个区域之间的转换关系如下： Working Directory： 工作区，就是你平时存放项目代码的地方，大概就是一个文件夹。 Index / Stage： 暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息 Repository： 仓库区（或版本库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本 Remote： 远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:1:2","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"1.3 工作流程 git的工作流程一般是这样的： 在工作目录中添加、修改文件； 将需要进行版本管理的文件放入暂存区域； 将暂存区域的文件提交到git仓库。 因此，git管理的文件有三种状态：已修改(modified),已暂存(staged),已提交(committed)。 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:1:3","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"1.4 文件的四种状态 版本控制就是对文件的版本控制，要对文件进行修改、提交等操作，首先要知道文件当前在什么状态，不然可能会提交了现在还不想提交的文件，或者要提交的文件没提交上。 Git不关心文件两个版本之间的具体差别，而是关心文件的整体是否有改变，若文件被改变，在添加提交时就生成文件新版本的快照，而判断文件整体是否改变的方法就是用。 SHA-1算法计算文件的校验和。 Untracked: 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过git add 状态变为Staged。 Unmodify: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为Modified。如果使用git rm移出版本库, 则成为Untracked文件 Modified: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过git add可进入暂存staged状态, 使用git checkout 则丢弃修改过,返回到unmodify状态, 这个git checkout即从库中取出文件, 覆盖当前修改 Staged: 暂存状态. 执行git commit则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为Unmodify状态. 执行git reset HEAD filename取消暂存,文件状态为Modified。 下面的图很好的解释了这四种状态的转变： 新建文件后 —\u003eUntracked 使用add命令将新建的文件加入到暂存区—\u003eStaged 使用commit命令将暂存区的文件提交到本地仓库—\u003eUnmodified 如果对Unmodified状态的文件进行修改—\u003e modified 如果对Unmodified状态的文件进行remove操作—\u003eUntracked ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:1:4","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"2. 使用 基本配置在上一章写了这里就略过了。 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:2:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"2.1 创建git仓库 创建git仓库包含两种方式： git init：本地创建后推送到远程服务器 git clone：远程服务器创建后克隆到本地 效果都是一样的。 git init 用 git init 在目录中创建新的 Git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。 执行后会在当前文件夹中多出一个.git文件夹，Git相关信息都在里面。 .git文件夹是隐藏的 如果看不到需要调整系统设置让其显示隐藏文件夹 $ git init Initialized empty Git repository in C:/Users/13452/Desktop/gitte/.git/ git clone 当然，也可以在远程服务器上拉取代码，拷贝一个 Git 仓库到本地 git clone [url] # 例如 git@github.com:lixd/daily-notes.git ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:2:1","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"2.2 代码提交 假如已经通过git clone从远程服务器上拉取了一下git仓库到本地了。 然后在本地进行了一些编辑，比如:新增了一个test.txt文件 git status 可以通过git status 查看当前文件的状态 由于是新增的文件，还未加入 git 追踪，所以当前test.txt为Untracked状态 $ git status On branch master No commits yet Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) test.txt nothing added to commit but untracked files present (use \"git add\" to track) git add 在本地(Working Directory)将文件修改完成后使用git add命令可将该文件添加到暂存区 (Index) # 添加单个文件 git add filename # 添加所有文件 git add . # 将test.txt文件添加到Index git add test.txt git commit 使用 git add 命令是将已经更新的内容写入缓存区， 而执行git commit将缓存区内容提交到本地仓库中(Repository)。 # 提交Index中的文件 执行后会进入写注释的界面 git commit # 提交时直接写注释 git commit -m\"注释\" # 例如提交test.txt文件 git commit -m\"新增test.txt文件\" git push 在执行git commit将缓存区内容添加到本地仓库中后，可以使用git push将本地的修改推送到服务器上的远程仓库中，这样其他人就可以同步了。 git push [主机名] [分支名] # 推送到origin主机的master分支 其中origin是默认的主机名 git push origin master git pull 在其他人提交代码后，可以通过git pull命令拉取服务器代码到本地。 git pull [主机名] [分支名] # 拉取origin主机的master分支最新代码 其中默认的主机名是origin git pull origin master # 直接git pull 则会拉取当前分支最新代码 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:2:2","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"3. 进阶操作 学会前面的几个操作基本就能满足日常的使用了(如果不出意外的话)。 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:3:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"3.1 分支操作 创建项目后默认在master分支, 即主分支。 应保证master分支代码永远是正确的，稳定的，可运行的 所以正常开发是不会直接在master分支上修改的。 创建分支 实际开发时一般会根据功能创建多个分支，或者每个开发者创建一个自己的分支。 # 创建分支branchName git branch branchName # 切换到分支branchName git checkout branchName # 创建并切换到分支branchName git checkout -b branchName # 例如创建一个dev分支 git branch dev git checkout dev # 或者用下面这句 二者是等效的 git checkout -b dev 合并(merge)分支 在新建的分支开发完后需要进行合并，将新的功能代码合并搭到master分支，当然也可以合并到任意分支。 # 会把branchName分支合并到`当前`分支 git merge branchName # 例如dev分支开发完成 合并到master分支 git checkout master gir merge dev 一般情况下并不会直接合并到master分支，而是先pull最新的master分支合并到自己的分支，然后没问题了在合并到master分支。 完整流程如下: #1 创建功能分支 (master) git checkout -b feature #2 功能迭代 (feature) git commit ... #3 合并最新主分支代码 (feature) git checkout master (master) git pull (master) git checkout feature (feature) git merge master # 解决冲突(如果有的话) #4 review，修改代码 再次提交 (feature) git commit #5 没问题后再次合并到主分支 (feature) git checkout master # --squash参数可加可不加 具体作用现在先不用管 (master) git merge feature --squash (master) git commit # # 推送到远端，正常结束 (master) git push origin # #6 如果上一步被拒绝，是因为master有更新的代码入库了，为了防止master上出现分线，需要重新执行第5步 删除分支 合并完成后即可删除开发时创建的分支。 # 删除分支branchName d-\u003edelete git branch -d branchName # 例如删除前面的功能分支 git branch -d feature ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:3:1","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"3.2 其他操作 git diff 执行git diff命令来查看文件与之前的区别。 # 查看本地工作区和index区域的文件的区别 git diff # 查看Index区域与Repository区域的区别 git diff --cached # 查看所有文件与本地仓库的区别 git diff HEAD # 只显示摘要而不是全部显示 git diff --stat git reset 撤销命令，git中比较重要的命令之一了。 git reset [恢复等级] [commitId] soft/mixed/hard git reset有三个参数，可以看做是三个恢复等级。 git reset –soft 仅仅将commit回退到了指定的提交 ，只修改Repository区域 git reset –mixed 用指定的commit覆盖Repository区域和Index区，之前所有暂存的内容都变为未暂存的状态 (默认为该参数) git reset –hard 使用指定的commit的内容覆盖Repository区域、Index区和工作区。(危险！！！ 此操作会丢弃工作区所做的修改！需谨慎！！！) 具体如图： commidID 表示将要恢复到哪个版本。有如下几种表示法 HEAD:表示当前最新的一次提交,(HEAD^)表示倒数第二次提交,(HEAD^^)表示倒数第三次提交，倒数第100次提交则是HEAD^^...^^^ 100个^,当然不会这么傻，还有另外一种写法HEAD~100 就是倒数第100次了。 当然还可以使用具体的commitID: 使用git log可以查看到提交历史，其中就包含了commitID $ git log ////这个是最新的一次提交的commitId commit 06f1cd144f57c38d6fdbed07616af8ed5d69a9ea(HEAD -\u003e hexo, origin/hexo, origin/HEAD) Author: lillusory \u003cxueduanli@163.com\u003e Date: Sat Feb 16 17:51:18 2019 +0800 添加Git工作区概念详解 commit 8f8908ff3edbba0d24d7eee7682e09d002faee6f //这个就是commitId Author: lillusory \u003cxueduanli@163.com\u003e Date: Fri Feb 15 19:10:06 2019 +0800 fix建造者模式两种写法 commit 71a44acd12d427f694f554df1d2f26ad59df5978 //这个就是commitId Author: lillusory \u003cxueduanli@163.com\u003e Date: Fri Feb 15 00:31:33 2019 +0800 fix 单例模式+Git 常用命令 commit 099675715979832baa107f9da080bfd38d3d63e0 //这个就是commitId Author: lillusory \u003cxueduanli@163.com\u003e Date: Thu Feb 14 23:26:10 2019 +0800 所以git reset有多种写法 git reset HEAD //Repository和Index恢复到最后一次提交的状态 不影响工作区 git reset HEAD test.txt //只恢复test.txt 文件 git reset --soft HEAD //Repository恢复到最后一次提交的状态 git reset --hard HEAD //Repository、Index和工作区都恢复到最后一次提交的状态 丢弃工作区所有内容 git reset 099675715979832baa107f9da080bfd38d3d63e0 //恢复到commitID版本 一般不用写完整的commitid 写前几位git就可以分辨出来了 git reflog 前面的git reset可以恢复到各个版本，但是若恢复到前面的版本了，那么在使用git log查看是就找不到后面的提交了，想要恢复到后面的版本时就可以使用git reflog查看，该命令可以看到所有的版本改动信息。 $ git log commit 86a08a6fbacffcf93f7b4dd94be4a21ca31682c4 (HEAD -\u003e master) Author: lillusory \u003cxueduanli@163.com\u003e Date: Sat Feb 16 18:29:48 2019 +0800 新增test.txt $ git reflog 86a08a6 HEAD@{1}: reset: moving to HEAD^ b9802c7 (HEAD -\u003e master) HEAD@{2}: commit: 添加内容1111 86a08a6 HEAD@{3}: commit (initial): 新增test.txt ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:3:2","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"3. 常用命令 # 新建仓库 git init git clone [url] # 代码提交 git add \u003cfilename\u003e git commit -m\"注释\" git push # 版本恢复 git reset # 代码拉取 git pull # 分支操作 git branch \u003cbranchName\u003e git checkout \u003cbranchName\u003e git merge \u003cbranchName\u003e # 信息查看 git status git log git reflog git config -l 最后附上一张网上找到的Git常用命令速查表 ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:4:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"4. 参考 http://www.runoob.com/git/git-basic-operations.html https://www.cnblogs.com/qdhxhz/p/9757390.html ","date":"2018-08-06","objectID":"/posts/git/02-git-use-guide/:5:0","tags":["Git"],"title":"Git教程(二)---常用命令","uri":"/posts/git/02-git-use-guide/"},{"categories":["Git"],"content":"Git 配置SSH key过程记录","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"本文主要记录了使用git之前的基本操作，首先配置全局用户名和邮箱，接着添加一个ssh key,这样就不用每天提交的时候都输一次密码了。 本地 Git 仓库和 远程(github)仓库之间的传输是通过 SSH 加密的，所以配置SSH key之后，上传代码到Github远程仓库时就不用输入密码了。 一般是在C盘用户目录下有一个 something 和 something.pub 来命名的一对文件，这个 something 通常就是 id_dsa 或 id_rsa。 有 .pub 后缀的文件就是公钥，另一个文件则是密钥。连接时必须提供一个公钥用于授权，没有的话就要生成一个。 ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:0:0","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"1. 配置用户名和邮箱 配置全局用户名和邮箱，git提交代码时用来显示你身份和联系方式，并不是github用户名和邮箱 # 记得改成自己的... git config --global user.name \"lixd\" git config --global user.email \"xueduanli@163.com\" ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:1:0","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"2. 生成SSH key ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:2:0","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"2.1 生成秘钥 执行ssh-keygen -t rsa -C \"你的邮箱地址\" 命令 生成ssh key 然后会叫你输入保存路径，直接按回车即可，保存在C盘用户目录下 然后会提示输入密码和确认密码，不用输入直接按两下回车即可 到这里SSH key就生成好了，接下来就是配置到github上。 ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:2:1","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"2.2 配置SSH key 登陆Github--\u003e点击头像--\u003eSettings--\u003eSSH and GPG keys--\u003e选择SSh keys上的New SSH keys--\u003ename 随便写，key就是刚才生成的文件中的所有内容。 文件默认是在C盘用户目录下，我的是C:\\Users\\13452\\.ssh 文件夹中应该会有两个文件 ：id_rsa和id_rsa.pub id_rsa.pub就是我们要的key, 一般以ssh-rsa开头，以你刚才输的邮箱结尾。 ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:2:2","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"2.3 测试 执行ssh -T git@github.com命令验证一下。 可能会提示，无法验证主机的真实性是否要建立连接，输入yes就行了。 如果，看到： Hi xxx! You’ve successfully authenticated, but GitHub does not # provide shell access. 恭喜你，你的设置已经成功了。 ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:2:3","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":["Git"],"content":"3. 参考 https://git-scm.com/book/zh/v2 ","date":"2018-07-01","objectID":"/posts/git/01-git-ssh-key-set/:3:0","tags":["Git"],"title":"Git教程(一)---配置及SSH key","uri":"/posts/git/01-git-ssh-key-set/"},{"categories":null,"content":"关于我 博客主要记录自己的学习记录 or 总结，当然也希望能帮到有需要的人。 强迫症，不喜欢看到杂乱的代码。 当前正在使用 Go 语言(容器云方向)，let’s go :) 。 以指譬教，以月譬法。 如人以手指月示人，彼人因指，当应看月。若复观指，以为月体，此人岂唯亡失月轮，亦亡其指。 ","date":"2017-10-01","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"学习记录 书名/课程 时间 备注 《MySQL 技术内幕》 2019 MySQL 基本操作，入门书籍 《并发编程的艺术》 2019 Java 并发 《深入理解 Java 虚拟机》 2019 JVM 相关 《微服务:从设计到部署》 2019 微服务如何落地 《RabbitMQ 实战指南》 2019 基本使用和高级特性讲解 《Redis 设计与实现》 2020 Redis 内部实现的详细讲解 《MySQL 技术内幕 InnoDB 存储引擎》 2020 InnoDB 索引内部实现及原理 《云原生分布式存储基石 etcd深入解析》 2020 etcd 内部实现详解 《MySQL 45 讲》 2020 MySQL核心技术与原理，硬核，推荐 《趣谈网络协议》 2020 真·趣谈网络协议，推荐 《重学操作系统》 2020 操作系统核心概述 《Go 微服务实战 38 讲》 2020 Go 微服务概述 《分布式链路追踪实战》 2020 链路追踪 Tracing 概述 《Redis 核心技术与实战》 2021 Redis 核心原理 《由浅入深吃透 Docker》 2021 Docker 原理概述 《深入剖析 Kubernetes》 2021 Kubernetes 核心原理，硬核 《etcd 实战课》 2021 Go 生态常用组件，etcd 核心原理 《Go 语言设计与实现》 2021 Go 语言实现分析，硬核 《Kafka 核心技术与实战》 2021 kafka 大致原理，适合入门 《Kubernetes 权威指南：从Docker 到 Kubernetes 实践全接触》 2021 比较全面，从概念、实践指南、核心原理、开发指导、运维指南等都有提到。k8s 迭代比较快，有基础的还是推荐直接看官方文档。 《程序是怎样跑起来的》 2022 计算机原理入门数据，重读一遍大学的感觉，从C语言到操作系统，到编译原理、汇编、CPU、IC等等。 《Go 语言高级编程》 2022 其中的 CGO 和 Go 汇编部分比较深入，其他的 Go 基础、RPC、Protobuf、Go Web及最后的分布式系统部分并没有想象中那么高级。 《Service Mesh 微服务架构设计》 2022 从微服务架构和治理角度出发，聚焦 ServiceMesh 的架构设计。不过由于时间有点久了，Istio 和 Envoy 都有较大更新，导致这部分内容出入比较大。 ","date":"2017-10-01","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"TODO 希望后续在继续跟进其他技术的情况下，能抽时间一两年读完下面几本经典。 论基础知识的重要性 《TCP/IP 详解》 《深入理解计算机系统》（CSAPP） 《数据密集型应用系统设计》（DDIA） 《重构—改善既有代码的设计》 《UNIX 编程艺术》 《UNIX 环境高级编程》 《UNIX 网络编程》 ","date":"2017-10-01","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"}]